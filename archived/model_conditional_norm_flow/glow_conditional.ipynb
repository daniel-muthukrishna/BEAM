{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 2242,
     "status": "ok",
     "timestamp": 1692391050462,
     "user": {
      "displayName": "Joseph Lupo",
      "userId": "18192058496751055649"
     },
     "user_tz": 240
    },
    "id": "AoYASP6SlsZe",
    "outputId": "4d279a5f-4afd-4985-f1cf-c7fa74d1dd2a"
   },
   "outputs": [],
   "source": [
    "# # only for colab\n",
    "# from google.colab import drive\n",
    "# import os\n",
    "\n",
    "# drive.mount(\"/content/gdrive\")\n",
    "# os.chdir('/content/gdrive/MyDrive/Projects/TESS/model_conditional_norm_flow/')\n",
    "# ! ls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "executionInfo": {
     "elapsed": 11,
     "status": "ok",
     "timestamp": 1692391050463,
     "user": {
      "displayName": "Joseph Lupo",
      "userId": "18192058496751055649"
     },
     "user_tz": 240
    },
    "id": "wWai4P8Il4oq"
   },
   "outputs": [],
   "source": [
    "# ! rm -r log*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "executionInfo": {
     "elapsed": 9,
     "status": "ok",
     "timestamp": 1692391050463,
     "user": {
      "displayName": "Joseph Lupo",
      "userId": "18192058496751055649"
     },
     "user_tz": 240
    },
    "id": "rkQEZJWdtZpu"
   },
   "outputs": [],
   "source": [
    "# Adapted from here: https://github.com/yolu1055/conditional-glow/tree/master"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "executionInfo": {
     "elapsed": 3075,
     "status": "ok",
     "timestamp": 1692391053531,
     "user": {
      "displayName": "Joseph Lupo",
      "userId": "18192058496751055649"
     },
     "user_tz": 240
    },
    "id": "MdI9aotpm1b3"
   },
   "outputs": [],
   "source": [
    "# @title Utils\n",
    "import os\n",
    "import torch\n",
    "import numpy as np\n",
    "\n",
    "def split_feature(tensor, type=\"split\"):\n",
    "    \"\"\"\n",
    "    type = [\"split\", \"cross\"]\n",
    "    \"\"\"\n",
    "    C = tensor.size(1)\n",
    "    if type == \"split\":\n",
    "        return tensor[:, :C // 2, ...], tensor[:, C // 2:, ...]\n",
    "    elif type == \"cross\":\n",
    "        return tensor[:, 0::2, ...], tensor[:, 1::2, ...]\n",
    "\n",
    "\n",
    "def save_model(model, optim, scheduler, dir, iteration):\n",
    "    path = os.path.join(dir, \"checkpoint_{}.pth.tar\".format(iteration))\n",
    "    state = {}\n",
    "    state[\"iteration\"] = iteration\n",
    "    state[\"modelname\"] = model.__class__.__name__\n",
    "    state[\"model\"] = model.state_dict()\n",
    "    state[\"optim\"] = optim.state_dict()\n",
    "    if scheduler is not None:\n",
    "        state[\"scheduler\"] = scheduler.state_dict()\n",
    "    else:\n",
    "        state[\"scheduler\"] = None\n",
    "\n",
    "    torch.save(state, path)\n",
    "\n",
    "\n",
    "def load_state(path, cuda):\n",
    "    if cuda:\n",
    "        print (\"load to gpu\")\n",
    "        state = torch.load(path)\n",
    "    else:\n",
    "        print (\"load to cpu\")\n",
    "        state = torch.load(path, map_location=lambda storage, loc: storage)\n",
    "\n",
    "    return state\n",
    "\n",
    "\n",
    "def _fast_hist(label_true, label_pred, n_class):\n",
    "    mask = (label_true >= 0) & (label_true < n_class)\n",
    "    hist = np.bincount(\n",
    "        n_class * label_true[mask].astype(int) +\n",
    "        label_pred[mask].astype(int), minlength=n_class ** 2).reshape(n_class, n_class)\n",
    "    return hist\n",
    "\n",
    "def compute_accuracy(label_trues, label_preds, n_class):\n",
    "\n",
    "    hist = np.zeros((n_class, n_class))\n",
    "    for lt, lp in zip(label_trues, label_preds):\n",
    "        hist += _fast_hist(lt.flatten(), lp.flatten(), n_class)\n",
    "    acc = np.diag(hist).sum() / hist.sum()\n",
    "    acc_cls = np.diag(hist) / hist.sum(axis=1)\n",
    "    acc_cls = np.nanmean(acc_cls)\n",
    "    iu = np.diag(hist) / (hist.sum(axis=1) + hist.sum(axis=0) - np.diag(hist))\n",
    "    mean_iu = np.nanmean(iu)\n",
    "    freq = hist.sum(axis=1) / hist.sum()\n",
    "    fwavacc = (freq[freq > 0] * iu[freq > 0]).sum()\n",
    "    return acc, acc_cls, mean_iu, fwavacc\n",
    "\n",
    "\n",
    "def count_parameters(model):\n",
    "    return sum(p.numel() for p in model.parameters() if p.requires_grad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "executionInfo": {
     "elapsed": 5,
     "status": "ok",
     "timestamp": 1692391053531,
     "user": {
      "displayName": "Joseph Lupo",
      "userId": "18192058496751055649"
     },
     "user_tz": 240
    },
    "id": "y7lYe37Em1eN"
   },
   "outputs": [],
   "source": [
    "# @title Modules\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "class ActNorm(nn.Module):\n",
    "\n",
    "    def __init__(self, num_channels):\n",
    "        super().__init__()\n",
    "\n",
    "        size = [1, num_channels, 1, 1]\n",
    "\n",
    "        bias = torch.normal(mean=torch.zeros(*size), std=torch.ones(*size)*0.05)\n",
    "        logs = torch.normal(mean=torch.zeros(*size), std=torch.ones(*size)*0.05)\n",
    "        self.register_parameter(\"bias\", nn.Parameter(torch.Tensor(bias), requires_grad=True))\n",
    "        self.register_parameter(\"logs\", nn.Parameter(torch.Tensor(logs), requires_grad=True))\n",
    "\n",
    "\n",
    "    def forward(self, input, logdet=0, reverse=False):\n",
    "        dimentions = input.size(2) * input.size(3)\n",
    "        if reverse == False:\n",
    "            input = input + self.bias\n",
    "            input = input * torch.exp(self.logs)\n",
    "            dlogdet = torch.sum(self.logs) * dimentions\n",
    "            logdet = logdet + dlogdet\n",
    "\n",
    "        if reverse == True:\n",
    "            input = input * torch.exp(-self.logs)\n",
    "            input = input - self.bias\n",
    "            dlogdet = - torch.sum(self.logs) * dimentions\n",
    "            logdet = logdet + dlogdet\n",
    "\n",
    "        return input, logdet\n",
    "\n",
    "\n",
    "class Conv2dZeros(nn.Conv2d):\n",
    "\n",
    "    def __init__(self, in_channel, out_channel, kernel_size=[3,3], stride=[1,1]):\n",
    "        padding = (kernel_size[0] - 1) // 2\n",
    "        super().__init__(in_channels=in_channel, out_channels=out_channel, kernel_size=kernel_size, stride=stride, padding=padding)\n",
    "        self.weight.data.normal_(mean=0.0, std=0.1)\n",
    "\n",
    "\n",
    "\n",
    "class Conv2dResize(nn.Conv2d):\n",
    "\n",
    "    def __init__(self, in_size, out_size):\n",
    "\n",
    "        stride = [in_size[1]//out_size[1], in_size[2]//out_size[2]]\n",
    "        kernel_size = Conv2dResize.compute_kernel_size(in_size, out_size, stride)\n",
    "        super().__init__(in_channels=in_size[0], out_channels=out_size[0], kernel_size=kernel_size, stride=stride)\n",
    "        self.weight.data.zero_()\n",
    "\n",
    "\n",
    "    @staticmethod\n",
    "    def compute_kernel_size(in_size, out_size, stride):\n",
    "        k0 = in_size[1] - (out_size[1] - 1) * stride[0]\n",
    "        k1 = in_size[2] - (out_size[2] - 1) * stride[1]\n",
    "        return[k0,k1]\n",
    "\n",
    "\n",
    "\n",
    "class Conv2dNorm(nn.Conv2d):\n",
    "\n",
    "    def __init__(self, in_channels, out_channels, kernel_size=[3, 3], stride=[1, 1]):\n",
    "\n",
    "        padding = (kernel_size[0] - 1) // 2\n",
    "        super().__init__(in_channels, out_channels, kernel_size, stride, padding)\n",
    "        #initialize weight\n",
    "        self.weight.data.normal_(mean=0.0, std=0.05)\n",
    "\n",
    "\n",
    "\n",
    "class CondActNorm(nn.Module):\n",
    "\n",
    "    def __init__(self, x_size, y_channels, x_hidden_channels, x_hidden_size):\n",
    "        super().__init__()\n",
    "\n",
    "        C_x,H_x,W_x = x_size\n",
    "\n",
    "        # conditioning network\n",
    "        self.x_Con = nn.Sequential(\n",
    "            Conv2dResize(in_size=[C_x,H_x,W_x], out_size=[x_hidden_channels, H_x//2, W_x//2]),\n",
    "            nn.ReLU(),\n",
    "            Conv2dResize(in_size=[x_hidden_channels, H_x//2, W_x//2], out_size=[x_hidden_channels, H_x//4, W_x//4]),\n",
    "            nn.ReLU(),\n",
    "            Conv2dResize(in_size=[x_hidden_channels, H_x//4, W_x//4], out_size=[x_hidden_channels, H_x//8, W_x//8]),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "\n",
    "        self.x_Linear = nn.Sequential(\n",
    "            LinearZeros(x_hidden_channels*H_x*W_x//(8*8), x_hidden_size),\n",
    "            nn.ReLU(),\n",
    "            LinearZeros(x_hidden_size, x_hidden_size),\n",
    "            nn.ReLU(),\n",
    "            LinearZeros(x_hidden_size, 2*y_channels),\n",
    "            nn.Tanh()\n",
    "        )\n",
    "\n",
    "\n",
    "    def forward(self, x, y, logdet=0, reverse=False):\n",
    "\n",
    "        B,C,H,W = x.size()\n",
    "\n",
    "        # generate weights\n",
    "        x = self.x_Con(x)\n",
    "        x = x.view(B, -1)\n",
    "        x = self.x_Linear(x)\n",
    "        x = x.view(B, -1, 1, 1)\n",
    "\n",
    "\n",
    "\n",
    "        logs, bias = split_feature(x)\n",
    "        dimentions = y.size(2) * y.size(3)\n",
    "\n",
    "\n",
    "        if not reverse:\n",
    "            # center and scale\n",
    "            y = y + bias\n",
    "            y = y * torch.exp(logs)\n",
    "            dlogdet = dimentions * torch.sum(logs, dim=(1,2,3))\n",
    "            logdet = logdet + dlogdet\n",
    "        else:\n",
    "            # scale and center\n",
    "            y = y * torch.exp(-logs)\n",
    "            y = y - bias\n",
    "            dlogdet = - dimentions * torch.sum(logs, dim=(1,2,3))\n",
    "            logdet = logdet + dlogdet\n",
    "\n",
    "        return y, logdet\n",
    "\n",
    "\n",
    "\n",
    "class Cond1x1Conv(nn.Module):\n",
    "\n",
    "    def __init__(self, x_size, x_hidden_channels, x_hidden_size, y_channels):\n",
    "\n",
    "        super().__init__()\n",
    "\n",
    "        C_x,H_x,W_x = x_size\n",
    "\n",
    "\n",
    "        # conditioning network\n",
    "        self.x_Con = nn.Sequential(\n",
    "            Conv2dResize(in_size=[C_x,H_x,W_x], out_size=[x_hidden_channels, H_x//2, W_x//2]),\n",
    "            nn.ReLU(),\n",
    "            Conv2dResize(in_size=[x_hidden_channels, H_x//2, W_x//2], out_size=[x_hidden_channels, H_x//4, W_x//4]),\n",
    "            nn.ReLU(),\n",
    "            Conv2dResize(in_size=[x_hidden_channels, H_x//4, W_x//4], out_size=[x_hidden_channels, H_x//8, W_x//8]),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "\n",
    "        self.x_Linear = nn.Sequential(\n",
    "            LinearZeros(x_hidden_channels*H_x*W_x//(8*8), x_hidden_size),\n",
    "            nn.ReLU(),\n",
    "            LinearZeros(x_hidden_size, x_hidden_size),\n",
    "            nn.ReLU(),\n",
    "            LinearNorm(x_hidden_size, y_channels*y_channels),\n",
    "            nn.Tanh()\n",
    "        )\n",
    "\n",
    "\n",
    "    def get_weight(self, x, y, reverse):\n",
    "        y_channels = y.size(1)\n",
    "        B,C,H,W = x.size()\n",
    "\n",
    "        x = self.x_Con(x)\n",
    "        x = x.view(B, -1)\n",
    "        x = self.x_Linear(x)\n",
    "        weight = x.view(B, y_channels, y_channels)\n",
    "\n",
    "        dimensions = y.size(2) * y.size(3)\n",
    "        dlogdet = torch.slogdet(weight)[1] * dimensions\n",
    "\n",
    "\n",
    "        if reverse == False:\n",
    "            weight = weight.view(B, y_channels, y_channels,1,1)\n",
    "\n",
    "        else:\n",
    "            weight = torch.inverse(weight.double()).float().view(B, y_channels, y_channels,1,1)\n",
    "\n",
    "        return weight, dlogdet\n",
    "\n",
    "\n",
    "\n",
    "    def forward(self, x, y, logdet=None, reverse=False):\n",
    "\n",
    "        weight, dlogdet = self.get_weight(x, y, reverse)\n",
    "        B,C,H,W = y.size()\n",
    "        y = y.view(1, B*C, H, W)\n",
    "        B_k, C_i_k, C_o_k, H_k, W_k = weight.size()\n",
    "        assert B == B_k and C == C_i_k and C == C_o_k, \"The input and kernel dimensions are different\"\n",
    "        weight = weight.reshape(B_k*C_i_k,C_o_k,H_k,W_k)\n",
    "\n",
    "        if reverse == False:\n",
    "            z = F.conv2d(y, weight, groups=B)\n",
    "            z = z.view(B,C,H,W)\n",
    "            if logdet is not None:\n",
    "                logdet = logdet + dlogdet\n",
    "\n",
    "            return z, logdet\n",
    "        else:\n",
    "            z = F.conv2d(y, weight, groups=B)\n",
    "            z = z.view(B,C,H,W)\n",
    "\n",
    "            if logdet is not None:\n",
    "                logdet = logdet - dlogdet\n",
    "\n",
    "            return z, logdet\n",
    "\n",
    "\n",
    "class Conv2dNormy(nn.Conv2d):\n",
    "\n",
    "    def __init__(self, in_channels, out_channels,\n",
    "                 kernel_size=[3, 3], stride=[1, 1]):\n",
    "        padding = [(kernel_size[0]-1)//2, (kernel_size[1]-1)//2]\n",
    "        super().__init__(in_channels, out_channels, kernel_size, stride,\n",
    "                         padding, bias=False)\n",
    "\n",
    "        #initialize weight\n",
    "        self.weight.data.normal_(mean=0.0, std=0.05)\n",
    "        self.actnorm = ActNorm(out_channels)\n",
    "\n",
    "\n",
    "    def forward(self, input):\n",
    "        x = super().forward(input)\n",
    "        x,_ = self.actnorm(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "class Conv2dZerosy(nn.Conv2d):\n",
    "\n",
    "    def __init__(self, in_channels, out_channels,\n",
    "                 kernel_size=[3, 3], stride=[1, 1]):\n",
    "\n",
    "        padding = [(kernel_size[0]-1)//2, (kernel_size[1]-1)//2]\n",
    "        super().__init__(in_channels, out_channels, kernel_size, stride, padding)\n",
    "\n",
    "        self.logscale_factor = 3.0\n",
    "        self.register_parameter(\"logs\", nn.Parameter(torch.zeros(out_channels, 1, 1)))\n",
    "        self.register_parameter(\"newbias\", nn.Parameter(torch.zeros(out_channels, 1, 1)))\n",
    "\n",
    "        # init\n",
    "        self.weight.data.zero_()\n",
    "        self.bias.data.zero_()\n",
    "\n",
    "    def forward(self, input):\n",
    "        output = super().forward(input)\n",
    "        output = output + self.newbias\n",
    "        output = output * torch.exp(self.logs * self.logscale_factor)\n",
    "        return output\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "class CondAffineCoupling(nn.Module):\n",
    "\n",
    "    def __init__(self, x_size, y_size, hidden_channels):\n",
    "        super().__init__()\n",
    "\n",
    "        self.resize_x = nn.Sequential(\n",
    "            Conv2dZeros(x_size[0], 16),\n",
    "            nn.ReLU(),\n",
    "            Conv2dResize((16,x_size[1],x_size[2]), out_size=y_size),\n",
    "            nn.ReLU(),\n",
    "            Conv2dZeros(y_size[0], y_size[0]),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "\n",
    "        self.f = nn.Sequential(\n",
    "            Conv2dNormy(y_size[0]*2, hidden_channels),\n",
    "            nn.ReLU(),\n",
    "            Conv2dNormy(hidden_channels, hidden_channels, kernel_size=[1, 1]),\n",
    "            nn.ReLU(),\n",
    "            Conv2dZerosy(hidden_channels, 2*y_size[0]),\n",
    "            nn.Tanh()\n",
    "        )\n",
    "\n",
    "\n",
    "    def forward(self, x, y, logdet=0.0, reverse=False):\n",
    "\n",
    "        z1, z2 = split_feature(y, \"split\")\n",
    "        x = self.resize_x(x)\n",
    "\n",
    "        # print(\"xshape, z1shape\", x.shape, z1.shape)\n",
    "        # print(x)\n",
    "\n",
    "        h = torch.cat((x,z1), dim=1)\n",
    "        h = self.f(h)\n",
    "        shift, scale = split_feature(h, \"cross\")\n",
    "        scale = torch.sigmoid(scale + 2.)\n",
    "        if reverse == False:\n",
    "            z2 = z2 + shift\n",
    "            z2 = z2 * scale\n",
    "            logdet = torch.sum(torch.log(scale), dim=(1, 2, 3)) + logdet\n",
    "\n",
    "        if reverse == True:\n",
    "            z2 = z2 / scale\n",
    "            z2 = z2 - shift\n",
    "            logdet = -torch.sum(torch.log(scale), dim=(1, 2, 3)) + logdet\n",
    "\n",
    "        z = torch.cat((z1, z2), dim=1)\n",
    "\n",
    "        return z, logdet\n",
    "\n",
    "\n",
    "\n",
    "class SqueezeLayer(nn.Module):\n",
    "    def __init__(self, factor):\n",
    "        super().__init__()\n",
    "        self.factor = factor\n",
    "\n",
    "    def forward(self, input, logdet=None, reverse=False):\n",
    "        if not reverse:\n",
    "            output = SqueezeLayer.squeeze2d(input, self.factor)\n",
    "            return output, logdet\n",
    "        else:\n",
    "            output = SqueezeLayer.unsqueeze2d(input, self.factor)\n",
    "            return output, logdet\n",
    "\n",
    "\n",
    "    @staticmethod\n",
    "    def squeeze2d(input, factor=2):\n",
    "        assert factor >= 1 and isinstance(factor, int)\n",
    "        if factor == 1:\n",
    "            return input\n",
    "        B, C, H, W = input.size()\n",
    "        assert H % factor == 0 and W % factor == 0, \"{}\".format((H, W))\n",
    "        x = input.view(B, C, H // factor, factor, W // factor, factor)\n",
    "        x = x.permute(0, 1, 3, 5, 2, 4).contiguous()\n",
    "        x = x.view(B, C * factor * factor, H // factor, W // factor)\n",
    "        return x\n",
    "\n",
    "\n",
    "    @staticmethod\n",
    "    def unsqueeze2d(input, factor=2):\n",
    "        assert factor >= 1 and isinstance(factor, int)\n",
    "        factor2 = factor ** 2\n",
    "        if factor == 1:\n",
    "            return input\n",
    "        B, C, H, W = input.size()\n",
    "        assert C % (factor2) == 0, \"{}\".format(C)\n",
    "        x = input.view(B, C // factor2, factor, factor, H, W)\n",
    "        x = x.permute(0, 1, 4, 2, 5, 3).contiguous()\n",
    "        x = x.view(B, C // (factor2), H * factor, W * factor)\n",
    "        return x\n",
    "\n",
    "\n",
    "class Split2d(nn.Module):\n",
    "    def __init__(self, num_channels):\n",
    "        super().__init__()\n",
    "\n",
    "        self.conv = nn.Sequential(\n",
    "            Conv2dZeros(num_channels // 2, num_channels),\n",
    "            nn.Tanh()\n",
    "        )\n",
    "\n",
    "    def split2d_prior(self, z):\n",
    "        h = self.conv(z)\n",
    "        return split_feature(h, \"cross\")\n",
    "\n",
    "    def forward(self, input, logdet=0., reverse=False, eps_std=None):\n",
    "        if not reverse:\n",
    "            z1, z2 = split_feature(input, \"split\")\n",
    "            mean, logs = self.split2d_prior(z1)\n",
    "            logdet = GaussianDiag.logp(mean, logs, z2) + logdet\n",
    "\n",
    "            return z1, logdet\n",
    "        else:\n",
    "            z1 = input\n",
    "            mean, logs = self.split2d_prior(z1)\n",
    "            z2 = GaussianDiag.sample(mean, logs, eps_std)\n",
    "            z = torch.cat((z1, z2), dim=1)\n",
    "\n",
    "            return z, logdet\n",
    "\n",
    "\n",
    "class GaussianDiag:\n",
    "    Log2PI = float(np.log(2 * np.pi))\n",
    "\n",
    "    @staticmethod\n",
    "    def likelihood(mean, logs, x):\n",
    "        return -0.5 * (logs * 2. + ((x - mean) ** 2.) / torch.exp(logs * 2.) + GaussianDiag.Log2PI)\n",
    "\n",
    "    @staticmethod\n",
    "    def logp(mean, logs, x):\n",
    "        likelihood = GaussianDiag.likelihood(mean, logs, x)\n",
    "        return torch.sum(likelihood, dim=(1, 2, 3))\n",
    "\n",
    "    @staticmethod\n",
    "    def sample(mean, logs, eps_std=None):\n",
    "        eps_std = eps_std or 1\n",
    "        eps = torch.normal(mean=torch.zeros_like(mean),\n",
    "                           std=torch.ones_like(logs) * eps_std)\n",
    "        return mean + torch.exp(logs) * eps\n",
    "\n",
    "    @staticmethod\n",
    "    def batchsample(batchsize, mean, logs, eps_std=None):\n",
    "        eps_std = eps_std or 1\n",
    "        sample = GaussianDiag.sample(mean, logs, eps_std)\n",
    "        for i in range(1, batchsize):\n",
    "            s = GaussianDiag.sample(mean, logs, eps_std)\n",
    "            sample = torch.cat((sample, s), dim=0)\n",
    "        return sample\n",
    "\n",
    "\n",
    "\n",
    "class LinearZeros(nn.Linear):\n",
    "\n",
    "    def __init__(self, in_channels, out_channels):\n",
    "        super().__init__(in_channels, out_channels)\n",
    "        self.weight.data.zero_()\n",
    "        self.bias.data.zero_()\n",
    "\n",
    "    def forward(self, input):\n",
    "        output = super().forward(input)\n",
    "        return output\n",
    "\n",
    "\n",
    "class LinearNorm(nn.Linear):\n",
    "\n",
    "    def __init__(self, in_channels, out_channels):\n",
    "        super().__init__(in_channels, out_channels)\n",
    "        self.weight.data.normal_(mean=0.0, std=0.1)\n",
    "        self.bias.data.normal_(mean=0.0, std=0.1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "executionInfo": {
     "elapsed": 4,
     "status": "ok",
     "timestamp": 1692391053531,
     "user": {
      "displayName": "Joseph Lupo",
      "userId": "18192058496751055649"
     },
     "user_tz": 240
    },
    "id": "6ntChUCZnc8W"
   },
   "outputs": [],
   "source": [
    "# @title Glow Model\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "\n",
    "class CondGlowStep(nn.Module):\n",
    "\n",
    "    def __init__(self, x_size, y_size, x_hidden_channels, x_hidden_size, y_hidden_channels):\n",
    "\n",
    "\n",
    "        super().__init__()\n",
    "\n",
    "        # 1. cond-actnorm\n",
    "        self.actnorm = CondActNorm(x_size=x_size, y_channels=y_size[0], x_hidden_channels=x_hidden_channels, x_hidden_size=x_hidden_size)\n",
    "\n",
    "        # 2. cond-1x1conv\n",
    "        self.invconv = Cond1x1Conv(x_size=x_size, x_hidden_channels=x_hidden_channels, x_hidden_size=x_hidden_size, y_channels=y_size[0])\n",
    "\n",
    "        # 3. cond-affine\n",
    "        self.affine = CondAffineCoupling(x_size=x_size, y_size=[y_size[0] // 2, y_size[1], y_size[2]], hidden_channels=y_hidden_channels)\n",
    "\n",
    "\n",
    "    def forward(self, x, y, logdet=None, reverse=False):\n",
    "        # print('------')\n",
    "        # print(x.shape)\n",
    "        # print(x)\n",
    "        # print('------')\n",
    "\n",
    "        if reverse is False:\n",
    "            # 1. cond-actnorm\n",
    "            y, logdet = self.actnorm(x, y, logdet, reverse=False)\n",
    "\n",
    "            # 2. cond-1x1conv\n",
    "            y, logdet = self.invconv(x, y, logdet, reverse=False)\n",
    "\n",
    "            # 3. cond-affine\n",
    "            y, logdet = self.affine(x, y, logdet, reverse=False)\n",
    "\n",
    "            # Return\n",
    "            return y, logdet\n",
    "\n",
    "\n",
    "        if reverse is True:\n",
    "            # 3. cond-affine\n",
    "            y, logdet = self.affine(x, y, logdet, reverse=True)\n",
    "\n",
    "            # 2. cond-1x1conv\n",
    "            y, logdet = self.invconv(x, y, logdet, reverse=True)\n",
    "\n",
    "            # 1. cond-actnorm\n",
    "            y, logdet = self.actnorm(x, y, logdet, reverse=True)\n",
    "\n",
    "            # Return\n",
    "            return y, logdet\n",
    "\n",
    "\n",
    "class CondGlow(nn.Module):\n",
    "\n",
    "    def __init__(self, x_size, y_size, x_hidden_channels, x_hidden_size, y_hidden_channels, K, L):\n",
    "\n",
    "\n",
    "        super().__init__()\n",
    "        self.layers = nn.ModuleList()\n",
    "        self.output_shapes = []\n",
    "        self.K = K\n",
    "        self.L = L\n",
    "        C, H, W = y_size\n",
    "\n",
    "        for l in range(0, L):\n",
    "\n",
    "            # 1. Squeeze\n",
    "            C, H, W = C * 4, H // 2, W // 2\n",
    "            y_size = [C,H,W]\n",
    "            self.layers.append(SqueezeLayer(factor=2))\n",
    "            self.output_shapes.append([-1, C, H, W])\n",
    "\n",
    "            # 2. K CGlowStep\n",
    "            for k in range(0, K):\n",
    "\n",
    "                self.layers.append(CondGlowStep(x_size = x_size,\n",
    "                                            y_size = y_size,\n",
    "                                            x_hidden_channels = x_hidden_channels,\n",
    "                                            x_hidden_size = x_hidden_size,\n",
    "                                            y_hidden_channels = y_hidden_channels,\n",
    "                                            )\n",
    "                                   )\n",
    "\n",
    "                self.output_shapes.append([-1, C, H, W])\n",
    "\n",
    "            # 3. Split\n",
    "            if l < L - 1:\n",
    "                self.layers.append(Split2d(num_channels=C))\n",
    "                self.output_shapes.append([-1, C // 2, H, W])\n",
    "                C = C // 2\n",
    "\n",
    "\n",
    "    def forward(self, x, y, logdet=0.0, reverse=False, eps_std=1.0):\n",
    "        if reverse == False:\n",
    "            return self.encode(x, y, logdet)\n",
    "        else:\n",
    "            return self.decode(x, y, logdet, eps_std)\n",
    "\n",
    "    def encode(self, x, y, logdet=0.0):\n",
    "        for layer, shape in zip(self.layers, self.output_shapes):\n",
    "            if isinstance(layer, Split2d) or isinstance(layer, SqueezeLayer):\n",
    "                y, logdet = layer(y, logdet, reverse=False)\n",
    "\n",
    "            else:\n",
    "                y, logdet = layer(x, y, logdet, reverse=False)\n",
    "        return y, logdet\n",
    "\n",
    "    def decode(self, x, y, logdet=0.0, eps_std=1.0):\n",
    "        for layer in reversed(self.layers):\n",
    "            if isinstance(layer, Split2d):\n",
    "                y, logdet = layer(y, logdet=logdet, reverse=True, eps_std=eps_std)\n",
    "\n",
    "            elif isinstance(layer, SqueezeLayer):\n",
    "                y, logdet = layer(y, logdet=logdet, reverse=True)\n",
    "\n",
    "            else:\n",
    "                y, logdet = layer(x, y, logdet=logdet, reverse=True)\n",
    "\n",
    "        return y, logdet\n",
    "\n",
    "\n",
    "class CondGlowModel(nn.Module):\n",
    "    BCE = nn.BCEWithLogitsLoss()\n",
    "    CE = nn.CrossEntropyLoss()\n",
    "\n",
    "    def __init__(self, args):\n",
    "        super().__init__()\n",
    "        self.flow = CondGlow(x_size=args.x_size,\n",
    "                            y_size=args.y_size,\n",
    "                            x_hidden_channels=args.x_hidden_channels,\n",
    "                            x_hidden_size=args.x_hidden_size,\n",
    "                            y_hidden_channels=args.y_hidden_channels,\n",
    "                            K=args.depth_flow,\n",
    "                            L=args.num_levels,\n",
    "                            )\n",
    "\n",
    "        self.learn_top = args.learn_top\n",
    "\n",
    "\n",
    "        self.register_parameter(\"new_mean\",\n",
    "                                nn.Parameter(torch.zeros(\n",
    "                                    [1,\n",
    "                                     self.flow.output_shapes[-1][1],\n",
    "                                     self.flow.output_shapes[-1][2],\n",
    "                                     self.flow.output_shapes[-1][3]])))\n",
    "\n",
    "\n",
    "        self.register_parameter(\"new_logs\",\n",
    "                                nn.Parameter(torch.zeros(\n",
    "                                    [1,\n",
    "                                     self.flow.output_shapes[-1][1],\n",
    "                                     self.flow.output_shapes[-1][2],\n",
    "                                     self.flow.output_shapes[-1][3]])))\n",
    "\n",
    "        self.n_bins = args.y_bins\n",
    "\n",
    "\n",
    "    def prior(self):\n",
    "\n",
    "        if self.learn_top:\n",
    "            return self.new_mean, self.new_logs\n",
    "        else:\n",
    "            return torch.zeros_like(self.new_mean), torch.zeros_like(self.new_logs)\n",
    "\n",
    "\n",
    "    def forward(self, x=0.0, y=None, eps_std=1.0, reverse=False):\n",
    "        if reverse == False:\n",
    "            # print(type(y))\n",
    "            # print(y.shape)\n",
    "            dimensions = y.size(1)*y.size(2)*y.size(3)\n",
    "            logdet = torch.zeros_like(y[:, 0, 0, 0])\n",
    "            logdet += float(-np.log(self.n_bins) * dimensions)\n",
    "            z, objective = self.flow(x, y, logdet=logdet, reverse=False)\n",
    "            mean, logs = self.prior()\n",
    "            objective += GaussianDiag.logp(mean, logs, z)\n",
    "            nll = -objective / float(np.log(2.) * dimensions)\n",
    "            return z, nll\n",
    "\n",
    "        else:\n",
    "            with torch.no_grad():\n",
    "                mean, logs = self.prior()\n",
    "                if y is None:\n",
    "                    y = GaussianDiag.batchsample(x.size(0), mean, logs, eps_std)\n",
    "                y, logdet = self.flow(x, y, eps_std=eps_std, reverse=True)\n",
    "            return y, logdet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {
    "executionInfo": {
     "elapsed": 1976,
     "status": "ok",
     "timestamp": 1692391055504,
     "user": {
      "displayName": "Joseph Lupo",
      "userId": "18192058496751055649"
     },
     "user_tz": 240
    },
    "id": "7xtTbLLNndDk"
   },
   "outputs": [],
   "source": [
    "# @title Learner\n",
    "import os\n",
    "import torch\n",
    "import datetime\n",
    "import numpy as np\n",
    "from torch.utils.data import DataLoader\n",
    "import time\n",
    "# from datasets import convert_to_img\n",
    "# from datasets import preprocess\n",
    "# from datasets import postprocess\n",
    "from torchvision.utils import save_image\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "class Trainer(object):\n",
    "\n",
    "    def __init__(self, graph, optim, scheduler, trainingset, validset, args, cuda):\n",
    "\n",
    "        # set path and date\n",
    "        date = str(datetime.datetime.now())\n",
    "        date = date[:date.rfind(\":\")].replace(\"-\", \"\")\\\n",
    "                                     .replace(\":\", \"\")\\\n",
    "                                     .replace(\" \", \"_\")\n",
    "        self.log_dir = os.path.join(args.log_root, \"log_\" + date)\n",
    "        if not os.path.exists(self.log_dir):\n",
    "            os.makedirs(self.log_dir)\n",
    "\n",
    "\n",
    "        self.checkpoints_dir = os.path.join(self.log_dir, \"checkpoints\")\n",
    "        if not os.path.exists(self.checkpoints_dir):\n",
    "            os.makedirs(self.checkpoints_dir)\n",
    "\n",
    "        self.images_dir = os.path.join(self.log_dir, \"images\")\n",
    "        if not os.path.exists(self.images_dir):\n",
    "            os.makedirs(self.images_dir)\n",
    "\n",
    "        self.valid_samples_dir = os.path.join(self.log_dir, \"valid_samples\")\n",
    "        if not os.path.exists(self.valid_samples_dir):\n",
    "            os.makedirs(self.valid_samples_dir)\n",
    "\n",
    "\n",
    "\n",
    "        # model\n",
    "        self.graph = graph\n",
    "        self.optim = optim\n",
    "        self.scheduler = scheduler\n",
    "\n",
    "        # gradient bound\n",
    "        self.max_grad_clip = args.max_grad_clip\n",
    "        self.max_grad_norm = args.max_grad_norm\n",
    "\n",
    "        # data\n",
    "        self.dataset_name = args.dataset_name\n",
    "        self.batch_size = args.batch_size\n",
    "        self.trainingset_loader = DataLoader(trainingset,\n",
    "                                      batch_size=self.batch_size,\n",
    "                                      shuffle=True,\n",
    "                                      drop_last=True)\n",
    "\n",
    "        self.validset_loader = DataLoader(validset,\n",
    "                                      batch_size=self.batch_size,\n",
    "                                      shuffle=False,\n",
    "                                      drop_last=False)\n",
    "\n",
    "        self.num_epochs = args.num_epochs\n",
    "        self.global_step = args.num_steps\n",
    "        self.label_scale = args.label_scale\n",
    "        self.label_bias = args.label_bias\n",
    "        self.x_bins = args.x_bins\n",
    "        self.y_bins = args.y_bins\n",
    "\n",
    "\n",
    "        self.num_epochs = args.num_epochs\n",
    "        self.nll_gap = args.nll_gap\n",
    "        self.inference_gap = args.inference_gap\n",
    "        self.checkpoints_gap = args.checkpoints_gap\n",
    "        self.save_gap = args.save_gap\n",
    "\n",
    "        # device\n",
    "        self.cuda = cuda\n",
    "\n",
    "    def validate(self):\n",
    "        print (\"Start Validating\")\n",
    "        self.graph.eval()\n",
    "        mean_loss = list()\n",
    "        samples = list()\n",
    "        samples_ffi_nums = list()\n",
    "        with torch.no_grad():\n",
    "            for i_batch, batch in enumerate(self.validset_loader):\n",
    "                x = batch[\"x\"]\n",
    "                y = batch[\"y\"]\n",
    "                ffi_num = batch[\"ffi_num\"]\n",
    "\n",
    "                if self.cuda:\n",
    "                    x = x.cuda()\n",
    "                    y = y.cuda()\n",
    "\n",
    "\n",
    "                # forward\n",
    "                z, nll = self.graph(x,y)\n",
    "                loss = torch.mean(nll)\n",
    "                mean_loss.append(loss.data.cpu().item())\n",
    "\n",
    "                # true label\n",
    "                y_true = y\n",
    "\n",
    "                # sample\n",
    "                batch_samples = []\n",
    "                for b in range(len(y)):\n",
    "                    row = []\n",
    "                    row.append(y_true[b])\n",
    "                    batch_samples.append(row)\n",
    "                for i in range(0, 5):\n",
    "                    y_sample,_ = self.graph(x, y=None, reverse=True)\n",
    "\n",
    "                    for b in range(len(y)):\n",
    "                        batch_samples[b].append(y_sample[b])\n",
    "\n",
    "                for b in range(len(y)):\n",
    "                    samples.append(batch_samples[b])\n",
    "                    samples_ffi_nums.append(ffi_num[b])\n",
    "\n",
    "\n",
    "\n",
    "        # ADD ffi number to below code\n",
    "        \n",
    "        # save samples\n",
    "        for i in range(0, len(samples)):\n",
    "            if i < 5:\n",
    "                fig, axes = plt.subplots(1, len(samples[i]), figsize=(40, 7))\n",
    "                for b in range(0,len(samples[i])):\n",
    "                    plottitle = 'y true, y\\'s' if b == 0 else f'(Using only x_cond) sample {b}, x\\'s'\n",
    "                    axes[b].set_title(plottitle + ' ffi: ' + samples_ffi_nums[b])\n",
    "                    im_pred = axes[b].imshow(samples[i][b].cpu()[0], cmap=\"gray\")\n",
    "                    fig.colorbar(im_pred, fraction=0.046, pad=0.04)\n",
    "                fig.savefig(os.path.join(self.valid_samples_dir, f\"global_step{self.global_step}_sample{i}_batch{b}.pdf\"))\n",
    "                plt.close()\n",
    "\n",
    "        # save loss\n",
    "        mean = np.mean(mean_loss)\n",
    "        with open(os.path.join(self.log_dir, \"valid_NLL.txt\"), \"a\") as nll_file:\n",
    "            nll_file.write(str(self.global_step) + \"\\t\" + \"{:.5f}\".format(mean) + \"\\n\")\n",
    "        print (\"Finish Validating\")\n",
    "        self.graph.train()\n",
    "\n",
    "\n",
    "    def train(self):\n",
    "\n",
    "        self.graph.train()\n",
    "\n",
    "        starttime = time.time()\n",
    "\n",
    "        # run\n",
    "        num_batchs = len(self.trainingset_loader)\n",
    "        total_its = self.num_epochs * num_batchs\n",
    "        for epoch in range(self.num_epochs):\n",
    "            mean_nll = 0.0\n",
    "            for _, batch in enumerate(self.trainingset_loader):\n",
    "                self.optim.zero_grad()\n",
    "\n",
    "                x = batch[\"x\"]\n",
    "                y = batch[\"y\"]\n",
    "                ffi_num = batch['ffi_num']\n",
    "\n",
    "                if self.cuda:\n",
    "                    x = x.cuda()\n",
    "                    y = y.cuda()\n",
    "\n",
    "                # forward\n",
    "                z, nll = self.graph(x, y)\n",
    "\n",
    "\n",
    "                # loss\n",
    "                loss = torch.mean(nll)\n",
    "                mean_nll = mean_nll + loss.data\n",
    "\n",
    "                # backward\n",
    "                self.graph.zero_grad()\n",
    "                self.optim.zero_grad()\n",
    "                loss.backward()\n",
    "\n",
    "                # operate grad\n",
    "                if self.max_grad_clip > 0:\n",
    "                    torch.nn.utils.clip_grad_value_(self.graph.parameters(), self.max_grad_clip)\n",
    "                if self.max_grad_norm > 0:\n",
    "                    torch.nn.utils.clip_grad_norm_(self.graph.parameters(), self.max_grad_norm)\n",
    "\n",
    "                # step\n",
    "                self.optim.step()\n",
    "\n",
    "                currenttime = time.time()\n",
    "                elapsed = currenttime - starttime\n",
    "                print(\"Iteration: {}/{} \\t Elapsed time: {:.2f} \\t Loss:{:.5f}\".format(self.global_step, total_its, elapsed, loss.data))\n",
    "\n",
    "                if self.global_step % self.nll_gap == 0:\n",
    "                    with open(os.path.join(self.log_dir, \"NLL.txt\"), \"a\") as nll_file:\n",
    "                        nll_file.write(str(self.global_step) + \" \\t \" + \"{:.2f} \\t {:.5f}\".format(elapsed, loss.data) + \"\\n\")\n",
    "\n",
    "\n",
    "\n",
    "                # checkpoint\n",
    "                if self.global_step % self.checkpoints_gap == 0 and self.global_step > 0:\n",
    "                    self.validate()\n",
    "\n",
    "                    # samples\n",
    "                    samples = []\n",
    "                    for b in range(self.batch_size):\n",
    "                        samples.append([])\n",
    "\n",
    "                    for i in range(0,5):\n",
    "                        y_sample,_ = self.graph(x, y=None, reverse=True)\n",
    "                        for b in range(self.batch_size):\n",
    "                            samples[b].append(y_sample[b])\n",
    "\n",
    "                    # inverse image\n",
    "                    y_inverse,_ = self.graph(x, y=z, reverse=True)\n",
    "\n",
    "                    # true label\n",
    "                    y_true = y\n",
    "\n",
    "\n",
    "                    # save images\n",
    "                    output = None\n",
    "                    for b in range(0, self.batch_size):\n",
    "\n",
    "                        fig, axes = plt.subplots(1, 3, figsize=(20, 8))\n",
    "\n",
    "                        if b < 5:\n",
    "                            axes[0].set_title('y Truth' + ' , ffi: ' + ffi_num[b])\n",
    "                            im_true = axes[0].imshow(y[b].cpu()[0], cmap=\"gray\")\n",
    "                            fig.colorbar(im_true, fraction=0.046, pad=0.04)\n",
    "\n",
    "                            axes[1].set_title('y inverse' + ' , ffi: ' + ffi_num[b])\n",
    "                            im_latent = axes[1].imshow(y_inverse[b].cpu()[0], cmap=\"gray\")\n",
    "                            fig.colorbar(im_latent, fraction=0.046, pad=0.04)\n",
    "\n",
    "                            axes[2].set_title('y Prediction sample' + ' , ffi: ' + ffi_num[b])\n",
    "                            im_pred = axes[2].imshow(samples[b][i].cpu()[0], cmap=\"gray\")\n",
    "                            fig.colorbar(im_pred, fraction=0.046, pad=0.04)\n",
    "\n",
    "                            fig.savefig(os.path.join(self.images_dir, f\"global_step-{self.global_step}_batch_{b}.pdf\"))\n",
    "                            plt.close()\n",
    "\n",
    "\n",
    "\n",
    "                # save model\n",
    "                if self.global_step % self.save_gap == 0 and self.global_step > 0:\n",
    "                    save_model(self.graph, self.optim, self.scheduler, self.checkpoints_dir, self.global_step)\n",
    "\n",
    "\n",
    "                self.global_step = self.global_step + 1\n",
    "\n",
    "            if self.scheduler is not None:\n",
    "                self.scheduler.step()\n",
    "            mean_nll = float(mean_nll / float(num_batchs))\n",
    "            with open(os.path.join(self.log_dir, \"Epoch_NLL.txt\"), \"a\") as f:\n",
    "                currenttime = time.time()\n",
    "                elapsed = currenttime - starttime\n",
    "                f.write(\"{} \\t {:.2f}\\t {:.5f}\".format(epoch, elapsed, mean_nll) + \"\\n\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "class Inferencer(object):\n",
    "\n",
    "    def __init__(self, model, dataset, args, cuda):\n",
    "\n",
    "        # set path and date\n",
    "        self.out_root = args.out_root\n",
    "        if not os.path.exists(self.out_root):\n",
    "            os.makedirs(self.out_root)\n",
    "\n",
    "        # cuda\n",
    "        self.cuda = cuda\n",
    "\n",
    "        # model\n",
    "        self.model = model\n",
    "\n",
    "\n",
    "        # data\n",
    "        self.dataset_name = args.dataset_name\n",
    "        self.batch_size = args.batch_size\n",
    "        self.data_loader = DataLoader(dataset,\n",
    "                                      batch_size=self.batch_size,\n",
    "                                      shuffle=False,\n",
    "                                      drop_last=False)\n",
    "\n",
    "        self.label_scale = args.label_scale\n",
    "        self.label_bias = args.label_bias\n",
    "        self.num_labels = args.num_labels\n",
    "        self.y_bins = args.y_bins\n",
    "        self.x_bins = args.x_bins\n",
    "\n",
    "\n",
    "    def sampled_based_prediction(self, n_samples):\n",
    "        metrics = []\n",
    "        start = time.time()\n",
    "        for i_batch, batch in enumerate(self.data_loader):\n",
    "            print(f\"Batch IDs: {i_batch}\")\n",
    "\n",
    "            x = batch[\"x\"]\n",
    "            y = batch[\"y\"]\n",
    "            ffi_num = batch['ffi_num']\n",
    "\n",
    "            if self.cuda:\n",
    "                x = x.cuda()\n",
    "                y = y.cuda()\n",
    "\n",
    "            sample_list = list()\n",
    "            nll_list = list()\n",
    "            for i in range(0, n_samples):\n",
    "\n",
    "                print(f\"Samples: {i}/{n_samples}\")\n",
    "\n",
    "                # z, nll = self.model(x, y, reverse=False)\n",
    "                # ypred, logdet = self.model(x, z, reverse=True)\n",
    "\n",
    "                y_sample,_ = self.model(x, reverse=True)\n",
    "                _, nll = self.model(x,y_sample)\n",
    "                loss = torch.mean(nll)\n",
    "                sample_list.append(y_sample)\n",
    "                nll_list.append(loss.data.cpu().numpy())\n",
    "\n",
    "            sample = torch.stack(sample_list)\n",
    "            sample = torch.mean(sample, dim=0, keepdim=False)\n",
    "            nll = np.mean(nll_list)\n",
    "\n",
    "            y_pred_imgs = sample\n",
    "            y_true_imgs = y\n",
    "\n",
    "            # save trues and preds\n",
    "            output = []\n",
    "            for i in range(0, len(y_true_imgs)):\n",
    "                true_img = y_true_imgs[i]\n",
    "                pred_img = y_pred_imgs[i]\n",
    "                row = [x[i].cpu(), true_img, pred_img]\n",
    "                output.append(row)\n",
    "\n",
    "                fig, axes = plt.subplots(1, 3, figsize=(20, 8))\n",
    "\n",
    "                axes[0].set_title('Truth' + ' , ffi: ' + ffi_num[0])\n",
    "                im_true = axes[0].imshow(true_img.cpu()[0], cmap=\"gray\")\n",
    "                fig.colorbar(im_true, fraction=0.046, pad=0.04)\n",
    "\n",
    "                axes[1].set_title('x_cond' + ' , ffi: ' + ffi_num[0])\n",
    "                im_latent = axes[1].imshow(x[i].cpu()[0], cmap=\"gray\")\n",
    "                fig.colorbar(im_latent, fraction=0.046, pad=0.04)\n",
    "\n",
    "                axes[2].set_title('Prediction' + ' , ffi: ' + ffi_num[0])\n",
    "                im_pred = axes[2].imshow(pred_img.cpu()[0], cmap=\"gray\")\n",
    "                fig.colorbar(im_pred, fraction=0.046, pad=0.04)\n",
    "\n",
    "                fig.savefig(os.path.join(self.out_root, f\"batch_{i_batch}-{i}.pdf\"))\n",
    "                plt.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {
    "executionInfo": {
     "elapsed": 15,
     "status": "ok",
     "timestamp": 1692391055505,
     "user": {
      "displayName": "Joseph Lupo",
      "userId": "18192058496751055649"
     },
     "user_tz": 240
    },
    "id": "T9jhxCYOndFo"
   },
   "outputs": [],
   "source": [
    "# # @title Datasets\n",
    "# import os\n",
    "# import numpy as np\n",
    "# import torch\n",
    "# import PIL.Image as Image\n",
    "# from torch.utils import data\n",
    "# import torchvision.transforms as transforms\n",
    "\n",
    "\n",
    "# class HorseDataset(data.Dataset):\n",
    "\n",
    "#     def __init__(self, dir, size, n_c, portion=\"train\"):\n",
    "#         self.dir = dir\n",
    "#         self.names = self.read_names(dir, portion)\n",
    "#         self.n_c = n_c\n",
    "#         self.size = size\n",
    "\n",
    "#     def read_names(self, dir, portion):\n",
    "\n",
    "#         path = os.path.join(dir, \"{}.txt\".format(portion))\n",
    "#         names = list()\n",
    "#         with open(path, \"r\") as f:\n",
    "#             for line in f:\n",
    "#                 line = line.strip()\n",
    "#                 name = {}\n",
    "#                 name[\"img\"] = os.path.join(dir, os.path.join(\"images\", line))\n",
    "#                 name[\"lbl\"] = os.path.join(dir, os.path.join(\"labels\", line))\n",
    "#                 names.append(name)\n",
    "#         return names\n",
    "\n",
    "#     def __len__(self):\n",
    "#         return len(self.names)\n",
    "\n",
    "\n",
    "#     def __getitem__(self, index):\n",
    "\n",
    "#         # path\n",
    "#         name = self.names[index]\n",
    "#         img_path = name[\"img\"]\n",
    "#         lbl_path = name[\"lbl\"]\n",
    "#         transform = transforms.Compose([transforms.Resize(self.size), transforms.ToTensor()])\n",
    "\n",
    "#         # img\n",
    "#         img = Image.open(img_path).convert(\"RGB\")\n",
    "#         img = transform(img)\n",
    "\n",
    "#         # lbl\n",
    "#         lbl = Image.open(lbl_path).convert(\"L\")\n",
    "#         lbl = transform(lbl)\n",
    "#         lbl = torch.round(lbl)\n",
    "#         if self.n_c > 1:\n",
    "#             lbl = lbl.repeat(self.n_c,1,1)\n",
    "\n",
    "#         return {\"x\":img, \"y\":lbl}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {
    "executionInfo": {
     "elapsed": 14,
     "status": "ok",
     "timestamp": 1692391055506,
     "user": {
      "displayName": "Joseph Lupo",
      "userId": "18192058496751055649"
     },
     "user_tz": 240
    },
    "id": "FR8B9zdYndHt"
   },
   "outputs": [],
   "source": [
    "# @title TESS Dataset\n",
    "import os\n",
    "from PIL import Image\n",
    "import torch\n",
    "import pickle\n",
    "from torch.utils.data import Dataset, DataLoader, random_split\n",
    "import torchvision.transforms as transforms\n",
    "\n",
    "class TESSDataset(Dataset):\n",
    "    def __init__(self):\n",
    "\n",
    "        # self.data = []\n",
    "        # self.labels = []\n",
    "\n",
    "        # get data\n",
    "        angle_folder = \"/pdo/users/jlupoiii/TESS/data/angles/\"\n",
    "        ccd_folder = \"/pdo/users/jlupoiii/TESS/data/ccds/\"\n",
    "\n",
    "        # data matrices\n",
    "        X = []\n",
    "        Y = []\n",
    "        ffi_nums = []\n",
    "\n",
    "        angles_dic = pickle.load(open(angle_folder+'angles_O13_data.pkl', \"rb\"))\n",
    "\n",
    "        for filename in os.listdir(ccd_folder):\n",
    "            if len(filename) < 40 or filename[27] != '3': continue\n",
    "\n",
    "            image_arr = pickle.load(open(ccd_folder+filename, \"rb\"))\n",
    "            ffi_num = filename[18:18+8]\n",
    "            try:\n",
    "                angles = angles_dic[ffi_num]\n",
    "                # print('Got ffi number', ffi_num)\n",
    "            except:\n",
    "                # print('Could not find ffi with number:', ffi_num)\n",
    "                continue\n",
    "            X.append(np.array([angles[10], angles[11], angles[18], angles[19], angles[22], angles[23], angles[24], angles[25]]))\n",
    "            Y.append(image_arr.flatten())\n",
    "            ffi_nums.append(ffi_num)\n",
    "\n",
    "        # X = np.array(X)\n",
    "        # Y = np.array(Y)\n",
    "        # ffis = np.array(ffis)\n",
    "        # we are calculating Y GIVEN X\n",
    "        self.data = [Image.fromarray(x) for x in X]\n",
    "        self.labels = [Image.fromarray(y) for y in Y]\n",
    "        self.ffi_nums = ffi_nums\n",
    "\n",
    "        # for s in self.labels:\n",
    "        #     print(s.size)\n",
    "        #     # print(np.array(s).reshape((16,16))\n",
    "            \n",
    "        #     plt.imshow(np.array(s).reshape((16,16)))\n",
    "        #     plt.show()\n",
    "        #     plt.close()\n",
    "\n",
    "\n",
    "\n",
    "        # for class_idx, class_name in enumerate(self.classes):\n",
    "        #     # print(class_idx)\n",
    "        #     horse_img_path = os.path.join(self.root_dir, \"horse\", class_name)\n",
    "        #     mask_img_path = os.path.join(self.root_dir, \"mask\", class_name)\n",
    "\n",
    "        #     horse_image = Image.open(horse_img_path).convert('RGB')\n",
    "        #     mask_image = Image.open(mask_img_path).convert('L')\n",
    "\n",
    "        #     self.data.extend([horse_image])\n",
    "        #     self.labels.extend([mask_image])\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        # horse_img_path, mask_img_path = self.data[idx], self.labels[idx]\n",
    "        # label = self.labels[idx]\n",
    "        # horse_image = Image.open(horse_img_path).convert('RGB')\n",
    "        # mask_image = Image.open(mask_img_path).convert('L')\n",
    "\n",
    "        angles_image = self.data[idx]\n",
    "        ffi_image = self.labels[idx]\n",
    "        ffi_num = self.ffi_nums[idx]\n",
    "\n",
    "        transform = transforms.Compose([\n",
    "            transforms.Resize((8, 8)),\n",
    "            transforms.ToTensor(),\n",
    "            # transforms.Normalize(mean=[0.456], std=[0.225])\n",
    "        ])\n",
    "        target_transform = transforms.Compose([\n",
    "            lambda s: np.array(s),\n",
    "            lambda s: s.reshape((16,16)),\n",
    "            # transforms.Resize((16, 16)),\n",
    "            transforms.ToTensor()\n",
    "        ])\n",
    "\n",
    "        angles_image = transform(angles_image)\n",
    "        ffi_image = target_transform(ffi_image)\n",
    "        \n",
    "        # return x=angles, y=ffis\n",
    "        # we are calculating X GIVEN Y\n",
    "        return {\"x\":angles_image, \"y\":ffi_image, \"ffi_num\": ffi_num}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 6458,
     "status": "ok",
     "timestamp": 1692391061951,
     "user": {
      "displayName": "Joseph Lupo",
      "userId": "18192058496751055649"
     },
     "user_tz": 240
    },
    "id": "4S1ymwTMm1gO",
    "outputId": "60acbfb8-8096-4873-a1f1-0f1719f8514a",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "693\n",
      "torch.Size([1, 8, 8])\n",
      "torch.Size([1, 16, 16])\n",
      "00007761\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAZgAAAGdCAYAAAAv9mXmAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjcuMiwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy8pXeV/AAAACXBIWXMAAA9hAAAPYQGoP6dpAAAXpElEQVR4nO3dcWyUhf3H8c+1pQ9M2xOQQjuOAooiYDtGgWB1oiCkQYL+wQjBrIJbfpJjgI2JaX6JmN8ix/6YwS2kAnPFxDHYlhXU/KADZkuMVEpJ8wNNEIRJJwJzkbu2yQ7sPb+/vK0DSp+j3z485f1KnsQ7n+vz0Rje3l3bC7mu6woAgD6W5fcAAMDARGAAACYIDADABIEBAJggMAAAEwQGAGCCwAAATBAYAICJnP6+YCqV0rlz55SXl6dQKNTflwcA3ATXddXe3q6ioiJlZfX8HKXfA3Pu3DlFIpH+viwAoA+1tbVp9OjRPZ7T74HJy8uTJI1f+7KynMH9fXkAwE1IJf+p0xv/J/1neU/6PTDfviyW5QxWNoEBgEDqzVscvMkPADBBYAAAJggMAMAEgQEAmCAwAAATBAYAYILAAABMEBgAgAkCAwAwQWAAACYIDADABIEBAJggMAAAEwQGAGCCwAAATBAYAICJjAKzadMmjR07VoMHD9bMmTN1+PDhvt4FAAg4z4HZuXOnqqqqtG7dOh09elSlpaWaP3++Ll68aLEPABBQngPz2muv6Sc/+YmWL1+uSZMm6Y033tB3vvMd/eY3v7HYBwAIKE+BuXz5slpaWjR37tx/fYGsLM2dO1eHDh265mOSyaQSiUS3AwAw8HkKzFdffaWuri6NHDmy2/0jR47U+fPnr/mYWCymcDicPiKRSOZrAQCBYf5dZNXV1YrH4+mjra3N+pIAgFtAjpeT7777bmVnZ+vChQvd7r9w4YJGjRp1zcc4jiPHcTJfCAAIJE/PYHJzczVt2jQdOHAgfV8qldKBAwc0a9asPh8HAAguT89gJKmqqkqVlZUqKyvTjBkztHHjRnV2dmr58uUW+wAAAeU5MEuWLNHf//53vfzyyzp//ry+973vae/evVe98Q8AuL15DowkrVq1SqtWrerrLQCAAYTfRQYAMEFgAAAmCAwAwASBAQCYIDAAABMEBgBggsAAAEwQGACACQIDADBBYAAAJggMAMAEgQEAmCAwAAATBAYAYILAAABMhFzXdfvzgolEQuFwWF9/Ol75efQNAIIk0Z7S0PtOKx6PKz8/v8dz+RMeAGCCwAAATBAYAIAJAgMAMEFgAAAmCAwAwASBAQCYIDAAABMEBgBggsAAAEwQGACACQIDADBBYAAAJggMAMAEgQEAmCAwAAATBAYAYILAAABMEBgAgAnPgTl48KAWLlyooqIihUIh7dq1y2AWACDoPAems7NTpaWl2rRpk8UeAMAAkeP1ARUVFaqoqLDYAgAYQDwHxqtkMqlkMpm+nUgkrC8JALgFmL/JH4vFFA6H00ckErG+JADgFmAemOrqasXj8fTR1tZmfUkAwC3A/CUyx3HkOI71ZQAAtxh+DgYAYMLzM5iOjg6dOnUqffvMmTNqbW3VsGHDNGbMmD4dBwAILs+BOXLkiB577LH07aqqKklSZWWltm3b1mfDAADB5jkws2fPluu6FlsAAAMI78EAAEwQGACACQIDADBBYAAAJggMAMAEgQEAmCAwAAATBAYAYILAAABMEBgAgAkCAwAwQWAAACYIDADABIEBAJggMAAAE54/D6avPPrqc8rOHezX5QEAGei6/E9J/92rc3kGAwAwQWAAACYIDADABIEBAJggMAAAEwQGAGCCwAAATBAYAIAJAgMAMEFgAAAmCAwAwASBAQCYIDAAABMEBgBggsAAAEwQGACACQIDADBBYAAAJggMAMCEp8DEYjFNnz5deXl5Kigo0FNPPaUTJ05YbQMABJinwDQ2NioajaqpqUn79u3TlStXNG/ePHV2dlrtAwAEVI6Xk/fu3dvt9rZt21RQUKCWlhb94Ac/6NNhAIBg8xSY/xSPxyVJw4YNu+45yWRSyWQyfTuRSNzMJQEAAZHxm/ypVEpr165VeXm5pkyZct3zYrGYwuFw+ohEIpleEgAQIBkHJhqN6vjx49qxY0eP51VXVysej6ePtra2TC8JAAiQjF4iW7Vqld577z0dPHhQo0eP7vFcx3HkOE5G4wAAweUpMK7r6qc//anq6urU0NCgcePGWe0CAAScp8BEo1Ft375du3fvVl5ens6fPy9JCofDGjJkiMlAAEAweXoPpqamRvF4XLNnz1ZhYWH62Llzp9U+AEBAeX6JDACA3uB3kQEATBAYAIAJAgMAMEFgAAAmCAwAwASBAQCYIDAAABMEBgBggsAAAEwQGACACQIDADBBYAAAJggMAMAEgQEAmCAwAAATBAYAYMLTB471pYr/+kDOnYP8ujwAIAPJjiv6v7d6dy7PYAAAJggMAMAEgQEAmCAwAAATBAYAYILAAABMEBgAgAkCAwAwQWAAACYIDADABIEBAJggMAAAEwQGAGCCwAAATBAYAIAJAgMAMEFgAAAmCAwAwISnwNTU1KikpET5+fnKz8/XrFmztGfPHqttAIAA8xSY0aNHa8OGDWppadGRI0f0+OOPa9GiRfr444+t9gEAAirHy8kLFy7sdvvVV19VTU2NmpqaNHny5D4dBgAINk+B+XddXV36wx/+oM7OTs2aNeu65yWTSSWTyfTtRCKR6SUBAAHi+U3+Y8eO6c4775TjOHr++edVV1enSZMmXff8WCymcDicPiKRyE0NBgAEg+fA3H///WptbdVHH32klStXqrKyUp988sl1z6+urlY8Hk8fbW1tNzUYABAMnl8iy83N1b333itJmjZtmpqbm/X6669r8+bN1zzfcRw5jnNzKwEAgXPTPweTSqW6vccCAIDk8RlMdXW1KioqNGbMGLW3t2v79u1qaGhQfX291T4AQEB5CszFixf1ox/9SF9++aXC4bBKSkpUX1+vJ554wmofACCgPAXmzTfftNoBABhg+F1kAAATBAYAYILAAABMEBgAgAkCAwAwQWAAACYIDADABIEBAJggMAAAEwQGAGCCwAAATBAYAIAJAgMAMEFgAAAmCAwAwASBAQCY8PSBY33po4dylRMa5NflAQAZ+MYN9fpcnsEAAEwQGACACQIDADBBYAAAJggMAMAEgQEAmCAwAAATBAYAYILAAABMEBgAgAkCAwAwQWAAACYIDADABIEBAJggMAAAEwQGAGCCwAAATBAYAICJmwrMhg0bFAqFtHbt2j6aAwAYKDIOTHNzszZv3qySkpK+3AMAGCAyCkxHR4eWLVumrVu3aujQoX29CQAwAGQUmGg0qgULFmju3Ll9vQcAMEDkeH3Ajh07dPToUTU3N/fq/GQyqWQymb6dSCS8XhIAEECensG0tbVpzZo1+u1vf6vBgwf36jGxWEzhcDh9RCKRjIYCAIIl5Lqu29uTd+3apaefflrZ2dnp+7q6uhQKhZSVlaVkMtnt70nXfgYTiUQ0W4uUExrUB/8IAID+8o17RQ3arXg8rvz8/B7P9fQS2Zw5c3Ts2LFu9y1fvlwTJ07USy+9dFVcJMlxHDmO4+UyAIABwFNg8vLyNGXKlG733XHHHRo+fPhV9wMAbm/8JD8AwITn7yL7Tw0NDX0wAwAw0PAMBgBggsAAAEwQGACACQIDADBBYAAAJggMAMAEgQEAmCAwAAATBAYAYILAAABMEBgAgAkCAwAwQWAAACYIDADABIEBAJggMAAAEwQGAGCCwAAATBAYAIAJAgMAMEFgAAAmCAwAwASBAQCYIDAAABMEBgBggsAAAEwQGACACQIDADBBYAAAJggMAMAEgQEAmCAwAAATBAYAYILAAABMEBgAgAkCAwAw4Skwr7zyikKhULdj4sSJVtsAAAGW4/UBkydP1v79+//1BXI8fwkAwG3Acx1ycnI0atQoiy0AgAHE83swJ0+eVFFRkcaPH69ly5bp7NmzPZ6fTCaVSCS6HQCAgc9TYGbOnKlt27Zp7969qqmp0ZkzZ/TII4+ovb39uo+JxWIKh8PpIxKJ3PRoAMCtL+S6rpvpgy9duqTi4mK99tpreu655655TjKZVDKZTN9OJBKKRCKarUXKCQ3K9NIAAB98415Rg3YrHo8rPz+/x3Nv6h36u+66S/fdd59OnTp13XMcx5HjODdzGQBAAN3Uz8F0dHTos88+U2FhYV/tAQAMEJ4C8+KLL6qxsVF//etf9eGHH+rpp59Wdna2li5darUPABBQnl4i+9vf/qalS5fqH//4h0aMGKGHH35YTU1NGjFihNU+AEBAeQrMjh07rHYAAAYYfhcZAMAEgQEAmCAwAAATBAYAYILAAABMEBgAgAkCAwAwQWAAACYIDADABIEBAJggMAAAEwQGAGCCwAAATBAYAIAJAgMAMEFgAAAmCAwAwASBAQCYIDAAABMEBgBggsAAAEwQGACACQIDADBBYAAAJggMAMAEgQEAmCAwAAATBAYAYILAAABMEBgAgAkCAwAwQWAAACYIDADABIEBAJggMAAAE54D88UXX+iZZ57R8OHDNWTIED344IM6cuSIxTYAQIDleDn566+/Vnl5uR577DHt2bNHI0aM0MmTJzV06FCrfQCAgPIUmJ///OeKRCKqra1N3zdu3Lg+HwUACD5PL5G98847Kisr0+LFi1VQUKCpU6dq69atPT4mmUwqkUh0OwAAA5+nwJw+fVo1NTWaMGGC6uvrtXLlSq1evVpvvfXWdR8Ti8UUDofTRyQSuenRAIBbX8h1Xbe3J+fm5qqsrEwffvhh+r7Vq1erublZhw4duuZjksmkkslk+nYikVAkEtFsLVJOaNBNTAcA9Ldv3Ctq0G7F43Hl5+f3eK6nZzCFhYWaNGlSt/seeOABnT179rqPcRxH+fn53Q4AwMDnKTDl5eU6ceJEt/s+/fRTFRcX9+koAEDweQrMCy+8oKamJq1fv16nTp3S9u3btWXLFkWjUat9AICA8hSY6dOnq66uTr/73e80ZcoU/exnP9PGjRu1bNkyq30AgIDy9HMwkvTkk0/qySeftNgCABhA+F1kAAATBAYAYILAAABMEBgAgAkCAwAwQWAAACYIDADABIEBAJggMAAAEwQGAGCCwAAATBAYAIAJAgMAMEFgAAAmCAwAwASBAQCYIDAAABMEBgBggsAAAEwQGACACQIDADBBYAAAJggMAMAEgQEAmCAwAAATBAYAYILAAABMEBgAgAkCAwAwQWAAACYIDADABIEBAJggMAAAEwQGAGCCwAAATHgKzNixYxUKha46otGo1T4AQEDleDm5ublZXV1d6dvHjx/XE088ocWLF/f5MABAsHkKzIgRI7rd3rBhg+655x49+uijfToKABB8ngLz7y5fvqy3335bVVVVCoVC1z0vmUwqmUymbycSiUwvCQAIkIzf5N+1a5cuXbqkZ599tsfzYrGYwuFw+ohEIpleEgAQICHXdd1MHjh//nzl5ubq3Xff7fG8az2DiUQimq1FygkNyuTSAACffONeUYN2Kx6PKz8/v8dzM3qJ7PPPP9f+/fv1pz/96YbnOo4jx3EyuQwAIMAyeomstrZWBQUFWrBgQV/vAQAMEJ4Dk0qlVFtbq8rKSuXkZPw9AgCAAc5zYPbv36+zZ89qxYoVFnsAAAOE56cg8+bNU4bfFwAAuI3wu8gAACYIDADABIEBAJggMAAAEwQGAGCCwAAATBAYAIAJAgMAMEFgAAAmCAwAwASBAQCYIDAAABMEBgBggsAAAEwQGACAiX7/SMpvP0vmG12R+FgZAAiUb3RFknr1uWD9Hpj29nZJ0gf63/6+NACgj7S3tyscDvd4Tsjt54+nTKVSOnfunPLy8hQKhfr0aycSCUUiEbW1tSk/P79Pv7Yldvcvdve/oG5n99Vc11V7e7uKioqUldXzuyz9/gwmKytLo0ePNr1Gfn5+oP5j+Ba7+xe7+19Qt7O7uxs9c/kWb/IDAEwQGACAiQEVGMdxtG7dOjmO4/cUT9jdv9jd/4K6nd03p9/f5AcA3B4G1DMYAMCtg8AAAEwQGACACQIDADAxYAKzadMmjR07VoMHD9bMmTN1+PBhvyfd0MGDB7Vw4UIVFRUpFApp165dfk/qlVgspunTpysvL08FBQV66qmndOLECb9n3VBNTY1KSkrSP3w2a9Ys7dmzx+9Znm3YsEGhUEhr1671e0qPXnnlFYVCoW7HxIkT/Z7VK1988YWeeeYZDR8+XEOGDNGDDz6oI0eO+D3rhsaOHXvVv/NQKKRoNOrLngERmJ07d6qqqkrr1q3T0aNHVVpaqvnz5+vixYt+T+tRZ2enSktLtWnTJr+neNLY2KhoNKqmpibt27dPV65c0bx589TZ2en3tB6NHj1aGzZsUEtLi44cOaLHH39cixYt0scff+z3tF5rbm7W5s2bVVJS4veUXpk8ebK+/PLL9PHBBx/4PemGvv76a5WXl2vQoEHas2ePPvnkE/3iF7/Q0KFD/Z52Q83Nzd3+fe/bt0+StHjxYn8GuQPAjBkz3Gg0mr7d1dXlFhUVubFYzMdV3khy6+rq/J6RkYsXL7qS3MbGRr+neDZ06FD317/+td8zeqW9vd2dMGGCu2/fPvfRRx9116xZ4/ekHq1bt84tLS31e4ZnL730kvvwww/7PaNPrFmzxr3nnnvcVCrly/UD/wzm8uXLamlp0dy5c9P3ZWVlae7cuTp06JCPy24f8XhckjRs2DCfl/ReV1eXduzYoc7OTs2aNcvvOb0SjUa1YMGCbv+t3+pOnjypoqIijR8/XsuWLdPZs2f9nnRD77zzjsrKyrR48WIVFBRo6tSp2rp1q9+zPLt8+bLefvttrVixos9/sXBvBT4wX331lbq6ujRy5Mhu948cOVLnz5/3adXtI5VKae3atSovL9eUKVP8nnNDx44d05133inHcfT888+rrq5OkyZN8nvWDe3YsUNHjx5VLBbze0qvzZw5U9u2bdPevXtVU1OjM2fO6JFHHkl/ZMet6vTp06qpqdGECRNUX1+vlStXavXq1Xrrrbf8nubJrl27dOnSJT377LO+bej336aMgSUajer48eOBeG1dku6//361trYqHo/rj3/8oyorK9XY2HhLR6atrU1r1qzRvn37NHjwYL/n9FpFRUX6r0tKSjRz5kwVFxfr97//vZ577jkfl/UslUqprKxM69evlyRNnTpVx48f1xtvvKHKykqf1/Xem2++qYqKChUVFfm2IfDPYO6++25lZ2frwoUL3e6/cOGCRo0a5dOq28OqVav03nvv6f333zf/CIa+kpubq3vvvVfTpk1TLBZTaWmpXn/9db9n9ailpUUXL17U97//feXk5CgnJ0eNjY365S9/qZycHHV1dfk9sVfuuusu3XfffTp16pTfU3pUWFh41f9wPPDAA4F4ee9bn3/+ufbv368f//jHvu4IfGByc3M1bdo0HThwIH1fKpXSgQMHAvPaetC4rqtVq1aprq5Of/nLXzRu3Di/J2UslUopmUz6PaNHc+bM0bFjx9Ta2po+ysrKtGzZMrW2tio7O9vvib3S0dGhzz77TIWFhX5P6VF5eflV33b/6aefqri42KdF3tXW1qqgoEALFizwdceAeImsqqpKlZWVKisr04wZM7Rx40Z1dnZq+fLlfk/rUUdHR7f/mztz5oxaW1s1bNgwjRkzxsdlPYtGo9q+fbt2796tvLy89Htd4XBYQ4YM8Xnd9VVXV6uiokJjxoxRe3u7tm/froaGBtXX1/s9rUd5eXlXvb91xx13aPjw4bf0+14vvviiFi5cqOLiYp07d07r1q1Tdna2li5d6ve0Hr3wwgt66KGHtH79ev3whz/U4cOHtWXLFm3ZssXvab2SSqVUW1uryspK5eT4/Ee8L9+7ZuBXv/qVO2bMGDc3N9edMWOG29TU5PekG3r//fddSVcdlZWVfk/r0bU2S3Jra2v9ntajFStWuMXFxW5ubq47YsQId86cOe6f//xnv2dlJAjfprxkyRK3sLDQzc3Ndb/73e+6S5YscU+dOuX3rF5599133SlTpriO47gTJ050t2zZ4vekXquvr3cluSdOnPB7isuv6wcAmAj8ezAAgFsTgQEAmCAwAAATBAYAYILAAABMEBgAgAkCAwAwQWAAACYIDADABIEBAJggMAAAEwQGAGDi/wHwCIC/Bp/kgwAAAABJRU5ErkJggg==",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# we are calculating Y GIVEN X\n",
    "tess_dataset = TESSDataset()\n",
    "print(len(tess_dataset))\n",
    "print(tess_dataset[1]['x'].shape)\n",
    "print(tess_dataset[1]['y'].shape)\n",
    "print(tess_dataset[1]['ffi_num'])\n",
    "\n",
    "plt.imshow(tess_dataset[1]['x'][0])\n",
    "plt.show()\n",
    "plt.close()\n",
    "\n",
    "plt.imshow(tess_dataset[1]['y'][0])\n",
    "plt.show()\n",
    "plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "kjh1WU3Mm1iF",
    "outputId": "46a92afc-dfbc-403e-9731-2b7708c7725c",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of param: 9793876\n",
      "Iteration: 0/277000 \t Elapsed time: 0.15 \t Loss:44.94140\n",
      "Iteration: 1/277000 \t Elapsed time: 0.30 \t Loss:44.59123\n",
      "Iteration: 2/277000 \t Elapsed time: 0.48 \t Loss:44.33024\n",
      "Iteration: 3/277000 \t Elapsed time: 0.62 \t Loss:44.10220\n",
      "Iteration: 4/277000 \t Elapsed time: 0.76 \t Loss:43.89305\n",
      "Iteration: 5/277000 \t Elapsed time: 0.90 \t Loss:43.69872\n",
      "Iteration: 6/277000 \t Elapsed time: 1.05 \t Loss:43.51212\n",
      "Iteration: 7/277000 \t Elapsed time: 1.19 \t Loss:43.33480\n",
      "Iteration: 8/277000 \t Elapsed time: 1.33 \t Loss:43.16740\n",
      "Iteration: 9/277000 \t Elapsed time: 1.48 \t Loss:43.00337\n",
      "Iteration: 10/277000 \t Elapsed time: 1.62 \t Loss:42.83094\n",
      "Iteration: 11/277000 \t Elapsed time: 1.76 \t Loss:42.67070\n",
      "Iteration: 12/277000 \t Elapsed time: 1.91 \t Loss:42.51951\n",
      "Iteration: 13/277000 \t Elapsed time: 2.05 \t Loss:42.37238\n",
      "Iteration: 14/277000 \t Elapsed time: 2.19 \t Loss:42.21589\n",
      "Iteration: 15/277000 \t Elapsed time: 2.33 \t Loss:42.07532\n",
      "Iteration: 16/277000 \t Elapsed time: 2.50 \t Loss:41.94734\n",
      "Iteration: 17/277000 \t Elapsed time: 2.64 \t Loss:41.78039\n",
      "Iteration: 18/277000 \t Elapsed time: 2.77 \t Loss:41.64191\n",
      "Iteration: 19/277000 \t Elapsed time: 2.91 \t Loss:41.51422\n",
      "Iteration: 20/277000 \t Elapsed time: 3.04 \t Loss:41.36667\n",
      "Iteration: 21/277000 \t Elapsed time: 3.18 \t Loss:41.21962\n",
      "Iteration: 22/277000 \t Elapsed time: 3.34 \t Loss:41.10541\n",
      "Iteration: 23/277000 \t Elapsed time: 3.47 \t Loss:40.96499\n",
      "Iteration: 24/277000 \t Elapsed time: 3.61 \t Loss:40.84892\n",
      "Iteration: 25/277000 \t Elapsed time: 3.75 \t Loss:40.72937\n",
      "Iteration: 26/277000 \t Elapsed time: 3.89 \t Loss:40.60868\n",
      "Iteration: 27/277000 \t Elapsed time: 4.03 \t Loss:40.49813\n",
      "Iteration: 28/277000 \t Elapsed time: 4.17 \t Loss:40.40248\n",
      "Iteration: 29/277000 \t Elapsed time: 4.31 \t Loss:40.28217\n",
      "Iteration: 30/277000 \t Elapsed time: 4.45 \t Loss:40.17852\n",
      "Iteration: 31/277000 \t Elapsed time: 4.59 \t Loss:40.07858\n",
      "Iteration: 32/277000 \t Elapsed time: 4.73 \t Loss:39.96898\n",
      "Iteration: 33/277000 \t Elapsed time: 4.91 \t Loss:39.87061\n",
      "Iteration: 34/277000 \t Elapsed time: 5.05 \t Loss:39.78760\n",
      "Iteration: 35/277000 \t Elapsed time: 5.19 \t Loss:39.68611\n",
      "Iteration: 36/277000 \t Elapsed time: 5.34 \t Loss:39.59636\n",
      "Iteration: 37/277000 \t Elapsed time: 5.48 \t Loss:39.50988\n",
      "Iteration: 38/277000 \t Elapsed time: 5.62 \t Loss:39.42661\n",
      "Iteration: 39/277000 \t Elapsed time: 5.78 \t Loss:39.34164\n",
      "Iteration: 40/277000 \t Elapsed time: 5.92 \t Loss:39.25944\n",
      "Iteration: 41/277000 \t Elapsed time: 6.10 \t Loss:39.18225\n",
      "Iteration: 42/277000 \t Elapsed time: 6.24 \t Loss:39.10130\n",
      "Iteration: 43/277000 \t Elapsed time: 6.40 \t Loss:39.02282\n",
      "Iteration: 44/277000 \t Elapsed time: 6.54 \t Loss:38.94716\n",
      "Iteration: 45/277000 \t Elapsed time: 6.67 \t Loss:38.87206\n",
      "Iteration: 46/277000 \t Elapsed time: 6.81 \t Loss:38.79993\n",
      "Iteration: 47/277000 \t Elapsed time: 6.95 \t Loss:38.72510\n",
      "Iteration: 48/277000 \t Elapsed time: 7.09 \t Loss:38.65372\n",
      "Iteration: 49/277000 \t Elapsed time: 7.23 \t Loss:38.58291\n",
      "Iteration: 50/277000 \t Elapsed time: 7.37 \t Loss:38.51286\n",
      "Iteration: 51/277000 \t Elapsed time: 7.50 \t Loss:38.44392\n",
      "Iteration: 52/277000 \t Elapsed time: 7.64 \t Loss:38.37611\n",
      "Iteration: 53/277000 \t Elapsed time: 7.78 \t Loss:38.30869\n",
      "Iteration: 54/277000 \t Elapsed time: 7.92 \t Loss:38.24166\n",
      "Iteration: 55/277000 \t Elapsed time: 8.06 \t Loss:38.17544\n",
      "Iteration: 56/277000 \t Elapsed time: 8.20 \t Loss:38.11015\n",
      "Iteration: 57/277000 \t Elapsed time: 8.34 \t Loss:38.04554\n",
      "Iteration: 58/277000 \t Elapsed time: 8.47 \t Loss:37.98126\n",
      "Iteration: 59/277000 \t Elapsed time: 8.60 \t Loss:37.91796\n",
      "Iteration: 60/277000 \t Elapsed time: 8.76 \t Loss:37.85495\n",
      "Iteration: 61/277000 \t Elapsed time: 8.92 \t Loss:37.79262\n",
      "Iteration: 62/277000 \t Elapsed time: 9.06 \t Loss:37.73053\n",
      "Iteration: 63/277000 \t Elapsed time: 9.25 \t Loss:37.66880\n",
      "Iteration: 64/277000 \t Elapsed time: 9.45 \t Loss:37.60821\n",
      "Iteration: 65/277000 \t Elapsed time: 9.59 \t Loss:37.54797\n",
      "Iteration: 66/277000 \t Elapsed time: 9.72 \t Loss:37.48752\n",
      "Iteration: 67/277000 \t Elapsed time: 9.86 \t Loss:37.42807\n",
      "Iteration: 68/277000 \t Elapsed time: 9.99 \t Loss:37.36903\n",
      "Iteration: 69/277000 \t Elapsed time: 10.13 \t Loss:37.31051\n",
      "Iteration: 70/277000 \t Elapsed time: 10.30 \t Loss:37.25233\n",
      "Iteration: 71/277000 \t Elapsed time: 10.43 \t Loss:37.19436\n",
      "Iteration: 72/277000 \t Elapsed time: 10.56 \t Loss:37.13705\n",
      "Iteration: 73/277000 \t Elapsed time: 10.70 \t Loss:37.07984\n",
      "Iteration: 74/277000 \t Elapsed time: 10.85 \t Loss:37.02333\n",
      "Iteration: 75/277000 \t Elapsed time: 10.98 \t Loss:36.96775\n",
      "Iteration: 76/277000 \t Elapsed time: 11.12 \t Loss:36.91151\n",
      "Iteration: 77/277000 \t Elapsed time: 11.25 \t Loss:36.85569\n",
      "Iteration: 78/277000 \t Elapsed time: 11.39 \t Loss:36.80032\n",
      "Iteration: 79/277000 \t Elapsed time: 11.53 \t Loss:36.74548\n",
      "Iteration: 80/277000 \t Elapsed time: 11.68 \t Loss:36.69110\n",
      "Iteration: 81/277000 \t Elapsed time: 11.82 \t Loss:36.63680\n",
      "Iteration: 82/277000 \t Elapsed time: 12.00 \t Loss:36.58276\n",
      "Iteration: 83/277000 \t Elapsed time: 12.49 \t Loss:36.52927\n",
      "Iteration: 84/277000 \t Elapsed time: 12.64 \t Loss:36.47583\n",
      "Iteration: 85/277000 \t Elapsed time: 12.78 \t Loss:36.42307\n",
      "Iteration: 86/277000 \t Elapsed time: 12.92 \t Loss:36.37048\n",
      "Iteration: 87/277000 \t Elapsed time: 13.05 \t Loss:36.31760\n",
      "Iteration: 88/277000 \t Elapsed time: 13.20 \t Loss:36.26548\n",
      "Iteration: 89/277000 \t Elapsed time: 13.34 \t Loss:36.21351\n",
      "Iteration: 90/277000 \t Elapsed time: 13.47 \t Loss:36.16191\n",
      "Iteration: 91/277000 \t Elapsed time: 13.61 \t Loss:36.11038\n",
      "Iteration: 92/277000 \t Elapsed time: 13.75 \t Loss:36.05916\n",
      "Iteration: 93/277000 \t Elapsed time: 13.89 \t Loss:36.00844\n",
      "Iteration: 94/277000 \t Elapsed time: 14.04 \t Loss:35.95760\n",
      "Iteration: 95/277000 \t Elapsed time: 14.18 \t Loss:35.90714\n",
      "Iteration: 96/277000 \t Elapsed time: 14.32 \t Loss:35.85702\n",
      "Iteration: 97/277000 \t Elapsed time: 14.46 \t Loss:35.80713\n",
      "Iteration: 98/277000 \t Elapsed time: 14.61 \t Loss:35.75739\n",
      "Iteration: 99/277000 \t Elapsed time: 14.76 \t Loss:35.70798\n",
      "Iteration: 100/277000 \t Elapsed time: 14.91 \t Loss:35.65877\n",
      "Iteration: 101/277000 \t Elapsed time: 15.07 \t Loss:35.61003\n",
      "Iteration: 102/277000 \t Elapsed time: 15.22 \t Loss:35.56087\n",
      "Iteration: 103/277000 \t Elapsed time: 15.36 \t Loss:35.51212\n",
      "Iteration: 104/277000 \t Elapsed time: 15.50 \t Loss:35.46368\n",
      "Iteration: 105/277000 \t Elapsed time: 15.64 \t Loss:35.41554\n",
      "Iteration: 106/277000 \t Elapsed time: 15.82 \t Loss:35.36758\n",
      "Iteration: 107/277000 \t Elapsed time: 15.96 \t Loss:35.31976\n",
      "Iteration: 108/277000 \t Elapsed time: 16.10 \t Loss:35.27232\n",
      "Iteration: 109/277000 \t Elapsed time: 16.25 \t Loss:35.22481\n",
      "Iteration: 110/277000 \t Elapsed time: 16.41 \t Loss:35.17782\n",
      "Iteration: 111/277000 \t Elapsed time: 16.55 \t Loss:35.13067\n",
      "Iteration: 112/277000 \t Elapsed time: 16.69 \t Loss:35.08397\n",
      "Iteration: 113/277000 \t Elapsed time: 16.86 \t Loss:35.03728\n",
      "Iteration: 114/277000 \t Elapsed time: 17.01 \t Loss:34.99084\n",
      "Iteration: 115/277000 \t Elapsed time: 17.15 \t Loss:34.94454\n",
      "Iteration: 116/277000 \t Elapsed time: 17.35 \t Loss:34.89869\n",
      "Iteration: 117/277000 \t Elapsed time: 17.49 \t Loss:34.85258\n",
      "Iteration: 118/277000 \t Elapsed time: 17.63 \t Loss:34.80687\n",
      "Iteration: 119/277000 \t Elapsed time: 17.77 \t Loss:34.76153\n",
      "Iteration: 120/277000 \t Elapsed time: 17.91 \t Loss:34.71597\n",
      "Iteration: 121/277000 \t Elapsed time: 18.05 \t Loss:34.67085\n",
      "Iteration: 122/277000 \t Elapsed time: 18.20 \t Loss:34.62608\n",
      "Iteration: 123/277000 \t Elapsed time: 18.34 \t Loss:34.58139\n",
      "Iteration: 124/277000 \t Elapsed time: 18.49 \t Loss:34.53624\n",
      "Iteration: 125/277000 \t Elapsed time: 18.64 \t Loss:34.49163\n",
      "Iteration: 126/277000 \t Elapsed time: 18.78 \t Loss:34.44743\n",
      "Iteration: 127/277000 \t Elapsed time: 18.92 \t Loss:34.40343\n",
      "Iteration: 128/277000 \t Elapsed time: 19.06 \t Loss:34.35880\n",
      "Iteration: 129/277000 \t Elapsed time: 19.21 \t Loss:34.31493\n",
      "Iteration: 130/277000 \t Elapsed time: 19.35 \t Loss:34.27113\n",
      "Iteration: 131/277000 \t Elapsed time: 19.50 \t Loss:34.22727\n",
      "Iteration: 132/277000 \t Elapsed time: 19.64 \t Loss:34.18376\n",
      "Iteration: 133/277000 \t Elapsed time: 19.79 \t Loss:34.14051\n",
      "Iteration: 134/277000 \t Elapsed time: 19.93 \t Loss:34.09733\n",
      "Iteration: 135/277000 \t Elapsed time: 20.07 \t Loss:34.05457\n",
      "Iteration: 136/277000 \t Elapsed time: 20.21 \t Loss:34.01152\n",
      "Iteration: 137/277000 \t Elapsed time: 20.37 \t Loss:33.96844\n",
      "Iteration: 138/277000 \t Elapsed time: 20.51 \t Loss:33.92582\n",
      "Iteration: 139/277000 \t Elapsed time: 20.65 \t Loss:33.88334\n",
      "Iteration: 140/277000 \t Elapsed time: 20.78 \t Loss:33.84076\n",
      "Iteration: 141/277000 \t Elapsed time: 20.92 \t Loss:33.79849\n",
      "Iteration: 142/277000 \t Elapsed time: 21.06 \t Loss:33.75643\n",
      "Iteration: 143/277000 \t Elapsed time: 21.20 \t Loss:33.71448\n",
      "Iteration: 144/277000 \t Elapsed time: 21.34 \t Loss:33.67244\n",
      "Iteration: 145/277000 \t Elapsed time: 21.48 \t Loss:33.63078\n",
      "Iteration: 146/277000 \t Elapsed time: 21.63 \t Loss:33.58953\n",
      "Iteration: 147/277000 \t Elapsed time: 21.76 \t Loss:33.54752\n",
      "Iteration: 148/277000 \t Elapsed time: 21.90 \t Loss:33.50613\n",
      "Iteration: 149/277000 \t Elapsed time: 22.04 \t Loss:33.46499\n",
      "Iteration: 150/277000 \t Elapsed time: 22.19 \t Loss:33.42388\n",
      "Iteration: 151/277000 \t Elapsed time: 22.35 \t Loss:33.38262\n",
      "Iteration: 152/277000 \t Elapsed time: 22.48 \t Loss:33.34178\n",
      "Iteration: 153/277000 \t Elapsed time: 22.61 \t Loss:33.30094\n",
      "Iteration: 154/277000 \t Elapsed time: 22.76 \t Loss:33.26031\n",
      "Iteration: 155/277000 \t Elapsed time: 22.90 \t Loss:33.21955\n",
      "Iteration: 156/277000 \t Elapsed time: 23.03 \t Loss:33.17905\n",
      "Iteration: 157/277000 \t Elapsed time: 23.17 \t Loss:33.13866\n",
      "Iteration: 158/277000 \t Elapsed time: 23.30 \t Loss:33.09847\n",
      "Iteration: 159/277000 \t Elapsed time: 23.46 \t Loss:33.05831\n",
      "Iteration: 160/277000 \t Elapsed time: 23.65 \t Loss:33.01815\n",
      "Iteration: 161/277000 \t Elapsed time: 23.79 \t Loss:32.97832\n",
      "Iteration: 162/277000 \t Elapsed time: 23.93 \t Loss:32.93863\n",
      "Iteration: 163/277000 \t Elapsed time: 24.08 \t Loss:32.89914\n",
      "Iteration: 164/277000 \t Elapsed time: 24.22 \t Loss:32.85932\n",
      "Iteration: 165/277000 \t Elapsed time: 24.37 \t Loss:32.81962\n",
      "Iteration: 166/277000 \t Elapsed time: 24.51 \t Loss:32.78015\n",
      "Iteration: 167/277000 \t Elapsed time: 24.65 \t Loss:32.74093\n",
      "Iteration: 168/277000 \t Elapsed time: 24.80 \t Loss:32.70177\n",
      "Iteration: 169/277000 \t Elapsed time: 24.94 \t Loss:32.66292\n",
      "Iteration: 170/277000 \t Elapsed time: 25.08 \t Loss:32.62374\n",
      "Iteration: 171/277000 \t Elapsed time: 25.23 \t Loss:32.58464\n",
      "Iteration: 172/277000 \t Elapsed time: 25.38 \t Loss:32.54526\n",
      "Iteration: 173/277000 \t Elapsed time: 25.52 \t Loss:32.50611\n",
      "Iteration: 174/277000 \t Elapsed time: 25.65 \t Loss:32.46744\n",
      "Iteration: 175/277000 \t Elapsed time: 25.84 \t Loss:32.42860\n",
      "Iteration: 176/277000 \t Elapsed time: 26.01 \t Loss:32.38927\n",
      "Iteration: 177/277000 \t Elapsed time: 26.15 \t Loss:32.35072\n",
      "Iteration: 178/277000 \t Elapsed time: 26.29 \t Loss:32.31243\n",
      "Iteration: 179/277000 \t Elapsed time: 26.43 \t Loss:32.27441\n",
      "Iteration: 180/277000 \t Elapsed time: 26.57 \t Loss:32.23617\n",
      "Iteration: 181/277000 \t Elapsed time: 26.73 \t Loss:32.19870\n",
      "Iteration: 182/277000 \t Elapsed time: 26.87 \t Loss:32.16031\n",
      "Iteration: 183/277000 \t Elapsed time: 27.01 \t Loss:32.12246\n",
      "Iteration: 184/277000 \t Elapsed time: 27.15 \t Loss:32.08457\n",
      "Iteration: 185/277000 \t Elapsed time: 27.30 \t Loss:32.04700\n",
      "Iteration: 186/277000 \t Elapsed time: 27.43 \t Loss:32.01000\n",
      "Iteration: 187/277000 \t Elapsed time: 27.56 \t Loss:31.97198\n",
      "Iteration: 188/277000 \t Elapsed time: 27.70 \t Loss:31.93457\n",
      "Iteration: 189/277000 \t Elapsed time: 27.85 \t Loss:31.89717\n",
      "Iteration: 190/277000 \t Elapsed time: 27.99 \t Loss:31.86019\n",
      "Iteration: 191/277000 \t Elapsed time: 28.13 \t Loss:31.82260\n",
      "Iteration: 192/277000 \t Elapsed time: 28.27 \t Loss:31.78518\n",
      "Iteration: 193/277000 \t Elapsed time: 28.40 \t Loss:31.74771\n",
      "Iteration: 194/277000 \t Elapsed time: 28.55 \t Loss:31.71079\n",
      "Iteration: 195/277000 \t Elapsed time: 28.68 \t Loss:31.67368\n",
      "Iteration: 196/277000 \t Elapsed time: 28.82 \t Loss:31.63679\n",
      "Iteration: 197/277000 \t Elapsed time: 28.97 \t Loss:31.60019\n",
      "Iteration: 198/277000 \t Elapsed time: 29.11 \t Loss:31.56364\n",
      "Iteration: 199/277000 \t Elapsed time: 29.25 \t Loss:31.52712\n",
      "Iteration: 200/277000 \t Elapsed time: 29.40 \t Loss:31.49101\n",
      "Iteration: 201/277000 \t Elapsed time: 29.54 \t Loss:31.45447\n",
      "Iteration: 202/277000 \t Elapsed time: 29.68 \t Loss:31.41838\n",
      "Iteration: 203/277000 \t Elapsed time: 29.83 \t Loss:31.38206\n",
      "Iteration: 204/277000 \t Elapsed time: 29.96 \t Loss:31.34600\n",
      "Iteration: 205/277000 \t Elapsed time: 30.17 \t Loss:31.31005\n",
      "Iteration: 206/277000 \t Elapsed time: 30.31 \t Loss:31.27417\n",
      "Iteration: 207/277000 \t Elapsed time: 30.45 \t Loss:31.23827\n",
      "Iteration: 208/277000 \t Elapsed time: 30.59 \t Loss:31.20260\n",
      "Iteration: 209/277000 \t Elapsed time: 30.73 \t Loss:31.16721\n",
      "Iteration: 210/277000 \t Elapsed time: 30.88 \t Loss:31.13130\n",
      "Iteration: 211/277000 \t Elapsed time: 31.02 \t Loss:31.09584\n",
      "Iteration: 212/277000 \t Elapsed time: 31.16 \t Loss:31.06065\n",
      "Iteration: 213/277000 \t Elapsed time: 31.29 \t Loss:31.02505\n",
      "Iteration: 214/277000 \t Elapsed time: 31.43 \t Loss:30.99002\n",
      "Iteration: 215/277000 \t Elapsed time: 31.58 \t Loss:30.95457\n",
      "Iteration: 216/277000 \t Elapsed time: 31.72 \t Loss:30.91969\n",
      "Iteration: 217/277000 \t Elapsed time: 31.86 \t Loss:30.88462\n",
      "Iteration: 218/277000 \t Elapsed time: 32.01 \t Loss:30.84933\n",
      "Iteration: 219/277000 \t Elapsed time: 32.22 \t Loss:30.81462\n",
      "Iteration: 220/277000 \t Elapsed time: 32.38 \t Loss:30.77952\n",
      "Iteration: 221/277000 \t Elapsed time: 32.52 \t Loss:30.74502\n",
      "Iteration: 222/277000 \t Elapsed time: 32.69 \t Loss:30.71013\n",
      "Iteration: 223/277000 \t Elapsed time: 32.83 \t Loss:30.67549\n",
      "Iteration: 224/277000 \t Elapsed time: 32.98 \t Loss:30.64090\n",
      "Iteration: 225/277000 \t Elapsed time: 33.13 \t Loss:30.60633\n",
      "Iteration: 226/277000 \t Elapsed time: 33.26 \t Loss:30.57175\n",
      "Iteration: 227/277000 \t Elapsed time: 33.40 \t Loss:30.53707\n",
      "Iteration: 228/277000 \t Elapsed time: 33.54 \t Loss:30.50281\n",
      "Iteration: 229/277000 \t Elapsed time: 33.68 \t Loss:30.46844\n",
      "Iteration: 230/277000 \t Elapsed time: 33.85 \t Loss:30.43446\n",
      "Iteration: 231/277000 \t Elapsed time: 34.00 \t Loss:30.40010\n",
      "Iteration: 232/277000 \t Elapsed time: 34.14 \t Loss:30.36632\n",
      "Iteration: 233/277000 \t Elapsed time: 34.28 \t Loss:30.33224\n",
      "Iteration: 234/277000 \t Elapsed time: 34.43 \t Loss:30.29850\n",
      "Iteration: 235/277000 \t Elapsed time: 34.57 \t Loss:30.26464\n",
      "Iteration: 236/277000 \t Elapsed time: 34.71 \t Loss:30.23098\n",
      "Iteration: 237/277000 \t Elapsed time: 34.85 \t Loss:30.19728\n",
      "Iteration: 238/277000 \t Elapsed time: 35.00 \t Loss:30.16386\n",
      "Iteration: 239/277000 \t Elapsed time: 35.14 \t Loss:30.13021\n",
      "Iteration: 240/277000 \t Elapsed time: 35.28 \t Loss:30.09661\n",
      "Iteration: 241/277000 \t Elapsed time: 35.42 \t Loss:30.06328\n",
      "Iteration: 242/277000 \t Elapsed time: 35.57 \t Loss:30.02991\n",
      "Iteration: 243/277000 \t Elapsed time: 35.74 \t Loss:29.99659\n",
      "Iteration: 244/277000 \t Elapsed time: 35.88 \t Loss:29.96358\n",
      "Iteration: 245/277000 \t Elapsed time: 36.01 \t Loss:29.93022\n",
      "Iteration: 246/277000 \t Elapsed time: 36.15 \t Loss:29.89708\n",
      "Iteration: 247/277000 \t Elapsed time: 36.29 \t Loss:29.86415\n",
      "Iteration: 248/277000 \t Elapsed time: 36.43 \t Loss:29.83114\n",
      "Iteration: 249/277000 \t Elapsed time: 36.61 \t Loss:29.79822\n",
      "Iteration: 250/277000 \t Elapsed time: 36.76 \t Loss:29.76544\n",
      "Iteration: 251/277000 \t Elapsed time: 36.90 \t Loss:29.73267\n",
      "Iteration: 252/277000 \t Elapsed time: 37.07 \t Loss:29.69992\n",
      "Iteration: 253/277000 \t Elapsed time: 37.21 \t Loss:29.66739\n",
      "Iteration: 254/277000 \t Elapsed time: 37.35 \t Loss:29.63476\n",
      "Iteration: 255/277000 \t Elapsed time: 37.50 \t Loss:29.60245\n",
      "Iteration: 256/277000 \t Elapsed time: 37.64 \t Loss:29.56976\n",
      "Iteration: 257/277000 \t Elapsed time: 37.78 \t Loss:29.53735\n",
      "Iteration: 258/277000 \t Elapsed time: 37.92 \t Loss:29.50504\n",
      "Iteration: 259/277000 \t Elapsed time: 38.06 \t Loss:29.47276\n",
      "Iteration: 260/277000 \t Elapsed time: 38.19 \t Loss:29.44070\n",
      "Iteration: 261/277000 \t Elapsed time: 38.33 \t Loss:29.40878\n",
      "Iteration: 262/277000 \t Elapsed time: 38.47 \t Loss:29.37627\n",
      "Iteration: 263/277000 \t Elapsed time: 38.62 \t Loss:29.34435\n",
      "Iteration: 264/277000 \t Elapsed time: 38.78 \t Loss:29.31223\n",
      "Iteration: 265/277000 \t Elapsed time: 38.92 \t Loss:29.28042\n",
      "Iteration: 266/277000 \t Elapsed time: 39.06 \t Loss:29.24852\n",
      "Iteration: 267/277000 \t Elapsed time: 39.21 \t Loss:29.21681\n",
      "Iteration: 268/277000 \t Elapsed time: 39.35 \t Loss:29.18488\n",
      "Iteration: 269/277000 \t Elapsed time: 39.49 \t Loss:29.15332\n",
      "Iteration: 270/277000 \t Elapsed time: 39.64 \t Loss:29.12168\n",
      "Iteration: 271/277000 \t Elapsed time: 39.78 \t Loss:29.09004\n",
      "Iteration: 272/277000 \t Elapsed time: 39.92 \t Loss:29.05862\n",
      "Iteration: 273/277000 \t Elapsed time: 40.07 \t Loss:29.02704\n",
      "Iteration: 274/277000 \t Elapsed time: 40.21 \t Loss:28.99558\n",
      "Iteration: 275/277000 \t Elapsed time: 40.35 \t Loss:28.96424\n",
      "Iteration: 276/277000 \t Elapsed time: 40.49 \t Loss:28.93298\n",
      "Iteration: 277/277000 \t Elapsed time: 40.65 \t Loss:28.90198\n",
      "Iteration: 278/277000 \t Elapsed time: 40.80 \t Loss:28.87051\n",
      "Iteration: 279/277000 \t Elapsed time: 40.94 \t Loss:28.83931\n",
      "Iteration: 280/277000 \t Elapsed time: 41.07 \t Loss:28.80834\n",
      "Iteration: 281/277000 \t Elapsed time: 41.23 \t Loss:28.77727\n",
      "Iteration: 282/277000 \t Elapsed time: 41.37 \t Loss:28.74630\n",
      "Iteration: 283/277000 \t Elapsed time: 41.56 \t Loss:28.71544\n",
      "Iteration: 284/277000 \t Elapsed time: 41.71 \t Loss:28.68453\n",
      "Iteration: 285/277000 \t Elapsed time: 41.85 \t Loss:28.65373\n",
      "Iteration: 286/277000 \t Elapsed time: 41.99 \t Loss:28.62290\n",
      "Iteration: 287/277000 \t Elapsed time: 42.15 \t Loss:28.59228\n",
      "Iteration: 288/277000 \t Elapsed time: 42.29 \t Loss:28.56148\n",
      "Iteration: 289/277000 \t Elapsed time: 42.44 \t Loss:28.53097\n",
      "Iteration: 290/277000 \t Elapsed time: 42.64 \t Loss:28.50031\n",
      "Iteration: 291/277000 \t Elapsed time: 42.78 \t Loss:28.46974\n",
      "Iteration: 292/277000 \t Elapsed time: 42.92 \t Loss:28.43934\n",
      "Iteration: 293/277000 \t Elapsed time: 43.06 \t Loss:28.40891\n",
      "Iteration: 294/277000 \t Elapsed time: 43.20 \t Loss:28.37847\n",
      "Iteration: 295/277000 \t Elapsed time: 43.33 \t Loss:28.34828\n",
      "Iteration: 296/277000 \t Elapsed time: 43.47 \t Loss:28.31807\n",
      "Iteration: 297/277000 \t Elapsed time: 43.62 \t Loss:28.28786\n",
      "Iteration: 298/277000 \t Elapsed time: 43.76 \t Loss:28.25779\n",
      "Iteration: 299/277000 \t Elapsed time: 43.90 \t Loss:28.22811\n",
      "Iteration: 300/277000 \t Elapsed time: 44.04 \t Loss:28.19749\n",
      "Iteration: 301/277000 \t Elapsed time: 44.18 \t Loss:28.16781\n",
      "Iteration: 302/277000 \t Elapsed time: 44.33 \t Loss:28.13799\n",
      "Iteration: 303/277000 \t Elapsed time: 44.47 \t Loss:28.10771\n",
      "Iteration: 304/277000 \t Elapsed time: 44.61 \t Loss:28.07841\n",
      "Iteration: 305/277000 \t Elapsed time: 44.75 \t Loss:28.04823\n",
      "Iteration: 306/277000 \t Elapsed time: 44.90 \t Loss:28.01852\n",
      "Iteration: 307/277000 \t Elapsed time: 45.04 \t Loss:27.98888\n",
      "Iteration: 308/277000 \t Elapsed time: 45.18 \t Loss:27.95912\n",
      "Iteration: 309/277000 \t Elapsed time: 45.32 \t Loss:27.93010\n",
      "Iteration: 310/277000 \t Elapsed time: 45.46 \t Loss:27.89982\n",
      "Iteration: 311/277000 \t Elapsed time: 45.60 \t Loss:27.87103\n",
      "Iteration: 312/277000 \t Elapsed time: 45.74 \t Loss:27.84116\n",
      "Iteration: 313/277000 \t Elapsed time: 45.91 \t Loss:27.81219\n",
      "Iteration: 314/277000 \t Elapsed time: 46.05 \t Loss:27.78326\n",
      "Iteration: 315/277000 \t Elapsed time: 46.20 \t Loss:27.75325\n",
      "Iteration: 316/277000 \t Elapsed time: 46.33 \t Loss:27.72511\n",
      "Iteration: 317/277000 \t Elapsed time: 46.47 \t Loss:27.69448\n",
      "Iteration: 318/277000 \t Elapsed time: 46.61 \t Loss:27.66611\n",
      "Iteration: 319/277000 \t Elapsed time: 46.75 \t Loss:27.63671\n",
      "Iteration: 320/277000 \t Elapsed time: 46.89 \t Loss:27.60760\n",
      "Iteration: 321/277000 \t Elapsed time: 47.03 \t Loss:27.57875\n",
      "Iteration: 322/277000 \t Elapsed time: 47.17 \t Loss:27.54942\n",
      "Iteration: 323/277000 \t Elapsed time: 47.40 \t Loss:27.52120\n",
      "Iteration: 324/277000 \t Elapsed time: 47.57 \t Loss:27.49153\n",
      "Iteration: 325/277000 \t Elapsed time: 47.71 \t Loss:27.46339\n",
      "Iteration: 326/277000 \t Elapsed time: 47.86 \t Loss:27.43378\n",
      "Iteration: 327/277000 \t Elapsed time: 47.99 \t Loss:27.40582\n",
      "Iteration: 328/277000 \t Elapsed time: 48.14 \t Loss:27.37657\n",
      "Iteration: 329/277000 \t Elapsed time: 48.28 \t Loss:27.34812\n",
      "Iteration: 330/277000 \t Elapsed time: 48.42 \t Loss:27.31963\n",
      "Iteration: 331/277000 \t Elapsed time: 48.76 \t Loss:27.29054\n",
      "Iteration: 332/277000 \t Elapsed time: 48.90 \t Loss:27.26287\n",
      "Iteration: 333/277000 \t Elapsed time: 49.04 \t Loss:27.23364\n",
      "Iteration: 334/277000 \t Elapsed time: 49.18 \t Loss:27.20541\n",
      "Iteration: 335/277000 \t Elapsed time: 49.32 \t Loss:27.17725\n",
      "Iteration: 336/277000 \t Elapsed time: 49.46 \t Loss:27.14830\n",
      "Iteration: 337/277000 \t Elapsed time: 49.60 \t Loss:27.12057\n",
      "Iteration: 338/277000 \t Elapsed time: 49.74 \t Loss:27.09199\n",
      "Iteration: 339/277000 \t Elapsed time: 49.88 \t Loss:27.06364\n",
      "Iteration: 340/277000 \t Elapsed time: 50.03 \t Loss:27.03580\n",
      "Iteration: 341/277000 \t Elapsed time: 50.17 \t Loss:27.00736\n",
      "Iteration: 342/277000 \t Elapsed time: 50.31 \t Loss:26.97938\n",
      "Iteration: 343/277000 \t Elapsed time: 50.45 \t Loss:26.95138\n",
      "Iteration: 344/277000 \t Elapsed time: 50.59 \t Loss:26.92321\n",
      "Iteration: 345/277000 \t Elapsed time: 50.73 \t Loss:26.89541\n",
      "Iteration: 346/277000 \t Elapsed time: 50.89 \t Loss:26.86741\n",
      "Iteration: 347/277000 \t Elapsed time: 51.03 \t Loss:26.83951\n",
      "Iteration: 348/277000 \t Elapsed time: 51.19 \t Loss:26.81184\n",
      "Iteration: 349/277000 \t Elapsed time: 51.33 \t Loss:26.78389\n",
      "Iteration: 350/277000 \t Elapsed time: 51.50 \t Loss:26.75620\n",
      "Iteration: 351/277000 \t Elapsed time: 51.65 \t Loss:26.72848\n",
      "Iteration: 352/277000 \t Elapsed time: 51.79 \t Loss:26.70075\n",
      "Iteration: 353/277000 \t Elapsed time: 51.93 \t Loss:26.67324\n",
      "Iteration: 354/277000 \t Elapsed time: 52.07 \t Loss:26.64557\n",
      "Iteration: 355/277000 \t Elapsed time: 52.21 \t Loss:26.61803\n",
      "Iteration: 356/277000 \t Elapsed time: 52.36 \t Loss:26.59060\n",
      "Iteration: 357/277000 \t Elapsed time: 52.50 \t Loss:26.56311\n",
      "Iteration: 358/277000 \t Elapsed time: 52.64 \t Loss:26.53572\n",
      "Iteration: 359/277000 \t Elapsed time: 52.78 \t Loss:26.50845\n",
      "Iteration: 360/277000 \t Elapsed time: 52.92 \t Loss:26.48104\n",
      "Iteration: 361/277000 \t Elapsed time: 53.06 \t Loss:26.45384\n",
      "Iteration: 362/277000 \t Elapsed time: 53.21 \t Loss:26.42657\n",
      "Iteration: 363/277000 \t Elapsed time: 53.56 \t Loss:26.39936\n",
      "Iteration: 364/277000 \t Elapsed time: 53.73 \t Loss:26.37222\n",
      "Iteration: 365/277000 \t Elapsed time: 53.89 \t Loss:26.34510\n",
      "Iteration: 366/277000 \t Elapsed time: 54.04 \t Loss:26.31801\n",
      "Iteration: 367/277000 \t Elapsed time: 54.18 \t Loss:26.29103\n",
      "Iteration: 368/277000 \t Elapsed time: 54.32 \t Loss:26.26397\n",
      "Iteration: 369/277000 \t Elapsed time: 54.45 \t Loss:26.23700\n",
      "Iteration: 370/277000 \t Elapsed time: 54.61 \t Loss:26.21008\n",
      "Iteration: 371/277000 \t Elapsed time: 54.75 \t Loss:26.18317\n",
      "Iteration: 372/277000 \t Elapsed time: 54.89 \t Loss:26.15632\n",
      "Iteration: 373/277000 \t Elapsed time: 55.03 \t Loss:26.12954\n",
      "Iteration: 374/277000 \t Elapsed time: 55.18 \t Loss:26.10278\n",
      "Iteration: 375/277000 \t Elapsed time: 55.32 \t Loss:26.07613\n",
      "Iteration: 376/277000 \t Elapsed time: 55.46 \t Loss:26.04943\n",
      "Iteration: 377/277000 \t Elapsed time: 55.60 \t Loss:26.02279\n",
      "Iteration: 378/277000 \t Elapsed time: 55.75 \t Loss:25.99616\n",
      "Iteration: 379/277000 \t Elapsed time: 55.90 \t Loss:25.96954\n",
      "Iteration: 380/277000 \t Elapsed time: 56.04 \t Loss:25.94307\n",
      "Iteration: 381/277000 \t Elapsed time: 56.18 \t Loss:25.91667\n",
      "Iteration: 382/277000 \t Elapsed time: 56.32 \t Loss:25.89029\n",
      "Iteration: 383/277000 \t Elapsed time: 56.47 \t Loss:25.86377\n",
      "Iteration: 384/277000 \t Elapsed time: 56.61 \t Loss:25.83738\n",
      "Iteration: 385/277000 \t Elapsed time: 56.75 \t Loss:25.81102\n",
      "Iteration: 386/277000 \t Elapsed time: 56.89 \t Loss:25.78477\n",
      "Iteration: 387/277000 \t Elapsed time: 57.03 \t Loss:25.75852\n",
      "Iteration: 388/277000 \t Elapsed time: 57.18 \t Loss:25.73229\n",
      "Iteration: 389/277000 \t Elapsed time: 57.33 \t Loss:25.70618\n",
      "Iteration: 390/277000 \t Elapsed time: 57.50 \t Loss:25.68000\n",
      "Iteration: 391/277000 \t Elapsed time: 57.65 \t Loss:25.65391\n",
      "Iteration: 392/277000 \t Elapsed time: 57.78 \t Loss:25.62785\n",
      "Iteration: 393/277000 \t Elapsed time: 57.92 \t Loss:25.60183\n",
      "Iteration: 394/277000 \t Elapsed time: 58.06 \t Loss:25.57586\n",
      "Iteration: 395/277000 \t Elapsed time: 58.20 \t Loss:25.54997\n",
      "Iteration: 396/277000 \t Elapsed time: 58.34 \t Loss:25.52403\n",
      "Iteration: 397/277000 \t Elapsed time: 58.51 \t Loss:25.49820\n",
      "Iteration: 398/277000 \t Elapsed time: 58.65 \t Loss:25.47233\n",
      "Iteration: 399/277000 \t Elapsed time: 58.79 \t Loss:25.44654\n",
      "Iteration: 400/277000 \t Elapsed time: 58.93 \t Loss:25.42079\n",
      "Iteration: 401/277000 \t Elapsed time: 59.07 \t Loss:25.39511\n",
      "Iteration: 402/277000 \t Elapsed time: 59.22 \t Loss:25.36945\n",
      "Iteration: 403/277000 \t Elapsed time: 59.37 \t Loss:25.34378\n",
      "Iteration: 404/277000 \t Elapsed time: 59.55 \t Loss:25.31818\n",
      "Iteration: 405/277000 \t Elapsed time: 59.70 \t Loss:25.29262\n",
      "Iteration: 406/277000 \t Elapsed time: 59.86 \t Loss:25.26707\n",
      "Iteration: 407/277000 \t Elapsed time: 60.00 \t Loss:25.24157\n",
      "Iteration: 408/277000 \t Elapsed time: 60.20 \t Loss:25.21610\n",
      "Iteration: 409/277000 \t Elapsed time: 60.34 \t Loss:25.19073\n",
      "Iteration: 410/277000 \t Elapsed time: 60.48 \t Loss:25.16533\n",
      "Iteration: 411/277000 \t Elapsed time: 60.63 \t Loss:25.13996\n",
      "Iteration: 412/277000 \t Elapsed time: 60.82 \t Loss:25.11465\n",
      "Iteration: 413/277000 \t Elapsed time: 60.97 \t Loss:25.08941\n",
      "Iteration: 414/277000 \t Elapsed time: 61.11 \t Loss:25.06414\n",
      "Iteration: 415/277000 \t Elapsed time: 61.25 \t Loss:25.03895\n",
      "Iteration: 416/277000 \t Elapsed time: 61.43 \t Loss:25.01378\n",
      "Iteration: 417/277000 \t Elapsed time: 61.61 \t Loss:24.98866\n",
      "Iteration: 418/277000 \t Elapsed time: 61.76 \t Loss:24.96359\n",
      "Iteration: 419/277000 \t Elapsed time: 61.90 \t Loss:24.93854\n",
      "Iteration: 420/277000 \t Elapsed time: 62.04 \t Loss:24.91348\n",
      "Iteration: 421/277000 \t Elapsed time: 62.25 \t Loss:24.88847\n",
      "Iteration: 422/277000 \t Elapsed time: 62.41 \t Loss:24.86357\n",
      "Iteration: 423/277000 \t Elapsed time: 62.55 \t Loss:24.83863\n",
      "Iteration: 424/277000 \t Elapsed time: 62.68 \t Loss:24.81376\n",
      "Iteration: 425/277000 \t Elapsed time: 62.84 \t Loss:24.78894\n",
      "Iteration: 426/277000 \t Elapsed time: 62.98 \t Loss:24.76415\n",
      "Iteration: 427/277000 \t Elapsed time: 63.11 \t Loss:24.73941\n",
      "Iteration: 428/277000 \t Elapsed time: 63.28 \t Loss:24.71469\n",
      "Iteration: 429/277000 \t Elapsed time: 63.41 \t Loss:24.68996\n",
      "Iteration: 430/277000 \t Elapsed time: 63.55 \t Loss:24.66523\n",
      "Iteration: 431/277000 \t Elapsed time: 63.70 \t Loss:24.64053\n",
      "Iteration: 432/277000 \t Elapsed time: 63.86 \t Loss:24.61600\n",
      "Iteration: 433/277000 \t Elapsed time: 63.99 \t Loss:24.59144\n",
      "Iteration: 434/277000 \t Elapsed time: 64.13 \t Loss:24.56686\n",
      "Iteration: 435/277000 \t Elapsed time: 64.27 \t Loss:24.54235\n",
      "Iteration: 436/277000 \t Elapsed time: 64.41 \t Loss:24.51783\n",
      "Iteration: 437/277000 \t Elapsed time: 64.56 \t Loss:24.49345\n",
      "Iteration: 438/277000 \t Elapsed time: 64.71 \t Loss:24.46905\n",
      "Iteration: 439/277000 \t Elapsed time: 64.86 \t Loss:24.44463\n",
      "Iteration: 440/277000 \t Elapsed time: 65.00 \t Loss:24.42031\n",
      "Iteration: 441/277000 \t Elapsed time: 65.14 \t Loss:24.39601\n",
      "Iteration: 442/277000 \t Elapsed time: 65.29 \t Loss:24.37178\n",
      "Iteration: 443/277000 \t Elapsed time: 65.43 \t Loss:24.34752\n",
      "Iteration: 444/277000 \t Elapsed time: 65.58 \t Loss:24.32331\n",
      "Iteration: 445/277000 \t Elapsed time: 65.73 \t Loss:24.29913\n",
      "Iteration: 446/277000 \t Elapsed time: 65.87 \t Loss:24.27504\n",
      "Iteration: 447/277000 \t Elapsed time: 66.02 \t Loss:24.25089\n",
      "Iteration: 448/277000 \t Elapsed time: 66.17 \t Loss:24.22686\n",
      "Iteration: 449/277000 \t Elapsed time: 66.31 \t Loss:24.20279\n",
      "Iteration: 450/277000 \t Elapsed time: 66.46 \t Loss:24.17885\n",
      "Iteration: 451/277000 \t Elapsed time: 66.60 \t Loss:24.15481\n",
      "Iteration: 452/277000 \t Elapsed time: 66.74 \t Loss:24.13093\n",
      "Iteration: 453/277000 \t Elapsed time: 66.88 \t Loss:24.10699\n",
      "Iteration: 454/277000 \t Elapsed time: 67.01 \t Loss:24.08312\n",
      "Iteration: 455/277000 \t Elapsed time: 67.16 \t Loss:24.05926\n",
      "Iteration: 456/277000 \t Elapsed time: 67.36 \t Loss:24.03546\n",
      "Iteration: 457/277000 \t Elapsed time: 67.52 \t Loss:24.01168\n",
      "Iteration: 458/277000 \t Elapsed time: 67.68 \t Loss:23.98795\n",
      "Iteration: 459/277000 \t Elapsed time: 67.83 \t Loss:23.96424\n",
      "Iteration: 460/277000 \t Elapsed time: 67.99 \t Loss:23.94054\n",
      "Iteration: 461/277000 \t Elapsed time: 68.14 \t Loss:23.91695\n",
      "Iteration: 462/277000 \t Elapsed time: 68.28 \t Loss:23.89333\n",
      "Iteration: 463/277000 \t Elapsed time: 68.43 \t Loss:23.86978\n",
      "Iteration: 464/277000 \t Elapsed time: 68.57 \t Loss:23.84629\n",
      "Iteration: 465/277000 \t Elapsed time: 68.72 \t Loss:23.82279\n",
      "Iteration: 466/277000 \t Elapsed time: 68.86 \t Loss:23.79929\n",
      "Iteration: 467/277000 \t Elapsed time: 69.00 \t Loss:23.77578\n",
      "Iteration: 468/277000 \t Elapsed time: 69.15 \t Loss:23.75229\n",
      "Iteration: 469/277000 \t Elapsed time: 69.30 \t Loss:23.72895\n",
      "Iteration: 470/277000 \t Elapsed time: 69.44 \t Loss:23.70558\n",
      "Iteration: 471/277000 \t Elapsed time: 69.58 \t Loss:23.68226\n",
      "Iteration: 472/277000 \t Elapsed time: 69.72 \t Loss:23.65890\n",
      "Iteration: 473/277000 \t Elapsed time: 69.86 \t Loss:23.63562\n",
      "Iteration: 474/277000 \t Elapsed time: 70.00 \t Loss:23.61238\n",
      "Iteration: 475/277000 \t Elapsed time: 70.17 \t Loss:23.58919\n",
      "Iteration: 476/277000 \t Elapsed time: 70.32 \t Loss:23.56601\n",
      "Iteration: 477/277000 \t Elapsed time: 70.47 \t Loss:23.54282\n",
      "Iteration: 478/277000 \t Elapsed time: 70.61 \t Loss:23.51971\n",
      "Iteration: 479/277000 \t Elapsed time: 70.75 \t Loss:23.49668\n",
      "Iteration: 480/277000 \t Elapsed time: 70.93 \t Loss:23.47360\n",
      "Iteration: 481/277000 \t Elapsed time: 71.08 \t Loss:23.45058\n",
      "Iteration: 482/277000 \t Elapsed time: 71.22 \t Loss:23.42756\n",
      "Iteration: 483/277000 \t Elapsed time: 71.36 \t Loss:23.40460\n",
      "Iteration: 484/277000 \t Elapsed time: 71.50 \t Loss:23.38167\n",
      "Iteration: 485/277000 \t Elapsed time: 71.66 \t Loss:23.35876\n",
      "Iteration: 486/277000 \t Elapsed time: 71.80 \t Loss:23.33588\n",
      "Iteration: 487/277000 \t Elapsed time: 71.94 \t Loss:23.31307\n",
      "Iteration: 488/277000 \t Elapsed time: 72.09 \t Loss:23.29021\n",
      "Iteration: 489/277000 \t Elapsed time: 72.23 \t Loss:23.26743\n",
      "Iteration: 490/277000 \t Elapsed time: 72.38 \t Loss:23.24467\n",
      "Iteration: 491/277000 \t Elapsed time: 72.53 \t Loss:23.22192\n",
      "Iteration: 492/277000 \t Elapsed time: 72.68 \t Loss:23.19925\n",
      "Iteration: 493/277000 \t Elapsed time: 72.82 \t Loss:23.17659\n",
      "Iteration: 494/277000 \t Elapsed time: 72.97 \t Loss:23.15395\n",
      "Iteration: 495/277000 \t Elapsed time: 73.18 \t Loss:23.13130\n",
      "Iteration: 496/277000 \t Elapsed time: 73.34 \t Loss:23.10875\n",
      "Iteration: 497/277000 \t Elapsed time: 73.48 \t Loss:23.08621\n",
      "Iteration: 498/277000 \t Elapsed time: 73.62 \t Loss:23.06364\n",
      "Iteration: 499/277000 \t Elapsed time: 73.75 \t Loss:23.04115\n",
      "Iteration: 500/277000 \t Elapsed time: 73.90 \t Loss:23.01868\n",
      "Iteration: 501/277000 \t Elapsed time: 74.04 \t Loss:22.99625\n",
      "Iteration: 502/277000 \t Elapsed time: 74.17 \t Loss:22.97384\n",
      "Iteration: 503/277000 \t Elapsed time: 74.30 \t Loss:22.95144\n",
      "Iteration: 504/277000 \t Elapsed time: 74.46 \t Loss:22.92911\n",
      "Iteration: 505/277000 \t Elapsed time: 74.60 \t Loss:22.90675\n",
      "Iteration: 506/277000 \t Elapsed time: 74.74 \t Loss:22.88447\n",
      "Iteration: 507/277000 \t Elapsed time: 74.89 \t Loss:22.86222\n",
      "Iteration: 508/277000 \t Elapsed time: 75.04 \t Loss:22.83998\n",
      "Iteration: 509/277000 \t Elapsed time: 75.19 \t Loss:22.81775\n",
      "Iteration: 510/277000 \t Elapsed time: 75.34 \t Loss:22.79556\n",
      "Iteration: 511/277000 \t Elapsed time: 75.52 \t Loss:22.77340\n",
      "Iteration: 512/277000 \t Elapsed time: 75.67 \t Loss:22.75131\n",
      "Iteration: 513/277000 \t Elapsed time: 75.81 \t Loss:22.72921\n",
      "Iteration: 514/277000 \t Elapsed time: 75.95 \t Loss:22.70714\n",
      "Iteration: 515/277000 \t Elapsed time: 76.10 \t Loss:22.68507\n",
      "Iteration: 516/277000 \t Elapsed time: 76.24 \t Loss:22.66305\n",
      "Iteration: 517/277000 \t Elapsed time: 76.39 \t Loss:22.64104\n",
      "Iteration: 518/277000 \t Elapsed time: 76.54 \t Loss:22.61909\n",
      "Iteration: 519/277000 \t Elapsed time: 76.69 \t Loss:22.59713\n",
      "Iteration: 520/277000 \t Elapsed time: 76.83 \t Loss:22.57522\n",
      "Iteration: 521/277000 \t Elapsed time: 76.99 \t Loss:22.55333\n",
      "Iteration: 522/277000 \t Elapsed time: 77.13 \t Loss:22.53149\n",
      "Iteration: 523/277000 \t Elapsed time: 77.27 \t Loss:22.50964\n",
      "Iteration: 524/277000 \t Elapsed time: 77.42 \t Loss:22.48786\n",
      "Iteration: 525/277000 \t Elapsed time: 77.56 \t Loss:22.46605\n",
      "Iteration: 526/277000 \t Elapsed time: 77.70 \t Loss:22.44431\n",
      "Iteration: 527/277000 \t Elapsed time: 77.84 \t Loss:22.42258\n",
      "Iteration: 528/277000 \t Elapsed time: 77.97 \t Loss:22.40089\n",
      "Iteration: 529/277000 \t Elapsed time: 78.11 \t Loss:22.37924\n",
      "Iteration: 530/277000 \t Elapsed time: 78.25 \t Loss:22.35769\n",
      "Iteration: 531/277000 \t Elapsed time: 78.39 \t Loss:22.33609\n",
      "Iteration: 532/277000 \t Elapsed time: 78.53 \t Loss:22.31460\n",
      "Iteration: 533/277000 \t Elapsed time: 78.67 \t Loss:22.29313\n",
      "Iteration: 534/277000 \t Elapsed time: 78.83 \t Loss:22.27161\n",
      "Iteration: 535/277000 \t Elapsed time: 78.98 \t Loss:22.25011\n",
      "Iteration: 536/277000 \t Elapsed time: 79.12 \t Loss:22.22863\n",
      "Iteration: 537/277000 \t Elapsed time: 79.26 \t Loss:22.20712\n",
      "Iteration: 538/277000 \t Elapsed time: 79.41 \t Loss:22.18558\n",
      "Iteration: 539/277000 \t Elapsed time: 79.56 \t Loss:22.16404\n",
      "Iteration: 540/277000 \t Elapsed time: 79.70 \t Loss:22.14272\n",
      "Iteration: 541/277000 \t Elapsed time: 79.88 \t Loss:22.12144\n",
      "Iteration: 542/277000 \t Elapsed time: 80.02 \t Loss:22.10011\n",
      "Iteration: 543/277000 \t Elapsed time: 80.20 \t Loss:22.07869\n",
      "Iteration: 544/277000 \t Elapsed time: 80.35 \t Loss:22.05731\n",
      "Iteration: 545/277000 \t Elapsed time: 80.49 \t Loss:22.03608\n",
      "Iteration: 546/277000 \t Elapsed time: 80.63 \t Loss:22.01490\n",
      "Iteration: 547/277000 \t Elapsed time: 80.78 \t Loss:21.99366\n",
      "Iteration: 548/277000 \t Elapsed time: 80.92 \t Loss:21.97244\n",
      "Iteration: 549/277000 \t Elapsed time: 81.07 \t Loss:21.95128\n",
      "Iteration: 550/277000 \t Elapsed time: 81.22 \t Loss:21.93025\n",
      "Iteration: 551/277000 \t Elapsed time: 81.38 \t Loss:21.90922\n",
      "Iteration: 552/277000 \t Elapsed time: 81.52 \t Loss:21.88792\n",
      "Iteration: 553/277000 \t Elapsed time: 81.66 \t Loss:21.86724\n",
      "Iteration: 554/277000 \t Elapsed time: 81.83 \t Loss:21.84632\n",
      "Iteration: 555/277000 \t Elapsed time: 81.98 \t Loss:21.82488\n",
      "Iteration: 556/277000 \t Elapsed time: 82.15 \t Loss:21.80397\n",
      "Iteration: 557/277000 \t Elapsed time: 82.29 \t Loss:21.78361\n",
      "Iteration: 558/277000 \t Elapsed time: 82.44 \t Loss:21.76297\n",
      "Iteration: 559/277000 \t Elapsed time: 82.64 \t Loss:21.74122\n",
      "Iteration: 560/277000 \t Elapsed time: 82.78 \t Loss:21.72039\n",
      "Iteration: 561/277000 \t Elapsed time: 82.92 \t Loss:21.69978\n",
      "Iteration: 562/277000 \t Elapsed time: 83.06 \t Loss:21.67862\n",
      "Iteration: 563/277000 \t Elapsed time: 83.20 \t Loss:21.65777\n",
      "Iteration: 564/277000 \t Elapsed time: 83.34 \t Loss:21.63721\n",
      "Iteration: 565/277000 \t Elapsed time: 83.48 \t Loss:21.61627\n",
      "Iteration: 566/277000 \t Elapsed time: 83.62 \t Loss:21.59545\n",
      "Iteration: 567/277000 \t Elapsed time: 83.76 \t Loss:21.57510\n",
      "Iteration: 568/277000 \t Elapsed time: 84.07 \t Loss:21.55405\n",
      "Iteration: 569/277000 \t Elapsed time: 84.22 \t Loss:21.53329\n",
      "Iteration: 570/277000 \t Elapsed time: 84.36 \t Loss:21.51280\n",
      "Iteration: 571/277000 \t Elapsed time: 84.54 \t Loss:21.49202\n",
      "Iteration: 572/277000 \t Elapsed time: 84.69 \t Loss:21.47142\n",
      "Iteration: 573/277000 \t Elapsed time: 84.83 \t Loss:21.45086\n",
      "Iteration: 574/277000 \t Elapsed time: 84.98 \t Loss:21.43034\n",
      "Iteration: 575/277000 \t Elapsed time: 85.12 \t Loss:21.40972\n",
      "Iteration: 576/277000 \t Elapsed time: 85.27 \t Loss:21.38915\n",
      "Iteration: 577/277000 \t Elapsed time: 85.41 \t Loss:21.36877\n",
      "Iteration: 578/277000 \t Elapsed time: 85.56 \t Loss:21.34822\n",
      "Iteration: 579/277000 \t Elapsed time: 85.71 \t Loss:21.32785\n",
      "Iteration: 580/277000 \t Elapsed time: 85.99 \t Loss:21.30742\n",
      "Iteration: 581/277000 \t Elapsed time: 86.13 \t Loss:21.28691\n",
      "Iteration: 582/277000 \t Elapsed time: 86.27 \t Loss:21.26659\n",
      "Iteration: 583/277000 \t Elapsed time: 86.42 \t Loss:21.24619\n",
      "Iteration: 584/277000 \t Elapsed time: 86.56 \t Loss:21.22585\n",
      "Iteration: 585/277000 \t Elapsed time: 86.70 \t Loss:21.20552\n",
      "Iteration: 586/277000 \t Elapsed time: 86.85 \t Loss:21.18520\n",
      "Iteration: 587/277000 \t Elapsed time: 86.99 \t Loss:21.16497\n",
      "Iteration: 588/277000 \t Elapsed time: 87.14 \t Loss:21.14472\n",
      "Iteration: 589/277000 \t Elapsed time: 87.28 \t Loss:21.12449\n",
      "Iteration: 590/277000 \t Elapsed time: 87.44 \t Loss:21.10433\n",
      "Iteration: 591/277000 \t Elapsed time: 87.59 \t Loss:21.08413\n",
      "Iteration: 592/277000 \t Elapsed time: 87.73 \t Loss:21.06393\n",
      "Iteration: 593/277000 \t Elapsed time: 87.86 \t Loss:21.04387\n",
      "Iteration: 594/277000 \t Elapsed time: 88.00 \t Loss:21.02370\n",
      "Iteration: 595/277000 \t Elapsed time: 88.14 \t Loss:21.00362\n",
      "Iteration: 596/277000 \t Elapsed time: 88.28 \t Loss:20.98352\n",
      "Iteration: 597/277000 \t Elapsed time: 88.42 \t Loss:20.96348\n",
      "Iteration: 598/277000 \t Elapsed time: 88.59 \t Loss:20.94350\n",
      "Iteration: 599/277000 \t Elapsed time: 88.73 \t Loss:20.92343\n",
      "Iteration: 600/277000 \t Elapsed time: 88.87 \t Loss:20.90347\n",
      "Iteration: 601/277000 \t Elapsed time: 89.01 \t Loss:20.88352\n",
      "Iteration: 602/277000 \t Elapsed time: 89.16 \t Loss:20.86355\n",
      "Iteration: 603/277000 \t Elapsed time: 89.30 \t Loss:20.84365\n",
      "Iteration: 604/277000 \t Elapsed time: 89.44 \t Loss:20.82374\n",
      "Iteration: 605/277000 \t Elapsed time: 89.58 \t Loss:20.80384\n",
      "Iteration: 606/277000 \t Elapsed time: 89.74 \t Loss:20.78397\n",
      "Iteration: 607/277000 \t Elapsed time: 89.88 \t Loss:20.76413\n",
      "Iteration: 608/277000 \t Elapsed time: 90.02 \t Loss:20.74431\n",
      "Iteration: 609/277000 \t Elapsed time: 90.17 \t Loss:20.72449\n",
      "Iteration: 610/277000 \t Elapsed time: 90.32 \t Loss:20.70481\n",
      "Iteration: 611/277000 \t Elapsed time: 90.49 \t Loss:20.68501\n",
      "Iteration: 612/277000 \t Elapsed time: 90.63 \t Loss:20.66524\n",
      "Iteration: 613/277000 \t Elapsed time: 90.77 \t Loss:20.64554\n",
      "Iteration: 614/277000 \t Elapsed time: 90.91 \t Loss:20.62586\n",
      "Iteration: 615/277000 \t Elapsed time: 91.05 \t Loss:20.60617\n",
      "Iteration: 616/277000 \t Elapsed time: 91.21 \t Loss:20.58654\n",
      "Iteration: 617/277000 \t Elapsed time: 91.37 \t Loss:20.56690\n",
      "Iteration: 618/277000 \t Elapsed time: 91.51 \t Loss:20.54730\n",
      "Iteration: 619/277000 \t Elapsed time: 91.66 \t Loss:20.52769\n",
      "Iteration: 620/277000 \t Elapsed time: 91.85 \t Loss:20.50812\n",
      "Iteration: 621/277000 \t Elapsed time: 91.99 \t Loss:20.48857\n",
      "Iteration: 622/277000 \t Elapsed time: 92.14 \t Loss:20.46908\n",
      "Iteration: 623/277000 \t Elapsed time: 92.28 \t Loss:20.44957\n",
      "Iteration: 624/277000 \t Elapsed time: 92.43 \t Loss:20.43005\n",
      "Iteration: 625/277000 \t Elapsed time: 92.57 \t Loss:20.41062\n",
      "Iteration: 626/277000 \t Elapsed time: 92.72 \t Loss:20.39115\n",
      "Iteration: 627/277000 \t Elapsed time: 92.85 \t Loss:20.37170\n",
      "Iteration: 628/277000 \t Elapsed time: 92.98 \t Loss:20.35233\n",
      "Iteration: 629/277000 \t Elapsed time: 93.12 \t Loss:20.33291\n",
      "Iteration: 630/277000 \t Elapsed time: 93.26 \t Loss:20.31358\n",
      "Iteration: 631/277000 \t Elapsed time: 93.42 \t Loss:20.29422\n",
      "Iteration: 632/277000 \t Elapsed time: 93.59 \t Loss:20.27491\n",
      "Iteration: 633/277000 \t Elapsed time: 93.73 \t Loss:20.25561\n",
      "Iteration: 634/277000 \t Elapsed time: 93.87 \t Loss:20.23634\n",
      "Iteration: 635/277000 \t Elapsed time: 94.01 \t Loss:20.21706\n",
      "Iteration: 636/277000 \t Elapsed time: 94.17 \t Loss:20.19773\n",
      "Iteration: 637/277000 \t Elapsed time: 94.31 \t Loss:20.17847\n",
      "Iteration: 638/277000 \t Elapsed time: 94.46 \t Loss:20.15921\n",
      "Iteration: 639/277000 \t Elapsed time: 94.60 \t Loss:20.14003\n",
      "Iteration: 640/277000 \t Elapsed time: 94.74 \t Loss:20.12084\n",
      "Iteration: 641/277000 \t Elapsed time: 94.89 \t Loss:20.10169\n",
      "Iteration: 642/277000 \t Elapsed time: 95.04 \t Loss:20.08252\n",
      "Iteration: 643/277000 \t Elapsed time: 95.19 \t Loss:20.06338\n",
      "Iteration: 644/277000 \t Elapsed time: 95.33 \t Loss:20.04428\n",
      "Iteration: 645/277000 \t Elapsed time: 95.47 \t Loss:20.02518\n",
      "Iteration: 646/277000 \t Elapsed time: 95.63 \t Loss:20.00612\n",
      "Iteration: 647/277000 \t Elapsed time: 95.78 \t Loss:19.98709\n",
      "Iteration: 648/277000 \t Elapsed time: 95.94 \t Loss:19.96803\n",
      "Iteration: 649/277000 \t Elapsed time: 96.08 \t Loss:19.94898\n",
      "Iteration: 650/277000 \t Elapsed time: 96.23 \t Loss:19.92997\n",
      "Iteration: 651/277000 \t Elapsed time: 96.38 \t Loss:19.91103\n",
      "Iteration: 652/277000 \t Elapsed time: 96.51 \t Loss:19.89209\n",
      "Iteration: 653/277000 \t Elapsed time: 96.68 \t Loss:19.87316\n",
      "Iteration: 654/277000 \t Elapsed time: 96.83 \t Loss:19.85420\n",
      "Iteration: 655/277000 \t Elapsed time: 96.97 \t Loss:19.83529\n",
      "Iteration: 656/277000 \t Elapsed time: 97.11 \t Loss:19.81641\n",
      "Iteration: 657/277000 \t Elapsed time: 97.25 \t Loss:19.79763\n",
      "Iteration: 658/277000 \t Elapsed time: 97.41 \t Loss:19.77875\n",
      "Iteration: 659/277000 \t Elapsed time: 97.57 \t Loss:19.75992\n",
      "Iteration: 660/277000 \t Elapsed time: 97.70 \t Loss:19.74108\n",
      "Iteration: 661/277000 \t Elapsed time: 97.85 \t Loss:19.72227\n",
      "Iteration: 662/277000 \t Elapsed time: 98.00 \t Loss:19.70350\n",
      "Iteration: 663/277000 \t Elapsed time: 98.14 \t Loss:19.68475\n",
      "Iteration: 664/277000 \t Elapsed time: 98.27 \t Loss:19.66602\n",
      "Iteration: 665/277000 \t Elapsed time: 98.41 \t Loss:19.64729\n",
      "Iteration: 666/277000 \t Elapsed time: 98.55 \t Loss:19.62859\n",
      "Iteration: 667/277000 \t Elapsed time: 98.69 \t Loss:19.60990\n",
      "Iteration: 668/277000 \t Elapsed time: 98.84 \t Loss:19.59128\n",
      "Iteration: 669/277000 \t Elapsed time: 99.00 \t Loss:19.57259\n",
      "Iteration: 670/277000 \t Elapsed time: 99.13 \t Loss:19.55400\n",
      "Iteration: 671/277000 \t Elapsed time: 99.27 \t Loss:19.53541\n",
      "Iteration: 672/277000 \t Elapsed time: 99.43 \t Loss:19.51680\n",
      "Iteration: 673/277000 \t Elapsed time: 99.57 \t Loss:19.49819\n",
      "Iteration: 674/277000 \t Elapsed time: 99.70 \t Loss:19.47964\n",
      "Iteration: 675/277000 \t Elapsed time: 99.84 \t Loss:19.46109\n",
      "Iteration: 676/277000 \t Elapsed time: 99.98 \t Loss:19.44256\n",
      "Iteration: 677/277000 \t Elapsed time: 100.15 \t Loss:19.42405\n",
      "Iteration: 678/277000 \t Elapsed time: 100.30 \t Loss:19.40557\n",
      "Iteration: 679/277000 \t Elapsed time: 100.43 \t Loss:19.38713\n",
      "Iteration: 680/277000 \t Elapsed time: 100.56 \t Loss:19.36865\n",
      "Iteration: 681/277000 \t Elapsed time: 100.72 \t Loss:19.35024\n",
      "Iteration: 682/277000 \t Elapsed time: 100.86 \t Loss:19.33181\n",
      "Iteration: 683/277000 \t Elapsed time: 101.00 \t Loss:19.31347\n",
      "Iteration: 684/277000 \t Elapsed time: 101.14 \t Loss:19.29512\n",
      "Iteration: 685/277000 \t Elapsed time: 101.28 \t Loss:19.27678\n",
      "Iteration: 686/277000 \t Elapsed time: 101.42 \t Loss:19.25841\n",
      "Iteration: 687/277000 \t Elapsed time: 101.56 \t Loss:19.24002\n",
      "Iteration: 688/277000 \t Elapsed time: 101.69 \t Loss:19.22167\n",
      "Iteration: 689/277000 \t Elapsed time: 101.82 \t Loss:19.20342\n",
      "Iteration: 690/277000 \t Elapsed time: 101.97 \t Loss:19.18505\n",
      "Iteration: 691/277000 \t Elapsed time: 102.11 \t Loss:19.16682\n",
      "Iteration: 692/277000 \t Elapsed time: 102.26 \t Loss:19.14858\n",
      "Iteration: 693/277000 \t Elapsed time: 102.41 \t Loss:19.13035\n",
      "Iteration: 694/277000 \t Elapsed time: 102.55 \t Loss:19.11206\n",
      "Iteration: 695/277000 \t Elapsed time: 102.69 \t Loss:19.09389\n",
      "Iteration: 696/277000 \t Elapsed time: 102.83 \t Loss:19.07565\n",
      "Iteration: 697/277000 \t Elapsed time: 102.97 \t Loss:19.05747\n",
      "Iteration: 698/277000 \t Elapsed time: 103.14 \t Loss:19.03939\n",
      "Iteration: 699/277000 \t Elapsed time: 103.27 \t Loss:19.02113\n",
      "Iteration: 700/277000 \t Elapsed time: 103.41 \t Loss:19.00300\n",
      "Iteration: 701/277000 \t Elapsed time: 103.55 \t Loss:18.98492\n",
      "Iteration: 702/277000 \t Elapsed time: 103.70 \t Loss:18.96692\n",
      "Iteration: 703/277000 \t Elapsed time: 103.85 \t Loss:18.94888\n",
      "Iteration: 704/277000 \t Elapsed time: 103.99 \t Loss:18.93073\n",
      "Iteration: 705/277000 \t Elapsed time: 104.13 \t Loss:18.91261\n",
      "Iteration: 706/277000 \t Elapsed time: 104.27 \t Loss:18.89465\n",
      "Iteration: 707/277000 \t Elapsed time: 104.42 \t Loss:18.87672\n",
      "Iteration: 708/277000 \t Elapsed time: 104.57 \t Loss:18.85872\n",
      "Iteration: 709/277000 \t Elapsed time: 104.72 \t Loss:18.84072\n",
      "Iteration: 710/277000 \t Elapsed time: 104.87 \t Loss:18.82271\n",
      "Iteration: 711/277000 \t Elapsed time: 105.01 \t Loss:18.80476\n",
      "Iteration: 712/277000 \t Elapsed time: 105.15 \t Loss:18.78683\n",
      "Iteration: 713/277000 \t Elapsed time: 105.29 \t Loss:18.76890\n",
      "Iteration: 714/277000 \t Elapsed time: 105.44 \t Loss:18.75094\n",
      "Iteration: 715/277000 \t Elapsed time: 105.58 \t Loss:18.73308\n",
      "Iteration: 716/277000 \t Elapsed time: 105.72 \t Loss:18.71523\n",
      "Iteration: 717/277000 \t Elapsed time: 105.87 \t Loss:18.69736\n",
      "Iteration: 718/277000 \t Elapsed time: 106.03 \t Loss:18.67948\n",
      "Iteration: 719/277000 \t Elapsed time: 106.21 \t Loss:18.66163\n",
      "Iteration: 720/277000 \t Elapsed time: 106.39 \t Loss:18.64386\n",
      "Iteration: 721/277000 \t Elapsed time: 106.54 \t Loss:18.62609\n",
      "Iteration: 722/277000 \t Elapsed time: 106.68 \t Loss:18.60831\n",
      "Iteration: 723/277000 \t Elapsed time: 106.82 \t Loss:18.59053\n",
      "Iteration: 724/277000 \t Elapsed time: 106.96 \t Loss:18.57278\n",
      "Iteration: 725/277000 \t Elapsed time: 107.10 \t Loss:18.55503\n",
      "Iteration: 726/277000 \t Elapsed time: 107.24 \t Loss:18.53732\n",
      "Iteration: 727/277000 \t Elapsed time: 107.39 \t Loss:18.51966\n",
      "Iteration: 728/277000 \t Elapsed time: 107.53 \t Loss:18.50213\n",
      "Iteration: 729/277000 \t Elapsed time: 107.68 \t Loss:18.48454\n",
      "Iteration: 730/277000 \t Elapsed time: 107.82 \t Loss:18.46697\n",
      "Iteration: 731/277000 \t Elapsed time: 107.96 \t Loss:18.44934\n",
      "Iteration: 732/277000 \t Elapsed time: 108.09 \t Loss:18.43152\n",
      "Iteration: 733/277000 \t Elapsed time: 108.23 \t Loss:18.41370\n",
      "Iteration: 734/277000 \t Elapsed time: 108.37 \t Loss:18.39611\n",
      "Iteration: 735/277000 \t Elapsed time: 108.51 \t Loss:18.37866\n",
      "Iteration: 736/277000 \t Elapsed time: 108.65 \t Loss:18.36115\n",
      "Iteration: 737/277000 \t Elapsed time: 108.79 \t Loss:18.34345\n",
      "Iteration: 738/277000 \t Elapsed time: 108.94 \t Loss:18.32588\n",
      "Iteration: 739/277000 \t Elapsed time: 109.07 \t Loss:18.30843\n",
      "Iteration: 740/277000 \t Elapsed time: 109.23 \t Loss:18.29107\n",
      "Iteration: 741/277000 \t Elapsed time: 109.37 \t Loss:18.27354\n",
      "Iteration: 742/277000 \t Elapsed time: 109.51 \t Loss:18.25595\n",
      "Iteration: 743/277000 \t Elapsed time: 109.64 \t Loss:18.23839\n",
      "Iteration: 744/277000 \t Elapsed time: 109.78 \t Loss:18.22100\n",
      "Iteration: 745/277000 \t Elapsed time: 109.92 \t Loss:18.20365\n",
      "Iteration: 746/277000 \t Elapsed time: 110.06 \t Loss:18.18643\n",
      "Iteration: 747/277000 \t Elapsed time: 110.21 \t Loss:18.16899\n",
      "Iteration: 748/277000 \t Elapsed time: 110.35 \t Loss:18.15152\n",
      "Iteration: 749/277000 \t Elapsed time: 110.49 \t Loss:18.13393\n",
      "Iteration: 750/277000 \t Elapsed time: 110.65 \t Loss:18.11659\n",
      "Iteration: 751/277000 \t Elapsed time: 110.80 \t Loss:18.09935\n",
      "Iteration: 752/277000 \t Elapsed time: 110.94 \t Loss:18.08207\n",
      "Iteration: 753/277000 \t Elapsed time: 111.08 \t Loss:18.06466\n",
      "Iteration: 754/277000 \t Elapsed time: 111.23 \t Loss:18.04726\n",
      "Iteration: 755/277000 \t Elapsed time: 111.37 \t Loss:18.02995\n",
      "Iteration: 756/277000 \t Elapsed time: 111.51 \t Loss:18.01276\n",
      "Iteration: 757/277000 \t Elapsed time: 111.68 \t Loss:17.99549\n",
      "Iteration: 758/277000 \t Elapsed time: 111.82 \t Loss:17.97813\n",
      "Iteration: 759/277000 \t Elapsed time: 111.96 \t Loss:17.96102\n",
      "Iteration: 760/277000 \t Elapsed time: 112.10 \t Loss:17.94423\n",
      "Iteration: 761/277000 \t Elapsed time: 112.27 \t Loss:17.92704\n",
      "Iteration: 762/277000 \t Elapsed time: 112.48 \t Loss:17.90930\n",
      "Iteration: 763/277000 \t Elapsed time: 112.64 \t Loss:17.89222\n",
      "Iteration: 764/277000 \t Elapsed time: 112.78 \t Loss:17.87551\n",
      "Iteration: 765/277000 \t Elapsed time: 112.91 \t Loss:17.85818\n",
      "Iteration: 766/277000 \t Elapsed time: 113.05 \t Loss:17.84061\n",
      "Iteration: 767/277000 \t Elapsed time: 113.20 \t Loss:17.82365\n",
      "Iteration: 768/277000 \t Elapsed time: 113.34 \t Loss:17.80678\n",
      "Iteration: 769/277000 \t Elapsed time: 113.48 \t Loss:17.78934\n",
      "Iteration: 770/277000 \t Elapsed time: 113.63 \t Loss:17.77221\n",
      "Iteration: 771/277000 \t Elapsed time: 113.76 \t Loss:17.75541\n",
      "Iteration: 772/277000 \t Elapsed time: 113.90 \t Loss:17.73825\n",
      "Iteration: 773/277000 \t Elapsed time: 114.04 \t Loss:17.72099\n",
      "Iteration: 774/277000 \t Elapsed time: 114.19 \t Loss:17.70410\n",
      "Iteration: 775/277000 \t Elapsed time: 114.33 \t Loss:17.68710\n",
      "Iteration: 776/277000 \t Elapsed time: 114.47 \t Loss:17.66998\n",
      "Iteration: 777/277000 \t Elapsed time: 114.62 \t Loss:17.65305\n",
      "Iteration: 778/277000 \t Elapsed time: 114.75 \t Loss:17.63615\n",
      "Iteration: 779/277000 \t Elapsed time: 114.89 \t Loss:17.61915\n",
      "Iteration: 780/277000 \t Elapsed time: 115.04 \t Loss:17.60212\n",
      "Iteration: 781/277000 \t Elapsed time: 115.18 \t Loss:17.58510\n",
      "Iteration: 782/277000 \t Elapsed time: 115.32 \t Loss:17.56827\n",
      "Iteration: 783/277000 \t Elapsed time: 115.46 \t Loss:17.55146\n",
      "Iteration: 784/277000 \t Elapsed time: 115.60 \t Loss:17.53447\n",
      "Iteration: 785/277000 \t Elapsed time: 115.74 \t Loss:17.51750\n",
      "Iteration: 786/277000 \t Elapsed time: 115.88 \t Loss:17.50068\n",
      "Iteration: 787/277000 \t Elapsed time: 116.02 \t Loss:17.48383\n",
      "Iteration: 788/277000 \t Elapsed time: 116.15 \t Loss:17.46692\n",
      "Iteration: 789/277000 \t Elapsed time: 116.29 \t Loss:17.45010\n",
      "Iteration: 790/277000 \t Elapsed time: 116.42 \t Loss:17.43336\n",
      "Iteration: 791/277000 \t Elapsed time: 116.58 \t Loss:17.41646\n",
      "Iteration: 792/277000 \t Elapsed time: 116.72 \t Loss:17.39969\n",
      "Iteration: 793/277000 \t Elapsed time: 116.86 \t Loss:17.38294\n",
      "Iteration: 794/277000 \t Elapsed time: 117.01 \t Loss:17.36612\n",
      "Iteration: 795/277000 \t Elapsed time: 117.16 \t Loss:17.34939\n",
      "Iteration: 796/277000 \t Elapsed time: 117.40 \t Loss:17.33265\n",
      "Iteration: 797/277000 \t Elapsed time: 117.54 \t Loss:17.31587\n",
      "Iteration: 798/277000 \t Elapsed time: 117.69 \t Loss:17.29919\n",
      "Iteration: 799/277000 \t Elapsed time: 117.84 \t Loss:17.28250\n",
      "Iteration: 800/277000 \t Elapsed time: 117.98 \t Loss:17.26576\n",
      "Iteration: 801/277000 \t Elapsed time: 118.13 \t Loss:17.24911\n",
      "Iteration: 802/277000 \t Elapsed time: 118.27 \t Loss:17.23249\n",
      "Iteration: 803/277000 \t Elapsed time: 118.42 \t Loss:17.21573\n",
      "Iteration: 804/277000 \t Elapsed time: 118.57 \t Loss:17.19907\n",
      "Iteration: 805/277000 \t Elapsed time: 118.71 \t Loss:17.18254\n",
      "Iteration: 806/277000 \t Elapsed time: 118.85 \t Loss:17.16582\n",
      "Iteration: 807/277000 \t Elapsed time: 119.00 \t Loss:17.14929\n",
      "Iteration: 808/277000 \t Elapsed time: 119.16 \t Loss:17.13269\n",
      "Iteration: 809/277000 \t Elapsed time: 119.48 \t Loss:17.11604\n",
      "Iteration: 810/277000 \t Elapsed time: 119.63 \t Loss:17.09955\n",
      "Iteration: 811/277000 \t Elapsed time: 119.77 \t Loss:17.08299\n",
      "Iteration: 812/277000 \t Elapsed time: 119.92 \t Loss:17.06637\n",
      "Iteration: 813/277000 \t Elapsed time: 120.06 \t Loss:17.04994\n",
      "Iteration: 814/277000 \t Elapsed time: 120.21 \t Loss:17.03340\n",
      "Iteration: 815/277000 \t Elapsed time: 120.38 \t Loss:17.01695\n",
      "Iteration: 816/277000 \t Elapsed time: 120.56 \t Loss:17.00047\n",
      "Iteration: 817/277000 \t Elapsed time: 120.71 \t Loss:16.98389\n",
      "Iteration: 818/277000 \t Elapsed time: 120.85 \t Loss:16.96750\n",
      "Iteration: 819/277000 \t Elapsed time: 120.99 \t Loss:16.95106\n",
      "Iteration: 820/277000 \t Elapsed time: 121.13 \t Loss:16.93452\n",
      "Iteration: 821/277000 \t Elapsed time: 121.27 \t Loss:16.91812\n",
      "Iteration: 822/277000 \t Elapsed time: 121.41 \t Loss:16.90178\n",
      "Iteration: 823/277000 \t Elapsed time: 121.55 \t Loss:16.88535\n",
      "Iteration: 824/277000 \t Elapsed time: 121.69 \t Loss:16.86894\n",
      "Iteration: 825/277000 \t Elapsed time: 121.84 \t Loss:16.85264\n",
      "Iteration: 826/277000 \t Elapsed time: 121.98 \t Loss:16.83636\n",
      "Iteration: 827/277000 \t Elapsed time: 122.16 \t Loss:16.81993\n",
      "Iteration: 828/277000 \t Elapsed time: 122.30 \t Loss:16.80357\n",
      "Iteration: 829/277000 \t Elapsed time: 122.45 \t Loss:16.78713\n",
      "Iteration: 830/277000 \t Elapsed time: 122.65 \t Loss:16.77070\n",
      "Iteration: 831/277000 \t Elapsed time: 122.81 \t Loss:16.75436\n",
      "Iteration: 832/277000 \t Elapsed time: 122.96 \t Loss:16.73811\n",
      "Iteration: 833/277000 \t Elapsed time: 123.10 \t Loss:16.72186\n",
      "Iteration: 834/277000 \t Elapsed time: 123.25 \t Loss:16.70560\n",
      "Iteration: 835/277000 \t Elapsed time: 123.40 \t Loss:16.68930\n",
      "Iteration: 836/277000 \t Elapsed time: 123.54 \t Loss:16.67303\n",
      "Iteration: 837/277000 \t Elapsed time: 123.67 \t Loss:16.65677\n",
      "Iteration: 838/277000 \t Elapsed time: 123.81 \t Loss:16.64052\n",
      "Iteration: 839/277000 \t Elapsed time: 123.95 \t Loss:16.62436\n",
      "Iteration: 840/277000 \t Elapsed time: 124.10 \t Loss:16.60814\n",
      "Iteration: 841/277000 \t Elapsed time: 124.24 \t Loss:16.59193\n",
      "Iteration: 842/277000 \t Elapsed time: 124.38 \t Loss:16.57573\n",
      "Iteration: 843/277000 \t Elapsed time: 124.51 \t Loss:16.55960\n",
      "Iteration: 844/277000 \t Elapsed time: 124.65 \t Loss:16.54343\n",
      "Iteration: 845/277000 \t Elapsed time: 124.79 \t Loss:16.52729\n",
      "Iteration: 846/277000 \t Elapsed time: 124.94 \t Loss:16.51119\n",
      "Iteration: 847/277000 \t Elapsed time: 125.08 \t Loss:16.49503\n",
      "Iteration: 848/277000 \t Elapsed time: 125.22 \t Loss:16.47894\n",
      "Iteration: 849/277000 \t Elapsed time: 125.36 \t Loss:16.46287\n",
      "Iteration: 850/277000 \t Elapsed time: 125.50 \t Loss:16.44676\n",
      "Iteration: 851/277000 \t Elapsed time: 125.65 \t Loss:16.43069\n",
      "Iteration: 852/277000 \t Elapsed time: 125.82 \t Loss:16.41461\n",
      "Iteration: 853/277000 \t Elapsed time: 125.95 \t Loss:16.39856\n",
      "Iteration: 854/277000 \t Elapsed time: 126.09 \t Loss:16.38252\n",
      "Iteration: 855/277000 \t Elapsed time: 126.24 \t Loss:16.36651\n",
      "Iteration: 856/277000 \t Elapsed time: 126.38 \t Loss:16.35052\n",
      "Iteration: 857/277000 \t Elapsed time: 126.57 \t Loss:16.33448\n",
      "Iteration: 858/277000 \t Elapsed time: 126.72 \t Loss:16.31850\n",
      "Iteration: 859/277000 \t Elapsed time: 126.87 \t Loss:16.30250\n",
      "Iteration: 860/277000 \t Elapsed time: 127.01 \t Loss:16.28655\n",
      "Iteration: 861/277000 \t Elapsed time: 127.15 \t Loss:16.27059\n",
      "Iteration: 862/277000 \t Elapsed time: 127.28 \t Loss:16.25464\n",
      "Iteration: 863/277000 \t Elapsed time: 127.50 \t Loss:16.23868\n",
      "Iteration: 864/277000 \t Elapsed time: 127.65 \t Loss:16.22276\n",
      "Iteration: 865/277000 \t Elapsed time: 127.78 \t Loss:16.20686\n",
      "Iteration: 866/277000 \t Elapsed time: 128.00 \t Loss:16.19095\n",
      "Iteration: 867/277000 \t Elapsed time: 128.15 \t Loss:16.17505\n",
      "Iteration: 868/277000 \t Elapsed time: 128.34 \t Loss:16.15917\n",
      "Iteration: 869/277000 \t Elapsed time: 128.49 \t Loss:16.14333\n",
      "Iteration: 870/277000 \t Elapsed time: 128.63 \t Loss:16.12749\n",
      "Iteration: 871/277000 \t Elapsed time: 128.77 \t Loss:16.11161\n",
      "Iteration: 872/277000 \t Elapsed time: 128.91 \t Loss:16.09576\n",
      "Iteration: 873/277000 \t Elapsed time: 129.05 \t Loss:16.07994\n",
      "Iteration: 874/277000 \t Elapsed time: 129.19 \t Loss:16.06413\n",
      "Iteration: 875/277000 \t Elapsed time: 129.33 \t Loss:16.04832\n",
      "Iteration: 876/277000 \t Elapsed time: 129.47 \t Loss:16.03253\n",
      "Iteration: 877/277000 \t Elapsed time: 129.61 \t Loss:16.01678\n",
      "Iteration: 878/277000 \t Elapsed time: 129.75 \t Loss:16.00100\n",
      "Iteration: 879/277000 \t Elapsed time: 129.91 \t Loss:15.98525\n",
      "Iteration: 880/277000 \t Elapsed time: 130.05 \t Loss:15.96949\n",
      "Iteration: 881/277000 \t Elapsed time: 130.24 \t Loss:15.95376\n",
      "Iteration: 882/277000 \t Elapsed time: 130.38 \t Loss:15.93807\n",
      "Iteration: 883/277000 \t Elapsed time: 130.52 \t Loss:15.92239\n",
      "Iteration: 884/277000 \t Elapsed time: 130.66 \t Loss:15.90667\n",
      "Iteration: 885/277000 \t Elapsed time: 130.80 \t Loss:15.89100\n",
      "Iteration: 886/277000 \t Elapsed time: 130.94 \t Loss:15.87531\n",
      "Iteration: 887/277000 \t Elapsed time: 131.08 \t Loss:15.85966\n",
      "Iteration: 888/277000 \t Elapsed time: 131.22 \t Loss:15.84402\n",
      "Iteration: 889/277000 \t Elapsed time: 131.37 \t Loss:15.82837\n",
      "Iteration: 890/277000 \t Elapsed time: 131.51 \t Loss:15.81277\n",
      "Iteration: 891/277000 \t Elapsed time: 131.68 \t Loss:15.79718\n",
      "Iteration: 892/277000 \t Elapsed time: 131.82 \t Loss:15.78161\n",
      "Iteration: 893/277000 \t Elapsed time: 131.96 \t Loss:15.76602\n",
      "Iteration: 894/277000 \t Elapsed time: 132.11 \t Loss:15.75031\n",
      "Iteration: 895/277000 \t Elapsed time: 132.25 \t Loss:15.73463\n",
      "Iteration: 896/277000 \t Elapsed time: 132.39 \t Loss:15.71904\n",
      "Iteration: 897/277000 \t Elapsed time: 132.60 \t Loss:15.70350\n",
      "Iteration: 898/277000 \t Elapsed time: 132.74 \t Loss:15.68800\n",
      "Iteration: 899/277000 \t Elapsed time: 132.89 \t Loss:15.67240\n",
      "Iteration: 900/277000 \t Elapsed time: 133.04 \t Loss:15.65689\n",
      "Iteration: 901/277000 \t Elapsed time: 133.19 \t Loss:15.64130\n",
      "Iteration: 902/277000 \t Elapsed time: 133.34 \t Loss:15.62583\n",
      "Iteration: 903/277000 \t Elapsed time: 133.48 \t Loss:15.61030\n",
      "Iteration: 904/277000 \t Elapsed time: 133.63 \t Loss:15.59480\n",
      "Iteration: 905/277000 \t Elapsed time: 133.76 \t Loss:15.57931\n",
      "Iteration: 906/277000 \t Elapsed time: 133.90 \t Loss:15.56388\n",
      "Iteration: 907/277000 \t Elapsed time: 134.05 \t Loss:15.54844\n",
      "Iteration: 908/277000 \t Elapsed time: 134.19 \t Loss:15.53302\n",
      "Iteration: 909/277000 \t Elapsed time: 134.33 \t Loss:15.51756\n",
      "Iteration: 910/277000 \t Elapsed time: 134.47 \t Loss:15.50210\n",
      "Iteration: 911/277000 \t Elapsed time: 134.61 \t Loss:15.48663\n",
      "Iteration: 912/277000 \t Elapsed time: 134.75 \t Loss:15.47123\n",
      "Iteration: 913/277000 \t Elapsed time: 134.90 \t Loss:15.45585\n",
      "Iteration: 914/277000 \t Elapsed time: 135.04 \t Loss:15.44045\n",
      "Iteration: 915/277000 \t Elapsed time: 135.18 \t Loss:15.42505\n",
      "Iteration: 916/277000 \t Elapsed time: 135.32 \t Loss:15.40967\n",
      "Iteration: 917/277000 \t Elapsed time: 135.47 \t Loss:15.39438\n",
      "Iteration: 918/277000 \t Elapsed time: 135.61 \t Loss:15.37899\n",
      "Iteration: 919/277000 \t Elapsed time: 135.75 \t Loss:15.36362\n",
      "Iteration: 920/277000 \t Elapsed time: 135.92 \t Loss:15.34831\n",
      "Iteration: 921/277000 \t Elapsed time: 136.06 \t Loss:15.33298\n",
      "Iteration: 922/277000 \t Elapsed time: 136.21 \t Loss:15.31775\n",
      "Iteration: 923/277000 \t Elapsed time: 136.35 \t Loss:15.30242\n",
      "Iteration: 924/277000 \t Elapsed time: 136.49 \t Loss:15.28715\n",
      "Iteration: 925/277000 \t Elapsed time: 136.63 \t Loss:15.27188\n",
      "Iteration: 926/277000 \t Elapsed time: 136.76 \t Loss:15.25664\n",
      "Iteration: 927/277000 \t Elapsed time: 136.91 \t Loss:15.24144\n",
      "Iteration: 928/277000 \t Elapsed time: 137.06 \t Loss:15.22626\n",
      "Iteration: 929/277000 \t Elapsed time: 137.20 \t Loss:15.21112\n",
      "Iteration: 930/277000 \t Elapsed time: 137.34 \t Loss:15.19575\n",
      "Iteration: 931/277000 \t Elapsed time: 137.47 \t Loss:15.18041\n",
      "Iteration: 932/277000 \t Elapsed time: 137.61 \t Loss:15.16518\n",
      "Iteration: 933/277000 \t Elapsed time: 137.75 \t Loss:15.15013\n",
      "Iteration: 934/277000 \t Elapsed time: 137.89 \t Loss:15.13508\n",
      "Iteration: 935/277000 \t Elapsed time: 138.03 \t Loss:15.12000\n",
      "Iteration: 936/277000 \t Elapsed time: 138.18 \t Loss:15.10481\n",
      "Iteration: 937/277000 \t Elapsed time: 138.32 \t Loss:15.08939\n",
      "Iteration: 938/277000 \t Elapsed time: 138.45 \t Loss:15.07419\n",
      "Iteration: 939/277000 \t Elapsed time: 138.60 \t Loss:15.05917\n",
      "Iteration: 940/277000 \t Elapsed time: 138.74 \t Loss:15.04420\n",
      "Iteration: 941/277000 \t Elapsed time: 138.89 \t Loss:15.02918\n",
      "Iteration: 942/277000 \t Elapsed time: 139.03 \t Loss:15.01390\n",
      "Iteration: 943/277000 \t Elapsed time: 139.17 \t Loss:14.99891\n",
      "Iteration: 944/277000 \t Elapsed time: 139.31 \t Loss:14.98397\n",
      "Iteration: 945/277000 \t Elapsed time: 139.46 \t Loss:14.96910\n",
      "Iteration: 946/277000 \t Elapsed time: 139.62 \t Loss:14.95390\n",
      "Iteration: 947/277000 \t Elapsed time: 139.76 \t Loss:14.93844\n",
      "Iteration: 948/277000 \t Elapsed time: 139.91 \t Loss:14.92341\n",
      "Iteration: 949/277000 \t Elapsed time: 140.05 \t Loss:14.90868\n",
      "Iteration: 950/277000 \t Elapsed time: 140.20 \t Loss:14.89371\n",
      "Iteration: 951/277000 \t Elapsed time: 140.34 \t Loss:14.87844\n",
      "Iteration: 952/277000 \t Elapsed time: 140.52 \t Loss:14.86323\n",
      "Iteration: 953/277000 \t Elapsed time: 140.66 \t Loss:14.84851\n",
      "Iteration: 954/277000 \t Elapsed time: 140.88 \t Loss:14.83355\n",
      "Iteration: 955/277000 \t Elapsed time: 141.04 \t Loss:14.81822\n",
      "Iteration: 956/277000 \t Elapsed time: 141.18 \t Loss:14.80338\n",
      "Iteration: 957/277000 \t Elapsed time: 141.33 \t Loss:14.78863\n",
      "Iteration: 958/277000 \t Elapsed time: 141.47 \t Loss:14.77330\n",
      "Iteration: 959/277000 \t Elapsed time: 141.61 \t Loss:14.75844\n",
      "Iteration: 960/277000 \t Elapsed time: 141.75 \t Loss:14.74358\n",
      "Iteration: 961/277000 \t Elapsed time: 141.89 \t Loss:14.72854\n",
      "Iteration: 962/277000 \t Elapsed time: 142.04 \t Loss:14.71357\n",
      "Iteration: 963/277000 \t Elapsed time: 142.18 \t Loss:14.69867\n",
      "Iteration: 964/277000 \t Elapsed time: 142.40 \t Loss:14.68372\n",
      "Iteration: 965/277000 \t Elapsed time: 142.57 \t Loss:14.66897\n",
      "Iteration: 966/277000 \t Elapsed time: 142.76 \t Loss:14.65408\n",
      "Iteration: 967/277000 \t Elapsed time: 142.93 \t Loss:14.63918\n",
      "Iteration: 968/277000 \t Elapsed time: 143.07 \t Loss:14.62427\n",
      "Iteration: 969/277000 \t Elapsed time: 143.21 \t Loss:14.60948\n",
      "Iteration: 970/277000 \t Elapsed time: 143.35 \t Loss:14.59457\n",
      "Iteration: 971/277000 \t Elapsed time: 143.54 \t Loss:14.57978\n",
      "Iteration: 972/277000 \t Elapsed time: 143.67 \t Loss:14.56491\n",
      "Iteration: 973/277000 \t Elapsed time: 143.83 \t Loss:14.55012\n",
      "Iteration: 974/277000 \t Elapsed time: 143.98 \t Loss:14.53532\n",
      "Iteration: 975/277000 \t Elapsed time: 144.11 \t Loss:14.52053\n",
      "Iteration: 976/277000 \t Elapsed time: 144.27 \t Loss:14.50570\n",
      "Iteration: 977/277000 \t Elapsed time: 144.42 \t Loss:14.49098\n",
      "Iteration: 978/277000 \t Elapsed time: 144.56 \t Loss:14.47618\n",
      "Iteration: 979/277000 \t Elapsed time: 144.70 \t Loss:14.46143\n",
      "Iteration: 980/277000 \t Elapsed time: 144.84 \t Loss:14.44670\n",
      "Iteration: 981/277000 \t Elapsed time: 144.99 \t Loss:14.43190\n",
      "Iteration: 982/277000 \t Elapsed time: 145.13 \t Loss:14.41717\n",
      "Iteration: 983/277000 \t Elapsed time: 145.27 \t Loss:14.40245\n",
      "Iteration: 984/277000 \t Elapsed time: 145.41 \t Loss:14.38779\n",
      "Iteration: 985/277000 \t Elapsed time: 145.56 \t Loss:14.37309\n",
      "Iteration: 986/277000 \t Elapsed time: 145.70 \t Loss:14.35837\n",
      "Iteration: 987/277000 \t Elapsed time: 145.84 \t Loss:14.34365\n",
      "Iteration: 988/277000 \t Elapsed time: 145.99 \t Loss:14.32902\n",
      "Iteration: 989/277000 \t Elapsed time: 146.17 \t Loss:14.31437\n",
      "Iteration: 990/277000 \t Elapsed time: 146.32 \t Loss:14.29972\n",
      "Iteration: 991/277000 \t Elapsed time: 146.45 \t Loss:14.28508\n",
      "Iteration: 992/277000 \t Elapsed time: 146.59 \t Loss:14.27043\n",
      "Iteration: 993/277000 \t Elapsed time: 146.73 \t Loss:14.25579\n",
      "Iteration: 994/277000 \t Elapsed time: 146.88 \t Loss:14.24113\n",
      "Iteration: 995/277000 \t Elapsed time: 147.02 \t Loss:14.22652\n",
      "Iteration: 996/277000 \t Elapsed time: 147.19 \t Loss:14.21192\n",
      "Iteration: 997/277000 \t Elapsed time: 147.36 \t Loss:14.19733\n",
      "Iteration: 998/277000 \t Elapsed time: 147.50 \t Loss:14.18279\n",
      "Iteration: 999/277000 \t Elapsed time: 147.66 \t Loss:14.16829\n",
      "Iteration: 1000/277000 \t Elapsed time: 147.79 \t Loss:14.15387\n",
      "Start Validating\n",
      "Finish Validating\n",
      "Iteration: 1001/277000 \t Elapsed time: 170.11 \t Loss:14.13943\n",
      "Iteration: 1002/277000 \t Elapsed time: 170.26 \t Loss:14.12489\n",
      "Iteration: 1003/277000 \t Elapsed time: 170.40 \t Loss:14.11022\n",
      "Iteration: 1004/277000 \t Elapsed time: 170.54 \t Loss:14.09548\n",
      "Iteration: 1005/277000 \t Elapsed time: 170.68 \t Loss:14.08094\n",
      "Iteration: 1006/277000 \t Elapsed time: 170.83 \t Loss:14.06645\n",
      "Iteration: 1007/277000 \t Elapsed time: 170.97 \t Loss:14.05211\n",
      "Iteration: 1008/277000 \t Elapsed time: 171.11 \t Loss:14.03759\n",
      "Iteration: 1009/277000 \t Elapsed time: 171.25 \t Loss:14.02306\n",
      "Iteration: 1010/277000 \t Elapsed time: 171.40 \t Loss:14.00844\n",
      "Iteration: 1011/277000 \t Elapsed time: 171.56 \t Loss:13.99398\n",
      "Iteration: 1012/277000 \t Elapsed time: 171.70 \t Loss:13.97962\n",
      "Iteration: 1013/277000 \t Elapsed time: 171.84 \t Loss:13.96516\n",
      "Iteration: 1014/277000 \t Elapsed time: 171.99 \t Loss:13.95063\n",
      "Iteration: 1015/277000 \t Elapsed time: 172.13 \t Loss:13.93616\n",
      "Iteration: 1016/277000 \t Elapsed time: 172.27 \t Loss:13.92175\n",
      "Iteration: 1017/277000 \t Elapsed time: 172.41 \t Loss:13.90743\n",
      "Iteration: 1018/277000 \t Elapsed time: 172.54 \t Loss:13.89306\n",
      "Iteration: 1019/277000 \t Elapsed time: 172.70 \t Loss:13.87858\n",
      "Iteration: 1020/277000 \t Elapsed time: 172.84 \t Loss:13.86422\n",
      "Iteration: 1021/277000 \t Elapsed time: 172.98 \t Loss:13.84982\n",
      "Iteration: 1022/277000 \t Elapsed time: 173.11 \t Loss:13.83549\n",
      "Iteration: 1023/277000 \t Elapsed time: 173.25 \t Loss:13.82107\n",
      "Iteration: 1024/277000 \t Elapsed time: 173.39 \t Loss:13.80669\n",
      "Iteration: 1025/277000 \t Elapsed time: 173.54 \t Loss:13.79234\n",
      "Iteration: 1026/277000 \t Elapsed time: 173.67 \t Loss:13.77795\n",
      "Iteration: 1027/277000 \t Elapsed time: 173.80 \t Loss:13.76365\n",
      "Iteration: 1028/277000 \t Elapsed time: 173.94 \t Loss:13.74931\n",
      "Iteration: 1029/277000 \t Elapsed time: 174.08 \t Loss:13.73501\n",
      "Iteration: 1030/277000 \t Elapsed time: 174.21 \t Loss:13.72068\n",
      "Iteration: 1031/277000 \t Elapsed time: 174.36 \t Loss:13.70640\n",
      "Iteration: 1032/277000 \t Elapsed time: 174.49 \t Loss:13.69210\n",
      "Iteration: 1033/277000 \t Elapsed time: 174.64 \t Loss:13.67779\n",
      "Iteration: 1034/277000 \t Elapsed time: 174.80 \t Loss:13.66352\n",
      "Iteration: 1035/277000 \t Elapsed time: 174.94 \t Loss:13.64923\n",
      "Iteration: 1036/277000 \t Elapsed time: 175.08 \t Loss:13.63500\n",
      "Iteration: 1037/277000 \t Elapsed time: 175.22 \t Loss:13.62074\n",
      "Iteration: 1038/277000 \t Elapsed time: 175.37 \t Loss:13.60650\n",
      "Iteration: 1039/277000 \t Elapsed time: 175.52 \t Loss:13.59226\n",
      "Iteration: 1040/277000 \t Elapsed time: 175.66 \t Loss:13.57807\n",
      "Iteration: 1041/277000 \t Elapsed time: 175.81 \t Loss:13.56383\n",
      "Iteration: 1042/277000 \t Elapsed time: 175.95 \t Loss:13.54967\n",
      "Iteration: 1043/277000 \t Elapsed time: 176.09 \t Loss:13.53550\n",
      "Iteration: 1044/277000 \t Elapsed time: 176.23 \t Loss:13.52126\n",
      "Iteration: 1045/277000 \t Elapsed time: 176.37 \t Loss:13.50707\n",
      "Iteration: 1046/277000 \t Elapsed time: 176.51 \t Loss:13.49289\n",
      "Iteration: 1047/277000 \t Elapsed time: 176.65 \t Loss:13.47874\n",
      "Iteration: 1048/277000 \t Elapsed time: 176.79 \t Loss:13.46468\n",
      "Iteration: 1049/277000 \t Elapsed time: 176.94 \t Loss:13.45041\n",
      "Iteration: 1050/277000 \t Elapsed time: 177.10 \t Loss:13.43629\n",
      "Iteration: 1051/277000 \t Elapsed time: 177.25 \t Loss:13.42216\n",
      "Iteration: 1052/277000 \t Elapsed time: 177.41 \t Loss:13.40802\n",
      "Iteration: 1053/277000 \t Elapsed time: 177.62 \t Loss:13.39393\n",
      "Iteration: 1054/277000 \t Elapsed time: 177.77 \t Loss:13.37980\n",
      "Iteration: 1055/277000 \t Elapsed time: 177.93 \t Loss:13.36576\n",
      "Iteration: 1056/277000 \t Elapsed time: 178.07 \t Loss:13.35164\n",
      "Iteration: 1057/277000 \t Elapsed time: 178.20 \t Loss:13.33754\n",
      "Iteration: 1058/277000 \t Elapsed time: 178.34 \t Loss:13.32347\n",
      "Iteration: 1059/277000 \t Elapsed time: 178.52 \t Loss:13.30940\n",
      "Iteration: 1060/277000 \t Elapsed time: 178.66 \t Loss:13.29533\n",
      "Iteration: 1061/277000 \t Elapsed time: 178.81 \t Loss:13.28130\n",
      "Iteration: 1062/277000 \t Elapsed time: 178.95 \t Loss:13.26726\n",
      "Iteration: 1063/277000 \t Elapsed time: 179.11 \t Loss:13.25322\n",
      "Iteration: 1064/277000 \t Elapsed time: 179.25 \t Loss:13.23924\n",
      "Iteration: 1065/277000 \t Elapsed time: 179.38 \t Loss:13.22523\n",
      "Iteration: 1066/277000 \t Elapsed time: 179.52 \t Loss:13.21127\n",
      "Iteration: 1067/277000 \t Elapsed time: 179.66 \t Loss:13.19732\n",
      "Iteration: 1068/277000 \t Elapsed time: 179.81 \t Loss:13.18354\n",
      "Iteration: 1069/277000 \t Elapsed time: 179.95 \t Loss:13.16969\n",
      "Iteration: 1070/277000 \t Elapsed time: 180.10 \t Loss:13.15579\n",
      "Iteration: 1071/277000 \t Elapsed time: 180.24 \t Loss:13.14171\n",
      "Iteration: 1072/277000 \t Elapsed time: 180.38 \t Loss:13.12747\n",
      "Iteration: 1073/277000 \t Elapsed time: 180.52 \t Loss:13.11332\n",
      "Iteration: 1074/277000 \t Elapsed time: 180.72 \t Loss:13.09947\n",
      "Iteration: 1075/277000 \t Elapsed time: 180.86 \t Loss:13.08568\n",
      "Iteration: 1076/277000 \t Elapsed time: 181.00 \t Loss:13.07179\n",
      "Iteration: 1077/277000 \t Elapsed time: 181.14 \t Loss:13.05771\n",
      "Iteration: 1078/277000 \t Elapsed time: 181.28 \t Loss:13.04370\n",
      "Iteration: 1079/277000 \t Elapsed time: 181.43 \t Loss:13.02984\n",
      "Iteration: 1080/277000 \t Elapsed time: 181.58 \t Loss:13.01603\n",
      "Iteration: 1081/277000 \t Elapsed time: 181.72 \t Loss:13.00213\n",
      "Iteration: 1082/277000 \t Elapsed time: 181.87 \t Loss:12.98812\n",
      "Iteration: 1083/277000 \t Elapsed time: 182.01 \t Loss:12.97425\n",
      "Iteration: 1084/277000 \t Elapsed time: 182.15 \t Loss:12.96044\n",
      "Iteration: 1085/277000 \t Elapsed time: 182.29 \t Loss:12.94659\n",
      "Iteration: 1086/277000 \t Elapsed time: 182.44 \t Loss:12.93267\n",
      "Iteration: 1087/277000 \t Elapsed time: 182.64 \t Loss:12.91881\n",
      "Iteration: 1088/277000 \t Elapsed time: 182.78 \t Loss:12.90502\n",
      "Iteration: 1089/277000 \t Elapsed time: 182.92 \t Loss:12.89121\n",
      "Iteration: 1090/277000 \t Elapsed time: 183.06 \t Loss:12.87747\n",
      "Iteration: 1091/277000 \t Elapsed time: 183.20 \t Loss:12.86358\n",
      "Iteration: 1092/277000 \t Elapsed time: 183.34 \t Loss:12.84979\n",
      "Iteration: 1093/277000 \t Elapsed time: 183.48 \t Loss:12.83598\n",
      "Iteration: 1094/277000 \t Elapsed time: 183.62 \t Loss:12.82226\n",
      "Iteration: 1095/277000 \t Elapsed time: 183.77 \t Loss:12.80840\n",
      "Iteration: 1096/277000 \t Elapsed time: 183.90 \t Loss:12.79465\n",
      "Iteration: 1097/277000 \t Elapsed time: 184.04 \t Loss:12.78088\n",
      "Iteration: 1098/277000 \t Elapsed time: 184.19 \t Loss:12.76714\n",
      "Iteration: 1099/277000 \t Elapsed time: 184.33 \t Loss:12.75336\n",
      "Iteration: 1100/277000 \t Elapsed time: 184.47 \t Loss:12.73965\n",
      "Iteration: 1101/277000 \t Elapsed time: 184.61 \t Loss:12.72590\n",
      "Iteration: 1102/277000 \t Elapsed time: 184.75 \t Loss:12.71219\n",
      "Iteration: 1103/277000 \t Elapsed time: 184.96 \t Loss:12.69849\n",
      "Iteration: 1104/277000 \t Elapsed time: 185.10 \t Loss:12.68476\n",
      "Iteration: 1105/277000 \t Elapsed time: 185.24 \t Loss:12.67108\n",
      "Iteration: 1106/277000 \t Elapsed time: 185.39 \t Loss:12.65734\n",
      "Iteration: 1107/277000 \t Elapsed time: 185.53 \t Loss:12.64370\n",
      "Iteration: 1108/277000 \t Elapsed time: 185.69 \t Loss:12.63004\n",
      "Iteration: 1109/277000 \t Elapsed time: 185.83 \t Loss:12.61631\n",
      "Iteration: 1110/277000 \t Elapsed time: 185.98 \t Loss:12.60272\n",
      "Iteration: 1111/277000 \t Elapsed time: 186.12 \t Loss:12.58903\n",
      "Iteration: 1112/277000 \t Elapsed time: 186.26 \t Loss:12.57533\n",
      "Iteration: 1113/277000 \t Elapsed time: 186.40 \t Loss:12.56167\n",
      "Iteration: 1114/277000 \t Elapsed time: 186.56 \t Loss:12.54805\n",
      "Iteration: 1115/277000 \t Elapsed time: 186.70 \t Loss:12.53443\n",
      "Iteration: 1116/277000 \t Elapsed time: 186.86 \t Loss:12.52079\n",
      "Iteration: 1117/277000 \t Elapsed time: 187.03 \t Loss:12.50721\n",
      "Iteration: 1118/277000 \t Elapsed time: 187.17 \t Loss:12.49365\n",
      "Iteration: 1119/277000 \t Elapsed time: 187.40 \t Loss:12.48001\n",
      "Iteration: 1120/277000 \t Elapsed time: 187.63 \t Loss:12.46639\n",
      "Iteration: 1121/277000 \t Elapsed time: 187.77 \t Loss:12.45280\n",
      "Iteration: 1122/277000 \t Elapsed time: 187.92 \t Loss:12.43923\n",
      "Iteration: 1123/277000 \t Elapsed time: 188.07 \t Loss:12.42568\n",
      "Iteration: 1124/277000 \t Elapsed time: 188.20 \t Loss:12.41210\n",
      "Iteration: 1125/277000 \t Elapsed time: 188.34 \t Loss:12.39864\n",
      "Iteration: 1126/277000 \t Elapsed time: 188.48 \t Loss:12.38513\n",
      "Iteration: 1127/277000 \t Elapsed time: 188.61 \t Loss:12.37148\n",
      "Iteration: 1128/277000 \t Elapsed time: 188.76 \t Loss:12.35798\n",
      "Iteration: 1129/277000 \t Elapsed time: 188.90 \t Loss:12.34445\n",
      "Iteration: 1130/277000 \t Elapsed time: 189.06 \t Loss:12.33106\n",
      "Iteration: 1131/277000 \t Elapsed time: 189.21 \t Loss:12.31743\n",
      "Iteration: 1132/277000 \t Elapsed time: 189.35 \t Loss:12.30391\n",
      "Iteration: 1133/277000 \t Elapsed time: 189.48 \t Loss:12.29042\n",
      "Iteration: 1134/277000 \t Elapsed time: 189.62 \t Loss:12.27695\n",
      "Iteration: 1135/277000 \t Elapsed time: 189.76 \t Loss:12.26341\n",
      "Iteration: 1136/277000 \t Elapsed time: 189.91 \t Loss:12.24994\n",
      "Iteration: 1137/277000 \t Elapsed time: 190.06 \t Loss:12.23647\n",
      "Iteration: 1138/277000 \t Elapsed time: 190.20 \t Loss:12.22303\n",
      "Iteration: 1139/277000 \t Elapsed time: 190.34 \t Loss:12.20965\n",
      "Iteration: 1140/277000 \t Elapsed time: 190.49 \t Loss:12.19613\n",
      "Iteration: 1141/277000 \t Elapsed time: 190.63 \t Loss:12.18268\n",
      "Iteration: 1142/277000 \t Elapsed time: 190.76 \t Loss:12.16925\n",
      "Iteration: 1143/277000 \t Elapsed time: 190.90 \t Loss:12.15589\n",
      "Iteration: 1144/277000 \t Elapsed time: 191.04 \t Loss:12.14249\n",
      "Iteration: 1145/277000 \t Elapsed time: 191.18 \t Loss:12.12912\n",
      "Iteration: 1146/277000 \t Elapsed time: 191.32 \t Loss:12.11583\n",
      "Iteration: 1147/277000 \t Elapsed time: 191.46 \t Loss:12.10243\n",
      "Iteration: 1148/277000 \t Elapsed time: 191.61 \t Loss:12.08916\n",
      "Iteration: 1149/277000 \t Elapsed time: 191.75 \t Loss:12.07575\n",
      "Iteration: 1150/277000 \t Elapsed time: 191.89 \t Loss:12.06238\n",
      "Iteration: 1151/277000 \t Elapsed time: 192.03 \t Loss:12.04886\n",
      "Iteration: 1152/277000 \t Elapsed time: 192.17 \t Loss:12.03558\n",
      "Iteration: 1153/277000 \t Elapsed time: 192.31 \t Loss:12.02213\n",
      "Iteration: 1154/277000 \t Elapsed time: 192.46 \t Loss:12.00885\n",
      "Iteration: 1155/277000 \t Elapsed time: 192.60 \t Loss:11.99546\n",
      "Iteration: 1156/277000 \t Elapsed time: 192.79 \t Loss:11.98219\n",
      "Iteration: 1157/277000 \t Elapsed time: 192.93 \t Loss:11.96894\n",
      "Iteration: 1158/277000 \t Elapsed time: 193.08 \t Loss:11.95560\n",
      "Iteration: 1159/277000 \t Elapsed time: 193.22 \t Loss:11.94226\n",
      "Iteration: 1160/277000 \t Elapsed time: 193.36 \t Loss:11.92900\n",
      "Iteration: 1161/277000 \t Elapsed time: 193.52 \t Loss:11.91563\n",
      "Iteration: 1162/277000 \t Elapsed time: 193.65 \t Loss:11.90255\n",
      "Iteration: 1163/277000 \t Elapsed time: 193.79 \t Loss:11.88916\n",
      "Iteration: 1164/277000 \t Elapsed time: 193.92 \t Loss:11.87594\n",
      "Iteration: 1165/277000 \t Elapsed time: 194.07 \t Loss:11.86251\n",
      "Iteration: 1166/277000 \t Elapsed time: 194.21 \t Loss:11.84928\n",
      "Iteration: 1167/277000 \t Elapsed time: 194.35 \t Loss:11.83610\n",
      "Iteration: 1168/277000 \t Elapsed time: 194.49 \t Loss:11.82284\n",
      "Iteration: 1169/277000 \t Elapsed time: 194.63 \t Loss:11.80984\n",
      "Iteration: 1170/277000 \t Elapsed time: 194.77 \t Loss:11.79645\n",
      "Iteration: 1171/277000 \t Elapsed time: 194.92 \t Loss:11.78335\n",
      "Iteration: 1172/277000 \t Elapsed time: 195.06 \t Loss:11.76999\n",
      "Iteration: 1173/277000 \t Elapsed time: 195.20 \t Loss:11.75669\n",
      "Iteration: 1174/277000 \t Elapsed time: 195.34 \t Loss:11.74363\n",
      "Iteration: 1175/277000 \t Elapsed time: 195.48 \t Loss:11.73046\n",
      "Iteration: 1176/277000 \t Elapsed time: 195.62 \t Loss:11.71743\n",
      "Iteration: 1177/277000 \t Elapsed time: 195.76 \t Loss:11.70435\n",
      "Iteration: 1178/277000 \t Elapsed time: 195.91 \t Loss:11.69096\n",
      "Iteration: 1179/277000 \t Elapsed time: 196.06 \t Loss:11.67768\n",
      "Iteration: 1180/277000 \t Elapsed time: 196.21 \t Loss:11.66458\n",
      "Iteration: 1181/277000 \t Elapsed time: 196.36 \t Loss:11.65160\n",
      "Iteration: 1182/277000 \t Elapsed time: 196.50 \t Loss:11.63839\n",
      "Iteration: 1183/277000 \t Elapsed time: 196.64 \t Loss:11.62517\n",
      "Iteration: 1184/277000 \t Elapsed time: 196.78 \t Loss:11.61209\n",
      "Iteration: 1185/277000 \t Elapsed time: 196.93 \t Loss:11.59890\n",
      "Iteration: 1186/277000 \t Elapsed time: 197.07 \t Loss:11.58592\n",
      "Iteration: 1187/277000 \t Elapsed time: 197.21 \t Loss:11.57272\n",
      "Iteration: 1188/277000 \t Elapsed time: 197.36 \t Loss:11.55968\n",
      "Iteration: 1189/277000 \t Elapsed time: 197.50 \t Loss:11.54660\n",
      "Iteration: 1190/277000 \t Elapsed time: 197.64 \t Loss:11.53347\n",
      "Iteration: 1191/277000 \t Elapsed time: 197.78 \t Loss:11.52052\n",
      "Iteration: 1192/277000 \t Elapsed time: 197.92 \t Loss:11.50749\n",
      "Iteration: 1193/277000 \t Elapsed time: 198.07 \t Loss:11.49444\n",
      "Iteration: 1194/277000 \t Elapsed time: 198.21 \t Loss:11.48137\n",
      "Iteration: 1195/277000 \t Elapsed time: 198.35 \t Loss:11.46824\n",
      "Iteration: 1196/277000 \t Elapsed time: 198.48 \t Loss:11.45529\n",
      "Iteration: 1197/277000 \t Elapsed time: 198.61 \t Loss:11.44249\n",
      "Iteration: 1198/277000 \t Elapsed time: 198.78 \t Loss:11.42963\n",
      "Iteration: 1199/277000 \t Elapsed time: 198.92 \t Loss:11.41637\n",
      "Iteration: 1200/277000 \t Elapsed time: 199.07 \t Loss:11.40316\n",
      "Iteration: 1201/277000 \t Elapsed time: 199.21 \t Loss:11.39028\n",
      "Iteration: 1202/277000 \t Elapsed time: 199.35 \t Loss:11.37748\n",
      "Iteration: 1203/277000 \t Elapsed time: 199.49 \t Loss:11.36434\n",
      "Iteration: 1204/277000 \t Elapsed time: 199.63 \t Loss:11.35116\n",
      "Iteration: 1205/277000 \t Elapsed time: 199.77 \t Loss:11.33833\n",
      "Iteration: 1206/277000 \t Elapsed time: 199.92 \t Loss:11.32553\n",
      "Iteration: 1207/277000 \t Elapsed time: 200.06 \t Loss:11.31238\n",
      "Iteration: 1208/277000 \t Elapsed time: 200.20 \t Loss:11.29923\n",
      "Iteration: 1209/277000 \t Elapsed time: 200.34 \t Loss:11.28635\n",
      "Iteration: 1210/277000 \t Elapsed time: 200.48 \t Loss:11.27356\n",
      "Iteration: 1211/277000 \t Elapsed time: 200.63 \t Loss:11.26058\n",
      "Iteration: 1212/277000 \t Elapsed time: 200.81 \t Loss:11.24762\n",
      "Iteration: 1213/277000 \t Elapsed time: 200.96 \t Loss:11.23479\n",
      "Iteration: 1214/277000 \t Elapsed time: 201.11 \t Loss:11.22195\n",
      "Iteration: 1215/277000 \t Elapsed time: 201.26 \t Loss:11.20907\n",
      "Iteration: 1216/277000 \t Elapsed time: 201.41 \t Loss:11.19615\n",
      "Iteration: 1217/277000 \t Elapsed time: 201.56 \t Loss:11.18324\n",
      "Iteration: 1218/277000 \t Elapsed time: 201.73 \t Loss:11.17056\n",
      "Iteration: 1219/277000 \t Elapsed time: 201.88 \t Loss:11.15759\n",
      "Iteration: 1220/277000 \t Elapsed time: 202.02 \t Loss:11.14449\n",
      "Iteration: 1221/277000 \t Elapsed time: 202.16 \t Loss:11.13153\n",
      "Iteration: 1222/277000 \t Elapsed time: 202.31 \t Loss:11.11871\n",
      "Iteration: 1223/277000 \t Elapsed time: 202.46 \t Loss:11.10593\n",
      "Iteration: 1224/277000 \t Elapsed time: 202.68 \t Loss:11.09306\n",
      "Iteration: 1225/277000 \t Elapsed time: 202.90 \t Loss:11.08007\n",
      "Iteration: 1226/277000 \t Elapsed time: 203.03 \t Loss:11.06726\n",
      "Iteration: 1227/277000 \t Elapsed time: 203.18 \t Loss:11.05445\n",
      "Iteration: 1228/277000 \t Elapsed time: 203.32 \t Loss:11.04159\n",
      "Iteration: 1229/277000 \t Elapsed time: 203.46 \t Loss:11.02887\n",
      "Iteration: 1230/277000 \t Elapsed time: 203.60 \t Loss:11.01604\n",
      "Iteration: 1231/277000 \t Elapsed time: 203.74 \t Loss:11.00330\n",
      "Iteration: 1232/277000 \t Elapsed time: 203.88 \t Loss:10.99037\n",
      "Iteration: 1233/277000 \t Elapsed time: 204.02 \t Loss:10.97750\n",
      "Iteration: 1234/277000 \t Elapsed time: 204.17 \t Loss:10.96482\n",
      "Iteration: 1235/277000 \t Elapsed time: 204.31 \t Loss:10.95203\n",
      "Iteration: 1236/277000 \t Elapsed time: 204.50 \t Loss:10.93929\n",
      "Iteration: 1237/277000 \t Elapsed time: 204.64 \t Loss:10.92645\n",
      "Iteration: 1238/277000 \t Elapsed time: 204.78 \t Loss:10.91369\n",
      "Iteration: 1239/277000 \t Elapsed time: 204.92 \t Loss:10.90097\n",
      "Iteration: 1240/277000 \t Elapsed time: 205.06 \t Loss:10.88811\n",
      "Iteration: 1241/277000 \t Elapsed time: 205.21 \t Loss:10.87545\n",
      "Iteration: 1242/277000 \t Elapsed time: 205.35 \t Loss:10.86275\n",
      "Iteration: 1243/277000 \t Elapsed time: 205.50 \t Loss:10.84998\n",
      "Iteration: 1244/277000 \t Elapsed time: 205.65 \t Loss:10.83722\n",
      "Iteration: 1245/277000 \t Elapsed time: 205.83 \t Loss:10.82451\n",
      "Iteration: 1246/277000 \t Elapsed time: 206.24 \t Loss:10.81178\n",
      "Iteration: 1247/277000 \t Elapsed time: 206.39 \t Loss:10.79907\n",
      "Iteration: 1248/277000 \t Elapsed time: 206.56 \t Loss:10.78639\n",
      "Iteration: 1249/277000 \t Elapsed time: 206.71 \t Loss:10.77370\n",
      "Iteration: 1250/277000 \t Elapsed time: 206.85 \t Loss:10.76097\n",
      "Iteration: 1251/277000 \t Elapsed time: 206.98 \t Loss:10.74832\n",
      "Iteration: 1252/277000 \t Elapsed time: 207.13 \t Loss:10.73576\n",
      "Iteration: 1253/277000 \t Elapsed time: 207.35 \t Loss:10.72307\n",
      "Iteration: 1254/277000 \t Elapsed time: 207.50 \t Loss:10.71043\n",
      "Iteration: 1255/277000 \t Elapsed time: 207.64 \t Loss:10.69795\n",
      "Iteration: 1256/277000 \t Elapsed time: 207.79 \t Loss:10.68522\n",
      "Iteration: 1257/277000 \t Elapsed time: 207.93 \t Loss:10.67278\n",
      "Iteration: 1258/277000 \t Elapsed time: 208.08 \t Loss:10.66012\n",
      "Iteration: 1259/277000 \t Elapsed time: 208.22 \t Loss:10.64733\n",
      "Iteration: 1260/277000 \t Elapsed time: 208.36 \t Loss:10.63480\n",
      "Iteration: 1261/277000 \t Elapsed time: 208.49 \t Loss:10.62217\n",
      "Iteration: 1262/277000 \t Elapsed time: 208.63 \t Loss:10.60973\n",
      "Iteration: 1263/277000 \t Elapsed time: 208.77 \t Loss:10.59711\n",
      "Iteration: 1264/277000 \t Elapsed time: 208.94 \t Loss:10.58451\n",
      "Iteration: 1265/277000 \t Elapsed time: 209.08 \t Loss:10.57208\n",
      "Iteration: 1266/277000 \t Elapsed time: 209.22 \t Loss:10.55940\n",
      "Iteration: 1267/277000 \t Elapsed time: 209.37 \t Loss:10.54692\n",
      "Iteration: 1268/277000 \t Elapsed time: 209.52 \t Loss:10.53436\n",
      "Iteration: 1269/277000 \t Elapsed time: 209.66 \t Loss:10.52176\n",
      "Iteration: 1270/277000 \t Elapsed time: 209.80 \t Loss:10.50925\n",
      "Iteration: 1271/277000 \t Elapsed time: 209.95 \t Loss:10.49673\n",
      "Iteration: 1272/277000 \t Elapsed time: 210.09 \t Loss:10.48429\n",
      "Iteration: 1273/277000 \t Elapsed time: 210.25 \t Loss:10.47178\n",
      "Iteration: 1274/277000 \t Elapsed time: 210.40 \t Loss:10.45929\n",
      "Iteration: 1275/277000 \t Elapsed time: 210.56 \t Loss:10.44686\n",
      "Iteration: 1276/277000 \t Elapsed time: 210.70 \t Loss:10.43430\n",
      "Iteration: 1277/277000 \t Elapsed time: 210.84 \t Loss:10.42181\n",
      "Iteration: 1278/277000 \t Elapsed time: 210.98 \t Loss:10.40931\n",
      "Iteration: 1279/277000 \t Elapsed time: 211.13 \t Loss:10.39690\n",
      "Iteration: 1280/277000 \t Elapsed time: 211.28 \t Loss:10.38444\n",
      "Iteration: 1281/277000 \t Elapsed time: 211.46 \t Loss:10.37198\n",
      "Iteration: 1282/277000 \t Elapsed time: 211.61 \t Loss:10.35951\n",
      "Iteration: 1283/277000 \t Elapsed time: 211.76 \t Loss:10.34703\n",
      "Iteration: 1284/277000 \t Elapsed time: 211.90 \t Loss:10.33469\n",
      "Iteration: 1285/277000 \t Elapsed time: 212.04 \t Loss:10.32225\n",
      "Iteration: 1286/277000 \t Elapsed time: 212.18 \t Loss:10.30980\n",
      "Iteration: 1287/277000 \t Elapsed time: 212.32 \t Loss:10.29744\n",
      "Iteration: 1288/277000 \t Elapsed time: 212.47 \t Loss:10.28503\n",
      "Iteration: 1289/277000 \t Elapsed time: 212.61 \t Loss:10.27270\n",
      "Iteration: 1290/277000 \t Elapsed time: 212.75 \t Loss:10.26033\n",
      "Iteration: 1291/277000 \t Elapsed time: 212.90 \t Loss:10.24794\n",
      "Iteration: 1292/277000 \t Elapsed time: 213.04 \t Loss:10.23552\n",
      "Iteration: 1293/277000 \t Elapsed time: 213.20 \t Loss:10.22311\n",
      "Iteration: 1294/277000 \t Elapsed time: 213.33 \t Loss:10.21059\n",
      "Iteration: 1295/277000 \t Elapsed time: 213.47 \t Loss:10.19880\n",
      "Iteration: 1296/277000 \t Elapsed time: 213.61 \t Loss:10.18649\n",
      "Iteration: 1297/277000 \t Elapsed time: 213.76 \t Loss:10.17392\n",
      "Iteration: 1298/277000 \t Elapsed time: 213.89 \t Loss:10.16144\n",
      "Iteration: 1299/277000 \t Elapsed time: 214.03 \t Loss:10.14931\n",
      "Iteration: 1300/277000 \t Elapsed time: 214.19 \t Loss:10.13667\n",
      "Iteration: 1301/277000 \t Elapsed time: 214.33 \t Loss:10.12456\n",
      "Iteration: 1302/277000 \t Elapsed time: 214.47 \t Loss:10.11205\n",
      "Iteration: 1303/277000 \t Elapsed time: 214.65 \t Loss:10.09982\n",
      "Iteration: 1304/277000 \t Elapsed time: 214.80 \t Loss:10.08750\n",
      "Iteration: 1305/277000 \t Elapsed time: 214.95 \t Loss:10.07530\n",
      "Iteration: 1306/277000 \t Elapsed time: 215.09 \t Loss:10.06316\n",
      "Iteration: 1307/277000 \t Elapsed time: 215.24 \t Loss:10.05066\n",
      "Iteration: 1308/277000 \t Elapsed time: 215.38 \t Loss:10.03845\n",
      "Iteration: 1309/277000 \t Elapsed time: 215.56 \t Loss:10.02609\n",
      "Iteration: 1310/277000 \t Elapsed time: 215.70 \t Loss:10.01382\n",
      "Iteration: 1311/277000 \t Elapsed time: 215.85 \t Loss:10.00149\n",
      "Iteration: 1312/277000 \t Elapsed time: 215.99 \t Loss:9.98928\n",
      "Iteration: 1313/277000 \t Elapsed time: 216.13 \t Loss:9.97700\n",
      "Iteration: 1314/277000 \t Elapsed time: 216.27 \t Loss:9.96492\n",
      "Iteration: 1315/277000 \t Elapsed time: 216.44 \t Loss:9.95261\n",
      "Iteration: 1316/277000 \t Elapsed time: 216.58 \t Loss:9.94031\n",
      "Iteration: 1317/277000 \t Elapsed time: 216.72 \t Loss:9.92803\n",
      "Iteration: 1318/277000 \t Elapsed time: 216.87 \t Loss:9.91584\n",
      "Iteration: 1319/277000 \t Elapsed time: 217.01 \t Loss:9.90375\n",
      "Iteration: 1320/277000 \t Elapsed time: 217.15 \t Loss:9.89153\n",
      "Iteration: 1321/277000 \t Elapsed time: 217.28 \t Loss:9.87927\n",
      "Iteration: 1322/277000 \t Elapsed time: 217.42 \t Loss:9.86720\n",
      "Iteration: 1323/277000 \t Elapsed time: 217.56 \t Loss:9.85511\n",
      "Iteration: 1324/277000 \t Elapsed time: 217.70 \t Loss:9.84291\n",
      "Iteration: 1325/277000 \t Elapsed time: 217.84 \t Loss:9.83072\n",
      "Iteration: 1326/277000 \t Elapsed time: 218.00 \t Loss:9.81853\n",
      "Iteration: 1327/277000 \t Elapsed time: 218.15 \t Loss:9.80627\n",
      "Iteration: 1328/277000 \t Elapsed time: 218.29 \t Loss:9.79411\n",
      "Iteration: 1329/277000 \t Elapsed time: 218.44 \t Loss:9.78185\n",
      "Iteration: 1330/277000 \t Elapsed time: 218.58 \t Loss:9.76984\n",
      "Iteration: 1331/277000 \t Elapsed time: 218.71 \t Loss:9.75777\n",
      "Iteration: 1332/277000 \t Elapsed time: 218.87 \t Loss:9.74557\n",
      "Iteration: 1333/277000 \t Elapsed time: 219.01 \t Loss:9.73350\n",
      "Iteration: 1334/277000 \t Elapsed time: 219.15 \t Loss:9.72140\n",
      "Iteration: 1335/277000 \t Elapsed time: 219.31 \t Loss:9.70928\n",
      "Iteration: 1336/277000 \t Elapsed time: 219.45 \t Loss:9.69715\n",
      "Iteration: 1337/277000 \t Elapsed time: 219.59 \t Loss:9.68501\n",
      "Iteration: 1338/277000 \t Elapsed time: 219.73 \t Loss:9.67292\n",
      "Iteration: 1339/277000 \t Elapsed time: 219.88 \t Loss:9.66081\n",
      "Iteration: 1340/277000 \t Elapsed time: 220.02 \t Loss:9.64870\n",
      "Iteration: 1341/277000 \t Elapsed time: 220.16 \t Loss:9.63671\n",
      "Iteration: 1342/277000 \t Elapsed time: 220.30 \t Loss:9.62474\n",
      "Iteration: 1343/277000 \t Elapsed time: 220.44 \t Loss:9.61267\n",
      "Iteration: 1344/277000 \t Elapsed time: 220.58 \t Loss:9.60065\n",
      "Iteration: 1345/277000 \t Elapsed time: 220.72 \t Loss:9.58856\n",
      "Iteration: 1346/277000 \t Elapsed time: 220.86 \t Loss:9.57658\n",
      "Iteration: 1347/277000 \t Elapsed time: 221.01 \t Loss:9.56448\n",
      "Iteration: 1348/277000 \t Elapsed time: 221.15 \t Loss:9.55238\n",
      "Iteration: 1349/277000 \t Elapsed time: 221.29 \t Loss:9.54040\n",
      "Iteration: 1350/277000 \t Elapsed time: 221.43 \t Loss:9.52850\n",
      "Iteration: 1351/277000 \t Elapsed time: 221.57 \t Loss:9.51658\n",
      "Iteration: 1352/277000 \t Elapsed time: 221.71 \t Loss:9.50447\n",
      "Iteration: 1353/277000 \t Elapsed time: 221.85 \t Loss:9.49258\n",
      "Iteration: 1354/277000 \t Elapsed time: 221.99 \t Loss:9.48088\n",
      "Iteration: 1355/277000 \t Elapsed time: 222.12 \t Loss:9.46893\n",
      "Iteration: 1356/277000 \t Elapsed time: 222.26 \t Loss:9.45682\n",
      "Iteration: 1357/277000 \t Elapsed time: 222.43 \t Loss:9.44487\n",
      "Iteration: 1358/277000 \t Elapsed time: 222.57 \t Loss:9.43309\n",
      "Iteration: 1359/277000 \t Elapsed time: 222.70 \t Loss:9.42112\n",
      "Iteration: 1360/277000 \t Elapsed time: 222.85 \t Loss:9.40917\n",
      "Iteration: 1361/277000 \t Elapsed time: 223.00 \t Loss:9.39687\n",
      "Iteration: 1362/277000 \t Elapsed time: 223.13 \t Loss:9.38482\n",
      "Iteration: 1363/277000 \t Elapsed time: 223.26 \t Loss:9.37291\n",
      "Iteration: 1364/277000 \t Elapsed time: 223.41 \t Loss:9.36125\n",
      "Iteration: 1365/277000 \t Elapsed time: 223.56 \t Loss:9.34992\n",
      "Iteration: 1366/277000 \t Elapsed time: 223.71 \t Loss:9.33867\n",
      "Iteration: 1367/277000 \t Elapsed time: 223.85 \t Loss:9.32622\n",
      "Iteration: 1368/277000 \t Elapsed time: 224.00 \t Loss:9.31437\n",
      "Iteration: 1369/277000 \t Elapsed time: 224.14 \t Loss:9.30186\n",
      "Iteration: 1370/277000 \t Elapsed time: 224.28 \t Loss:9.28994\n",
      "Iteration: 1371/277000 \t Elapsed time: 224.42 \t Loss:9.27814\n",
      "Iteration: 1372/277000 \t Elapsed time: 224.56 \t Loss:9.26632\n",
      "Iteration: 1373/277000 \t Elapsed time: 224.73 \t Loss:9.25432\n",
      "Iteration: 1374/277000 \t Elapsed time: 224.87 \t Loss:9.24252\n",
      "Iteration: 1375/277000 \t Elapsed time: 225.01 \t Loss:9.23038\n",
      "Iteration: 1376/277000 \t Elapsed time: 225.20 \t Loss:9.21849\n",
      "Iteration: 1377/277000 \t Elapsed time: 225.35 \t Loss:9.20690\n",
      "Iteration: 1378/277000 \t Elapsed time: 225.49 \t Loss:9.19503\n",
      "Iteration: 1379/277000 \t Elapsed time: 225.63 \t Loss:9.18322\n",
      "Iteration: 1380/277000 \t Elapsed time: 225.77 \t Loss:9.17156\n",
      "Iteration: 1381/277000 \t Elapsed time: 225.91 \t Loss:9.15964\n",
      "Iteration: 1382/277000 \t Elapsed time: 226.07 \t Loss:9.14776\n",
      "Iteration: 1383/277000 \t Elapsed time: 226.21 \t Loss:9.13564\n",
      "Iteration: 1384/277000 \t Elapsed time: 226.38 \t Loss:9.12390\n",
      "Iteration: 1385/277000 \t Elapsed time: 226.55 \t Loss:9.11204\n",
      "Iteration: 1386/277000 \t Elapsed time: 226.69 \t Loss:9.10022\n",
      "Iteration: 1387/277000 \t Elapsed time: 226.85 \t Loss:9.08852\n",
      "Iteration: 1388/277000 \t Elapsed time: 226.99 \t Loss:9.07674\n",
      "Iteration: 1389/277000 \t Elapsed time: 227.37 \t Loss:9.06510\n",
      "Iteration: 1390/277000 \t Elapsed time: 227.51 \t Loss:9.05356\n",
      "Iteration: 1391/277000 \t Elapsed time: 227.67 \t Loss:9.04191\n",
      "Iteration: 1392/277000 \t Elapsed time: 227.82 \t Loss:9.03016\n",
      "Iteration: 1393/277000 \t Elapsed time: 227.97 \t Loss:9.01813\n",
      "Iteration: 1394/277000 \t Elapsed time: 228.12 \t Loss:9.00619\n",
      "Iteration: 1395/277000 \t Elapsed time: 228.26 \t Loss:8.99462\n",
      "Iteration: 1396/277000 \t Elapsed time: 228.40 \t Loss:8.98323\n",
      "Iteration: 1397/277000 \t Elapsed time: 228.54 \t Loss:8.97203\n",
      "Iteration: 1398/277000 \t Elapsed time: 228.68 \t Loss:8.96014\n",
      "Iteration: 1399/277000 \t Elapsed time: 228.84 \t Loss:8.94800\n",
      "Iteration: 1400/277000 \t Elapsed time: 229.03 \t Loss:8.93576\n",
      "Iteration: 1401/277000 \t Elapsed time: 229.18 \t Loss:8.92426\n",
      "Iteration: 1402/277000 \t Elapsed time: 229.33 \t Loss:8.91302\n",
      "Iteration: 1403/277000 \t Elapsed time: 229.47 \t Loss:8.90148\n",
      "Iteration: 1404/277000 \t Elapsed time: 229.61 \t Loss:8.88943\n",
      "Iteration: 1405/277000 \t Elapsed time: 229.75 \t Loss:8.87721\n",
      "Iteration: 1406/277000 \t Elapsed time: 229.89 \t Loss:8.86573\n",
      "Iteration: 1407/277000 \t Elapsed time: 230.04 \t Loss:8.85465\n",
      "Iteration: 1408/277000 \t Elapsed time: 230.18 \t Loss:8.84279\n",
      "Iteration: 1409/277000 \t Elapsed time: 230.32 \t Loss:8.83072\n",
      "Iteration: 1410/277000 \t Elapsed time: 230.45 \t Loss:8.81891\n",
      "Iteration: 1411/277000 \t Elapsed time: 230.60 \t Loss:8.80739\n",
      "Iteration: 1412/277000 \t Elapsed time: 230.74 \t Loss:8.79597\n",
      "Iteration: 1413/277000 \t Elapsed time: 230.88 \t Loss:8.78423\n",
      "Iteration: 1414/277000 \t Elapsed time: 231.02 \t Loss:8.77246\n",
      "Iteration: 1415/277000 \t Elapsed time: 231.17 \t Loss:8.76067\n",
      "Iteration: 1416/277000 \t Elapsed time: 231.31 \t Loss:8.74905\n",
      "Iteration: 1417/277000 \t Elapsed time: 231.45 \t Loss:8.73749\n",
      "Iteration: 1418/277000 \t Elapsed time: 231.59 \t Loss:8.72612\n",
      "Iteration: 1419/277000 \t Elapsed time: 231.73 \t Loss:8.71498\n",
      "Iteration: 1420/277000 \t Elapsed time: 231.92 \t Loss:8.70368\n",
      "Iteration: 1421/277000 \t Elapsed time: 232.06 \t Loss:8.69269\n",
      "Iteration: 1422/277000 \t Elapsed time: 232.21 \t Loss:8.68017\n",
      "Iteration: 1423/277000 \t Elapsed time: 232.35 \t Loss:8.66792\n",
      "Iteration: 1424/277000 \t Elapsed time: 232.50 \t Loss:8.65627\n",
      "Iteration: 1425/277000 \t Elapsed time: 232.67 \t Loss:8.64509\n",
      "Iteration: 1426/277000 \t Elapsed time: 232.80 \t Loss:8.63390\n",
      "Iteration: 1427/277000 \t Elapsed time: 232.93 \t Loss:8.62233\n",
      "Iteration: 1428/277000 \t Elapsed time: 233.07 \t Loss:8.61028\n",
      "Iteration: 1429/277000 \t Elapsed time: 233.21 \t Loss:8.59832\n",
      "Iteration: 1430/277000 \t Elapsed time: 233.35 \t Loss:8.58723\n",
      "Iteration: 1431/277000 \t Elapsed time: 233.50 \t Loss:8.57631\n",
      "Iteration: 1432/277000 \t Elapsed time: 233.64 \t Loss:8.56451\n",
      "Iteration: 1433/277000 \t Elapsed time: 233.78 \t Loss:8.55239\n",
      "Iteration: 1434/277000 \t Elapsed time: 233.93 \t Loss:8.54060\n",
      "Iteration: 1435/277000 \t Elapsed time: 234.08 \t Loss:8.53007\n",
      "Iteration: 1436/277000 \t Elapsed time: 234.22 \t Loss:8.51925\n",
      "Iteration: 1437/277000 \t Elapsed time: 234.36 \t Loss:8.50664\n",
      "Iteration: 1438/277000 \t Elapsed time: 234.57 \t Loss:8.49490\n",
      "Iteration: 1439/277000 \t Elapsed time: 234.72 \t Loss:8.48361\n",
      "Iteration: 1440/277000 \t Elapsed time: 234.86 \t Loss:8.47223\n",
      "Iteration: 1441/277000 \t Elapsed time: 235.00 \t Loss:8.46129\n",
      "Iteration: 1442/277000 \t Elapsed time: 235.15 \t Loss:8.44910\n",
      "Iteration: 1443/277000 \t Elapsed time: 235.30 \t Loss:8.43721\n",
      "Iteration: 1444/277000 \t Elapsed time: 235.44 \t Loss:8.42683\n",
      "Iteration: 1445/277000 \t Elapsed time: 235.59 \t Loss:8.41595\n",
      "Iteration: 1446/277000 \t Elapsed time: 235.74 \t Loss:8.40381\n",
      "Iteration: 1447/277000 \t Elapsed time: 235.89 \t Loss:8.39114\n",
      "Iteration: 1448/277000 \t Elapsed time: 236.04 \t Loss:8.38123\n",
      "Iteration: 1449/277000 \t Elapsed time: 236.19 \t Loss:8.37154\n",
      "Iteration: 1450/277000 \t Elapsed time: 236.33 \t Loss:8.35781\n",
      "Iteration: 1451/277000 \t Elapsed time: 236.48 \t Loss:8.34568\n",
      "Iteration: 1452/277000 \t Elapsed time: 236.62 \t Loss:8.33571\n",
      "Iteration: 1453/277000 \t Elapsed time: 236.76 \t Loss:8.32426\n",
      "Iteration: 1454/277000 \t Elapsed time: 236.90 \t Loss:8.31127\n",
      "Iteration: 1455/277000 \t Elapsed time: 237.04 \t Loss:8.30064\n",
      "Iteration: 1456/277000 \t Elapsed time: 237.19 \t Loss:8.28964\n",
      "Iteration: 1457/277000 \t Elapsed time: 237.34 \t Loss:8.27713\n",
      "Iteration: 1458/277000 \t Elapsed time: 237.51 \t Loss:8.26586\n",
      "Iteration: 1459/277000 \t Elapsed time: 237.65 \t Loss:8.25442\n",
      "Iteration: 1460/277000 \t Elapsed time: 237.79 \t Loss:8.24313\n",
      "Iteration: 1461/277000 \t Elapsed time: 237.93 \t Loss:8.23161\n",
      "Iteration: 1462/277000 \t Elapsed time: 238.08 \t Loss:8.22029\n",
      "Iteration: 1463/277000 \t Elapsed time: 238.22 \t Loss:8.20911\n",
      "Iteration: 1464/277000 \t Elapsed time: 238.36 \t Loss:8.19730\n",
      "Iteration: 1465/277000 \t Elapsed time: 238.50 \t Loss:8.18588\n",
      "Iteration: 1466/277000 \t Elapsed time: 238.64 \t Loss:8.17483\n",
      "Iteration: 1467/277000 \t Elapsed time: 238.79 \t Loss:8.16355\n",
      "Iteration: 1468/277000 \t Elapsed time: 238.93 \t Loss:8.15200\n",
      "Iteration: 1469/277000 \t Elapsed time: 239.08 \t Loss:8.14031\n",
      "Iteration: 1470/277000 \t Elapsed time: 239.22 \t Loss:8.12905\n",
      "Iteration: 1471/277000 \t Elapsed time: 239.36 \t Loss:8.11789\n",
      "Iteration: 1472/277000 \t Elapsed time: 239.50 \t Loss:8.10639\n",
      "Iteration: 1473/277000 \t Elapsed time: 239.65 \t Loss:8.09509\n",
      "Iteration: 1474/277000 \t Elapsed time: 239.81 \t Loss:8.08392\n",
      "Iteration: 1475/277000 \t Elapsed time: 239.97 \t Loss:8.07299\n",
      "Iteration: 1476/277000 \t Elapsed time: 240.13 \t Loss:8.06139\n",
      "Iteration: 1477/277000 \t Elapsed time: 240.27 \t Loss:8.04972\n",
      "Iteration: 1478/277000 \t Elapsed time: 240.41 \t Loss:8.03848\n",
      "Iteration: 1479/277000 \t Elapsed time: 240.56 \t Loss:8.02754\n",
      "Iteration: 1480/277000 \t Elapsed time: 240.70 \t Loss:8.01647\n",
      "Iteration: 1481/277000 \t Elapsed time: 240.85 \t Loss:8.00492\n",
      "Iteration: 1482/277000 \t Elapsed time: 240.99 \t Loss:7.99325\n",
      "Iteration: 1483/277000 \t Elapsed time: 241.13 \t Loss:7.98204\n",
      "Iteration: 1484/277000 \t Elapsed time: 241.27 \t Loss:7.97104\n",
      "Iteration: 1485/277000 \t Elapsed time: 241.42 \t Loss:7.95996\n",
      "Iteration: 1486/277000 \t Elapsed time: 241.57 \t Loss:7.94839\n",
      "Iteration: 1487/277000 \t Elapsed time: 241.71 \t Loss:7.93685\n",
      "Iteration: 1488/277000 \t Elapsed time: 241.85 \t Loss:7.92566\n",
      "Iteration: 1489/277000 \t Elapsed time: 241.99 \t Loss:7.91442\n",
      "Iteration: 1490/277000 \t Elapsed time: 242.13 \t Loss:7.90352\n",
      "Iteration: 1491/277000 \t Elapsed time: 242.38 \t Loss:7.89219\n",
      "Iteration: 1492/277000 \t Elapsed time: 242.53 \t Loss:7.88076\n",
      "Iteration: 1493/277000 \t Elapsed time: 242.67 \t Loss:7.86933\n",
      "Iteration: 1494/277000 \t Elapsed time: 242.81 \t Loss:7.85800\n",
      "Iteration: 1495/277000 \t Elapsed time: 243.02 \t Loss:7.84702\n",
      "Iteration: 1496/277000 \t Elapsed time: 243.17 \t Loss:7.83589\n",
      "Iteration: 1497/277000 \t Elapsed time: 243.32 \t Loss:7.82480\n",
      "Iteration: 1498/277000 \t Elapsed time: 243.45 \t Loss:7.81378\n",
      "Iteration: 1499/277000 \t Elapsed time: 243.60 \t Loss:7.80261\n",
      "Iteration: 1500/277000 \t Elapsed time: 243.73 \t Loss:7.79138\n",
      "Iteration: 1501/277000 \t Elapsed time: 243.87 \t Loss:7.77981\n",
      "Iteration: 1502/277000 \t Elapsed time: 244.01 \t Loss:7.76855\n",
      "Iteration: 1503/277000 \t Elapsed time: 244.15 \t Loss:7.75725\n",
      "Iteration: 1504/277000 \t Elapsed time: 244.30 \t Loss:7.74611\n",
      "Iteration: 1505/277000 \t Elapsed time: 244.44 \t Loss:7.73493\n",
      "Iteration: 1506/277000 \t Elapsed time: 244.58 \t Loss:7.72381\n",
      "Iteration: 1507/277000 \t Elapsed time: 244.72 \t Loss:7.71297\n",
      "Iteration: 1508/277000 \t Elapsed time: 244.86 \t Loss:7.70231\n",
      "Iteration: 1509/277000 \t Elapsed time: 245.00 \t Loss:7.69230\n",
      "Iteration: 1510/277000 \t Elapsed time: 245.14 \t Loss:7.68248\n",
      "Iteration: 1511/277000 \t Elapsed time: 245.28 \t Loss:7.67216\n",
      "Iteration: 1512/277000 \t Elapsed time: 245.43 \t Loss:7.65927\n",
      "Iteration: 1513/277000 \t Elapsed time: 245.57 \t Loss:7.64642\n",
      "Iteration: 1514/277000 \t Elapsed time: 245.71 \t Loss:7.63566\n",
      "Iteration: 1515/277000 \t Elapsed time: 245.86 \t Loss:7.62574\n",
      "Iteration: 1516/277000 \t Elapsed time: 246.00 \t Loss:7.61401\n",
      "Iteration: 1517/277000 \t Elapsed time: 246.14 \t Loss:7.60191\n",
      "Iteration: 1518/277000 \t Elapsed time: 246.28 \t Loss:7.59150\n",
      "Iteration: 1519/277000 \t Elapsed time: 246.42 \t Loss:7.58058\n",
      "Iteration: 1520/277000 \t Elapsed time: 246.56 \t Loss:7.56890\n",
      "Iteration: 1521/277000 \t Elapsed time: 246.70 \t Loss:7.55815\n",
      "Iteration: 1522/277000 \t Elapsed time: 246.89 \t Loss:7.54867\n",
      "Iteration: 1523/277000 \t Elapsed time: 247.03 \t Loss:7.53729\n",
      "Iteration: 1524/277000 \t Elapsed time: 247.17 \t Loss:7.52498\n",
      "Iteration: 1525/277000 \t Elapsed time: 247.31 \t Loss:7.51560\n",
      "Iteration: 1526/277000 \t Elapsed time: 247.46 \t Loss:7.50496\n",
      "Iteration: 1527/277000 \t Elapsed time: 247.61 \t Loss:7.49186\n",
      "Iteration: 1528/277000 \t Elapsed time: 247.75 \t Loss:7.48227\n",
      "Iteration: 1529/277000 \t Elapsed time: 247.96 \t Loss:7.47127\n",
      "Iteration: 1530/277000 \t Elapsed time: 248.10 \t Loss:7.45906\n",
      "Iteration: 1531/277000 \t Elapsed time: 248.25 \t Loss:7.44917\n",
      "Iteration: 1532/277000 \t Elapsed time: 248.38 \t Loss:7.43835\n",
      "Iteration: 1533/277000 \t Elapsed time: 248.52 \t Loss:7.42597\n",
      "Iteration: 1534/277000 \t Elapsed time: 248.65 \t Loss:7.41630\n",
      "Iteration: 1535/277000 \t Elapsed time: 248.79 \t Loss:7.40620\n",
      "Iteration: 1536/277000 \t Elapsed time: 248.93 \t Loss:7.39303\n",
      "Iteration: 1537/277000 \t Elapsed time: 249.08 \t Loss:7.38365\n",
      "Iteration: 1538/277000 \t Elapsed time: 249.22 \t Loss:7.37271\n",
      "Iteration: 1539/277000 \t Elapsed time: 249.36 \t Loss:7.36002\n",
      "Iteration: 1540/277000 \t Elapsed time: 249.50 \t Loss:7.35041\n",
      "Iteration: 1541/277000 \t Elapsed time: 249.64 \t Loss:7.33889\n",
      "Iteration: 1542/277000 \t Elapsed time: 249.78 \t Loss:7.32751\n",
      "Iteration: 1543/277000 \t Elapsed time: 249.93 \t Loss:7.31861\n",
      "Iteration: 1544/277000 \t Elapsed time: 250.08 \t Loss:7.30766\n",
      "Iteration: 1545/277000 \t Elapsed time: 250.23 \t Loss:7.29482\n",
      "Iteration: 1546/277000 \t Elapsed time: 250.37 \t Loss:7.28464\n",
      "Iteration: 1547/277000 \t Elapsed time: 250.51 \t Loss:7.27367\n",
      "Iteration: 1548/277000 \t Elapsed time: 250.66 \t Loss:7.26211\n",
      "Iteration: 1549/277000 \t Elapsed time: 250.80 \t Loss:7.25231\n",
      "Iteration: 1550/277000 \t Elapsed time: 250.95 \t Loss:7.24071\n",
      "Iteration: 1551/277000 \t Elapsed time: 251.10 \t Loss:7.22962\n",
      "Iteration: 1552/277000 \t Elapsed time: 251.24 \t Loss:7.21918\n",
      "Iteration: 1553/277000 \t Elapsed time: 251.39 \t Loss:7.20813\n",
      "Iteration: 1554/277000 \t Elapsed time: 251.62 \t Loss:7.19722\n",
      "Iteration: 1555/277000 \t Elapsed time: 251.77 \t Loss:7.18643\n",
      "Iteration: 1556/277000 \t Elapsed time: 251.91 \t Loss:7.17556\n",
      "Iteration: 1557/277000 \t Elapsed time: 252.07 \t Loss:7.16451\n",
      "Iteration: 1558/277000 \t Elapsed time: 252.23 \t Loss:7.15390\n",
      "Iteration: 1559/277000 \t Elapsed time: 252.38 \t Loss:7.14297\n",
      "Iteration: 1560/277000 \t Elapsed time: 252.55 \t Loss:7.13200\n",
      "Iteration: 1561/277000 \t Elapsed time: 252.71 \t Loss:7.12117\n",
      "Iteration: 1562/277000 \t Elapsed time: 252.85 \t Loss:7.11030\n",
      "Iteration: 1563/277000 \t Elapsed time: 252.98 \t Loss:7.09970\n",
      "Iteration: 1564/277000 \t Elapsed time: 253.12 \t Loss:7.08873\n",
      "Iteration: 1565/277000 \t Elapsed time: 253.26 \t Loss:7.07803\n",
      "Iteration: 1566/277000 \t Elapsed time: 253.42 \t Loss:7.06720\n",
      "Iteration: 1567/277000 \t Elapsed time: 253.57 \t Loss:7.05643\n",
      "Iteration: 1568/277000 \t Elapsed time: 253.71 \t Loss:7.04574\n",
      "Iteration: 1569/277000 \t Elapsed time: 253.86 \t Loss:7.03479\n",
      "Iteration: 1570/277000 \t Elapsed time: 253.99 \t Loss:7.02401\n",
      "Iteration: 1571/277000 \t Elapsed time: 254.13 \t Loss:7.01334\n",
      "Iteration: 1572/277000 \t Elapsed time: 254.27 \t Loss:7.00257\n",
      "Iteration: 1573/277000 \t Elapsed time: 254.42 \t Loss:6.99186\n",
      "Iteration: 1574/277000 \t Elapsed time: 254.56 \t Loss:6.98112\n",
      "Iteration: 1575/277000 \t Elapsed time: 254.98 \t Loss:6.97032\n",
      "Iteration: 1576/277000 \t Elapsed time: 255.12 \t Loss:6.95955\n",
      "Iteration: 1577/277000 \t Elapsed time: 255.27 \t Loss:6.94888\n",
      "Iteration: 1578/277000 \t Elapsed time: 255.40 \t Loss:6.93812\n",
      "Iteration: 1579/277000 \t Elapsed time: 255.58 \t Loss:6.92754\n",
      "Iteration: 1580/277000 \t Elapsed time: 255.72 \t Loss:6.91718\n",
      "Iteration: 1581/277000 \t Elapsed time: 255.86 \t Loss:6.90689\n",
      "Iteration: 1582/277000 \t Elapsed time: 256.00 \t Loss:6.89571\n",
      "Iteration: 1583/277000 \t Elapsed time: 256.15 \t Loss:6.88476\n",
      "Iteration: 1584/277000 \t Elapsed time: 256.29 \t Loss:6.87406\n",
      "Iteration: 1585/277000 \t Elapsed time: 256.43 \t Loss:6.86413\n",
      "Iteration: 1586/277000 \t Elapsed time: 256.58 \t Loss:6.85504\n",
      "Iteration: 1587/277000 \t Elapsed time: 256.71 \t Loss:6.84698\n",
      "Iteration: 1588/277000 \t Elapsed time: 256.85 \t Loss:6.84036\n",
      "Iteration: 1589/277000 \t Elapsed time: 257.00 \t Loss:6.83001\n",
      "Iteration: 1590/277000 \t Elapsed time: 257.14 \t Loss:6.81318\n",
      "Iteration: 1591/277000 \t Elapsed time: 257.28 \t Loss:6.80003\n",
      "Iteration: 1592/277000 \t Elapsed time: 257.44 \t Loss:6.79437\n",
      "Iteration: 1593/277000 \t Elapsed time: 257.68 \t Loss:6.78289\n",
      "Iteration: 1594/277000 \t Elapsed time: 257.85 \t Loss:6.76768\n",
      "Iteration: 1595/277000 \t Elapsed time: 257.99 \t Loss:6.76008\n",
      "Iteration: 1596/277000 \t Elapsed time: 258.13 \t Loss:6.74954\n",
      "Iteration: 1597/277000 \t Elapsed time: 258.27 \t Loss:6.73623\n",
      "Iteration: 1598/277000 \t Elapsed time: 258.45 \t Loss:6.72815\n",
      "Iteration: 1599/277000 \t Elapsed time: 258.59 \t Loss:6.71641\n",
      "Iteration: 1600/277000 \t Elapsed time: 258.73 \t Loss:6.70448\n",
      "Iteration: 1601/277000 \t Elapsed time: 258.87 \t Loss:6.69618\n",
      "Iteration: 1602/277000 \t Elapsed time: 259.00 \t Loss:6.68406\n",
      "Iteration: 1603/277000 \t Elapsed time: 259.16 \t Loss:6.67333\n",
      "Iteration: 1604/277000 \t Elapsed time: 259.31 \t Loss:6.66395\n",
      "Iteration: 1605/277000 \t Elapsed time: 259.46 \t Loss:6.65193\n",
      "Iteration: 1606/277000 \t Elapsed time: 259.60 \t Loss:6.64210\n",
      "Iteration: 1607/277000 \t Elapsed time: 259.74 \t Loss:6.63150\n",
      "Iteration: 1608/277000 \t Elapsed time: 259.88 \t Loss:6.61989\n",
      "Iteration: 1609/277000 \t Elapsed time: 260.02 \t Loss:6.61066\n",
      "Iteration: 1610/277000 \t Elapsed time: 260.17 \t Loss:6.59935\n",
      "Iteration: 1611/277000 \t Elapsed time: 260.31 \t Loss:6.58864\n",
      "Iteration: 1612/277000 \t Elapsed time: 260.46 \t Loss:6.57877\n",
      "Iteration: 1613/277000 \t Elapsed time: 260.60 \t Loss:6.56738\n",
      "Iteration: 1614/277000 \t Elapsed time: 260.74 \t Loss:6.55750\n",
      "Iteration: 1615/277000 \t Elapsed time: 260.89 \t Loss:6.54676\n",
      "Iteration: 1616/277000 \t Elapsed time: 261.05 \t Loss:6.53603\n",
      "Iteration: 1617/277000 \t Elapsed time: 261.20 \t Loss:6.52600\n",
      "Iteration: 1618/277000 \t Elapsed time: 261.34 \t Loss:6.51503\n",
      "Iteration: 1619/277000 \t Elapsed time: 261.48 \t Loss:6.50466\n",
      "Iteration: 1620/277000 \t Elapsed time: 261.62 \t Loss:6.49439\n",
      "Iteration: 1621/277000 \t Elapsed time: 261.76 \t Loss:6.48357\n",
      "Iteration: 1622/277000 \t Elapsed time: 261.90 \t Loss:6.47332\n",
      "Iteration: 1623/277000 \t Elapsed time: 262.05 \t Loss:6.46283\n",
      "Iteration: 1624/277000 \t Elapsed time: 262.19 \t Loss:6.45228\n",
      "Iteration: 1625/277000 \t Elapsed time: 262.33 \t Loss:6.44196\n",
      "Iteration: 1626/277000 \t Elapsed time: 262.47 \t Loss:6.43151\n",
      "Iteration: 1627/277000 \t Elapsed time: 262.61 \t Loss:6.42099\n",
      "Iteration: 1628/277000 \t Elapsed time: 262.75 \t Loss:6.41085\n",
      "Iteration: 1629/277000 \t Elapsed time: 262.97 \t Loss:6.40056\n",
      "Iteration: 1630/277000 \t Elapsed time: 263.11 \t Loss:6.39043\n",
      "Iteration: 1631/277000 \t Elapsed time: 263.25 \t Loss:6.38053\n",
      "Iteration: 1632/277000 \t Elapsed time: 263.40 \t Loss:6.37040\n",
      "Iteration: 1633/277000 \t Elapsed time: 263.53 \t Loss:6.35928\n",
      "Iteration: 1634/277000 \t Elapsed time: 263.67 \t Loss:6.34832\n",
      "Iteration: 1635/277000 \t Elapsed time: 263.80 \t Loss:6.33786\n",
      "Iteration: 1636/277000 \t Elapsed time: 263.93 \t Loss:6.32770\n",
      "Iteration: 1637/277000 \t Elapsed time: 264.08 \t Loss:6.31915\n",
      "Iteration: 1638/277000 \t Elapsed time: 264.22 \t Loss:6.31171\n",
      "Iteration: 1639/277000 \t Elapsed time: 264.35 \t Loss:6.30129\n",
      "Iteration: 1640/277000 \t Elapsed time: 264.48 \t Loss:6.28635\n",
      "Iteration: 1641/277000 \t Elapsed time: 264.64 \t Loss:6.27890\n",
      "Iteration: 1642/277000 \t Elapsed time: 264.80 \t Loss:6.26895\n",
      "Iteration: 1643/277000 \t Elapsed time: 264.98 \t Loss:6.25604\n",
      "Iteration: 1644/277000 \t Elapsed time: 265.12 \t Loss:6.24919\n",
      "Iteration: 1645/277000 \t Elapsed time: 265.27 \t Loss:6.23639\n",
      "Iteration: 1646/277000 \t Elapsed time: 265.41 \t Loss:6.22530\n",
      "Iteration: 1647/277000 \t Elapsed time: 265.55 \t Loss:6.21803\n",
      "Iteration: 1648/277000 \t Elapsed time: 265.69 \t Loss:6.20504\n",
      "Iteration: 1649/277000 \t Elapsed time: 265.84 \t Loss:6.19651\n",
      "Iteration: 1650/277000 \t Elapsed time: 266.00 \t Loss:6.18511\n",
      "Iteration: 1651/277000 \t Elapsed time: 266.14 \t Loss:6.17338\n",
      "Iteration: 1652/277000 \t Elapsed time: 266.28 \t Loss:6.16528\n",
      "Iteration: 1653/277000 \t Elapsed time: 266.42 \t Loss:6.15335\n",
      "Iteration: 1654/277000 \t Elapsed time: 266.58 \t Loss:6.14493\n",
      "Iteration: 1655/277000 \t Elapsed time: 266.73 \t Loss:6.13328\n",
      "Iteration: 1656/277000 \t Elapsed time: 266.86 \t Loss:6.12308\n",
      "Iteration: 1657/277000 \t Elapsed time: 267.01 \t Loss:6.11260\n",
      "Iteration: 1658/277000 \t Elapsed time: 267.15 \t Loss:6.10162\n",
      "Iteration: 1659/277000 \t Elapsed time: 267.29 \t Loss:6.09225\n",
      "Iteration: 1660/277000 \t Elapsed time: 267.43 \t Loss:6.08148\n",
      "Iteration: 1661/277000 \t Elapsed time: 267.58 \t Loss:6.07258\n",
      "Iteration: 1662/277000 \t Elapsed time: 267.71 \t Loss:6.06138\n",
      "Iteration: 1663/277000 \t Elapsed time: 267.85 \t Loss:6.05059\n",
      "Iteration: 1664/277000 \t Elapsed time: 267.98 \t Loss:6.04061\n",
      "Iteration: 1665/277000 \t Elapsed time: 268.12 \t Loss:6.03021\n",
      "Iteration: 1666/277000 \t Elapsed time: 268.26 \t Loss:6.02062\n",
      "Iteration: 1667/277000 \t Elapsed time: 268.40 \t Loss:6.01066\n",
      "Iteration: 1668/277000 \t Elapsed time: 268.54 \t Loss:6.00136\n",
      "Iteration: 1669/277000 \t Elapsed time: 268.68 \t Loss:5.99222\n",
      "Iteration: 1670/277000 \t Elapsed time: 268.82 \t Loss:5.98221\n",
      "Iteration: 1671/277000 \t Elapsed time: 268.96 \t Loss:5.97246\n",
      "Iteration: 1672/277000 \t Elapsed time: 269.09 \t Loss:5.96170\n",
      "Iteration: 1673/277000 \t Elapsed time: 269.24 \t Loss:5.95043\n",
      "Iteration: 1674/277000 \t Elapsed time: 269.38 \t Loss:5.93920\n",
      "Iteration: 1675/277000 \t Elapsed time: 269.51 \t Loss:5.92842\n",
      "Iteration: 1676/277000 \t Elapsed time: 269.65 \t Loss:5.91857\n",
      "Iteration: 1677/277000 \t Elapsed time: 269.80 \t Loss:5.90936\n",
      "Iteration: 1678/277000 \t Elapsed time: 269.95 \t Loss:5.89951\n",
      "Iteration: 1679/277000 \t Elapsed time: 270.09 \t Loss:5.88936\n",
      "Iteration: 1680/277000 \t Elapsed time: 270.23 \t Loss:5.87878\n",
      "Iteration: 1681/277000 \t Elapsed time: 270.36 \t Loss:5.86820\n",
      "Iteration: 1682/277000 \t Elapsed time: 270.57 \t Loss:5.85787\n",
      "Iteration: 1683/277000 \t Elapsed time: 270.74 \t Loss:5.84787\n",
      "Iteration: 1684/277000 \t Elapsed time: 270.92 \t Loss:5.83819\n",
      "Iteration: 1685/277000 \t Elapsed time: 271.07 \t Loss:5.82903\n",
      "Iteration: 1686/277000 \t Elapsed time: 271.21 \t Loss:5.82013\n",
      "Iteration: 1687/277000 \t Elapsed time: 271.35 \t Loss:5.81047\n",
      "Iteration: 1688/277000 \t Elapsed time: 271.49 \t Loss:5.80032\n",
      "Iteration: 1689/277000 \t Elapsed time: 271.63 \t Loss:5.78953\n",
      "Iteration: 1690/277000 \t Elapsed time: 271.78 \t Loss:5.77895\n",
      "Iteration: 1691/277000 \t Elapsed time: 271.92 \t Loss:5.76792\n",
      "Iteration: 1692/277000 \t Elapsed time: 272.06 \t Loss:5.75778\n",
      "Iteration: 1693/277000 \t Elapsed time: 272.20 \t Loss:5.74777\n",
      "Iteration: 1694/277000 \t Elapsed time: 272.34 \t Loss:5.73770\n",
      "Iteration: 1695/277000 \t Elapsed time: 272.47 \t Loss:5.72736\n",
      "Iteration: 1696/277000 \t Elapsed time: 272.61 \t Loss:5.71717\n",
      "Iteration: 1697/277000 \t Elapsed time: 272.81 \t Loss:5.70843\n",
      "Iteration: 1698/277000 \t Elapsed time: 272.96 \t Loss:5.70397\n",
      "Iteration: 1699/277000 \t Elapsed time: 273.09 \t Loss:5.70612\n",
      "Iteration: 1700/277000 \t Elapsed time: 273.22 \t Loss:5.70606\n",
      "Iteration: 1701/277000 \t Elapsed time: 273.36 \t Loss:5.67421\n",
      "Iteration: 1702/277000 \t Elapsed time: 273.50 \t Loss:5.66841\n",
      "Iteration: 1703/277000 \t Elapsed time: 273.63 \t Loss:5.66263\n",
      "Iteration: 1704/277000 \t Elapsed time: 273.78 \t Loss:5.64018\n",
      "Iteration: 1705/277000 \t Elapsed time: 273.92 \t Loss:5.64284\n",
      "Iteration: 1706/277000 \t Elapsed time: 274.05 \t Loss:5.62294\n",
      "Iteration: 1707/277000 \t Elapsed time: 274.19 \t Loss:5.61852\n",
      "Iteration: 1708/277000 \t Elapsed time: 274.33 \t Loss:5.60367\n",
      "Iteration: 1709/277000 \t Elapsed time: 274.47 \t Loss:5.59466\n",
      "Iteration: 1710/277000 \t Elapsed time: 274.61 \t Loss:5.58217\n",
      "Iteration: 1711/277000 \t Elapsed time: 274.76 \t Loss:5.57372\n",
      "Iteration: 1712/277000 \t Elapsed time: 274.90 \t Loss:5.56378\n",
      "Iteration: 1713/277000 \t Elapsed time: 275.05 \t Loss:5.55431\n",
      "Iteration: 1714/277000 \t Elapsed time: 275.22 \t Loss:5.54317\n",
      "Iteration: 1715/277000 \t Elapsed time: 275.37 \t Loss:5.53289\n",
      "Iteration: 1716/277000 \t Elapsed time: 275.51 \t Loss:5.52234\n",
      "Iteration: 1717/277000 \t Elapsed time: 275.65 \t Loss:5.51373\n",
      "Iteration: 1718/277000 \t Elapsed time: 275.81 \t Loss:5.50343\n",
      "Iteration: 1719/277000 \t Elapsed time: 275.97 \t Loss:5.49435\n",
      "Iteration: 1720/277000 \t Elapsed time: 276.15 \t Loss:5.48307\n",
      "Iteration: 1721/277000 \t Elapsed time: 276.30 \t Loss:5.47354\n",
      "Iteration: 1722/277000 \t Elapsed time: 276.45 \t Loss:5.46305\n",
      "Iteration: 1723/277000 \t Elapsed time: 276.60 \t Loss:5.45390\n",
      "Iteration: 1724/277000 \t Elapsed time: 276.76 \t Loss:5.44419\n",
      "Iteration: 1725/277000 \t Elapsed time: 276.90 \t Loss:5.43618\n",
      "Iteration: 1726/277000 \t Elapsed time: 277.04 \t Loss:5.42543\n",
      "Iteration: 1727/277000 \t Elapsed time: 277.18 \t Loss:5.41746\n",
      "Iteration: 1728/277000 \t Elapsed time: 277.32 \t Loss:5.40731\n",
      "Iteration: 1729/277000 \t Elapsed time: 277.45 \t Loss:5.39612\n",
      "Iteration: 1730/277000 \t Elapsed time: 277.60 \t Loss:5.38716\n",
      "Iteration: 1731/277000 \t Elapsed time: 277.76 \t Loss:5.37675\n",
      "Iteration: 1732/277000 \t Elapsed time: 277.91 \t Loss:5.36735\n",
      "Iteration: 1733/277000 \t Elapsed time: 278.05 \t Loss:5.35789\n",
      "Iteration: 1734/277000 \t Elapsed time: 278.19 \t Loss:5.34880\n",
      "Iteration: 1735/277000 \t Elapsed time: 278.33 \t Loss:5.33930\n",
      "Iteration: 1736/277000 \t Elapsed time: 278.47 \t Loss:5.33080\n",
      "Iteration: 1737/277000 \t Elapsed time: 278.61 \t Loss:5.32179\n",
      "Iteration: 1738/277000 \t Elapsed time: 278.76 \t Loss:5.31298\n",
      "Iteration: 1739/277000 \t Elapsed time: 278.89 \t Loss:5.30307\n",
      "Iteration: 1740/277000 \t Elapsed time: 279.02 \t Loss:5.29083\n",
      "Iteration: 1741/277000 \t Elapsed time: 279.18 \t Loss:5.27916\n",
      "Iteration: 1742/277000 \t Elapsed time: 279.35 \t Loss:5.26890\n",
      "Iteration: 1743/277000 \t Elapsed time: 279.55 \t Loss:5.26100\n",
      "Iteration: 1744/277000 \t Elapsed time: 279.70 \t Loss:5.25215\n",
      "Iteration: 1745/277000 \t Elapsed time: 279.84 \t Loss:5.24307\n",
      "Iteration: 1746/277000 \t Elapsed time: 279.98 \t Loss:5.23153\n",
      "Iteration: 1747/277000 \t Elapsed time: 280.12 \t Loss:5.22093\n",
      "Iteration: 1748/277000 \t Elapsed time: 280.26 \t Loss:5.21146\n",
      "Iteration: 1749/277000 \t Elapsed time: 280.40 \t Loss:5.20188\n",
      "Iteration: 1750/277000 \t Elapsed time: 280.53 \t Loss:5.19321\n",
      "Iteration: 1751/277000 \t Elapsed time: 280.67 \t Loss:5.18279\n",
      "Iteration: 1752/277000 \t Elapsed time: 280.86 \t Loss:5.17244\n",
      "Iteration: 1753/277000 \t Elapsed time: 281.00 \t Loss:5.16314\n",
      "Iteration: 1754/277000 \t Elapsed time: 281.14 \t Loss:5.15376\n",
      "Iteration: 1755/277000 \t Elapsed time: 281.28 \t Loss:5.14516\n",
      "Iteration: 1756/277000 \t Elapsed time: 281.44 \t Loss:5.13701\n",
      "Iteration: 1757/277000 \t Elapsed time: 281.61 \t Loss:5.12849\n",
      "Iteration: 1758/277000 \t Elapsed time: 281.75 \t Loss:5.12252\n",
      "Iteration: 1759/277000 \t Elapsed time: 281.89 \t Loss:5.11411\n",
      "Iteration: 1760/277000 \t Elapsed time: 282.03 \t Loss:5.10396\n",
      "Iteration: 1761/277000 \t Elapsed time: 282.18 \t Loss:5.09006\n",
      "Iteration: 1762/277000 \t Elapsed time: 282.32 \t Loss:5.07891\n",
      "Iteration: 1763/277000 \t Elapsed time: 282.46 \t Loss:5.07192\n",
      "Iteration: 1764/277000 \t Elapsed time: 282.61 \t Loss:5.06503\n",
      "Iteration: 1765/277000 \t Elapsed time: 282.75 \t Loss:5.05572\n",
      "Iteration: 1766/277000 \t Elapsed time: 282.88 \t Loss:5.04230\n",
      "Iteration: 1767/277000 \t Elapsed time: 283.02 \t Loss:5.03216\n",
      "Iteration: 1768/277000 \t Elapsed time: 283.16 \t Loss:5.02380\n",
      "Iteration: 1769/277000 \t Elapsed time: 283.32 \t Loss:5.01498\n",
      "Iteration: 1770/277000 \t Elapsed time: 283.45 \t Loss:5.00330\n",
      "Iteration: 1771/277000 \t Elapsed time: 283.60 \t Loss:4.99224\n",
      "Iteration: 1772/277000 \t Elapsed time: 283.73 \t Loss:4.98424\n",
      "Iteration: 1773/277000 \t Elapsed time: 283.87 \t Loss:4.97598\n",
      "Iteration: 1774/277000 \t Elapsed time: 284.02 \t Loss:4.96534\n",
      "Iteration: 1775/277000 \t Elapsed time: 284.16 \t Loss:4.95512\n",
      "Iteration: 1776/277000 \t Elapsed time: 284.31 \t Loss:4.94767\n",
      "Iteration: 1777/277000 \t Elapsed time: 284.46 \t Loss:4.93936\n",
      "Iteration: 1778/277000 \t Elapsed time: 284.60 \t Loss:4.92817\n",
      "Iteration: 1779/277000 \t Elapsed time: 284.74 \t Loss:4.91776\n",
      "Iteration: 1780/277000 \t Elapsed time: 284.89 \t Loss:4.90892\n",
      "Iteration: 1781/277000 \t Elapsed time: 285.03 \t Loss:4.89837\n",
      "Iteration: 1782/277000 \t Elapsed time: 285.18 \t Loss:4.88988\n",
      "Iteration: 1783/277000 \t Elapsed time: 285.33 \t Loss:4.88109\n",
      "Iteration: 1784/277000 \t Elapsed time: 285.47 \t Loss:4.87027\n",
      "Iteration: 1785/277000 \t Elapsed time: 285.61 \t Loss:4.85999\n",
      "Iteration: 1786/277000 \t Elapsed time: 285.77 \t Loss:4.85264\n",
      "Iteration: 1787/277000 \t Elapsed time: 285.93 \t Loss:4.84719\n",
      "Iteration: 1788/277000 \t Elapsed time: 286.10 \t Loss:4.84131\n",
      "Iteration: 1789/277000 \t Elapsed time: 286.24 \t Loss:4.84424\n",
      "Iteration: 1790/277000 \t Elapsed time: 286.43 \t Loss:4.85964\n",
      "Iteration: 1791/277000 \t Elapsed time: 286.62 \t Loss:4.90400\n",
      "Iteration: 1792/277000 \t Elapsed time: 286.78 \t Loss:4.86856\n",
      "Iteration: 1793/277000 \t Elapsed time: 286.92 \t Loss:4.81006\n",
      "Iteration: 1794/277000 \t Elapsed time: 287.07 \t Loss:4.85493\n",
      "Iteration: 1795/277000 \t Elapsed time: 287.22 \t Loss:4.79130\n",
      "Iteration: 1796/277000 \t Elapsed time: 287.36 \t Loss:4.79019\n",
      "Iteration: 1797/277000 \t Elapsed time: 287.50 \t Loss:4.77588\n",
      "Iteration: 1798/277000 \t Elapsed time: 287.65 \t Loss:4.75947\n",
      "Iteration: 1799/277000 \t Elapsed time: 287.79 \t Loss:4.75810\n",
      "Iteration: 1800/277000 \t Elapsed time: 287.94 \t Loss:4.74016\n",
      "Iteration: 1801/277000 \t Elapsed time: 288.07 \t Loss:4.72999\n",
      "Iteration: 1802/277000 \t Elapsed time: 288.21 \t Loss:4.72698\n",
      "Iteration: 1803/277000 \t Elapsed time: 288.39 \t Loss:4.72536\n",
      "Iteration: 1804/277000 \t Elapsed time: 288.59 \t Loss:4.72857\n",
      "Iteration: 1805/277000 \t Elapsed time: 288.73 \t Loss:4.69614\n",
      "Iteration: 1806/277000 \t Elapsed time: 288.88 \t Loss:4.69586\n",
      "Iteration: 1807/277000 \t Elapsed time: 289.02 \t Loss:4.69089\n",
      "Iteration: 1808/277000 \t Elapsed time: 289.15 \t Loss:4.66910\n",
      "Iteration: 1809/277000 \t Elapsed time: 289.30 \t Loss:4.65731\n",
      "Iteration: 1810/277000 \t Elapsed time: 289.46 \t Loss:4.65937\n",
      "Iteration: 1811/277000 \t Elapsed time: 289.61 \t Loss:4.63293\n",
      "Iteration: 1812/277000 \t Elapsed time: 289.79 \t Loss:4.63600\n",
      "Iteration: 1813/277000 \t Elapsed time: 289.93 \t Loss:4.61760\n",
      "Iteration: 1814/277000 \t Elapsed time: 290.07 \t Loss:4.60955\n",
      "Iteration: 1815/277000 \t Elapsed time: 290.21 \t Loss:4.60142\n",
      "Iteration: 1816/277000 \t Elapsed time: 290.36 \t Loss:4.58874\n",
      "Iteration: 1817/277000 \t Elapsed time: 290.54 \t Loss:4.58369\n",
      "Iteration: 1818/277000 \t Elapsed time: 290.68 \t Loss:4.57202\n",
      "Iteration: 1819/277000 \t Elapsed time: 290.82 \t Loss:4.56171\n",
      "Iteration: 1820/277000 \t Elapsed time: 290.98 \t Loss:4.55491\n",
      "Iteration: 1821/277000 \t Elapsed time: 291.13 \t Loss:4.54180\n",
      "Iteration: 1822/277000 \t Elapsed time: 291.27 \t Loss:4.53537\n",
      "Iteration: 1823/277000 \t Elapsed time: 291.41 \t Loss:4.52500\n",
      "Iteration: 1824/277000 \t Elapsed time: 291.54 \t Loss:4.51517\n",
      "Iteration: 1825/277000 \t Elapsed time: 291.69 \t Loss:4.50827\n",
      "Iteration: 1826/277000 \t Elapsed time: 291.83 \t Loss:4.49795\n",
      "Iteration: 1827/277000 \t Elapsed time: 291.97 \t Loss:4.48920\n",
      "Iteration: 1828/277000 \t Elapsed time: 292.11 \t Loss:4.48017\n",
      "Iteration: 1829/277000 \t Elapsed time: 292.25 \t Loss:4.47050\n",
      "Iteration: 1830/277000 \t Elapsed time: 292.39 \t Loss:4.46276\n",
      "Iteration: 1831/277000 \t Elapsed time: 292.60 \t Loss:4.45308\n",
      "Iteration: 1832/277000 \t Elapsed time: 292.74 \t Loss:4.44343\n",
      "Iteration: 1833/277000 \t Elapsed time: 292.88 \t Loss:4.43593\n",
      "Iteration: 1834/277000 \t Elapsed time: 293.02 \t Loss:4.42593\n",
      "Iteration: 1835/277000 \t Elapsed time: 293.16 \t Loss:4.41682\n",
      "Iteration: 1836/277000 \t Elapsed time: 293.30 \t Loss:4.40897\n",
      "Iteration: 1837/277000 \t Elapsed time: 293.43 \t Loss:4.39904\n",
      "Iteration: 1838/277000 \t Elapsed time: 293.57 \t Loss:4.39059\n",
      "Iteration: 1839/277000 \t Elapsed time: 293.71 \t Loss:4.38291\n",
      "Iteration: 1840/277000 \t Elapsed time: 293.85 \t Loss:4.37319\n",
      "Iteration: 1841/277000 \t Elapsed time: 293.98 \t Loss:4.36393\n",
      "Iteration: 1842/277000 \t Elapsed time: 294.11 \t Loss:4.35501\n",
      "Iteration: 1843/277000 \t Elapsed time: 294.25 \t Loss:4.34664\n",
      "Iteration: 1844/277000 \t Elapsed time: 294.40 \t Loss:4.33857\n",
      "Iteration: 1845/277000 \t Elapsed time: 294.54 \t Loss:4.32866\n",
      "Iteration: 1846/277000 \t Elapsed time: 294.69 \t Loss:4.32011\n",
      "Iteration: 1847/277000 \t Elapsed time: 294.84 \t Loss:4.31150\n",
      "Iteration: 1848/277000 \t Elapsed time: 294.98 \t Loss:4.30366\n",
      "Iteration: 1849/277000 \t Elapsed time: 295.12 \t Loss:4.29364\n",
      "Iteration: 1850/277000 \t Elapsed time: 295.28 \t Loss:4.28513\n",
      "Iteration: 1851/277000 \t Elapsed time: 295.43 \t Loss:4.27696\n",
      "Iteration: 1852/277000 \t Elapsed time: 295.57 \t Loss:4.26703\n",
      "Iteration: 1853/277000 \t Elapsed time: 295.72 \t Loss:4.25815\n",
      "Iteration: 1854/277000 \t Elapsed time: 295.87 \t Loss:4.24964\n",
      "Iteration: 1855/277000 \t Elapsed time: 296.05 \t Loss:4.24166\n",
      "Iteration: 1856/277000 \t Elapsed time: 296.20 \t Loss:4.23312\n",
      "Iteration: 1857/277000 \t Elapsed time: 296.34 \t Loss:4.22428\n",
      "Iteration: 1858/277000 \t Elapsed time: 296.48 \t Loss:4.21660\n",
      "Iteration: 1859/277000 \t Elapsed time: 296.62 \t Loss:4.21290\n",
      "Iteration: 1860/277000 \t Elapsed time: 296.76 \t Loss:4.20601\n",
      "Iteration: 1861/277000 \t Elapsed time: 296.90 \t Loss:4.20267\n",
      "Iteration: 1862/277000 \t Elapsed time: 297.05 \t Loss:4.20376\n",
      "Iteration: 1863/277000 \t Elapsed time: 297.20 \t Loss:4.22472\n",
      "Iteration: 1864/277000 \t Elapsed time: 297.34 \t Loss:4.19941\n",
      "Iteration: 1865/277000 \t Elapsed time: 297.48 \t Loss:4.17091\n",
      "Iteration: 1866/277000 \t Elapsed time: 297.62 \t Loss:4.15843\n",
      "Iteration: 1867/277000 \t Elapsed time: 297.76 \t Loss:4.16034\n",
      "Iteration: 1868/277000 \t Elapsed time: 297.90 \t Loss:4.15146\n",
      "Iteration: 1869/277000 \t Elapsed time: 298.04 \t Loss:4.13728\n",
      "Iteration: 1870/277000 \t Elapsed time: 298.18 \t Loss:4.12841\n",
      "Iteration: 1871/277000 \t Elapsed time: 298.32 \t Loss:4.12084\n",
      "Iteration: 1872/277000 \t Elapsed time: 298.46 \t Loss:4.11333\n",
      "Iteration: 1873/277000 \t Elapsed time: 298.60 \t Loss:4.09476\n",
      "Iteration: 1874/277000 \t Elapsed time: 298.75 \t Loss:4.09494\n",
      "Iteration: 1875/277000 \t Elapsed time: 298.89 \t Loss:4.07631\n",
      "Iteration: 1876/277000 \t Elapsed time: 299.03 \t Loss:4.08162\n",
      "Iteration: 1877/277000 \t Elapsed time: 299.17 \t Loss:4.06274\n",
      "Iteration: 1878/277000 \t Elapsed time: 299.31 \t Loss:4.05563\n",
      "Iteration: 1879/277000 \t Elapsed time: 299.45 \t Loss:4.04741\n",
      "Iteration: 1880/277000 \t Elapsed time: 299.59 \t Loss:4.03445\n",
      "Iteration: 1881/277000 \t Elapsed time: 299.73 \t Loss:4.03137\n",
      "Iteration: 1882/277000 \t Elapsed time: 299.87 \t Loss:4.01659\n",
      "Iteration: 1883/277000 \t Elapsed time: 300.00 \t Loss:4.01005\n",
      "Iteration: 1884/277000 \t Elapsed time: 300.14 \t Loss:3.99957\n",
      "Iteration: 1885/277000 \t Elapsed time: 300.29 \t Loss:3.99247\n",
      "Iteration: 1886/277000 \t Elapsed time: 300.43 \t Loss:3.98501\n",
      "Iteration: 1887/277000 \t Elapsed time: 300.59 \t Loss:3.97381\n",
      "Iteration: 1888/277000 \t Elapsed time: 300.73 \t Loss:3.96729\n",
      "Iteration: 1889/277000 \t Elapsed time: 300.87 \t Loss:3.95518\n",
      "Iteration: 1890/277000 \t Elapsed time: 301.01 \t Loss:3.95305\n",
      "Iteration: 1891/277000 \t Elapsed time: 301.16 \t Loss:3.94085\n",
      "Iteration: 1892/277000 \t Elapsed time: 301.30 \t Loss:3.93428\n",
      "Iteration: 1893/277000 \t Elapsed time: 301.45 \t Loss:3.92317\n",
      "Iteration: 1894/277000 \t Elapsed time: 301.59 \t Loss:3.91519\n",
      "Iteration: 1895/277000 \t Elapsed time: 301.74 \t Loss:3.90545\n",
      "Iteration: 1896/277000 \t Elapsed time: 301.88 \t Loss:3.89632\n",
      "Iteration: 1897/277000 \t Elapsed time: 302.03 \t Loss:3.88899\n",
      "Iteration: 1898/277000 \t Elapsed time: 302.16 \t Loss:3.87995\n",
      "Iteration: 1899/277000 \t Elapsed time: 302.34 \t Loss:3.87557\n",
      "Iteration: 1900/277000 \t Elapsed time: 302.47 \t Loss:3.86638\n",
      "Iteration: 1901/277000 \t Elapsed time: 302.62 \t Loss:3.85580\n",
      "Iteration: 1902/277000 \t Elapsed time: 302.76 \t Loss:3.84687\n",
      "Iteration: 1903/277000 \t Elapsed time: 302.90 \t Loss:3.83703\n",
      "Iteration: 1904/277000 \t Elapsed time: 303.32 \t Loss:3.83087\n",
      "Iteration: 1905/277000 \t Elapsed time: 303.45 \t Loss:3.82241\n",
      "Iteration: 1906/277000 \t Elapsed time: 303.59 \t Loss:3.81342\n",
      "Iteration: 1907/277000 \t Elapsed time: 303.77 \t Loss:3.80525\n",
      "Iteration: 1908/277000 \t Elapsed time: 303.92 \t Loss:3.79484\n",
      "Iteration: 1909/277000 \t Elapsed time: 304.06 \t Loss:3.78695\n",
      "Iteration: 1910/277000 \t Elapsed time: 304.20 \t Loss:3.78092\n",
      "Iteration: 1911/277000 \t Elapsed time: 304.35 \t Loss:3.77399\n",
      "Iteration: 1912/277000 \t Elapsed time: 304.49 \t Loss:3.76567\n",
      "Iteration: 1913/277000 \t Elapsed time: 304.63 \t Loss:3.75675\n",
      "Iteration: 1914/277000 \t Elapsed time: 304.77 \t Loss:3.74633\n",
      "Iteration: 1915/277000 \t Elapsed time: 304.91 \t Loss:3.73726\n",
      "Iteration: 1916/277000 \t Elapsed time: 305.05 \t Loss:3.72923\n",
      "Iteration: 1917/277000 \t Elapsed time: 305.26 \t Loss:3.72081\n",
      "Iteration: 1918/277000 \t Elapsed time: 305.40 \t Loss:3.71570\n",
      "Iteration: 1919/277000 \t Elapsed time: 305.54 \t Loss:3.70496\n",
      "Iteration: 1920/277000 \t Elapsed time: 305.69 \t Loss:3.69766\n",
      "Iteration: 1921/277000 \t Elapsed time: 305.84 \t Loss:3.68847\n",
      "Iteration: 1922/277000 \t Elapsed time: 306.00 \t Loss:3.67930\n",
      "Iteration: 1923/277000 \t Elapsed time: 306.13 \t Loss:3.67104\n",
      "Iteration: 1924/277000 \t Elapsed time: 306.27 \t Loss:3.66169\n",
      "Iteration: 1925/277000 \t Elapsed time: 306.42 \t Loss:3.65269\n",
      "Iteration: 1926/277000 \t Elapsed time: 306.61 \t Loss:3.66682\n",
      "Iteration: 1927/277000 \t Elapsed time: 306.75 \t Loss:3.66072\n",
      "Iteration: 1928/277000 \t Elapsed time: 306.89 \t Loss:3.63499\n",
      "Iteration: 1929/277000 \t Elapsed time: 307.03 \t Loss:3.64126\n",
      "Iteration: 1930/277000 \t Elapsed time: 307.18 \t Loss:3.64812\n",
      "Iteration: 1931/277000 \t Elapsed time: 307.32 \t Loss:3.65584\n",
      "Iteration: 1932/277000 \t Elapsed time: 307.45 \t Loss:3.65571\n",
      "Iteration: 1933/277000 \t Elapsed time: 307.60 \t Loss:3.63607\n",
      "Iteration: 1934/277000 \t Elapsed time: 307.80 \t Loss:3.62180\n",
      "Iteration: 1935/277000 \t Elapsed time: 307.94 \t Loss:3.64687\n",
      "Iteration: 1936/277000 \t Elapsed time: 308.07 \t Loss:3.69491\n",
      "Iteration: 1937/277000 \t Elapsed time: 308.20 \t Loss:3.59586\n",
      "Iteration: 1938/277000 \t Elapsed time: 308.34 \t Loss:3.60303\n",
      "Iteration: 1939/277000 \t Elapsed time: 308.56 \t Loss:3.61341\n",
      "Iteration: 1940/277000 \t Elapsed time: 308.74 \t Loss:3.62305\n",
      "Iteration: 1941/277000 \t Elapsed time: 308.91 \t Loss:3.66935\n",
      "Iteration: 1942/277000 \t Elapsed time: 309.05 \t Loss:3.55697\n",
      "Iteration: 1943/277000 \t Elapsed time: 309.19 \t Loss:3.57641\n",
      "Iteration: 1944/277000 \t Elapsed time: 309.33 \t Loss:3.56458\n",
      "Iteration: 1945/277000 \t Elapsed time: 309.47 \t Loss:3.52323\n",
      "Iteration: 1946/277000 \t Elapsed time: 309.61 \t Loss:3.55169\n",
      "Iteration: 1947/277000 \t Elapsed time: 309.76 \t Loss:3.53890\n",
      "Iteration: 1948/277000 \t Elapsed time: 309.90 \t Loss:3.52542\n",
      "Iteration: 1949/277000 \t Elapsed time: 310.05 \t Loss:3.48841\n",
      "Iteration: 1950/277000 \t Elapsed time: 310.20 \t Loss:3.49401\n",
      "Iteration: 1951/277000 \t Elapsed time: 310.35 \t Loss:3.49298\n",
      "Iteration: 1952/277000 \t Elapsed time: 310.48 \t Loss:3.46853\n",
      "Iteration: 1953/277000 \t Elapsed time: 310.62 \t Loss:3.45917\n",
      "Iteration: 1954/277000 \t Elapsed time: 310.77 \t Loss:3.45868\n",
      "Iteration: 1955/277000 \t Elapsed time: 310.92 \t Loss:3.44092\n",
      "Iteration: 1956/277000 \t Elapsed time: 311.06 \t Loss:3.43045\n",
      "Iteration: 1957/277000 \t Elapsed time: 311.19 \t Loss:3.42862\n",
      "Iteration: 1958/277000 \t Elapsed time: 311.32 \t Loss:3.41432\n",
      "Iteration: 1959/277000 \t Elapsed time: 311.46 \t Loss:3.40235\n",
      "Iteration: 1960/277000 \t Elapsed time: 311.60 \t Loss:3.40031\n",
      "Iteration: 1961/277000 \t Elapsed time: 311.74 \t Loss:3.38477\n",
      "Iteration: 1962/277000 \t Elapsed time: 311.88 \t Loss:3.37772\n",
      "Iteration: 1963/277000 \t Elapsed time: 312.03 \t Loss:3.36890\n",
      "Iteration: 1964/277000 \t Elapsed time: 312.17 \t Loss:3.36203\n",
      "Iteration: 1965/277000 \t Elapsed time: 312.31 \t Loss:3.35229\n",
      "Iteration: 1966/277000 \t Elapsed time: 312.45 \t Loss:3.34526\n",
      "Iteration: 1967/277000 \t Elapsed time: 312.61 \t Loss:3.33622\n",
      "Iteration: 1968/277000 \t Elapsed time: 312.74 \t Loss:3.32873\n",
      "Iteration: 1969/277000 \t Elapsed time: 312.93 \t Loss:3.32013\n",
      "Iteration: 1970/277000 \t Elapsed time: 313.06 \t Loss:3.31273\n",
      "Iteration: 1971/277000 \t Elapsed time: 313.20 \t Loss:3.30752\n",
      "Iteration: 1972/277000 \t Elapsed time: 313.33 \t Loss:3.29688\n",
      "Iteration: 1973/277000 \t Elapsed time: 313.47 \t Loss:3.28844\n",
      "Iteration: 1974/277000 \t Elapsed time: 313.63 \t Loss:3.28288\n",
      "Iteration: 1975/277000 \t Elapsed time: 313.78 \t Loss:3.27680\n",
      "Iteration: 1976/277000 \t Elapsed time: 313.92 \t Loss:3.26556\n",
      "Iteration: 1977/277000 \t Elapsed time: 314.06 \t Loss:3.25875\n",
      "Iteration: 1978/277000 \t Elapsed time: 314.21 \t Loss:3.25124\n",
      "Iteration: 1979/277000 \t Elapsed time: 314.35 \t Loss:3.24269\n",
      "Iteration: 1980/277000 \t Elapsed time: 314.49 \t Loss:3.23545\n",
      "Iteration: 1981/277000 \t Elapsed time: 314.63 \t Loss:3.22900\n",
      "Iteration: 1982/277000 \t Elapsed time: 314.78 \t Loss:3.22171\n",
      "Iteration: 1983/277000 \t Elapsed time: 314.93 \t Loss:3.21157\n",
      "Iteration: 1984/277000 \t Elapsed time: 315.08 \t Loss:3.20318\n",
      "Iteration: 1985/277000 \t Elapsed time: 315.22 \t Loss:3.19752\n",
      "Iteration: 1986/277000 \t Elapsed time: 315.36 \t Loss:3.18731\n",
      "Iteration: 1987/277000 \t Elapsed time: 315.50 \t Loss:3.18120\n",
      "Iteration: 1988/277000 \t Elapsed time: 315.64 \t Loss:3.17424\n",
      "Iteration: 1989/277000 \t Elapsed time: 315.84 \t Loss:3.16413\n",
      "Iteration: 1990/277000 \t Elapsed time: 316.00 \t Loss:3.16130\n",
      "Iteration: 1991/277000 \t Elapsed time: 316.15 \t Loss:3.15071\n",
      "Iteration: 1992/277000 \t Elapsed time: 316.31 \t Loss:3.14345\n",
      "Iteration: 1993/277000 \t Elapsed time: 316.46 \t Loss:3.13668\n",
      "Iteration: 1994/277000 \t Elapsed time: 316.60 \t Loss:3.13049\n",
      "Iteration: 1995/277000 \t Elapsed time: 316.74 \t Loss:3.11805\n",
      "Iteration: 1996/277000 \t Elapsed time: 316.88 \t Loss:3.11443\n",
      "Iteration: 1997/277000 \t Elapsed time: 317.03 \t Loss:3.10357\n",
      "Iteration: 1998/277000 \t Elapsed time: 317.17 \t Loss:3.09592\n",
      "Iteration: 1999/277000 \t Elapsed time: 317.31 \t Loss:3.09104\n",
      "Iteration: 2000/277000 \t Elapsed time: 317.45 \t Loss:3.07922\n",
      "Start Validating\n",
      "Finish Validating\n",
      "Iteration: 2001/277000 \t Elapsed time: 339.98 \t Loss:3.07227\n",
      "Iteration: 2002/277000 \t Elapsed time: 340.13 \t Loss:3.06455\n",
      "Iteration: 2003/277000 \t Elapsed time: 340.26 \t Loss:3.05681\n",
      "Iteration: 2004/277000 \t Elapsed time: 340.41 \t Loss:3.04913\n",
      "Iteration: 2005/277000 \t Elapsed time: 340.55 \t Loss:3.04509\n",
      "Iteration: 2006/277000 \t Elapsed time: 340.71 \t Loss:3.03813\n",
      "Iteration: 2007/277000 \t Elapsed time: 340.86 \t Loss:3.02944\n",
      "Iteration: 2008/277000 \t Elapsed time: 341.00 \t Loss:3.02738\n",
      "Iteration: 2009/277000 \t Elapsed time: 341.14 \t Loss:3.02341\n",
      "Iteration: 2010/277000 \t Elapsed time: 341.28 \t Loss:3.01974\n",
      "Iteration: 2011/277000 \t Elapsed time: 341.42 \t Loss:3.01389\n",
      "Iteration: 2012/277000 \t Elapsed time: 341.56 \t Loss:3.01236\n",
      "Iteration: 2013/277000 \t Elapsed time: 341.72 \t Loss:3.00271\n",
      "Iteration: 2014/277000 \t Elapsed time: 341.86 \t Loss:2.98348\n",
      "Iteration: 2015/277000 \t Elapsed time: 342.01 \t Loss:2.96822\n",
      "Iteration: 2016/277000 \t Elapsed time: 342.15 \t Loss:2.96470\n",
      "Iteration: 2017/277000 \t Elapsed time: 342.29 \t Loss:2.96043\n",
      "Iteration: 2018/277000 \t Elapsed time: 342.43 \t Loss:2.94977\n",
      "Iteration: 2019/277000 \t Elapsed time: 342.61 \t Loss:2.94052\n",
      "Iteration: 2020/277000 \t Elapsed time: 342.84 \t Loss:2.93256\n",
      "Iteration: 2021/277000 \t Elapsed time: 342.99 \t Loss:2.92384\n",
      "Iteration: 2022/277000 \t Elapsed time: 343.12 \t Loss:2.91892\n",
      "Iteration: 2023/277000 \t Elapsed time: 343.26 \t Loss:2.90790\n",
      "Iteration: 2024/277000 \t Elapsed time: 343.40 \t Loss:2.90017\n",
      "Iteration: 2025/277000 \t Elapsed time: 343.54 \t Loss:2.89588\n",
      "Iteration: 2026/277000 \t Elapsed time: 343.68 \t Loss:2.88682\n",
      "Iteration: 2027/277000 \t Elapsed time: 343.84 \t Loss:2.87616\n",
      "Iteration: 2028/277000 \t Elapsed time: 343.98 \t Loss:2.87024\n",
      "Iteration: 2029/277000 \t Elapsed time: 344.12 \t Loss:2.86153\n",
      "Iteration: 2030/277000 \t Elapsed time: 344.27 \t Loss:2.85509\n",
      "Iteration: 2031/277000 \t Elapsed time: 344.41 \t Loss:2.84647\n",
      "Iteration: 2032/277000 \t Elapsed time: 344.55 \t Loss:2.83970\n",
      "Iteration: 2033/277000 \t Elapsed time: 344.69 \t Loss:2.83335\n",
      "Iteration: 2034/277000 \t Elapsed time: 344.84 \t Loss:2.82615\n",
      "Iteration: 2035/277000 \t Elapsed time: 345.00 \t Loss:2.81780\n",
      "Iteration: 2036/277000 \t Elapsed time: 345.14 \t Loss:2.80926\n",
      "Iteration: 2037/277000 \t Elapsed time: 345.29 \t Loss:2.80072\n",
      "Iteration: 2038/277000 \t Elapsed time: 345.47 \t Loss:2.80026\n",
      "Iteration: 2039/277000 \t Elapsed time: 345.62 \t Loss:2.78988\n",
      "Iteration: 2040/277000 \t Elapsed time: 345.77 \t Loss:2.77996\n",
      "Iteration: 2041/277000 \t Elapsed time: 345.90 \t Loss:2.77577\n",
      "Iteration: 2042/277000 \t Elapsed time: 346.04 \t Loss:2.76920\n",
      "Iteration: 2043/277000 \t Elapsed time: 346.19 \t Loss:2.76542\n",
      "Iteration: 2044/277000 \t Elapsed time: 346.33 \t Loss:2.76796\n",
      "Iteration: 2045/277000 \t Elapsed time: 346.47 \t Loss:2.77340\n",
      "Iteration: 2046/277000 \t Elapsed time: 346.61 \t Loss:2.76686\n",
      "Iteration: 2047/277000 \t Elapsed time: 346.75 \t Loss:2.77051\n",
      "Iteration: 2048/277000 \t Elapsed time: 346.89 \t Loss:2.75946\n",
      "Iteration: 2049/277000 \t Elapsed time: 347.04 \t Loss:2.73577\n",
      "Iteration: 2050/277000 \t Elapsed time: 347.17 \t Loss:2.71993\n",
      "Iteration: 2051/277000 \t Elapsed time: 347.31 \t Loss:2.72172\n",
      "Iteration: 2052/277000 \t Elapsed time: 347.45 \t Loss:2.73827\n",
      "Iteration: 2053/277000 \t Elapsed time: 347.60 \t Loss:2.72076\n",
      "Iteration: 2054/277000 \t Elapsed time: 347.75 \t Loss:2.68794\n",
      "Iteration: 2055/277000 \t Elapsed time: 347.90 \t Loss:2.70067\n",
      "Iteration: 2056/277000 \t Elapsed time: 348.05 \t Loss:2.67495\n",
      "Iteration: 2057/277000 \t Elapsed time: 348.18 \t Loss:2.67407\n",
      "Iteration: 2058/277000 \t Elapsed time: 348.31 \t Loss:2.67892\n",
      "Iteration: 2059/277000 \t Elapsed time: 348.44 \t Loss:2.65522\n",
      "Iteration: 2060/277000 \t Elapsed time: 348.59 \t Loss:2.65284\n",
      "Iteration: 2061/277000 \t Elapsed time: 348.74 \t Loss:2.64053\n",
      "Iteration: 2062/277000 \t Elapsed time: 348.88 \t Loss:2.63530\n",
      "Iteration: 2063/277000 \t Elapsed time: 349.02 \t Loss:2.62194\n",
      "Iteration: 2064/277000 \t Elapsed time: 349.17 \t Loss:2.61506\n",
      "Iteration: 2065/277000 \t Elapsed time: 349.32 \t Loss:2.61167\n",
      "Iteration: 2066/277000 \t Elapsed time: 349.45 \t Loss:2.60733\n",
      "Iteration: 2067/277000 \t Elapsed time: 349.59 \t Loss:2.59649\n",
      "Iteration: 2068/277000 \t Elapsed time: 349.83 \t Loss:2.58969\n",
      "Iteration: 2069/277000 \t Elapsed time: 349.98 \t Loss:2.58403\n",
      "Iteration: 2070/277000 \t Elapsed time: 350.13 \t Loss:2.56941\n",
      "Iteration: 2071/277000 \t Elapsed time: 350.27 \t Loss:2.56530\n",
      "Iteration: 2072/277000 \t Elapsed time: 350.42 \t Loss:2.56598\n",
      "Iteration: 2073/277000 \t Elapsed time: 350.58 \t Loss:2.55383\n",
      "Iteration: 2074/277000 \t Elapsed time: 350.72 \t Loss:2.55581\n",
      "Iteration: 2075/277000 \t Elapsed time: 350.86 \t Loss:2.55780\n",
      "Iteration: 2076/277000 \t Elapsed time: 351.00 \t Loss:2.53191\n",
      "Iteration: 2077/277000 \t Elapsed time: 351.14 \t Loss:2.53082\n",
      "Iteration: 2078/277000 \t Elapsed time: 351.28 \t Loss:2.51372\n",
      "Iteration: 2079/277000 \t Elapsed time: 351.42 \t Loss:2.54488\n",
      "Iteration: 2080/277000 \t Elapsed time: 351.56 \t Loss:2.52726\n",
      "Iteration: 2081/277000 \t Elapsed time: 351.69 \t Loss:2.50472\n",
      "Iteration: 2082/277000 \t Elapsed time: 351.83 \t Loss:2.49866\n",
      "Iteration: 2083/277000 \t Elapsed time: 351.99 \t Loss:2.49280\n",
      "Iteration: 2084/277000 \t Elapsed time: 352.13 \t Loss:2.48933\n",
      "Iteration: 2085/277000 \t Elapsed time: 352.27 \t Loss:2.47828\n",
      "Iteration: 2086/277000 \t Elapsed time: 352.40 \t Loss:2.48009\n",
      "Iteration: 2087/277000 \t Elapsed time: 352.53 \t Loss:2.45650\n",
      "Iteration: 2088/277000 \t Elapsed time: 352.74 \t Loss:2.45423\n",
      "Iteration: 2089/277000 \t Elapsed time: 352.89 \t Loss:2.45003\n",
      "Iteration: 2090/277000 \t Elapsed time: 353.04 \t Loss:2.43120\n",
      "Iteration: 2091/277000 \t Elapsed time: 353.18 \t Loss:2.43259\n",
      "Iteration: 2092/277000 \t Elapsed time: 353.31 \t Loss:2.41946\n",
      "Iteration: 2093/277000 \t Elapsed time: 353.45 \t Loss:2.41012\n",
      "Iteration: 2094/277000 \t Elapsed time: 353.59 \t Loss:2.42669\n",
      "Iteration: 2095/277000 \t Elapsed time: 353.73 \t Loss:2.41061\n",
      "Iteration: 2096/277000 \t Elapsed time: 353.87 \t Loss:2.40293\n",
      "Iteration: 2097/277000 \t Elapsed time: 354.01 \t Loss:2.40986\n",
      "Iteration: 2098/277000 \t Elapsed time: 354.14 \t Loss:2.39258\n",
      "Iteration: 2099/277000 \t Elapsed time: 354.28 \t Loss:2.38151\n",
      "Iteration: 2100/277000 \t Elapsed time: 354.42 \t Loss:2.37324\n",
      "Iteration: 2101/277000 \t Elapsed time: 354.56 \t Loss:2.36901\n",
      "Iteration: 2102/277000 \t Elapsed time: 354.70 \t Loss:2.37336\n",
      "Iteration: 2103/277000 \t Elapsed time: 354.85 \t Loss:2.36238\n",
      "Iteration: 2104/277000 \t Elapsed time: 354.99 \t Loss:2.36680\n",
      "Iteration: 2105/277000 \t Elapsed time: 355.14 \t Loss:2.34768\n",
      "Iteration: 2106/277000 \t Elapsed time: 355.27 \t Loss:2.34035\n",
      "Iteration: 2107/277000 \t Elapsed time: 355.41 \t Loss:2.34290\n",
      "Iteration: 2108/277000 \t Elapsed time: 355.57 \t Loss:2.32046\n",
      "Iteration: 2109/277000 \t Elapsed time: 355.75 \t Loss:2.32082\n",
      "Iteration: 2110/277000 \t Elapsed time: 355.92 \t Loss:2.31298\n",
      "Iteration: 2111/277000 \t Elapsed time: 356.09 \t Loss:2.31524\n",
      "Iteration: 2112/277000 \t Elapsed time: 356.23 \t Loss:2.29207\n",
      "Iteration: 2113/277000 \t Elapsed time: 356.37 \t Loss:2.28826\n",
      "Iteration: 2114/277000 \t Elapsed time: 356.51 \t Loss:2.27645\n",
      "Iteration: 2115/277000 \t Elapsed time: 356.66 \t Loss:2.26396\n",
      "Iteration: 2116/277000 \t Elapsed time: 356.84 \t Loss:2.26001\n",
      "Iteration: 2117/277000 \t Elapsed time: 357.01 \t Loss:2.25073\n",
      "Iteration: 2118/277000 \t Elapsed time: 357.15 \t Loss:2.23889\n",
      "Iteration: 2119/277000 \t Elapsed time: 357.30 \t Loss:2.23468\n",
      "Iteration: 2120/277000 \t Elapsed time: 357.45 \t Loss:2.23507\n",
      "Iteration: 2121/277000 \t Elapsed time: 357.60 \t Loss:2.22115\n",
      "Iteration: 2122/277000 \t Elapsed time: 357.75 \t Loss:2.22940\n",
      "Iteration: 2123/277000 \t Elapsed time: 357.89 \t Loss:2.23174\n",
      "Iteration: 2124/277000 \t Elapsed time: 358.02 \t Loss:2.21638\n",
      "Iteration: 2125/277000 \t Elapsed time: 358.16 \t Loss:2.26816\n",
      "Iteration: 2126/277000 \t Elapsed time: 358.30 \t Loss:2.41531\n",
      "Iteration: 2127/277000 \t Elapsed time: 358.45 \t Loss:2.74485\n",
      "Iteration: 2128/277000 \t Elapsed time: 358.59 \t Loss:3.09603\n",
      "Iteration: 2129/277000 \t Elapsed time: 358.73 \t Loss:2.74407\n",
      "Iteration: 2130/277000 \t Elapsed time: 358.87 \t Loss:2.40190\n",
      "Iteration: 2131/277000 \t Elapsed time: 359.00 \t Loss:2.42637\n",
      "Iteration: 2132/277000 \t Elapsed time: 359.14 \t Loss:2.40518\n",
      "Iteration: 2133/277000 \t Elapsed time: 359.28 \t Loss:2.39388\n",
      "Iteration: 2134/277000 \t Elapsed time: 359.42 \t Loss:2.25076\n",
      "Iteration: 2135/277000 \t Elapsed time: 359.58 \t Loss:2.36105\n",
      "Iteration: 2136/277000 \t Elapsed time: 359.72 \t Loss:2.22794\n",
      "Iteration: 2137/277000 \t Elapsed time: 359.86 \t Loss:2.20346\n",
      "Iteration: 2138/277000 \t Elapsed time: 360.00 \t Loss:2.27699\n",
      "Iteration: 2139/277000 \t Elapsed time: 360.13 \t Loss:2.21161\n",
      "Iteration: 2140/277000 \t Elapsed time: 360.27 \t Loss:2.22535\n",
      "Iteration: 2141/277000 \t Elapsed time: 360.41 \t Loss:2.19551\n",
      "Iteration: 2142/277000 \t Elapsed time: 360.59 \t Loss:2.16011\n",
      "Iteration: 2143/277000 \t Elapsed time: 360.73 \t Loss:2.17377\n",
      "Iteration: 2144/277000 \t Elapsed time: 360.87 \t Loss:2.19133\n",
      "Iteration: 2145/277000 \t Elapsed time: 361.03 \t Loss:2.14098\n",
      "Iteration: 2146/277000 \t Elapsed time: 361.17 \t Loss:2.14025\n",
      "Iteration: 2147/277000 \t Elapsed time: 361.31 \t Loss:2.11172\n",
      "Iteration: 2148/277000 \t Elapsed time: 361.45 \t Loss:2.11288\n",
      "Iteration: 2149/277000 \t Elapsed time: 361.60 \t Loss:2.08685\n",
      "Iteration: 2150/277000 \t Elapsed time: 361.74 \t Loss:2.11191\n",
      "Iteration: 2151/277000 \t Elapsed time: 361.88 \t Loss:2.09672\n",
      "Iteration: 2152/277000 \t Elapsed time: 362.03 \t Loss:2.07321\n",
      "Iteration: 2153/277000 \t Elapsed time: 362.17 \t Loss:2.07019\n",
      "Iteration: 2154/277000 \t Elapsed time: 362.31 \t Loss:2.05845\n",
      "Iteration: 2155/277000 \t Elapsed time: 362.45 \t Loss:2.04626\n",
      "Iteration: 2156/277000 \t Elapsed time: 362.66 \t Loss:2.04342\n",
      "Iteration: 2157/277000 \t Elapsed time: 362.80 \t Loss:2.04656\n",
      "Iteration: 2158/277000 \t Elapsed time: 362.94 \t Loss:2.05604\n",
      "Iteration: 2159/277000 \t Elapsed time: 363.07 \t Loss:2.01947\n",
      "Iteration: 2160/277000 \t Elapsed time: 363.21 \t Loss:2.02322\n",
      "Iteration: 2161/277000 \t Elapsed time: 363.35 \t Loss:2.02146\n",
      "Iteration: 2162/277000 \t Elapsed time: 363.49 \t Loss:2.01298\n",
      "Iteration: 2163/277000 \t Elapsed time: 363.63 \t Loss:2.00155\n",
      "Iteration: 2164/277000 \t Elapsed time: 363.78 \t Loss:1.98696\n",
      "Iteration: 2165/277000 \t Elapsed time: 363.93 \t Loss:1.98194\n",
      "Iteration: 2166/277000 \t Elapsed time: 364.07 \t Loss:1.97384\n",
      "Iteration: 2167/277000 \t Elapsed time: 364.20 \t Loss:1.96477\n",
      "Iteration: 2168/277000 \t Elapsed time: 364.34 \t Loss:1.98950\n",
      "Iteration: 2169/277000 \t Elapsed time: 364.47 \t Loss:1.95685\n",
      "Iteration: 2170/277000 \t Elapsed time: 364.62 \t Loss:1.97356\n",
      "Iteration: 2171/277000 \t Elapsed time: 364.80 \t Loss:1.94725\n",
      "Iteration: 2172/277000 \t Elapsed time: 364.94 \t Loss:1.95139\n",
      "Iteration: 2173/277000 \t Elapsed time: 365.08 \t Loss:1.93127\n",
      "Iteration: 2174/277000 \t Elapsed time: 365.22 \t Loss:1.92828\n",
      "Iteration: 2175/277000 \t Elapsed time: 365.36 \t Loss:1.91534\n",
      "Iteration: 2176/277000 \t Elapsed time: 365.50 \t Loss:1.92902\n",
      "Iteration: 2177/277000 \t Elapsed time: 365.64 \t Loss:1.90312\n",
      "Iteration: 2178/277000 \t Elapsed time: 365.78 \t Loss:1.92439\n",
      "Iteration: 2179/277000 \t Elapsed time: 365.99 \t Loss:1.91039\n",
      "Iteration: 2180/277000 \t Elapsed time: 366.13 \t Loss:1.89004\n",
      "Iteration: 2181/277000 \t Elapsed time: 366.26 \t Loss:1.91899\n",
      "Iteration: 2182/277000 \t Elapsed time: 366.40 \t Loss:1.87744\n",
      "Iteration: 2183/277000 \t Elapsed time: 366.54 \t Loss:1.87225\n",
      "Iteration: 2184/277000 \t Elapsed time: 366.68 \t Loss:1.86722\n",
      "Iteration: 2185/277000 \t Elapsed time: 366.83 \t Loss:1.86202\n",
      "Iteration: 2186/277000 \t Elapsed time: 366.97 \t Loss:1.86032\n",
      "Iteration: 2187/277000 \t Elapsed time: 367.12 \t Loss:1.84231\n",
      "Iteration: 2188/277000 \t Elapsed time: 367.25 \t Loss:1.86584\n",
      "Iteration: 2189/277000 \t Elapsed time: 367.40 \t Loss:1.82928\n",
      "Iteration: 2190/277000 \t Elapsed time: 367.53 \t Loss:1.82664\n",
      "Iteration: 2191/277000 \t Elapsed time: 367.68 \t Loss:1.82293\n",
      "Iteration: 2192/277000 \t Elapsed time: 367.81 \t Loss:1.81016\n",
      "Iteration: 2193/277000 \t Elapsed time: 367.95 \t Loss:1.80506\n",
      "Iteration: 2194/277000 \t Elapsed time: 368.08 \t Loss:1.82000\n",
      "Iteration: 2195/277000 \t Elapsed time: 368.22 \t Loss:1.79005\n",
      "Iteration: 2196/277000 \t Elapsed time: 368.36 \t Loss:1.81013\n",
      "Iteration: 2197/277000 \t Elapsed time: 368.50 \t Loss:1.78281\n",
      "Iteration: 2198/277000 \t Elapsed time: 368.64 \t Loss:1.77307\n",
      "Iteration: 2199/277000 \t Elapsed time: 368.79 \t Loss:1.78308\n",
      "Iteration: 2200/277000 \t Elapsed time: 368.93 \t Loss:1.76212\n",
      "Iteration: 2201/277000 \t Elapsed time: 369.07 \t Loss:1.77841\n",
      "Iteration: 2202/277000 \t Elapsed time: 369.21 \t Loss:1.76259\n",
      "Iteration: 2203/277000 \t Elapsed time: 369.35 \t Loss:1.74612\n",
      "Iteration: 2204/277000 \t Elapsed time: 369.49 \t Loss:1.73713\n",
      "Iteration: 2205/277000 \t Elapsed time: 369.63 \t Loss:1.74440\n",
      "Iteration: 2206/277000 \t Elapsed time: 369.78 \t Loss:1.76641\n",
      "Iteration: 2207/277000 \t Elapsed time: 369.92 \t Loss:1.71929\n",
      "Iteration: 2208/277000 \t Elapsed time: 370.06 \t Loss:1.71409\n",
      "Iteration: 2209/277000 \t Elapsed time: 370.20 \t Loss:1.70641\n",
      "Iteration: 2210/277000 \t Elapsed time: 370.34 \t Loss:1.70620\n",
      "Iteration: 2211/277000 \t Elapsed time: 370.48 \t Loss:1.70769\n",
      "Iteration: 2212/277000 \t Elapsed time: 370.62 \t Loss:1.69057\n",
      "Iteration: 2213/277000 \t Elapsed time: 370.76 \t Loss:1.68549\n",
      "Iteration: 2214/277000 \t Elapsed time: 370.90 \t Loss:1.69111\n",
      "Iteration: 2215/277000 \t Elapsed time: 371.04 \t Loss:1.67467\n",
      "Iteration: 2216/277000 \t Elapsed time: 371.19 \t Loss:1.66504\n",
      "Iteration: 2217/277000 \t Elapsed time: 371.34 \t Loss:1.68923\n",
      "Iteration: 2218/277000 \t Elapsed time: 371.49 \t Loss:1.69658\n",
      "Iteration: 2219/277000 \t Elapsed time: 371.64 \t Loss:1.65054\n",
      "Iteration: 2220/277000 \t Elapsed time: 371.78 \t Loss:1.65046\n",
      "Iteration: 2221/277000 \t Elapsed time: 371.92 \t Loss:1.63977\n",
      "Iteration: 2222/277000 \t Elapsed time: 372.05 \t Loss:1.63627\n",
      "Iteration: 2223/277000 \t Elapsed time: 372.18 \t Loss:1.66956\n",
      "Iteration: 2224/277000 \t Elapsed time: 372.35 \t Loss:1.61897\n",
      "Iteration: 2225/277000 \t Elapsed time: 372.86 \t Loss:1.61451\n",
      "Iteration: 2226/277000 \t Elapsed time: 373.02 \t Loss:1.60891\n",
      "Iteration: 2227/277000 \t Elapsed time: 373.17 \t Loss:1.60044\n",
      "Iteration: 2228/277000 \t Elapsed time: 373.32 \t Loss:1.59319\n",
      "Iteration: 2229/277000 \t Elapsed time: 373.46 \t Loss:1.58887\n",
      "Iteration: 2230/277000 \t Elapsed time: 373.61 \t Loss:1.58211\n",
      "Iteration: 2231/277000 \t Elapsed time: 373.75 \t Loss:1.57648\n",
      "Iteration: 2232/277000 \t Elapsed time: 373.89 \t Loss:1.56987\n",
      "Iteration: 2233/277000 \t Elapsed time: 374.03 \t Loss:1.56358\n",
      "Iteration: 2234/277000 \t Elapsed time: 374.17 \t Loss:1.55655\n",
      "Iteration: 2235/277000 \t Elapsed time: 374.31 \t Loss:1.55840\n",
      "Iteration: 2236/277000 \t Elapsed time: 374.44 \t Loss:1.54706\n",
      "Iteration: 2237/277000 \t Elapsed time: 374.58 \t Loss:1.54428\n",
      "Iteration: 2238/277000 \t Elapsed time: 374.72 \t Loss:1.56803\n",
      "Iteration: 2239/277000 \t Elapsed time: 374.87 \t Loss:1.58474\n",
      "Iteration: 2240/277000 \t Elapsed time: 375.01 \t Loss:1.52484\n",
      "Iteration: 2241/277000 \t Elapsed time: 375.16 \t Loss:1.52307\n",
      "Iteration: 2242/277000 \t Elapsed time: 375.32 \t Loss:1.51481\n",
      "Iteration: 2243/277000 \t Elapsed time: 375.45 \t Loss:1.50664\n",
      "Iteration: 2244/277000 \t Elapsed time: 375.60 \t Loss:1.52632\n",
      "Iteration: 2245/277000 \t Elapsed time: 375.75 \t Loss:1.50026\n",
      "Iteration: 2246/277000 \t Elapsed time: 375.89 \t Loss:1.50812\n",
      "Iteration: 2247/277000 \t Elapsed time: 376.04 \t Loss:1.58828\n",
      "Iteration: 2248/277000 \t Elapsed time: 376.18 \t Loss:1.53143\n",
      "Iteration: 2249/277000 \t Elapsed time: 376.32 \t Loss:1.52056\n",
      "Iteration: 2250/277000 \t Elapsed time: 376.61 \t Loss:1.49116\n",
      "Iteration: 2251/277000 \t Elapsed time: 376.74 \t Loss:1.48995\n",
      "Iteration: 2252/277000 \t Elapsed time: 376.88 \t Loss:1.57144\n",
      "Iteration: 2253/277000 \t Elapsed time: 377.01 \t Loss:1.49673\n",
      "Iteration: 2254/277000 \t Elapsed time: 377.15 \t Loss:1.47108\n",
      "Iteration: 2255/277000 \t Elapsed time: 377.30 \t Loss:1.46920\n",
      "Iteration: 2256/277000 \t Elapsed time: 377.58 \t Loss:1.49007\n",
      "Iteration: 2257/277000 \t Elapsed time: 377.73 \t Loss:1.44965\n",
      "Iteration: 2258/277000 \t Elapsed time: 377.86 \t Loss:1.46317\n",
      "Iteration: 2259/277000 \t Elapsed time: 378.00 \t Loss:1.42944\n",
      "Iteration: 2260/277000 \t Elapsed time: 378.13 \t Loss:1.41818\n",
      "Iteration: 2261/277000 \t Elapsed time: 378.27 \t Loss:1.44373\n",
      "Iteration: 2262/277000 \t Elapsed time: 378.42 \t Loss:1.44696\n",
      "Iteration: 2263/277000 \t Elapsed time: 378.56 \t Loss:1.40954\n",
      "Iteration: 2264/277000 \t Elapsed time: 378.70 \t Loss:1.40672\n",
      "Iteration: 2265/277000 \t Elapsed time: 378.84 \t Loss:1.39712\n",
      "Iteration: 2266/277000 \t Elapsed time: 378.98 \t Loss:1.41726\n",
      "Iteration: 2267/277000 \t Elapsed time: 379.13 \t Loss:1.38447\n",
      "Iteration: 2268/277000 \t Elapsed time: 379.27 \t Loss:1.40112\n",
      "Iteration: 2269/277000 \t Elapsed time: 379.41 \t Loss:1.43043\n",
      "Iteration: 2270/277000 \t Elapsed time: 379.56 \t Loss:1.36568\n",
      "Iteration: 2271/277000 \t Elapsed time: 379.70 \t Loss:1.40613\n",
      "Iteration: 2272/277000 \t Elapsed time: 379.84 \t Loss:1.37080\n",
      "Iteration: 2273/277000 \t Elapsed time: 379.98 \t Loss:1.34577\n",
      "Iteration: 2274/277000 \t Elapsed time: 380.13 \t Loss:1.35458\n",
      "Iteration: 2275/277000 \t Elapsed time: 380.27 \t Loss:1.37668\n",
      "Iteration: 2276/277000 \t Elapsed time: 380.41 \t Loss:1.33611\n",
      "Iteration: 2277/277000 \t Elapsed time: 380.55 \t Loss:1.34586\n",
      "Iteration: 2278/277000 \t Elapsed time: 380.69 \t Loss:1.37261\n",
      "Iteration: 2279/277000 \t Elapsed time: 380.87 \t Loss:1.32217\n",
      "Iteration: 2280/277000 \t Elapsed time: 381.02 \t Loss:1.32546\n",
      "Iteration: 2281/277000 \t Elapsed time: 381.16 \t Loss:1.30289\n",
      "Iteration: 2282/277000 \t Elapsed time: 381.29 \t Loss:1.34421\n",
      "Iteration: 2283/277000 \t Elapsed time: 381.43 \t Loss:1.34336\n",
      "Iteration: 2284/277000 \t Elapsed time: 381.58 \t Loss:1.28121\n",
      "Iteration: 2285/277000 \t Elapsed time: 381.72 \t Loss:1.30788\n",
      "Iteration: 2286/277000 \t Elapsed time: 381.90 \t Loss:1.28790\n",
      "Iteration: 2287/277000 \t Elapsed time: 382.04 \t Loss:1.27147\n",
      "Iteration: 2288/277000 \t Elapsed time: 382.18 \t Loss:1.30349\n",
      "Iteration: 2289/277000 \t Elapsed time: 382.32 \t Loss:1.25875\n",
      "Iteration: 2290/277000 \t Elapsed time: 382.46 \t Loss:1.29904\n",
      "Iteration: 2291/277000 \t Elapsed time: 382.64 \t Loss:1.25086\n",
      "Iteration: 2292/277000 \t Elapsed time: 382.81 \t Loss:1.23896\n",
      "Iteration: 2293/277000 \t Elapsed time: 382.95 \t Loss:1.29479\n",
      "Iteration: 2294/277000 \t Elapsed time: 383.08 \t Loss:1.26765\n",
      "Iteration: 2295/277000 \t Elapsed time: 383.23 \t Loss:1.30409\n",
      "Iteration: 2296/277000 \t Elapsed time: 383.38 \t Loss:1.26908\n",
      "Iteration: 2297/277000 \t Elapsed time: 383.52 \t Loss:1.28274\n",
      "Iteration: 2298/277000 \t Elapsed time: 383.65 \t Loss:1.25471\n",
      "Iteration: 2299/277000 \t Elapsed time: 383.79 \t Loss:1.21534\n",
      "Iteration: 2300/277000 \t Elapsed time: 383.94 \t Loss:1.24828\n",
      "Iteration: 2301/277000 \t Elapsed time: 384.07 \t Loss:1.23436\n",
      "Iteration: 2302/277000 \t Elapsed time: 384.21 \t Loss:1.19893\n",
      "Iteration: 2303/277000 \t Elapsed time: 384.35 \t Loss:1.22848\n",
      "Iteration: 2304/277000 \t Elapsed time: 384.49 \t Loss:1.23259\n",
      "Iteration: 2305/277000 \t Elapsed time: 384.63 \t Loss:1.20355\n",
      "Iteration: 2306/277000 \t Elapsed time: 384.78 \t Loss:1.21278\n",
      "Iteration: 2307/277000 \t Elapsed time: 384.93 \t Loss:1.20713\n",
      "Iteration: 2308/277000 \t Elapsed time: 385.08 \t Loss:1.16330\n",
      "Iteration: 2309/277000 \t Elapsed time: 385.22 \t Loss:1.15019\n",
      "Iteration: 2310/277000 \t Elapsed time: 385.36 \t Loss:1.18919\n",
      "Iteration: 2311/277000 \t Elapsed time: 385.50 \t Loss:1.14589\n",
      "Iteration: 2312/277000 \t Elapsed time: 385.64 \t Loss:1.13477\n",
      "Iteration: 2313/277000 \t Elapsed time: 385.78 \t Loss:1.12891\n",
      "Iteration: 2314/277000 \t Elapsed time: 385.92 \t Loss:1.13837\n",
      "Iteration: 2315/277000 \t Elapsed time: 386.06 \t Loss:1.12465\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[85], line 119\u001b[0m\n\u001b[1;32m    117\u001b[0m \u001b[38;5;66;03m# begin to train\u001b[39;00m\n\u001b[1;32m    118\u001b[0m trainer \u001b[38;5;241m=\u001b[39m Trainer(model, optim, scheduler, training_set, valid_set, args, cuda)\n\u001b[0;32m--> 119\u001b[0m \u001b[43mtrainer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[81], line 170\u001b[0m, in \u001b[0;36mTrainer.train\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    167\u001b[0m     y \u001b[38;5;241m=\u001b[39m y\u001b[38;5;241m.\u001b[39mcuda()\n\u001b[1;32m    169\u001b[0m \u001b[38;5;66;03m# forward\u001b[39;00m\n\u001b[0;32m--> 170\u001b[0m z, nll \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgraph\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    173\u001b[0m \u001b[38;5;66;03m# loss\u001b[39;00m\n\u001b[1;32m    174\u001b[0m loss \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mmean(nll)\n",
      "File \u001b[0;32m~/.local/lib/python3.9/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1502\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "Cell \u001b[0;32mIn[28], line 177\u001b[0m, in \u001b[0;36mCondGlowModel.forward\u001b[0;34m(self, x, y, eps_std, reverse)\u001b[0m\n\u001b[1;32m    175\u001b[0m logdet \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mzeros_like(y[:, \u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m0\u001b[39m])\n\u001b[1;32m    176\u001b[0m logdet \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;28mfloat\u001b[39m(\u001b[38;5;241m-\u001b[39mnp\u001b[38;5;241m.\u001b[39mlog(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mn_bins) \u001b[38;5;241m*\u001b[39m dimensions)\n\u001b[0;32m--> 177\u001b[0m z, objective \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mflow\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlogdet\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlogdet\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mreverse\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[1;32m    178\u001b[0m mean, logs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mprior()\n\u001b[1;32m    179\u001b[0m objective \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m GaussianDiag\u001b[38;5;241m.\u001b[39mlogp(mean, logs, z)\n",
      "File \u001b[0;32m~/.local/lib/python3.9/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1502\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "Cell \u001b[0;32mIn[28], line 99\u001b[0m, in \u001b[0;36mCondGlow.forward\u001b[0;34m(self, x, y, logdet, reverse, eps_std)\u001b[0m\n\u001b[1;32m     97\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, x, y, logdet\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.0\u001b[39m, reverse\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m, eps_std\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1.0\u001b[39m):\n\u001b[1;32m     98\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m reverse \u001b[38;5;241m==\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m:\n\u001b[0;32m---> 99\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mencode\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlogdet\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    100\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    101\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdecode(x, y, logdet, eps_std)\n",
      "Cell \u001b[0;32mIn[28], line 109\u001b[0m, in \u001b[0;36mCondGlow.encode\u001b[0;34m(self, x, y, logdet)\u001b[0m\n\u001b[1;32m    106\u001b[0m         y, logdet \u001b[38;5;241m=\u001b[39m layer(y, logdet, reverse\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[1;32m    108\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 109\u001b[0m         y, logdet \u001b[38;5;241m=\u001b[39m \u001b[43mlayer\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlogdet\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mreverse\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[1;32m    110\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m y, logdet\n",
      "File \u001b[0;32m~/.local/lib/python3.9/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1502\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "Cell \u001b[0;32mIn[28], line 37\u001b[0m, in \u001b[0;36mCondGlowStep.forward\u001b[0;34m(self, x, y, logdet, reverse)\u001b[0m\n\u001b[1;32m     34\u001b[0m y, logdet \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39minvconv(x, y, logdet, reverse\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[1;32m     36\u001b[0m \u001b[38;5;66;03m# 3. cond-affine\u001b[39;00m\n\u001b[0;32m---> 37\u001b[0m y, logdet \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43maffine\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlogdet\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mreverse\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[1;32m     39\u001b[0m \u001b[38;5;66;03m# Return\u001b[39;00m\n\u001b[1;32m     40\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m y, logdet\n",
      "File \u001b[0;32m~/.local/lib/python3.9/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1502\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "Cell \u001b[0;32mIn[27], line 291\u001b[0m, in \u001b[0;36mCondAffineCoupling.forward\u001b[0;34m(self, x, y, logdet, reverse)\u001b[0m\n\u001b[1;32m    287\u001b[0m \u001b[38;5;66;03m# print(\"xshape, z1shape\", x.shape, z1.shape)\u001b[39;00m\n\u001b[1;32m    288\u001b[0m \u001b[38;5;66;03m# print(x)\u001b[39;00m\n\u001b[1;32m    290\u001b[0m h \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mcat((x,z1), dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[0;32m--> 291\u001b[0m h \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mf\u001b[49m\u001b[43m(\u001b[49m\u001b[43mh\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    292\u001b[0m shift, scale \u001b[38;5;241m=\u001b[39m split_feature(h, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcross\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    293\u001b[0m scale \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39msigmoid(scale \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m2.\u001b[39m)\n",
      "File \u001b[0;32m~/.local/lib/python3.9/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1502\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/.local/lib/python3.9/site-packages/torch/nn/modules/container.py:217\u001b[0m, in \u001b[0;36mSequential.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    215\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m):\n\u001b[1;32m    216\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m module \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m:\n\u001b[0;32m--> 217\u001b[0m         \u001b[38;5;28minput\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[43mmodule\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m    218\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28minput\u001b[39m\n",
      "File \u001b[0;32m~/.local/lib/python3.9/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1502\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "Cell \u001b[0;32mIn[27], line 229\u001b[0m, in \u001b[0;36mConv2dNormy.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    227\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m):\n\u001b[1;32m    228\u001b[0m     x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39mforward(\u001b[38;5;28minput\u001b[39m)\n\u001b[0;32m--> 229\u001b[0m     x,_ \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mactnorm\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    230\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m x\n",
      "File \u001b[0;32m~/.local/lib/python3.9/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1502\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "Cell \u001b[0;32mIn[27], line 26\u001b[0m, in \u001b[0;36mActNorm.forward\u001b[0;34m(self, input, logdet, reverse)\u001b[0m\n\u001b[1;32m     24\u001b[0m     \u001b[38;5;28minput\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28minput\u001b[39m \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbias\n\u001b[1;32m     25\u001b[0m     \u001b[38;5;28minput\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28minput\u001b[39m \u001b[38;5;241m*\u001b[39m torch\u001b[38;5;241m.\u001b[39mexp(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlogs)\n\u001b[0;32m---> 26\u001b[0m     dlogdet \u001b[38;5;241m=\u001b[39m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msum\u001b[49m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlogs) \u001b[38;5;241m*\u001b[39m dimentions\n\u001b[1;32m     27\u001b[0m     logdet \u001b[38;5;241m=\u001b[39m logdet \u001b[38;5;241m+\u001b[39m dlogdet\n\u001b[1;32m     29\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m reverse \u001b[38;5;241m==\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m:\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# @title Train\n",
    "import argparse\n",
    "import torch\n",
    "from torchvision import datasets\n",
    "# import Learner\n",
    "# import datasets\n",
    "# import utils\n",
    "# from CGlowModel import CondGlowModel\n",
    "\n",
    "\n",
    "if __name__==\"__main__\":\n",
    "\n",
    "    parser = argparse.ArgumentParser(description='train c-Glow')\n",
    "\n",
    "    # input output path\n",
    "    parser.add_argument(\"-d\", \"--dataset_name\", type=str, default=\"horse\")\n",
    "    parser.add_argument(\"-r\", \"--dataset_root\", type=str, default=\"weizmann_horse_db\")\n",
    "\n",
    "    # log root\n",
    "    parser.add_argument(\"--log_root\", type=str, default=\"\")\n",
    "\n",
    "    # C-Glow parameters\n",
    "    # we are calculating Y GIVEN X\n",
    "    parser.add_argument(\"--x_size\", type=tuple, default=(1,8,8))\n",
    "    parser.add_argument(\"--y_size\", type=tuple, default=(1,16,16))\n",
    "    parser.add_argument(\"--x_hidden_channels\", type=int, default=128)\n",
    "    parser.add_argument(\"--x_hidden_size\", type=int, default=64)\n",
    "    parser.add_argument(\"--y_hidden_channels\", type=int, default=256)\n",
    "    parser.add_argument(\"-K\", \"--depth_flow\", type=int, default=8)\n",
    "    parser.add_argument(\"-L\", \"--num_levels\", type=int, default=3)\n",
    "    parser.add_argument(\"--learn_top\", type=bool, default=False)\n",
    "\n",
    "    # Dataset preprocess parameters\n",
    "    parser.add_argument(\"--label_scale\", type=float, default=1)\n",
    "    parser.add_argument(\"--label_bias\", type=float, default=0.5)\n",
    "    parser.add_argument(\"--x_bins\", type=float, default=256.0)\n",
    "    parser.add_argument(\"--y_bins\", type=float, default=2.0)\n",
    "\n",
    "\n",
    "    # Optimizer parameters\n",
    "    parser.add_argument(\"--optimizer\", type=str, default=\"adam\")\n",
    "    parser.add_argument(\"--lr\", type=float, default=0.0002)\n",
    "    parser.add_argument(\"--betas\", type=tuple, default=(0.9,0.9999))\n",
    "    parser.add_argument(\"--eps\", type=float, default=1e-8)\n",
    "    parser.add_argument(\"--regularizer\", type=float, default=0.0)\n",
    "    parser.add_argument(\"--num_steps\", type=int, default=0)\n",
    "\n",
    "    # Trainer parameters\n",
    "    parser.add_argument(\"--num_epochs\", type=int, default=1000)\n",
    "    parser.add_argument(\"--batch_size\", type=int, default=2)\n",
    "    parser.add_argument(\"--max_grad_clip\", type=float, default=5)\n",
    "    parser.add_argument(\"--max_grad_norm\", type=float, default=0)\n",
    "    parser.add_argument(\"--checkpoints_gap\", type=int, default=1000)\n",
    "    parser.add_argument(\"--nll_gap\", type=int, default=1)\n",
    "    parser.add_argument(\"--inference_gap\", type=int, default=1000)\n",
    "    parser.add_argument(\"--save_gap\", type=int, default=1000)\n",
    "\n",
    "    # model path\n",
    "    parser.add_argument(\"--model_path\", type=str, default=\"\")#\"/pdo/users/jlupoiii/TESS/model_conditional_norm_flow/log_20230821_2340/checkpoints/checkpoint_4000.pth.tar\") # \"/pdo/users/jlupoiii/TESS/model_conditional_norm_flow/log_20230821_2340/checkpoints/checkpoint_4000.pth.tar\"\n",
    "\n",
    "    # args = parser.parse_args()\n",
    "    args = parser.parse_known_args()[0]\n",
    "    cuda = torch.cuda.is_available()\n",
    "\n",
    "\n",
    "    # # HORSE DATASET\n",
    "    # dataset_root = 'weizmann_horse_db/'\n",
    "    # # Define any image transformations you want to apply (resize, normalization, etc.)\n",
    "    # transform = transforms.Compose([\n",
    "    #     transforms.Resize((64, 64)),\n",
    "    #     transforms.ToTensor(),\n",
    "    #     # transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "    # ])\n",
    "    # target_transform = transforms.Compose([\n",
    "    #     transforms.Resize((64, 64)),\n",
    "    #     transforms.ToTensor(),\n",
    "    #     # transforms.Normalize(mean=[0.456], std=[0.225])\n",
    "    # ])\n",
    "    # # Create the dataset\n",
    "    # training_set = WeizmannHorseDataset(root_dir=dataset_root, transform=transform, target_transform=target_transform)\n",
    "    # valid_set = WeizmannHorseDataset(root_dir=dataset_root, transform=transform, target_transform=target_transform)\n",
    "\n",
    "    # create TESS dataset\n",
    "    torch.manual_seed(42)\n",
    "    full_dataset = TESSDataset()\n",
    "    train_ratio = 0.8\n",
    "    num_train_samples = int(train_ratio * len(full_dataset))\n",
    "    num_valid_samples = len(full_dataset) - num_train_samples\n",
    "    training_set, valid_set = random_split(full_dataset, [num_train_samples, num_valid_samples])\n",
    "\n",
    "    model = CondGlowModel(args)\n",
    "    if cuda:\n",
    "        model = model.cuda()\n",
    "\n",
    "    # from utils import count_parameters\n",
    "    print(\"number of param: {}\".format(count_parameters(model)))\n",
    "\n",
    "    # optimizer\n",
    "    optim = torch.optim.Adam(model.parameters(), lr=args.lr,betas=args.betas, weight_decay=args.regularizer)\n",
    "\n",
    "    # scheduler\n",
    "    scheduler = None\n",
    "\n",
    "    if args.model_path != \"\":\n",
    "        state = load_state(args.model_path, cuda)\n",
    "        optim.load_state_dict(state[\"optim\"])\n",
    "        model.load_state_dict(state[\"model\"])\n",
    "        args.steps = state[\"iteration\"] + 1\n",
    "        if scheduler is not None and state.get(\"scheduler\", None) is not None:\n",
    "            scheduler.load_state_dict(state[\"scheduler\"])\n",
    "        del state\n",
    "        print('Realoaded model stored at', args.model_path, 'starting at step', args.steps)\n",
    "\n",
    "    # begin to train\n",
    "    trainer = Trainer(model, optim, scheduler, training_set, valid_set, args, cuda)\n",
    "    trainer.train()\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "-pMvfdKknvWb"
   },
   "outputs": [],
   "source": [
    "# plt.imshow(y_sample.cpu()[0][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {
    "id": "E0vt1suwnvYt"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "load to gpu\n",
      "Batch IDs: 0\n",
      "Samples: 0/10\n",
      "Samples: 1/10\n",
      "Samples: 2/10\n",
      "Samples: 3/10\n",
      "Samples: 4/10\n",
      "Samples: 5/10\n",
      "Samples: 6/10\n",
      "Samples: 7/10\n",
      "Samples: 8/10\n",
      "Samples: 9/10\n"
     ]
    },
    {
     "ename": "PermissionError",
     "evalue": "[Errno 13] Permission denied: 'out_dir/batch_0-0.pdf'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mPermissionError\u001b[0m                           Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[89], line 70\u001b[0m\n\u001b[1;32m     67\u001b[0m predictor \u001b[38;5;241m=\u001b[39m Inferencer(model, valid_set, args, cuda)\n\u001b[1;32m     69\u001b[0m \u001b[38;5;66;03m# predict\u001b[39;00m\n\u001b[0;32m---> 70\u001b[0m \u001b[43mpredictor\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msampled_based_prediction\u001b[49m\u001b[43m(\u001b[49m\u001b[43margs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnum_samples\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[88], line 353\u001b[0m, in \u001b[0;36mInferencer.sampled_based_prediction\u001b[0;34m(self, n_samples)\u001b[0m\n\u001b[1;32m    350\u001b[0m im_pred \u001b[38;5;241m=\u001b[39m axes[\u001b[38;5;241m2\u001b[39m]\u001b[38;5;241m.\u001b[39mimshow(pred_img\u001b[38;5;241m.\u001b[39mcpu()[\u001b[38;5;241m0\u001b[39m], cmap\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mgray\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    351\u001b[0m fig\u001b[38;5;241m.\u001b[39mcolorbar(im_pred, fraction\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.046\u001b[39m, pad\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.04\u001b[39m)\n\u001b[0;32m--> 353\u001b[0m \u001b[43mfig\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msavefig\u001b[49m\u001b[43m(\u001b[49m\u001b[43mos\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpath\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mjoin\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mout_root\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43mf\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mbatch_\u001b[39;49m\u001b[38;5;132;43;01m{\u001b[39;49;00m\u001b[43mi_batch\u001b[49m\u001b[38;5;132;43;01m}\u001b[39;49;00m\u001b[38;5;124;43m-\u001b[39;49m\u001b[38;5;132;43;01m{\u001b[39;49;00m\u001b[43mi\u001b[49m\u001b[38;5;132;43;01m}\u001b[39;49;00m\u001b[38;5;124;43m.pdf\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    354\u001b[0m plt\u001b[38;5;241m.\u001b[39mclose()\n",
      "File \u001b[0;32m~/.local/lib/python3.9/site-packages/matplotlib/figure.py:3378\u001b[0m, in \u001b[0;36mFigure.savefig\u001b[0;34m(self, fname, transparent, **kwargs)\u001b[0m\n\u001b[1;32m   3374\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m ax \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39maxes:\n\u001b[1;32m   3375\u001b[0m         stack\u001b[38;5;241m.\u001b[39menter_context(\n\u001b[1;32m   3376\u001b[0m             ax\u001b[38;5;241m.\u001b[39mpatch\u001b[38;5;241m.\u001b[39m_cm_set(facecolor\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mnone\u001b[39m\u001b[38;5;124m'\u001b[39m, edgecolor\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mnone\u001b[39m\u001b[38;5;124m'\u001b[39m))\n\u001b[0;32m-> 3378\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcanvas\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mprint_figure\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfname\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.local/lib/python3.9/site-packages/matplotlib/backend_bases.py:2366\u001b[0m, in \u001b[0;36mFigureCanvasBase.print_figure\u001b[0;34m(self, filename, dpi, facecolor, edgecolor, orientation, format, bbox_inches, pad_inches, bbox_extra_artists, backend, **kwargs)\u001b[0m\n\u001b[1;32m   2362\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   2363\u001b[0m     \u001b[38;5;66;03m# _get_renderer may change the figure dpi (as vector formats\u001b[39;00m\n\u001b[1;32m   2364\u001b[0m     \u001b[38;5;66;03m# force the figure dpi to 72), so we need to set it again here.\u001b[39;00m\n\u001b[1;32m   2365\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m cbook\u001b[38;5;241m.\u001b[39m_setattr_cm(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfigure, dpi\u001b[38;5;241m=\u001b[39mdpi):\n\u001b[0;32m-> 2366\u001b[0m         result \u001b[38;5;241m=\u001b[39m \u001b[43mprint_method\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   2367\u001b[0m \u001b[43m            \u001b[49m\u001b[43mfilename\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2368\u001b[0m \u001b[43m            \u001b[49m\u001b[43mfacecolor\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfacecolor\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2369\u001b[0m \u001b[43m            \u001b[49m\u001b[43medgecolor\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43medgecolor\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2370\u001b[0m \u001b[43m            \u001b[49m\u001b[43morientation\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43morientation\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2371\u001b[0m \u001b[43m            \u001b[49m\u001b[43mbbox_inches_restore\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m_bbox_inches_restore\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2372\u001b[0m \u001b[43m            \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   2373\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m   2374\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m bbox_inches \u001b[38;5;129;01mand\u001b[39;00m restore_bbox:\n",
      "File \u001b[0;32m~/.local/lib/python3.9/site-packages/matplotlib/backend_bases.py:2232\u001b[0m, in \u001b[0;36mFigureCanvasBase._switch_canvas_and_return_print_method.<locals>.<lambda>\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m   2228\u001b[0m     optional_kws \u001b[38;5;241m=\u001b[39m {  \u001b[38;5;66;03m# Passed by print_figure for other renderers.\u001b[39;00m\n\u001b[1;32m   2229\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdpi\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfacecolor\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124medgecolor\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124morientation\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m   2230\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbbox_inches_restore\u001b[39m\u001b[38;5;124m\"\u001b[39m}\n\u001b[1;32m   2231\u001b[0m     skip \u001b[38;5;241m=\u001b[39m optional_kws \u001b[38;5;241m-\u001b[39m {\u001b[38;5;241m*\u001b[39minspect\u001b[38;5;241m.\u001b[39msignature(meth)\u001b[38;5;241m.\u001b[39mparameters}\n\u001b[0;32m-> 2232\u001b[0m     print_method \u001b[38;5;241m=\u001b[39m functools\u001b[38;5;241m.\u001b[39mwraps(meth)(\u001b[38;5;28;01mlambda\u001b[39;00m \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs: \u001b[43mmeth\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   2233\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43m{\u001b[49m\u001b[43mk\u001b[49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mv\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mk\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mv\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mkwargs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mitems\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mk\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mnot\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mskip\u001b[49m\u001b[43m}\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[1;32m   2234\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:  \u001b[38;5;66;03m# Let third-parties do as they see fit.\u001b[39;00m\n\u001b[1;32m   2235\u001b[0m     print_method \u001b[38;5;241m=\u001b[39m meth\n",
      "File \u001b[0;32m~/.local/lib/python3.9/site-packages/matplotlib/backends/backend_pdf.py:2813\u001b[0m, in \u001b[0;36mFigureCanvasPdf.print_pdf\u001b[0;34m(self, filename, bbox_inches_restore, metadata)\u001b[0m\n\u001b[1;32m   2811\u001b[0m     file \u001b[38;5;241m=\u001b[39m filename\u001b[38;5;241m.\u001b[39m_file\n\u001b[1;32m   2812\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 2813\u001b[0m     file \u001b[38;5;241m=\u001b[39m \u001b[43mPdfFile\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfilename\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmetadata\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmetadata\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   2814\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   2815\u001b[0m     file\u001b[38;5;241m.\u001b[39mnewPage(width, height)\n",
      "File \u001b[0;32m~/.local/lib/python3.9/site-packages/matplotlib/backends/backend_pdf.py:714\u001b[0m, in \u001b[0;36mPdfFile.__init__\u001b[0;34m(self, filename, metadata)\u001b[0m\n\u001b[1;32m    712\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moriginal_file_like \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    713\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtell_base \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[0;32m--> 714\u001b[0m fh, opened \u001b[38;5;241m=\u001b[39m \u001b[43mcbook\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto_filehandle\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfilename\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mwb\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mreturn_opened\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[1;32m    715\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m opened:\n\u001b[1;32m    716\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n",
      "File \u001b[0;32m~/.local/lib/python3.9/site-packages/matplotlib/cbook/__init__.py:494\u001b[0m, in \u001b[0;36mto_filehandle\u001b[0;34m(fname, flag, return_opened, encoding)\u001b[0m\n\u001b[1;32m    492\u001b[0m         fh \u001b[38;5;241m=\u001b[39m bz2\u001b[38;5;241m.\u001b[39mBZ2File(fname, flag)\n\u001b[1;32m    493\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 494\u001b[0m         fh \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mopen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mfname\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mflag\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mencoding\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mencoding\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    495\u001b[0m     opened \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[1;32m    496\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(fname, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mseek\u001b[39m\u001b[38;5;124m'\u001b[39m):\n",
      "\u001b[0;31mPermissionError\u001b[0m: [Errno 13] Permission denied: 'out_dir/batch_0-0.pdf'"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 2000x800 with 6 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import argparse\n",
    "import torch\n",
    "# import Learner\n",
    "# import datasets\n",
    "# import utils\n",
    "# from CGlowModel import CondGlowModel\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "\n",
    "    parser = argparse.ArgumentParser(description='predict c-Glow')\n",
    "\n",
    "    # model parameters\n",
    "    # parser.add_argument(\"--x_size\", type=tuple, default=(3,64,64))\n",
    "    # parser.add_argument(\"--y_size\", type=tuple, default=(1,64,64))\n",
    "    parser.add_argument(\"--x_size\", type=tuple, default=(1,8,8))\n",
    "    parser.add_argument(\"--y_size\", type=tuple, default=(1,16,16))\n",
    "    parser.add_argument(\"--x_hidden_channels\", type=int, default=128)\n",
    "    parser.add_argument(\"--x_hidden_size\", type=int, default=64)\n",
    "    parser.add_argument(\"--y_hidden_channels\", type=int, default=256)\n",
    "    parser.add_argument(\"-K\", \"--depth_flow\", type=int, default=8)\n",
    "    parser.add_argument(\"-L\", \"--num_levels\", type=int, default=3)\n",
    "    parser.add_argument(\"--learn_top\", type=bool, default=False)\n",
    "\n",
    "    # dataset\n",
    "    parser.add_argument(\"-d\", \"--dataset_name\", type=str, default=\"horse\")\n",
    "    parser.add_argument(\"-r\", \"--dataset_root\", type=str, default=\"\")\n",
    "    parser.add_argument(\"--label_scale\", type=float, default=1)\n",
    "    parser.add_argument(\"--label_bias\", type=float, default=0.5)\n",
    "    parser.add_argument(\"--num_labels\", type=int, default=2)\n",
    "    parser.add_argument(\"--x_bins\", type=float, default=256.0)\n",
    "    parser.add_argument(\"--y_bins\", type=float, default=2.0)\n",
    "\n",
    "    # output\n",
    "    parser.add_argument(\"-o\", \"--out_root\", type=str, default=\"out_dir\")\n",
    "\n",
    "    # model path\n",
    "    parser.add_argument(\"--model_path\", type=str, default=\"/pdo/users/jlupoiii/TESS/model_conditional_norm_flow/log_20230823_0134/checkpoints/checkpoint_1000.pth.tar\")\n",
    "\n",
    "    # predictor parameters\n",
    "    parser.add_argument(\"--batch_size\", type=int, default=10)\n",
    "    parser.add_argument(\"--num_samples\", type=int, default=10)\n",
    "\n",
    "\n",
    "    # args = parser.parse_args()\n",
    "    args = parser.parse_known_args()[0]\n",
    "    cuda = torch.cuda.is_available()\n",
    "\n",
    "    # create TESS dataset\n",
    "    torch.manual_seed(42)\n",
    "    full_dataset = TESSDataset()\n",
    "    train_ratio = 0.8\n",
    "    num_train_samples = int(train_ratio * len(full_dataset))\n",
    "    num_valid_samples = len(full_dataset) - num_train_samples\n",
    "    training_set, valid_set = random_split(full_dataset, [num_train_samples, num_valid_samples])\n",
    "    \n",
    "    # model\n",
    "    model = CondGlowModel(args)\n",
    "    if cuda:\n",
    "        model = model.cuda()\n",
    "    state = load_state(args.model_path, cuda)\n",
    "    model.load_state_dict(state[\"model\"])\n",
    "    del state\n",
    "\n",
    "    # predictor\n",
    "    predictor = Inferencer(model, valid_set, args, cuda)\n",
    "\n",
    "    # predict\n",
    "    predictor.sampled_based_prediction(args.num_samples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "-ofzNuC1nvaz"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "authorship_tag": "ABX9TyMbQ56iI/lZFYZCaH1MfPGD",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "pytorch_env",
   "language": "python",
   "name": "pytorch_env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.17"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
