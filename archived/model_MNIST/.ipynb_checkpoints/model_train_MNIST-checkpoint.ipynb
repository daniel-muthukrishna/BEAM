{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c94ceca2-c68e-447b-8344-8c4a488fa7b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "''' \n",
    "This script does conditional image generation on MNIST, using a diffusion model\n",
    "\n",
    "This code is modified from,\n",
    "https://github.com/cloneofsimo/minDiffusion\n",
    "\n",
    "Diffusion model is based on DDPM,\n",
    "https://arxiv.org/abs/2006.11239\n",
    "\n",
    "The conditioning idea is taken from 'Classifier-Free Diffusion Guidance',\n",
    "https://arxiv.org/abs/2207.12598\n",
    "\n",
    "This technique also features in ImageGen 'Photorealistic Text-to-Image Diffusion Modelswith Deep Language Understanding',\n",
    "https://arxiv.org/abs/2205.11487\n",
    "\n",
    "'''\n",
    "\n",
    "from typing import Dict, Tuple\n",
    "from tqdm import tqdm\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import models, transforms\n",
    "from torchvision.datasets import MNIST\n",
    "from torchvision.utils import save_image, make_grid\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.animation import FuncAnimation, PillowWriter\n",
    "import numpy as np\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "17801783-327d-4807-80ea-089d1b3b041f",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ResidualConvBlock(nn.Module):\n",
    "    def __init__(\n",
    "        self, in_channels: int, out_channels: int, is_res: bool = False\n",
    "    ) -> None:\n",
    "        super().__init__()\n",
    "        '''\n",
    "        standard ResNet style convolutional block\n",
    "        '''\n",
    "        self.same_channels = in_channels==out_channels\n",
    "        self.is_res = is_res\n",
    "        self.conv1 = nn.Sequential(\n",
    "            nn.Conv2d(in_channels, out_channels, 3, 1, 1),\n",
    "            nn.BatchNorm2d(out_channels),\n",
    "            nn.GELU(),\n",
    "        )\n",
    "        self.conv2 = nn.Sequential(\n",
    "            nn.Conv2d(out_channels, out_channels, 3, 1, 1),\n",
    "            nn.BatchNorm2d(out_channels),\n",
    "            nn.GELU(),\n",
    "        )\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        if self.is_res:\n",
    "            x1 = self.conv1(x)\n",
    "            x2 = self.conv2(x1)\n",
    "            # this adds on correct residual in case channels have increased\n",
    "            if self.same_channels:\n",
    "                out = x + x2\n",
    "            else:\n",
    "                out = x1 + x2 \n",
    "            return out / 1.414\n",
    "        else:\n",
    "            x1 = self.conv1(x)\n",
    "            x2 = self.conv2(x1)\n",
    "            return x2\n",
    "\n",
    "\n",
    "class UnetDown(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels):\n",
    "        super(UnetDown, self).__init__()\n",
    "        '''\n",
    "        process and downscale the image feature maps\n",
    "        '''\n",
    "        layers = [ResidualConvBlock(in_channels, out_channels), nn.MaxPool2d(2)]\n",
    "        self.model = nn.Sequential(*layers)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.model(x)\n",
    "\n",
    "\n",
    "class UnetUp(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels):\n",
    "        super(UnetUp, self).__init__()\n",
    "        '''\n",
    "        process and upscale the image feature maps\n",
    "        '''\n",
    "        layers = [\n",
    "            nn.ConvTranspose2d(in_channels, out_channels, 2, 2),\n",
    "            ResidualConvBlock(out_channels, out_channels),\n",
    "            ResidualConvBlock(out_channels, out_channels),\n",
    "        ]\n",
    "        self.model = nn.Sequential(*layers)\n",
    "\n",
    "    def forward(self, x, skip):\n",
    "        x = torch.cat((x, skip), 1)\n",
    "        x = self.model(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "class EmbedFC(nn.Module):\n",
    "    def __init__(self, input_dim, emb_dim):\n",
    "        super(EmbedFC, self).__init__()\n",
    "        '''\n",
    "        generic one layer FC NN for embedding things  \n",
    "        '''\n",
    "        self.input_dim = input_dim\n",
    "        layers = [\n",
    "            nn.Linear(input_dim, emb_dim),\n",
    "            nn.GELU(),\n",
    "            nn.Linear(emb_dim, emb_dim),\n",
    "        ]\n",
    "        self.model = nn.Sequential(*layers)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x.view(-1, self.input_dim)\n",
    "        return self.model(x)\n",
    "\n",
    "\n",
    "class ContextUnet(nn.Module):\n",
    "    def __init__(self, in_channels, n_feat = 256, n_classes=10):\n",
    "        super(ContextUnet, self).__init__()\n",
    "\n",
    "        self.in_channels = in_channels\n",
    "        self.n_feat = n_feat\n",
    "        self.n_classes = n_classes\n",
    "\n",
    "        self.init_conv = ResidualConvBlock(in_channels, n_feat, is_res=True)\n",
    "\n",
    "        self.down1 = UnetDown(n_feat, n_feat)\n",
    "        self.down2 = UnetDown(n_feat, 2 * n_feat)\n",
    "\n",
    "        self.to_vec = nn.Sequential(nn.AvgPool2d(7), nn.GELU())\n",
    "\n",
    "        self.timeembed1 = EmbedFC(1, 2*n_feat)\n",
    "        self.timeembed2 = EmbedFC(1, 1*n_feat)\n",
    "        self.contextembed1 = EmbedFC(n_classes, 2*n_feat)\n",
    "        self.contextembed2 = EmbedFC(n_classes, 1*n_feat)\n",
    "\n",
    "        self.up0 = nn.Sequential(\n",
    "            # nn.ConvTranspose2d(6 * n_feat, 2 * n_feat, 7, 7), # when concat temb and cemb end up w 6*n_feat\n",
    "            nn.ConvTranspose2d(2 * n_feat, 2 * n_feat, 7, 7), # otherwise just have 2*n_feat\n",
    "            nn.GroupNorm(8, 2 * n_feat),\n",
    "            nn.ReLU(),\n",
    "        )\n",
    "\n",
    "        self.up1 = UnetUp(4 * n_feat, n_feat)\n",
    "        self.up2 = UnetUp(2 * n_feat, n_feat)\n",
    "        self.out = nn.Sequential(\n",
    "            nn.Conv2d(2 * n_feat, n_feat, 3, 1, 1),\n",
    "            nn.GroupNorm(8, n_feat),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(n_feat, self.in_channels, 3, 1, 1),\n",
    "        )\n",
    "\n",
    "    def forward(self, x, c, t, context_mask):\n",
    "        # x is (noisy) image, c is context label, t is timestep, \n",
    "        # context_mask says which samples to block the context on\n",
    "\n",
    "        x = self.init_conv(x)\n",
    "        down1 = self.down1(x)\n",
    "        down2 = self.down2(down1)\n",
    "        hiddenvec = self.to_vec(down2)\n",
    "\n",
    "        # convert context to one hot embedding\n",
    "        c = nn.functional.one_hot(c, num_classes=self.n_classes).type(torch.float)\n",
    "        \n",
    "        # mask out context if context_mask == 1\n",
    "        context_mask = context_mask[:, None]\n",
    "        context_mask = context_mask.repeat(1,self.n_classes)\n",
    "        context_mask = (-1*(1-context_mask)) # need to flip 0 <-> 1\n",
    "        c = c * context_mask\n",
    "        \n",
    "        # embed context, time step\n",
    "        cemb1 = self.contextembed1(c).view(-1, self.n_feat * 2, 1, 1)\n",
    "        temb1 = self.timeembed1(t).view(-1, self.n_feat * 2, 1, 1)\n",
    "        cemb2 = self.contextembed2(c).view(-1, self.n_feat, 1, 1)\n",
    "        temb2 = self.timeembed2(t).view(-1, self.n_feat, 1, 1)\n",
    "        \n",
    "        # could concatenate the context embedding here instead of adaGN\n",
    "        # hiddenvec = torch.cat((hiddenvec, temb1, cemb1), 1)\n",
    "\n",
    "        up1 = self.up0(hiddenvec)\n",
    "        # up2 = self.up1(up1, down2) # if want to avoid add and multiply embeddings\n",
    "        up2 = self.up1(cemb1*up1+ temb1, down2)  # add and multiply embeddings\n",
    "        up3 = self.up2(cemb2*up2+ temb2, down1)\n",
    "        out = self.out(torch.cat((up3, x), 1))\n",
    "        return out\n",
    "\n",
    "\n",
    "def ddpm_schedules(beta1, beta2, T):\n",
    "    \"\"\"\n",
    "    Returns pre-computed schedules for DDPM sampling, training process.\n",
    "    \"\"\"\n",
    "    assert beta1 < beta2 < 1.0, \"beta1 and beta2 must be in (0, 1)\"\n",
    "\n",
    "    beta_t = (beta2 - beta1) * torch.arange(0, T + 1, dtype=torch.float32) / T + beta1\n",
    "    sqrt_beta_t = torch.sqrt(beta_t)\n",
    "    alpha_t = 1 - beta_t\n",
    "    log_alpha_t = torch.log(alpha_t)\n",
    "    alphabar_t = torch.cumsum(log_alpha_t, dim=0).exp()\n",
    "\n",
    "    sqrtab = torch.sqrt(alphabar_t)\n",
    "    oneover_sqrta = 1 / torch.sqrt(alpha_t)\n",
    "\n",
    "    sqrtmab = torch.sqrt(1 - alphabar_t)\n",
    "    mab_over_sqrtmab_inv = (1 - alpha_t) / sqrtmab\n",
    "\n",
    "    return {\n",
    "        \"alpha_t\": alpha_t,  # \\alpha_t\n",
    "        \"oneover_sqrta\": oneover_sqrta,  # 1/\\sqrt{\\alpha_t}\n",
    "        \"sqrt_beta_t\": sqrt_beta_t,  # \\sqrt{\\beta_t}\n",
    "        \"alphabar_t\": alphabar_t,  # \\bar{\\alpha_t}\n",
    "        \"sqrtab\": sqrtab,  # \\sqrt{\\bar{\\alpha_t}}\n",
    "        \"sqrtmab\": sqrtmab,  # \\sqrt{1-\\bar{\\alpha_t}}\n",
    "        \"mab_over_sqrtmab\": mab_over_sqrtmab_inv,  # (1-\\alpha_t)/\\sqrt{1-\\bar{\\alpha_t}}\n",
    "    }\n",
    "\n",
    "\n",
    "class DDPM(nn.Module):\n",
    "    def __init__(self, nn_model, betas, n_T, device, drop_prob=0.1):\n",
    "        super(DDPM, self).__init__()\n",
    "        self.nn_model = nn_model.to(device)\n",
    "\n",
    "        # register_buffer allows accessing dictionary produced by ddpm_schedules\n",
    "        # e.g. can access self.sqrtab later\n",
    "        for k, v in ddpm_schedules(betas[0], betas[1], n_T).items():\n",
    "            self.register_buffer(k, v)\n",
    "\n",
    "        self.n_T = n_T\n",
    "        self.device = device\n",
    "        self.drop_prob = drop_prob\n",
    "        self.loss_mse = nn.MSELoss()\n",
    "\n",
    "    def forward(self, x, c):\n",
    "        \"\"\"\n",
    "        this method is used in training, so samples t and noise randomly\n",
    "        \"\"\"\n",
    "        _ts = torch.randint(1, self.n_T+1, (x.shape[0],)).to(self.device)  # t ~ Uniform(0, n_T)\n",
    "        noise = torch.randn_like(x)  # eps ~ N(0, 1)\n",
    "        \n",
    "        x_t = (\n",
    "            self.sqrtab[_ts, None, None, None] * x\n",
    "            + self.sqrtmab[_ts, None, None, None] * noise\n",
    "        )  # This is the x_t, which is sqrt(alphabar) x_0 + sqrt(1-alphabar) * eps\n",
    "        # We should predict the \"error term\" from this x_t. Loss is what we return.\n",
    "\n",
    "        # dropout context with some probability\n",
    "        context_mask = torch.bernoulli(torch.zeros_like(c)+self.drop_prob).to(self.device)\n",
    "        \n",
    "        # return MSE between added noise, and our predicted noise\n",
    "        return self.loss_mse(noise, self.nn_model(x_t, c, _ts / self.n_T, context_mask))\n",
    "\n",
    "    def sample(self, n_sample, size, device, guide_w = 0.0):\n",
    "        # we follow the guidance sampling scheme described in 'Classifier-Free Diffusion Guidance'\n",
    "        # to make the fwd passes efficient, we concat two versions of the dataset,\n",
    "        # one with context_mask=0 and the other context_mask=1\n",
    "        # we then mix the outputs with the guidance scale, w\n",
    "        # where w>0 means more guidance\n",
    "\n",
    "        x_i = torch.randn(n_sample, *size).to(device)  # x_T ~ N(0, 1), sample initial noise\n",
    "        c_i = torch.arange(0,10).to(device) # context for us just cycles throught the mnist labels\n",
    "        c_i = c_i.repeat(int(n_sample/c_i.shape[0]))\n",
    "        print('c_i shape', c_i.shape)\n",
    "        \n",
    "        # don't drop context at test time\n",
    "        context_mask = torch.zeros_like(c_i).to(device)\n",
    "\n",
    "        # double the batch\n",
    "        c_i = c_i.repeat(2)\n",
    "        context_mask = context_mask.repeat(2)\n",
    "        context_mask[n_sample:] = 1. # makes second half of batch context free\n",
    "\n",
    "        x_i_store = [] # keep track of generated steps in case want to plot something \n",
    "        print()\n",
    "        for i in range(self.n_T, 0, -1):\n",
    "            print(f'sampling timestep {i}',end='\\r')\n",
    "            t_is = torch.tensor([i / self.n_T]).to(device)\n",
    "            t_is = t_is.repeat(n_sample,1,1,1)\n",
    "\n",
    "            # double batch\n",
    "            x_i = x_i.repeat(2,1,1,1)\n",
    "            t_is = t_is.repeat(2,1,1,1)\n",
    "\n",
    "            z = torch.randn(n_sample, *size).to(device) if i > 1 else 0\n",
    "            \n",
    "            # split predictions and compute weighting\n",
    "            eps = self.nn_model(x_i, c_i, t_is, context_mask)\n",
    "            eps1 = eps[:n_sample]\n",
    "            eps2 = eps[n_sample:]\n",
    "            \n",
    "            eps = (1+guide_w)*eps1 - guide_w*eps2\n",
    "            x_i = x_i[:n_sample]\n",
    "            x_i = (\n",
    "                self.oneover_sqrta[i] * (x_i - eps * self.mab_over_sqrtmab[i])\n",
    "                + self.sqrt_beta_t[i] * z\n",
    "            )\n",
    "            if i%20==0 or i==self.n_T or i<8:\n",
    "                x_i_store.append(x_i.detach().cpu().numpy())\n",
    "        \n",
    "        x_i_store = np.array(x_i_store)\n",
    "        return x_i, x_i_store\n",
    "\n",
    "\n",
    "    def sample_c(self, c_i, n_sample, size, device, guide_w = 0.0):\n",
    "        # we follow the guidance sampling scheme described in 'Classifier-Free Diffusion Guidance'\n",
    "        # to make the fwd passes efficient, we concat two versions of the dataset,\n",
    "        # one with context_mask=0 and the other context_mask=1\n",
    "        # we then mix the outputs with the guidance scale, w\n",
    "        # where w>0 means more guidance\n",
    "\n",
    "        x_i = torch.randn(n_sample, *size).to(device)  # x_T ~ N(0, 1), sample initial noise\n",
    "        # c_i = torch.arange(0,10).to(device) # context for us just cycles throught the mnist labels\n",
    "        c_i = c_i.repeat(int(n_sample/c_i.shape[0]))\n",
    "\n",
    "        # don't drop context at test time\n",
    "        context_mask = torch.zeros_like(c_i).to(device)\n",
    "\n",
    "        # double the batch\n",
    "        c_i = c_i.repeat(2)\n",
    "        context_mask = context_mask.repeat(2)\n",
    "        context_mask[n_sample:] = 1. # makes second half of batch context free\n",
    "\n",
    "        x_i_store = [] # keep track of generated steps in case want to plot something \n",
    "        print()\n",
    "        for i in range(self.n_T, 0, -1):\n",
    "            print(f'sampling timestep {i}',end='\\r')\n",
    "            t_is = torch.tensor([i / self.n_T]).to(device)\n",
    "            t_is = t_is.repeat(n_sample,1,1,1)\n",
    "\n",
    "            # double batch\n",
    "            x_i = x_i.repeat(2,1,1,1)\n",
    "            t_is = t_is.repeat(2,1,1,1)\n",
    "\n",
    "            z = torch.randn(n_sample, *size).to(device) if i > 1 else 0\n",
    "            \n",
    "            # split predictions and compute weighting\n",
    "            eps = self.nn_model(x_i, c_i, t_is, context_mask)\n",
    "            eps1 = eps[:n_sample]\n",
    "            eps2 = eps[n_sample:]\n",
    "            \n",
    "            eps = (1+guide_w)*eps1 - guide_w*eps2\n",
    "            x_i = x_i[:n_sample]\n",
    "            x_i = (\n",
    "                self.oneover_sqrta[i] * (x_i - eps * self.mab_over_sqrtmab[i])\n",
    "                + self.sqrt_beta_t[i] * z\n",
    "            )\n",
    "            if i%20==0 or i==self.n_T or i<8:\n",
    "                x_i_store.append(x_i.detach().cpu().numpy())\n",
    "        \n",
    "        x_i_store = np.array(x_i_store)\n",
    "        return x_i, x_i_store"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "46563040-30e8-43f8-9995-f12d95d1ebfd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loss: 0.1713: 100%|█████████████████████████████| 59/59 [00:21<00:00,  2.71it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "c_i shape torch.Size([40])\n",
      "\n",
      "x_gen, x_real torch.Size([40, 1, 28, 28]) torch.Size([40, 1, 28, 28])\n",
      "grid type <built-in method type of Tensor object at 0x7fe332f55ef0> torch.Size([3, 242, 302])\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[7], line 107\u001b[0m\n\u001b[1;32m    105\u001b[0m save_dir \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m./diffusion_outputs_edited_sample/\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m    106\u001b[0m os\u001b[38;5;241m.\u001b[39mmakedirs(save_dir, exist_ok\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[0;32m--> 107\u001b[0m \u001b[43mtrain_mnist\u001b[49m\u001b[43m(\u001b[49m\u001b[43msave_dir\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[7], line 79\u001b[0m, in \u001b[0;36mtrain_mnist\u001b[0;34m(save_dir)\u001b[0m\n\u001b[1;32m     77\u001b[0m grid \u001b[38;5;241m=\u001b[39m make_grid(x_all\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m1\u001b[39m, nrow\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m10\u001b[39m)\n\u001b[1;32m     78\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mgrid type\u001b[39m\u001b[38;5;124m'\u001b[39m, grid\u001b[38;5;241m.\u001b[39mtype, grid\u001b[38;5;241m.\u001b[39mshape)\n\u001b[0;32m---> 79\u001b[0m \u001b[43msave_image\u001b[49m\u001b[43m(\u001b[49m\u001b[43mgrid\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msave_dir\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;124;43mf\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mimage_ep\u001b[39;49m\u001b[38;5;132;43;01m{\u001b[39;49;00m\u001b[43mep\u001b[49m\u001b[38;5;132;43;01m}\u001b[39;49;00m\u001b[38;5;124;43m_w\u001b[39;49m\u001b[38;5;132;43;01m{\u001b[39;49;00m\u001b[43mw\u001b[49m\u001b[38;5;132;43;01m}\u001b[39;49;00m\u001b[38;5;124;43m.png\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m     80\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124msaved image at \u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;241m+\u001b[39m save_dir \u001b[38;5;241m+\u001b[39m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mimage_ep\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mep\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m_w\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mw\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.png\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     82\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m ep\u001b[38;5;241m%\u001b[39m\u001b[38;5;241m5\u001b[39m\u001b[38;5;241m==\u001b[39m\u001b[38;5;241m0\u001b[39m \u001b[38;5;129;01mor\u001b[39;00m ep \u001b[38;5;241m==\u001b[39m \u001b[38;5;28mint\u001b[39m(n_epoch\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m):\n\u001b[1;32m     83\u001b[0m     \u001b[38;5;66;03m# create gif of images evolving over time, based on x_gen_store\u001b[39;00m\n",
      "File \u001b[0;32m~/.local/lib/python3.9/site-packages/torch/utils/_contextlib.py:115\u001b[0m, in \u001b[0;36mcontext_decorator.<locals>.decorate_context\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    112\u001b[0m \u001b[38;5;129m@functools\u001b[39m\u001b[38;5;241m.\u001b[39mwraps(func)\n\u001b[1;32m    113\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdecorate_context\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m    114\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m ctx_factory():\n\u001b[0;32m--> 115\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.local/lib/python3.9/site-packages/torchvision/utils.py:151\u001b[0m, in \u001b[0;36msave_image\u001b[0;34m(tensor, fp, format, **kwargs)\u001b[0m\n\u001b[1;32m    149\u001b[0m ndarr \u001b[38;5;241m=\u001b[39m grid\u001b[38;5;241m.\u001b[39mmul(\u001b[38;5;241m255\u001b[39m)\u001b[38;5;241m.\u001b[39madd_(\u001b[38;5;241m0.5\u001b[39m)\u001b[38;5;241m.\u001b[39mclamp_(\u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m255\u001b[39m)\u001b[38;5;241m.\u001b[39mpermute(\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m2\u001b[39m, \u001b[38;5;241m0\u001b[39m)\u001b[38;5;241m.\u001b[39mto(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcpu\u001b[39m\u001b[38;5;124m\"\u001b[39m, torch\u001b[38;5;241m.\u001b[39muint8)\u001b[38;5;241m.\u001b[39mnumpy()\n\u001b[1;32m    150\u001b[0m im \u001b[38;5;241m=\u001b[39m Image\u001b[38;5;241m.\u001b[39mfromarray(ndarr)\n\u001b[0;32m--> 151\u001b[0m \u001b[43mim\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msave\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfp\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mformat\u001b[39;49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mformat\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.local/lib/python3.9/site-packages/PIL/Image.py:2410\u001b[0m, in \u001b[0;36mImage.save\u001b[0;34m(self, fp, format, **params)\u001b[0m\n\u001b[1;32m   2408\u001b[0m         fp \u001b[38;5;241m=\u001b[39m builtins\u001b[38;5;241m.\u001b[39mopen(filename, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mr+b\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m   2409\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 2410\u001b[0m         fp \u001b[38;5;241m=\u001b[39m \u001b[43mbuiltins\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mopen\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfilename\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mw+b\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m   2412\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   2413\u001b[0m     save_handler(\u001b[38;5;28mself\u001b[39m, fp, filename)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "def train_mnist(save_dir):\n",
    "\n",
    "    # hardcoding these here\n",
    "    n_epoch = 20\n",
    "    batch_size = 1024 # 256\n",
    "    n_T = 400 # 500\n",
    "    device = \"cuda:0\"\n",
    "    n_classes = 10\n",
    "    n_feat = 128 # 128 ok, 256 better (but slower)\n",
    "    lrate = 1e-4\n",
    "    save_model = False\n",
    "    # save_dir = './diffusion_outputs/'\n",
    "    ws_test = [0.0, 0.5, 2.0] # strength of generative guidance\n",
    "\n",
    "    ddpm = DDPM(nn_model=ContextUnet(in_channels=1, n_feat=n_feat, n_classes=n_classes), betas=(1e-4, 0.02), n_T=n_T, device=device, drop_prob=0.1)\n",
    "    ddpm.to(device)\n",
    "\n",
    "    # optionally load a model\n",
    "    # ddpm.load_state_dict(torch.load(\"./data/diffusion_outputs/ddpm_unet01_mnist_9.pth\"))\n",
    "\n",
    "    tf = transforms.Compose([transforms.ToTensor()]) # mnist is already normalised 0 to 1\n",
    "\n",
    "    dataset = MNIST(\"./data\", train=True, download=True, transform=tf)\n",
    "    dataloader = DataLoader(dataset, batch_size=batch_size, shuffle=True, num_workers=5)\n",
    "    optim = torch.optim.Adam(ddpm.parameters(), lr=lrate)\n",
    "\n",
    "    for ep in range(n_epoch):\n",
    "        print(f'epoch {ep}')\n",
    "        ddpm.train()\n",
    "\n",
    "        # linear lrate decay\n",
    "        optim.param_groups[0]['lr'] = lrate*(1-ep/n_epoch)\n",
    "\n",
    "        pbar = tqdm(dataloader)\n",
    "        loss_ema = None\n",
    "        for x, c in pbar:\n",
    "            # print('x', type(x), x.shape)\n",
    "            # print('c', type(c), c.shape, c)\n",
    "            # print('-----')\n",
    "            \n",
    "            optim.zero_grad()\n",
    "            x = x.to(device)\n",
    "            c = c.to(device)\n",
    "            loss = ddpm(x, c)\n",
    "            loss.backward()\n",
    "            if loss_ema is None:\n",
    "                loss_ema = loss.item()\n",
    "            else:\n",
    "                loss_ema = 0.95 * loss_ema + 0.05 * loss.item()\n",
    "            pbar.set_description(f\"loss: {loss_ema:.4f}\")\n",
    "            optim.step()\n",
    "        \n",
    "        # for eval, save an image of currently generated samples (top rows)\n",
    "        # followed by real images (bottom rows)\n",
    "        ddpm.eval()\n",
    "        with torch.no_grad():\n",
    "            n_sample = 4*n_classes\n",
    "            for w_i, w in enumerate(ws_test):\n",
    "                x_gen, x_gen_store = ddpm.sample(n_sample, (1, 28, 28), device, guide_w=w)\n",
    "                # x_gen, x_gen_store = ddpm.sample_c(torch.full((10,), 4).to(device), n_sample, (1, 28, 28), device, guide_w=w)\n",
    "        \n",
    "                # append some real images at bottom, order by class also\n",
    "                x_real = torch.Tensor(x_gen.shape).to(device)\n",
    "\n",
    "                print('x_gen, x_real', x_gen.shape, x_real.shape)\n",
    "\n",
    "                # populates x_real\n",
    "                for k in range(n_classes):\n",
    "                    for j in range(int(n_sample/n_classes)):\n",
    "                        try: \n",
    "                            idx = torch.squeeze((c == k).nonzero())[j]\n",
    "                        except:\n",
    "                            idx = 0\n",
    "                        x_real[k+(j*n_classes)] = x[idx]\n",
    "\n",
    "                x_all = torch.cat([x_gen, x_real])\n",
    "                grid = make_grid(x_all*-1 + 1, nrow=10)\n",
    "                print('grid type', grid.type, grid.shape)\n",
    "                save_image(grid, save_dir + f\"image_ep{ep}_w{w}.png\")\n",
    "                print('saved image at ' + save_dir + f\"image_ep{ep}_w{w}.png\")\n",
    "\n",
    "                if ep%5==0 or ep == int(n_epoch-1):\n",
    "                    # create gif of images evolving over time, based on x_gen_store\n",
    "                    fig, axs = plt.subplots(nrows=int(n_sample/n_classes), ncols=n_classes,sharex=True,sharey=True,figsize=(8,3))\n",
    "                    def animate_diff(i, x_gen_store):\n",
    "                        print(f'gif animating frame {i} of {x_gen_store.shape[0]}', end='\\r')\n",
    "                        plots = []\n",
    "                        for row in range(int(n_sample/n_classes)):\n",
    "                            for col in range(n_classes):\n",
    "                                axs[row, col].clear()\n",
    "                                axs[row, col].set_xticks([])\n",
    "                                axs[row, col].set_yticks([])\n",
    "                                # plots.append(axs[row, col].imshow(x_gen_store[i,(row*n_classes)+col,0],cmap='gray'))\n",
    "                                plots.append(axs[row, col].imshow(-x_gen_store[i,(row*n_classes)+col,0],cmap='gray',vmin=(-x_gen_store[i]).min(), vmax=(-x_gen_store[i]).max()))\n",
    "                        return plots\n",
    "                    ani = FuncAnimation(fig, animate_diff, fargs=[x_gen_store],  interval=200, blit=False, repeat=True, frames=x_gen_store.shape[0])    \n",
    "                    ani.save(save_dir + f\"gif_ep{ep}_w{w}.gif\", dpi=100, writer=PillowWriter(fps=5))\n",
    "                    print('saved image at ' + save_dir + f\"gif_ep{ep}_w{w}.gif\")\n",
    "        # optionally save model\n",
    "        if save_model and ep == int(n_epoch-1):\n",
    "            torch.save(ddpm.state_dict(), save_dir + f\"model_{ep}.pth\")\n",
    "            print('saved model at ' + save_dir + f\"model_{ep}.pth\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    save_dir = './diffusion_outputs_edited_sample/'\n",
    "    os.makedirs(save_dir, exist_ok=True)\n",
    "    train_mnist(save_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "231179fa-ac05-4d75-878d-af522b31e165",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b9e92dd-99e1-4452-826e-e2f676f790e9",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch_env",
   "language": "python",
   "name": "pytorch_env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.17"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
