{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 193,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 2242,
     "status": "ok",
     "timestamp": 1692391050462,
     "user": {
      "displayName": "Joseph Lupo",
      "userId": "18192058496751055649"
     },
     "user_tz": 240
    },
    "id": "AoYASP6SlsZe",
    "outputId": "4d279a5f-4afd-4985-f1cf-c7fa74d1dd2a"
   },
   "outputs": [],
   "source": [
    "# # only for colab\n",
    "# from google.colab import drive\n",
    "# import os\n",
    "# drive.mount(\"/content/gdrive\")\n",
    "# os.chdir('/content/gdrive/MyDrive/Projects/TESS/model_conditional_norm_flow/')\n",
    "# ! ls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 194,
   "metadata": {
    "executionInfo": {
     "elapsed": 11,
     "status": "ok",
     "timestamp": 1692391050463,
     "user": {
      "displayName": "Joseph Lupo",
      "userId": "18192058496751055649"
     },
     "user_tz": 240
    },
    "id": "wWai4P8Il4oq"
   },
   "outputs": [],
   "source": [
    "# ! rm -r log*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 195,
   "metadata": {
    "executionInfo": {
     "elapsed": 9,
     "status": "ok",
     "timestamp": 1692391050463,
     "user": {
      "displayName": "Joseph Lupo",
      "userId": "18192058496751055649"
     },
     "user_tz": 240
    },
    "id": "rkQEZJWdtZpu"
   },
   "outputs": [],
   "source": [
    "# Adapted from here: https://github.com/yolu1055/conditional-glow/tree/master"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 196,
   "metadata": {
    "executionInfo": {
     "elapsed": 3075,
     "status": "ok",
     "timestamp": 1692391053531,
     "user": {
      "displayName": "Joseph Lupo",
      "userId": "18192058496751055649"
     },
     "user_tz": 240
    },
    "id": "MdI9aotpm1b3"
   },
   "outputs": [],
   "source": [
    "# @title Utils\n",
    "import os\n",
    "import torch\n",
    "import numpy as np\n",
    "\n",
    "def split_feature(tensor, type=\"split\"):\n",
    "    \"\"\"\n",
    "    type = [\"split\", \"cross\"]\n",
    "    \"\"\"\n",
    "    C = tensor.size(1)\n",
    "    if type == \"split\":\n",
    "        return tensor[:, :C // 2, ...], tensor[:, C // 2:, ...]\n",
    "    elif type == \"cross\":\n",
    "        return tensor[:, 0::2, ...], tensor[:, 1::2, ...]\n",
    "\n",
    "\n",
    "def save_model(model, optim, scheduler, dir, iteration):\n",
    "    path = os.path.join(dir, \"checkpoint_{}.pth.tar\".format(iteration))\n",
    "    state = {}\n",
    "    state[\"iteration\"] = iteration\n",
    "    state[\"modelname\"] = model.__class__.__name__\n",
    "    state[\"model\"] = model.state_dict()\n",
    "    state[\"optim\"] = optim.state_dict()\n",
    "    if scheduler is not None:\n",
    "        state[\"scheduler\"] = scheduler.state_dict()\n",
    "    else:\n",
    "        state[\"scheduler\"] = None\n",
    "\n",
    "    torch.save(state, path)\n",
    "\n",
    "\n",
    "def load_state(path, cuda):\n",
    "    if cuda:\n",
    "        print (\"load to gpu\")\n",
    "        state = torch.load(path)\n",
    "    else:\n",
    "        print (\"load to cpu\")\n",
    "        state = torch.load(path, map_location=lambda storage, loc: storage)\n",
    "\n",
    "    return state\n",
    "\n",
    "\n",
    "def _fast_hist(label_true, label_pred, n_class):\n",
    "    mask = (label_true >= 0) & (label_true < n_class)\n",
    "    hist = np.bincount(\n",
    "        n_class * label_true[mask].astype(int) +\n",
    "        label_pred[mask].astype(int), minlength=n_class ** 2).reshape(n_class, n_class)\n",
    "    return hist\n",
    "\n",
    "def compute_accuracy(label_trues, label_preds, n_class):\n",
    "\n",
    "    hist = np.zeros((n_class, n_class))\n",
    "    for lt, lp in zip(label_trues, label_preds):\n",
    "        hist += _fast_hist(lt.flatten(), lp.flatten(), n_class)\n",
    "    acc = np.diag(hist).sum() / hist.sum()\n",
    "    acc_cls = np.diag(hist) / hist.sum(axis=1)\n",
    "    acc_cls = np.nanmean(acc_cls)\n",
    "    iu = np.diag(hist) / (hist.sum(axis=1) + hist.sum(axis=0) - np.diag(hist))\n",
    "    mean_iu = np.nanmean(iu)\n",
    "    freq = hist.sum(axis=1) / hist.sum()\n",
    "    fwavacc = (freq[freq > 0] * iu[freq > 0]).sum()\n",
    "    return acc, acc_cls, mean_iu, fwavacc\n",
    "\n",
    "\n",
    "def count_parameters(model):\n",
    "    return sum(p.numel() for p in model.parameters() if p.requires_grad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 197,
   "metadata": {
    "executionInfo": {
     "elapsed": 5,
     "status": "ok",
     "timestamp": 1692391053531,
     "user": {
      "displayName": "Joseph Lupo",
      "userId": "18192058496751055649"
     },
     "user_tz": 240
    },
    "id": "y7lYe37Em1eN"
   },
   "outputs": [],
   "source": [
    "# @title Modules\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "class ActNorm(nn.Module):\n",
    "\n",
    "    def __init__(self, num_channels):\n",
    "        super().__init__()\n",
    "\n",
    "        size = [1, num_channels, 1, 1]\n",
    "\n",
    "        bias = torch.normal(mean=torch.zeros(*size), std=torch.ones(*size)*0.05)\n",
    "        logs = torch.normal(mean=torch.zeros(*size), std=torch.ones(*size)*0.05)\n",
    "        self.register_parameter(\"bias\", nn.Parameter(torch.Tensor(bias), requires_grad=True))\n",
    "        self.register_parameter(\"logs\", nn.Parameter(torch.Tensor(logs), requires_grad=True))\n",
    "\n",
    "\n",
    "    def forward(self, input, logdet=0, reverse=False):\n",
    "        dimentions = input.size(2) * input.size(3)\n",
    "        if reverse == False:\n",
    "            input = input + self.bias\n",
    "            input = input * torch.exp(self.logs)\n",
    "            dlogdet = torch.sum(self.logs) * dimentions\n",
    "            logdet = logdet + dlogdet\n",
    "\n",
    "        if reverse == True:\n",
    "            input = input * torch.exp(-self.logs)\n",
    "            input = input - self.bias\n",
    "            dlogdet = - torch.sum(self.logs) * dimentions\n",
    "            logdet = logdet + dlogdet\n",
    "\n",
    "        return input, logdet\n",
    "\n",
    "\n",
    "class Conv2dZeros(nn.Conv2d):\n",
    "\n",
    "    def __init__(self, in_channel, out_channel, kernel_size=[3,3], stride=[1,1]):\n",
    "        padding = (kernel_size[0] - 1) // 2\n",
    "        super().__init__(in_channels=in_channel, out_channels=out_channel, kernel_size=kernel_size, stride=stride, padding=padding)\n",
    "        self.weight.data.normal_(mean=0.0, std=0.1)\n",
    "\n",
    "\n",
    "class Conv1dZeros(nn.Conv1d):\n",
    "\n",
    "    def __init__(self, in_channel, out_channel, kernel_size=[1], stride=[1]):\n",
    "        padding = (kernel_size[0] - 1) // 2\n",
    "        super().__init__(in_channels=in_channel, out_channels=out_channel, kernel_size=kernel_size, stride=stride, padding=padding)\n",
    "        self.weight.data.normal_(mean=0.0, std=0.1)\n",
    "\n",
    "\n",
    "class Conv2dResize(nn.Conv2d):\n",
    "\n",
    "    def __init__(self, in_size, out_size):\n",
    "\n",
    "        stride = [in_size[1]//out_size[1], in_size[2]//out_size[2]]\n",
    "        kernel_size = Conv2dResize.compute_kernel_size(in_size, out_size, stride)\n",
    "        super().__init__(in_channels=in_size[0], out_channels=out_size[0], kernel_size=kernel_size, stride=stride)\n",
    "        self.weight.data.zero_()\n",
    "\n",
    "\n",
    "    @staticmethod\n",
    "    def compute_kernel_size(in_size, out_size, stride):\n",
    "        k0 = in_size[1] - (out_size[1] - 1) * stride[0]\n",
    "        k1 = in_size[2] - (out_size[2] - 1) * stride[1]\n",
    "        return[k0,k1]\n",
    "\n",
    "\n",
    "class Conv1dResize(nn.Conv1d):\n",
    "\n",
    "    def __init__(self, in_size, out_size):\n",
    "\n",
    "        stride = [in_size[1]//out_size[1]]\n",
    "        kernel_size = Conv1dResize.compute_kernel_size(in_size, out_size, stride)\n",
    "        super().__init__(in_channels=in_size[0], out_channels=out_size[0], kernel_size=kernel_size, stride=stride)\n",
    "        self.weight.data.zero_()\n",
    "\n",
    "\n",
    "    @staticmethod\n",
    "    def compute_kernel_size(in_size, out_size, stride):\n",
    "        k0 = in_size[1] - (out_size[1] - 1) * stride[0]\n",
    "        return[k0]\n",
    "\n",
    "\n",
    "class Conv2dNorm(nn.Conv2d):\n",
    "\n",
    "    def __init__(self, in_channels, out_channels, kernel_size=[3, 3], stride=[1, 1]):\n",
    "\n",
    "        padding = (kernel_size[0] - 1) // 2\n",
    "        super().__init__(in_channels, out_channels, kernel_size, stride, padding)\n",
    "        #initialize weight\n",
    "        self.weight.data.normal_(mean=0.0, std=0.05)\n",
    "\n",
    "\n",
    "class Conv1dNorm(nn.Conv1d):\n",
    "\n",
    "    def __init__(self, in_channels, out_channels, kernel_size=[1], stride=[1]):\n",
    "\n",
    "        padding = (kernel_size[0] - 1) // 2\n",
    "        super().__init__(in_channels, out_channels, kernel_size, stride, padding)\n",
    "        #initialize weight\n",
    "        self.weight.data.normal_(mean=0.0, std=0.05)\n",
    "\n",
    "\n",
    "class CondActNorm(nn.Module):\n",
    "\n",
    "    def __init__(self, x_size, y_channels, x_hidden_channels, x_hidden_size):\n",
    "        super().__init__()\n",
    "\n",
    "        C_x,H_x = x_size\n",
    "\n",
    "        # conditioning network\n",
    "        self.x_Con = nn.Sequential(\n",
    "            Conv1dResize(in_size=[C_x,H_x], out_size=[x_hidden_channels, H_x//1]),\n",
    "            nn.ReLU(),\n",
    "            Conv1dResize(in_size=[x_hidden_channels, H_x//1], out_size=[x_hidden_channels, H_x//2]),\n",
    "            nn.ReLU(),\n",
    "            Conv1dResize(in_size=[x_hidden_channels, H_x//2], out_size=[x_hidden_channels, H_x//2]),\n",
    "            nn.ReLU(),\n",
    "            Conv1dResize(in_size=[x_hidden_channels, H_x//2], out_size=[x_hidden_channels, H_x//2]),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "\n",
    "        self.x_Linear = nn.Sequential(\n",
    "            LinearZeros(x_hidden_channels*H_x//(2), x_hidden_size),\n",
    "            nn.ReLU(),\n",
    "            LinearZeros(x_hidden_size, x_hidden_size),\n",
    "            nn.ReLU(),\n",
    "            LinearZeros(x_hidden_size, 2*y_channels),\n",
    "            nn.Tanh()\n",
    "        )\n",
    "\n",
    "\n",
    "    def forward(self, x, y, logdet=0, reverse=False):\n",
    "\n",
    "        B,C,H = x.size()\n",
    "\n",
    "        # generate weights \n",
    "        x = self.x_Con(x)\n",
    "        x = x.view(B, -1)\n",
    "        x = self.x_Linear(x)\n",
    "        x = x.view(B, -1, 1, 1)\n",
    "\n",
    "\n",
    "\n",
    "        logs, bias = split_feature(x)\n",
    "        dimentions = y.size(2) * y.size(3)\n",
    "\n",
    "\n",
    "        if not reverse:\n",
    "            # center and scale\n",
    "            y = y + bias\n",
    "            y = y * torch.exp(logs)\n",
    "            dlogdet = dimentions * torch.sum(logs, dim=(1,2,3))\n",
    "            logdet = logdet + dlogdet\n",
    "        else:\n",
    "            # scale and center\n",
    "            y = y * torch.exp(-logs)\n",
    "            y = y - bias\n",
    "            dlogdet = - dimentions * torch.sum(logs, dim=(1,2,3))\n",
    "            logdet = logdet + dlogdet\n",
    "\n",
    "        return y, logdet\n",
    "\n",
    "\n",
    "\n",
    "class Cond1x1Conv(nn.Module):\n",
    "\n",
    "    def __init__(self, x_size, x_hidden_channels, x_hidden_size, y_channels):\n",
    "\n",
    "        super().__init__()\n",
    "\n",
    "        C_x,H_x = x_size\n",
    "\n",
    "\n",
    "        # conditioning network\n",
    "        self.x_Con = nn.Sequential(\n",
    "            Conv1dResize(in_size=[C_x,H_x], out_size=[x_hidden_channels, H_x//1]),\n",
    "            nn.ReLU(),\n",
    "            Conv1dResize(in_size=[x_hidden_channels, H_x//1], out_size=[x_hidden_channels, H_x//2]),\n",
    "            nn.ReLU(),\n",
    "            Conv1dResize(in_size=[x_hidden_channels, H_x//2], out_size=[x_hidden_channels, H_x//2]),\n",
    "            nn.ReLU(),\n",
    "            Conv1dResize(in_size=[x_hidden_channels, H_x//2], out_size=[x_hidden_channels, H_x//2]),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "\n",
    "        self.x_Linear = nn.Sequential(\n",
    "            LinearZeros(x_hidden_channels*H_x//(2), x_hidden_size),\n",
    "            nn.ReLU(),\n",
    "            LinearZeros(x_hidden_size, x_hidden_size),\n",
    "            nn.ReLU(),\n",
    "            LinearNorm(x_hidden_size, y_channels*y_channels),\n",
    "            nn.Tanh()\n",
    "        )\n",
    "\n",
    "\n",
    "    def get_weight(self, x, y, reverse):\n",
    "        y_channels = y.size(1)\n",
    "        B,C,H = x.size()\n",
    "\n",
    "        x = self.x_Con(x)\n",
    "        x = x.view(B, -1)\n",
    "        x = self.x_Linear(x)\n",
    "        weight = x.view(B, y_channels, y_channels)\n",
    "\n",
    "        dimensions = y.size(2) * y.size(3)\n",
    "        dlogdet = torch.slogdet(weight)[1] * dimensions\n",
    "\n",
    "\n",
    "        if reverse == False:\n",
    "            weight = weight.view(B, y_channels, y_channels,1,1)\n",
    "\n",
    "        else:\n",
    "            weight = torch.inverse(weight.double()).float().view(B, y_channels, y_channels,1,1)\n",
    "\n",
    "        return weight, dlogdet\n",
    "\n",
    "\n",
    "\n",
    "    def forward(self, x, y, logdet=None, reverse=False):\n",
    "\n",
    "        weight, dlogdet = self.get_weight(x, y, reverse)\n",
    "        B,C,H,W = y.size()\n",
    "        y = y.view(1, B*C, H, W)\n",
    "        B_k, C_i_k, C_o_k, H_k, W_k = weight.size()\n",
    "        assert B == B_k and C == C_i_k and C == C_o_k, \"The input and kernel dimensions are different\"\n",
    "        weight = weight.reshape(B_k*C_i_k,C_o_k,H_k,W_k)\n",
    "\n",
    "        if reverse == False:\n",
    "            z = F.conv2d(y, weight, groups=B)\n",
    "            z = z.view(B,C,H,W)\n",
    "            if logdet is not None:\n",
    "                logdet = logdet + dlogdet\n",
    "\n",
    "            return z, logdet\n",
    "        else:\n",
    "            z = F.conv2d(y, weight, groups=B)\n",
    "            z = z.view(B,C,H,W)\n",
    "\n",
    "            if logdet is not None:\n",
    "                logdet = logdet - dlogdet\n",
    "\n",
    "            return z, logdet\n",
    "\n",
    "\n",
    "class Conv2dNormy(nn.Conv2d):\n",
    "\n",
    "    def __init__(self, in_channels, out_channels,\n",
    "                 kernel_size=[3, 3], stride=[1, 1]):\n",
    "        padding = [(kernel_size[0]-1)//2, (kernel_size[1]-1)//2]\n",
    "        super().__init__(in_channels, out_channels, kernel_size, stride,\n",
    "                         padding, bias=False)\n",
    "\n",
    "        #initialize weight\n",
    "        self.weight.data.normal_(mean=0.0, std=0.05)\n",
    "        self.actnorm = ActNorm(out_channels)\n",
    "\n",
    "\n",
    "    def forward(self, input):\n",
    "        x = super().forward(input)\n",
    "        x,_ = self.actnorm(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "class Conv2dZerosy(nn.Conv2d):\n",
    "\n",
    "    def __init__(self, in_channels, out_channels,\n",
    "                 kernel_size=[3, 3], stride=[1, 1]):\n",
    "\n",
    "        padding = [(kernel_size[0]-1)//2, (kernel_size[1]-1)//2]\n",
    "        super().__init__(in_channels, out_channels, kernel_size, stride, padding)\n",
    "\n",
    "        self.logscale_factor = 3.0\n",
    "        self.register_parameter(\"logs\", nn.Parameter(torch.zeros(out_channels, 1, 1)))\n",
    "        self.register_parameter(\"newbias\", nn.Parameter(torch.zeros(out_channels, 1, 1)))\n",
    "\n",
    "        # init\n",
    "        self.weight.data.zero_()\n",
    "        self.bias.data.zero_()\n",
    "\n",
    "    def forward(self, input):\n",
    "        output = super().forward(input)\n",
    "        output = output + self.newbias\n",
    "        output = output * torch.exp(self.logs * self.logscale_factor)\n",
    "        return output\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "class CondAffineCoupling(nn.Module):\n",
    "\n",
    "    def __init__(self, x_size, y_size, hidden_channels):\n",
    "        super().__init__()\n",
    "\n",
    "        self.resize_x = nn.Sequential(\n",
    "            Conv1dZeros(x_size[0], 32),\n",
    "            nn.ReLU(),\n",
    "            Conv1dZeros(32, 16),\n",
    "            nn.ReLU(),\n",
    "            Conv1dResize((16,x_size[1]), out_size=y_size),\n",
    "            nn.ReLU(),\n",
    "            Conv1dZeros(y_size[0], y_size[0]*y_size[1]),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "\n",
    "        self.f = nn.Sequential(\n",
    "            Conv2dNormy(y_size[0]*2, hidden_channels),\n",
    "            nn.ReLU(),\n",
    "            Conv2dNormy(hidden_channels, hidden_channels, kernel_size=[1, 1]),\n",
    "            nn.ReLU(),\n",
    "            Conv2dZerosy(hidden_channels, 2*y_size[0]),\n",
    "            nn.Tanh()\n",
    "        )\n",
    "\n",
    "\n",
    "    def forward(self, x, y, logdet=0.0, reverse=False):\n",
    "\n",
    "        z1, z2 = split_feature(y, \"split\")\n",
    "        x = self.resize_x(x)\n",
    "\n",
    "        x = x.view(z1.shape)\n",
    "\n",
    "        h = torch.cat((x,z1), dim=1)\n",
    "        h = self.f(h)\n",
    "        shift, scale = split_feature(h, \"cross\")\n",
    "        scale = torch.sigmoid(scale + 2.)\n",
    "        if reverse == False:\n",
    "            z2 = z2 + shift\n",
    "            z2 = z2 * scale\n",
    "            logdet = torch.sum(torch.log(scale), dim=(1, 2, 3)) + logdet\n",
    "\n",
    "        if reverse == True:\n",
    "            z2 = z2 / scale\n",
    "            z2 = z2 - shift\n",
    "            logdet = -torch.sum(torch.log(scale), dim=(1, 2, 3)) + logdet\n",
    "\n",
    "        z = torch.cat((z1, z2), dim=1)\n",
    "\n",
    "        return z, logdet\n",
    "\n",
    "\n",
    "\n",
    "class SqueezeLayer(nn.Module):\n",
    "    def __init__(self, factor):\n",
    "        super().__init__()\n",
    "        self.factor = factor\n",
    "\n",
    "    def forward(self, input, logdet=None, reverse=False):\n",
    "        if not reverse:\n",
    "            output = SqueezeLayer.squeeze2d(input, self.factor)\n",
    "            return output, logdet\n",
    "        else:\n",
    "            output = SqueezeLayer.unsqueeze2d(input, self.factor)\n",
    "            return output, logdet\n",
    "\n",
    "\n",
    "    @staticmethod\n",
    "    def squeeze2d(input, factor=2):\n",
    "        assert factor >= 1 and isinstance(factor, int)\n",
    "        if factor == 1:\n",
    "            return input\n",
    "        B, C, H, W = input.size()\n",
    "        assert H % factor == 0 and W % factor == 0, \"{}\".format((H, W))\n",
    "        x = input.view(B, C, H // factor, factor, W // factor, factor)\n",
    "        x = x.permute(0, 1, 3, 5, 2, 4).contiguous()\n",
    "        x = x.view(B, C * factor * factor, H // factor, W // factor)\n",
    "        return x\n",
    "\n",
    "\n",
    "    @staticmethod\n",
    "    def unsqueeze2d(input, factor=2):\n",
    "        assert factor >= 1 and isinstance(factor, int)\n",
    "        factor2 = factor ** 2\n",
    "        if factor == 1:\n",
    "            return input\n",
    "        B, C, H, W = input.size()\n",
    "        assert C % (factor2) == 0, \"{}\".format(C)\n",
    "        x = input.view(B, C // factor2, factor, factor, H, W)\n",
    "        x = x.permute(0, 1, 4, 2, 5, 3).contiguous()\n",
    "        x = x.view(B, C // (factor2), H * factor, W * factor)\n",
    "        return x\n",
    "\n",
    "\n",
    "class Split2d(nn.Module):\n",
    "    def __init__(self, num_channels):\n",
    "        super().__init__()\n",
    "\n",
    "        self.conv = nn.Sequential(\n",
    "            Conv2dZeros(num_channels // 2, num_channels),\n",
    "            nn.Tanh()\n",
    "        )\n",
    "\n",
    "    def split2d_prior(self, z):\n",
    "        h = self.conv(z)\n",
    "        return split_feature(h, \"cross\")\n",
    "\n",
    "    def forward(self, input, logdet=0., reverse=False, eps_std=None):\n",
    "        if not reverse:\n",
    "            z1, z2 = split_feature(input, \"split\")\n",
    "            mean, logs = self.split2d_prior(z1)\n",
    "            logdet = GaussianDiag.logp(mean, logs, z2) + logdet\n",
    "\n",
    "            return z1, logdet\n",
    "        else:\n",
    "            z1 = input\n",
    "            mean, logs = self.split2d_prior(z1)\n",
    "            z2 = GaussianDiag.sample(mean, logs, eps_std)\n",
    "            z = torch.cat((z1, z2), dim=1)\n",
    "\n",
    "            return z, logdet\n",
    "\n",
    "\n",
    "class GaussianDiag:\n",
    "    Log2PI = float(np.log(2 * np.pi))\n",
    "\n",
    "    @staticmethod\n",
    "    def likelihood(mean, logs, x):\n",
    "        return -0.5 * (logs * 2. + ((x - mean) ** 2.) / torch.exp(logs * 2.) + GaussianDiag.Log2PI)\n",
    "\n",
    "    @staticmethod\n",
    "    def logp(mean, logs, x):\n",
    "        likelihood = GaussianDiag.likelihood(mean, logs, x)\n",
    "        return torch.sum(likelihood, dim=(1, 2, 3))\n",
    "\n",
    "    @staticmethod\n",
    "    def sample(mean, logs, eps_std=None):\n",
    "        eps_std = eps_std or 1\n",
    "        eps = torch.normal(mean=torch.zeros_like(mean),\n",
    "                           std=torch.ones_like(logs) * eps_std)\n",
    "        return mean + torch.exp(logs) * eps\n",
    "\n",
    "    @staticmethod\n",
    "    def batchsample(batchsize, mean, logs, eps_std=None):\n",
    "        eps_std = eps_std or 1\n",
    "        sample = GaussianDiag.sample(mean, logs, eps_std)\n",
    "        for i in range(1, batchsize):\n",
    "            s = GaussianDiag.sample(mean, logs, eps_std)\n",
    "            sample = torch.cat((sample, s), dim=0)\n",
    "        return sample\n",
    "\n",
    "\n",
    "\n",
    "class LinearZeros(nn.Linear):\n",
    "\n",
    "    def __init__(self, in_channels, out_channels):\n",
    "        super().__init__(in_channels, out_channels)\n",
    "        self.weight.data.zero_()\n",
    "        self.bias.data.zero_()\n",
    "\n",
    "    def forward(self, input):\n",
    "        output = super().forward(input)\n",
    "        return output\n",
    "\n",
    "\n",
    "class LinearNorm(nn.Linear):\n",
    "\n",
    "    def __init__(self, in_channels, out_channels):\n",
    "        super().__init__(in_channels, out_channels)\n",
    "        self.weight.data.normal_(mean=0.0, std=0.1)\n",
    "        self.bias.data.normal_(mean=0.0, std=0.1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 198,
   "metadata": {
    "executionInfo": {
     "elapsed": 4,
     "status": "ok",
     "timestamp": 1692391053531,
     "user": {
      "displayName": "Joseph Lupo",
      "userId": "18192058496751055649"
     },
     "user_tz": 240
    },
    "id": "6ntChUCZnc8W"
   },
   "outputs": [],
   "source": [
    "# @title Glow Model\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "\n",
    "class CondGlowStep(nn.Module):\n",
    "\n",
    "    def __init__(self, x_size, y_size, x_hidden_channels, x_hidden_size, y_hidden_channels):\n",
    "\n",
    "\n",
    "        super().__init__()\n",
    "\n",
    "        # 1. cond-actnorm\n",
    "        self.actnorm = CondActNorm(x_size=x_size, y_channels=y_size[0], x_hidden_channels=x_hidden_channels, x_hidden_size=x_hidden_size)\n",
    "\n",
    "        # 2. cond-1x1conv\n",
    "        self.invconv = Cond1x1Conv(x_size=x_size, x_hidden_channels=x_hidden_channels, x_hidden_size=x_hidden_size, y_channels=y_size[0])\n",
    "\n",
    "        # 3. cond-affine\n",
    "        self.affine = CondAffineCoupling(x_size=x_size, y_size=[y_size[0] // 2, y_size[1], y_size[2]], hidden_channels=y_hidden_channels)\n",
    "\n",
    "\n",
    "    def forward(self, x, y, logdet=None, reverse=False):\n",
    "        \n",
    "        if reverse is False:\n",
    "            # 1. cond-actnorm\n",
    "            y, logdet = self.actnorm(x, y, logdet, reverse=False)\n",
    "\n",
    "            # 2. cond-1x1conv\n",
    "            y, logdet = self.invconv(x, y, logdet, reverse=False)\n",
    "\n",
    "            # 3. cond-affine\n",
    "            y, logdet = self.affine(x, y, logdet, reverse=False)\n",
    "\n",
    "            # Return\n",
    "            return y, logdet\n",
    "\n",
    "\n",
    "        if reverse is True:\n",
    "            # 3. cond-affine\n",
    "            y, logdet = self.affine(x, y, logdet, reverse=True)\n",
    "\n",
    "            # 2. cond-1x1conv\n",
    "            y, logdet = self.invconv(x, y, logdet, reverse=True)\n",
    "\n",
    "            # 1. cond-actnorm\n",
    "            y, logdet = self.actnorm(x, y, logdet, reverse=True)\n",
    "\n",
    "            # Return\n",
    "            return y, logdet\n",
    "\n",
    "\n",
    "class CondGlow(nn.Module):\n",
    "\n",
    "    def __init__(self, x_size, y_size, x_hidden_channels, x_hidden_size, y_hidden_channels, K, L):\n",
    "\n",
    "\n",
    "        super().__init__()\n",
    "        self.layers = nn.ModuleList()\n",
    "        self.output_shapes = []\n",
    "        self.K = K\n",
    "        self.L = L\n",
    "        C, H, W = y_size\n",
    "\n",
    "        for l in range(0, L):\n",
    "\n",
    "            # 1. Squeeze\n",
    "            C, H, W = C * 4, H // 2, W // 2\n",
    "            y_size = [C,H,W]\n",
    "            self.layers.append(SqueezeLayer(factor=2))\n",
    "            self.output_shapes.append([-1, C, H, W])\n",
    "\n",
    "            # 2. K CGlowStep\n",
    "            for k in range(0, K):\n",
    "\n",
    "                self.layers.append(CondGlowStep(x_size = x_size,\n",
    "                                            y_size = y_size,\n",
    "                                            x_hidden_channels = x_hidden_channels,\n",
    "                                            x_hidden_size = x_hidden_size,\n",
    "                                            y_hidden_channels = y_hidden_channels,\n",
    "                                            )\n",
    "                                   )\n",
    "\n",
    "                self.output_shapes.append([-1, C, H, W])\n",
    "\n",
    "            # 3. Split\n",
    "            if l < L - 1:\n",
    "                self.layers.append(Split2d(num_channels=C))\n",
    "                self.output_shapes.append([-1, C // 2, H, W])\n",
    "                C = C // 2\n",
    "\n",
    "\n",
    "    def forward(self, x, y, logdet=0.0, reverse=False, eps_std=1.0):\n",
    "        if reverse == False:\n",
    "            return self.encode(x, y, logdet)\n",
    "        else:\n",
    "            return self.decode(x, y, logdet, eps_std)\n",
    "\n",
    "    def encode(self, x, y, logdet=0.0):\n",
    "        for layer, shape in zip(self.layers, self.output_shapes):\n",
    "            if isinstance(layer, Split2d) or isinstance(layer, SqueezeLayer):\n",
    "                y, logdet = layer(y, logdet, reverse=False)\n",
    "\n",
    "            else:\n",
    "                y, logdet = layer(x, y, logdet, reverse=False)\n",
    "        return y, logdet\n",
    "\n",
    "    def decode(self, x, y, logdet=0.0, eps_std=1.0):\n",
    "        for layer in reversed(self.layers):\n",
    "            if isinstance(layer, Split2d):\n",
    "                y, logdet = layer(y, logdet=logdet, reverse=True, eps_std=eps_std)\n",
    "\n",
    "            elif isinstance(layer, SqueezeLayer):\n",
    "                y, logdet = layer(y, logdet=logdet, reverse=True)\n",
    "\n",
    "            else:\n",
    "                y, logdet = layer(x, y, logdet=logdet, reverse=True)\n",
    "\n",
    "        return y, logdet\n",
    "\n",
    "\n",
    "class CondGlowModel(nn.Module):\n",
    "    BCE = nn.BCEWithLogitsLoss()\n",
    "    CE = nn.CrossEntropyLoss()\n",
    "\n",
    "    def __init__(self, args):\n",
    "        super().__init__()\n",
    "        self.flow = CondGlow(x_size=args.x_size,\n",
    "                            y_size=args.y_size,\n",
    "                            x_hidden_channels=args.x_hidden_channels,\n",
    "                            x_hidden_size=args.x_hidden_size,\n",
    "                            y_hidden_channels=args.y_hidden_channels,\n",
    "                            K=args.depth_flow,\n",
    "                            L=args.num_levels,\n",
    "                            )\n",
    "\n",
    "        self.learn_top = args.learn_top\n",
    "\n",
    "\n",
    "        self.register_parameter(\"new_mean\",\n",
    "                                nn.Parameter(torch.zeros(\n",
    "                                    [1,\n",
    "                                     self.flow.output_shapes[-1][1],\n",
    "                                     self.flow.output_shapes[-1][2],\n",
    "                                     self.flow.output_shapes[-1][3]])))\n",
    "\n",
    "\n",
    "        self.register_parameter(\"new_logs\",\n",
    "                                nn.Parameter(torch.zeros(\n",
    "                                    [1,\n",
    "                                     self.flow.output_shapes[-1][1],\n",
    "                                     self.flow.output_shapes[-1][2],\n",
    "                                     self.flow.output_shapes[-1][3]])))\n",
    "\n",
    "        self.n_bins = args.y_bins\n",
    "\n",
    "\n",
    "    def prior(self):\n",
    "\n",
    "        if self.learn_top:\n",
    "            return self.new_mean, self.new_logs\n",
    "        else:\n",
    "            return torch.zeros_like(self.new_mean), torch.zeros_like(self.new_logs)\n",
    "\n",
    "\n",
    "    def forward(self, x=0.0, y=None, eps_std=1.0, reverse=False):\n",
    "        if reverse == False:\n",
    "            dimensions = y.size(1)*y.size(2)*y.size(3)\n",
    "            logdet = torch.zeros_like(y[:, 0, 0, 0])\n",
    "            logdet += float(-np.log(self.n_bins) * dimensions)\n",
    "            z, objective = self.flow(x, y, logdet=logdet, reverse=False)\n",
    "            mean, logs = self.prior()\n",
    "            objective += GaussianDiag.logp(mean, logs, z)\n",
    "            nll = -objective / float(np.log(2.) * dimensions)\n",
    "            return z, nll\n",
    "\n",
    "        else:\n",
    "            with torch.no_grad():\n",
    "                mean, logs = self.prior()\n",
    "                if y is None:\n",
    "                    y = GaussianDiag.batchsample(x.size(0), mean, logs, eps_std)\n",
    "                y, logdet = self.flow(x, y, eps_std=eps_std, reverse=True)\n",
    "            return y, logdet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 199,
   "metadata": {
    "executionInfo": {
     "elapsed": 1976,
     "status": "ok",
     "timestamp": 1692391055504,
     "user": {
      "displayName": "Joseph Lupo",
      "userId": "18192058496751055649"
     },
     "user_tz": 240
    },
    "id": "7xtTbLLNndDk"
   },
   "outputs": [],
   "source": [
    "# @title Learner\n",
    "import os\n",
    "import torch\n",
    "import datetime\n",
    "import numpy as np\n",
    "from torch.utils.data import DataLoader\n",
    "import time\n",
    "# from datasets import convert_to_img\n",
    "# from datasets import preprocess\n",
    "# from datasets import postprocess\n",
    "from torchvision.utils import save_image\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "class Trainer(object):\n",
    "\n",
    "    def __init__(self, graph, optim, scheduler, trainingset, validset, args, cuda):\n",
    "\n",
    "        # set path and date\n",
    "        date = str(datetime.datetime.now())\n",
    "        date = date[:date.rfind(\":\")].replace(\"-\", \"\")\\\n",
    "                                     .replace(\":\", \"\")\\\n",
    "                                     .replace(\" \", \"_\")\n",
    "        self.log_dir = os.path.join(args.log_root, \"log_\" + date)\n",
    "        if not os.path.exists(self.log_dir):\n",
    "            os.makedirs(self.log_dir)\n",
    "\n",
    "\n",
    "        self.checkpoints_dir = os.path.join(self.log_dir, \"checkpoints\")\n",
    "        if not os.path.exists(self.checkpoints_dir):\n",
    "            os.makedirs(self.checkpoints_dir)\n",
    "\n",
    "        # # old directories for samples and inverses\n",
    "        # self.images_dir = os.path.join(self.log_dir, \"images_training_set\")\n",
    "        # if not os.path.exists(self.images_dir):\n",
    "        #     os.makedirs(self.images_dir)\n",
    "        # self.valid_samples_dir = os.path.join(self.log_dir, \"valid_samples\")\n",
    "        # if not os.path.exists(self.valid_samples_dir):\n",
    "        #     os.makedirs(self.valid_samples_dir)\n",
    "\n",
    "        # new directories\n",
    "\n",
    "        # # do not need anymore, combined inverse into samples\n",
    "        # # images_dir -> images_inverse_train_dir\n",
    "        # self.images_inverse_train_dir = os.path.join(self.log_dir, \"images_inverse_train\")\n",
    "        # if not os.path.exists(self.images_inverse_train_dir):\n",
    "        #     os.makedirs(self.images_inverse_train_dir)\n",
    "        # self.images_inverse_valid_dir = os.path.join(self.log_dir, \"images_inverse_valid\")\n",
    "        # if not os.path.exists(self.images_inverse_valid_dir):\n",
    "        #     os.makedirs(self.images_inverse_valid_dir)\n",
    "\n",
    "        # valid_samples_dir -> images_samples_valid_dir\n",
    "        self.images_samples_valid_dir = os.path.join(self.log_dir, \"images_samples_valid\")\n",
    "        if not os.path.exists(self.images_samples_valid_dir):\n",
    "            os.makedirs(self.images_samples_valid_dir)\n",
    "\n",
    "        self.images_samples_train_dir = os.path.join(self.log_dir, \"images_samples_train\")\n",
    "        if not os.path.exists(self.images_samples_train_dir):\n",
    "            os.makedirs(self.images_samples_train_dir)\n",
    "\n",
    "\n",
    "\n",
    "        # model\n",
    "        self.graph = graph\n",
    "        self.optim = optim\n",
    "        self.scheduler = scheduler\n",
    "\n",
    "        # gradient bound\n",
    "        self.max_grad_clip = args.max_grad_clip\n",
    "        self.max_grad_norm = args.max_grad_norm\n",
    "\n",
    "        # data\n",
    "        self.dataset_name = args.dataset_name\n",
    "        self.batch_size = args.batch_size\n",
    "        self.trainingset_loader = DataLoader(trainingset,\n",
    "                                      batch_size=self.batch_size,\n",
    "                                      shuffle=True,\n",
    "                                      drop_last=True)\n",
    "\n",
    "        self.validset_loader = DataLoader(validset,\n",
    "                                      batch_size=self.batch_size,\n",
    "                                      shuffle=False,\n",
    "                                      drop_last=False)\n",
    "\n",
    "        print('training, validation lengths: ', len(self.trainingset_loader), len(self.validset_loader))\n",
    "\n",
    "        self.num_epochs = args.num_epochs\n",
    "        self.global_step = args.num_steps\n",
    "        self.label_scale = args.label_scale\n",
    "        self.label_bias = args.label_bias\n",
    "        self.x_bins = args.x_bins\n",
    "        self.y_bins = args.y_bins\n",
    "\n",
    "\n",
    "        self.num_epochs = args.num_epochs\n",
    "        self.nll_gap = args.nll_gap\n",
    "        self.inference_gap = args.inference_gap\n",
    "        self.checkpoints_gap = args.checkpoints_gap\n",
    "        self.save_gap = args.save_gap\n",
    "\n",
    "        # device\n",
    "        self.cuda = cuda\n",
    "\n",
    "    def validate(self):\n",
    "        print (\"Start Validating\")\n",
    "        self.graph.eval()\n",
    "\n",
    "        # for validation set\n",
    "        mean_loss = list()\n",
    "        samples = list()\n",
    "        samples_ffi_nums = list()\n",
    "        samples_x = list()\n",
    "        samples_orbit = list()\n",
    "        with torch.no_grad():\n",
    "            for i_batch, batch in enumerate(self.validset_loader):\n",
    "                \n",
    "                x = batch[\"x\"]\n",
    "                y = batch[\"y\"]\n",
    "                ffi_num = batch[\"ffi_num\"]\n",
    "                orbit = batch[\"orbit\"]\n",
    "\n",
    "                if self.cuda:\n",
    "                    x = x.cuda()\n",
    "                    y = y.cuda()\n",
    "\n",
    "\n",
    "                # forward\n",
    "                z, nll = self.graph(x,y)\n",
    "                loss = torch.mean(nll)\n",
    "                mean_loss.append(loss.data.cpu().item())\n",
    "\n",
    "                # true label\n",
    "                y_true = y\n",
    "\n",
    "                # sample\n",
    "                batch_samples = []\n",
    "                batch_samples_ffi_num = []\n",
    "                batch_samples_x = []\n",
    "                batch_samples_orbit = []\n",
    "                for b in range(len(y)):\n",
    "                    row = []\n",
    "                    row.append(y_true[b])\n",
    "                    batch_samples.append(row)\n",
    "\n",
    "                    batch_samples_ffi_num.append([ffi_num[b]])\n",
    "                    batch_samples_x.append([x[b]])\n",
    "                    batch_samples_orbit.append([orbit[b]])\n",
    "                    \n",
    "                for i in range(0, 5):\n",
    "                    y_sample,_ = self.graph(x, y=None, reverse=True)\n",
    "\n",
    "                    for b in range(len(y)):\n",
    "                        batch_samples[b].append(y_sample[b])\n",
    "\n",
    "                        batch_samples_ffi_num[b].append(ffi_num[b])\n",
    "                        batch_samples_x[b].append(x[b])\n",
    "                        batch_samples_orbit[b].append(orbit[b])\n",
    "\n",
    "                for b in range(len(y)):\n",
    "                    samples.append(batch_samples[b])\n",
    "                    samples_ffi_nums.append(batch_samples_ffi_num[b])\n",
    "                    samples_x.append(batch_samples_x[b])\n",
    "                    samples_orbit.append(batch_samples_orbit[b])\n",
    "                    \n",
    "        # save samples\n",
    "        for i in range(0, len(samples)):\n",
    "            if i < 5:\n",
    "                fig, axes = plt.subplots(1, len(samples[i]), figsize=(40, 7))\n",
    "                for b in range(0,len(samples[i])):\n",
    "                    \n",
    "                    # create title\n",
    "                    ffi = samples_ffi_nums[i][b]\n",
    "                    orbit = samples_orbit[i][b]\n",
    "                    plottitle = ''\n",
    "                    if b == 0: plottitle = 'y true: VALIDATION SET'\n",
    "                    elif b == 1: plottitle = 'y inverse (use x and y)'\n",
    "                    else: plottitle = f'(Using only x_cond) sample {b-1}'\n",
    "                    data = [round(float(elt), 2) for elt in samples_x[i][b][0]]\n",
    "                    data_line1 = str({'1/ED':data[0], '1/MD':data[1], '1/ED^2':data[2], '1/MD^2':data[3]})\n",
    "                    data_line2 = str({'Eel':data[4], 'Eaz':data[5], 'Mel':data[6], 'Maz':data[7]})\n",
    "                    data_line3 = str({'E3el':data[8], 'E3az':data[9], 'M3el':data[10], 'M3az':data[11]})\n",
    "                    plottitle += '\\nffi: ' + ffi + ' , O' + orbit + '\\n' + data_line1 + '\\n' + data_line2 + '\\n' + data_line3\n",
    "                    axes[b].set_title(plottitle)\n",
    "\n",
    "                    # plot plots\n",
    "                    im_pred = axes[b].imshow(samples[i][b].cpu()[0], cmap=\"gray\", vmin=0, vmax=1)\n",
    "                    fig.colorbar(im_pred, fraction=0.046, pad=0.04)\n",
    "                fig.savefig(os.path.join(self.images_samples_valid_dir, f\"valid_global_step{self.global_step}_sample{i}_batch{b}.pdf\"))\n",
    "                plt.close()\n",
    "\n",
    "        # save loss\n",
    "        mean = np.mean(mean_loss)\n",
    "        with open(os.path.join(self.log_dir, \"checkpoint_NLL_valid.txt\"), \"a\") as nll_file:\n",
    "            nll_file.write(str(self.global_step) + \"\\t\" + \"{:.5f}\".format(mean) + \"\\n\")\n",
    "\n",
    "        \n",
    "        print (\"Finish Validating\")\n",
    "        self.graph.train()\n",
    "\n",
    "\n",
    "    def train(self):\n",
    "\n",
    "        self.graph.train()\n",
    "\n",
    "        starttime = time.time()\n",
    "\n",
    "        # for plotting training loss and predicitons on training set, done every checkpoint\n",
    "        mean_loss = list()\n",
    "        samples = list()\n",
    "        samples_ffi_nums = list()\n",
    "        samples_x = list()\n",
    "        samples_orbit = list()\n",
    "\n",
    "        # run\n",
    "        num_batchs = len(self.trainingset_loader)\n",
    "        total_its = self.num_epochs * num_batchs\n",
    "        for epoch in range(self.num_epochs):\n",
    "            mean_nll = 0.0\n",
    "            for i_batch, batch in enumerate(self.trainingset_loader):\n",
    "                \n",
    "                self.optim.zero_grad()\n",
    "\n",
    "                x = batch[\"x\"]\n",
    "                y = batch[\"y\"]\n",
    "                ffi_num = batch['ffi_num']\n",
    "                orbit = batch['orbit']\n",
    "\n",
    "                if self.cuda:\n",
    "                    x = x.cuda()\n",
    "                    y = y.cuda()\n",
    "\n",
    "                # forward\n",
    "                z, nll = self.graph(x, y)\n",
    "\n",
    "\n",
    "                # loss\n",
    "                loss = torch.mean(nll)\n",
    "                mean_nll = mean_nll + loss.data\n",
    "\n",
    "                # backward\n",
    "                self.graph.zero_grad()\n",
    "                self.optim.zero_grad()\n",
    "                loss.backward()\n",
    "\n",
    "                # operate grad\n",
    "                if self.max_grad_clip > 0:\n",
    "                    torch.nn.utils.clip_grad_value_(self.graph.parameters(), self.max_grad_clip)\n",
    "                if self.max_grad_norm > 0:\n",
    "                    torch.nn.utils.clip_grad_norm_(self.graph.parameters(), self.max_grad_norm)\n",
    "\n",
    "                # step\n",
    "                self.optim.step()\n",
    "\n",
    "                currenttime = time.time()\n",
    "                elapsed = currenttime - starttime\n",
    "                print(\"Iteration: {}/{} \\t Elapsed time: {:.2f} \\t Loss:{:.5f}\".format(self.global_step, total_its, elapsed, loss.data))\n",
    "\n",
    "                if self.global_step % self.nll_gap == 0:\n",
    "                    with open(os.path.join(self.log_dir, \"step_NLL_train.txt\"), \"a\") as nll_file:\n",
    "                        nll_file.write(str(self.global_step) + \" \\t \" + \"{:.2f} \\t {:.5f}\".format(elapsed, loss.data) + \"\\n\")\n",
    "\n",
    "\n",
    "\n",
    "                # checkpoint\n",
    "                if self.global_step % self.checkpoints_gap == 0 and self.global_step > 0:\n",
    "                    self.validate()\n",
    "\n",
    "                    # records training loss and predictions on training set\n",
    "                    mean_loss = list()\n",
    "                    samples = list()\n",
    "                    samples_ffi_nums = list()\n",
    "                    samples_x = list()\n",
    "                    samples_orbit = list()\n",
    "                    with torch.no_grad():\n",
    "\n",
    "                        # have to do another trainingset_loader because we have to use torch.nograd()\n",
    "                        # that turns off the gradient descent\n",
    "                        for i_batch, batch in enumerate(self.trainingset_loader):\n",
    "                            \n",
    "                            mean_loss.append(loss.data.cpu().item())\n",
    "            \n",
    "                            # true label\n",
    "                            y_true = y\n",
    "            \n",
    "                            # sample\n",
    "                            batch_samples = []\n",
    "                            batch_samples_ffi_num = []\n",
    "                            batch_samples_x = []\n",
    "                            batch_samples_orbit = []\n",
    "                            for b in range(len(y)):\n",
    "                                row = []\n",
    "                                row.append(y_true[b])\n",
    "                                batch_samples.append(row)\n",
    "            \n",
    "                                batch_samples_ffi_num.append([ffi_num[b]])\n",
    "                                batch_samples_x.append([x[b]])\n",
    "                                batch_samples_orbit.append([orbit[b]])\n",
    "                                \n",
    "                            for i in range(0, 5):\n",
    "                                y_sample,_ = self.graph(x, y=None, reverse=True)\n",
    "            \n",
    "                                for b in range(len(y)):\n",
    "                                    batch_samples[b].append(y_sample[b])\n",
    "            \n",
    "                                    batch_samples_ffi_num[b].append(ffi_num[b])\n",
    "                                    batch_samples_x[b].append(x[b])\n",
    "                                    batch_samples_orbit[b].append(orbit[b])\n",
    "            \n",
    "                            for b in range(len(y)):\n",
    "                                samples.append(batch_samples[b])\n",
    "                                samples_ffi_nums.append(batch_samples_ffi_num[b])\n",
    "                                samples_x.append(batch_samples_x[b])\n",
    "                                samples_orbit.append(batch_samples_orbit[b])\n",
    "                                \n",
    "                    # save samples\n",
    "                    for i in range(0, len(samples)):\n",
    "                        if i < 5:\n",
    "                            fig, axes = plt.subplots(1, len(samples[i]), figsize=(40, 7))\n",
    "                            for b in range(0,len(samples[i])):\n",
    "                                \n",
    "                                # create title\n",
    "                                ffi = samples_ffi_nums[i][b]\n",
    "                                orbit = samples_orbit[i][b]\n",
    "                                plottitle = ''\n",
    "                                if b == 0: plottitle = 'y true: TRAINING SET'\n",
    "                                elif b == 1: plottitle = 'y inverse (use x and y)'\n",
    "                                else: plottitle = f'(Using only x_cond) sample {b-1}'\n",
    "                                data = [round(float(elt), 2) for elt in samples_x[i][b][0]]\n",
    "                                data_line1 = str({'1/ED':data[0], '1/MD':data[1], '1/ED^2':data[2], '1/MD^2':data[3]})\n",
    "                                data_line2 = str({'Eel':data[4], 'Eaz':data[5], 'Mel':data[6], 'Maz':data[7]})\n",
    "                                data_line3 = str({'E3el':data[8], 'E3az':data[9], 'M3el':data[10], 'M3az':data[11]})\n",
    "                                plottitle += '\\nffi: ' + ffi + ' , O' + orbit + '\\n' + data_line1 + '\\n' + data_line2 + '\\n' + data_line3\n",
    "                                axes[b].set_title(plottitle)\n",
    "\n",
    "                                # plot plots\n",
    "                                im_pred = axes[b].imshow(samples[i][b].cpu()[0], cmap=\"gray\", vmin=0, vmax=1)\n",
    "                                fig.colorbar(im_pred, fraction=0.046, pad=0.04)\n",
    "                            fig.savefig(os.path.join(self.images_samples_train_dir, f\"train_global_step{self.global_step}_sample{i}_batch{b}.pdf\"))\n",
    "                            plt.close()\n",
    "            \n",
    "                    # save loss\n",
    "                    mean = np.mean(mean_loss)\n",
    "                    with open(os.path.join(self.log_dir, \"checkpoint_NLL_train.txt\"), \"a\") as nll_file:\n",
    "                        nll_file.write(str(self.global_step) + \"\\t\" + \"{:.5f}\".format(mean) + \"\\n\")\n",
    "\n",
    "\n",
    "                    # # samples\n",
    "                    # samples = []\n",
    "                    # for b in range(self.batch_size):\n",
    "                    #     samples.append([])\n",
    "\n",
    "                    # for i in range(0,5):\n",
    "                    #     y_sample,_ = self.graph(x, y=None, reverse=True)\n",
    "                    #     for b in range(self.batch_size):\n",
    "                    #         samples[b].append(y_sample[b])\n",
    "\n",
    "                    # # inverse image\n",
    "                    # y_inverse,_ = self.graph(x, y=z, reverse=True)\n",
    "\n",
    "                    # # true label\n",
    "                    # y_true = y\n",
    "\n",
    "\n",
    "                    # # save images\n",
    "                    # output = None\n",
    "                    # for b in range(0, self.batch_size):\n",
    "\n",
    "                    #     fig, axes = plt.subplots(1, 3, figsize=(20, 8))\n",
    "\n",
    "                    #     if b < 5:\n",
    "                    #         angles_str = str([round(float(elt), 2) for elt in x[b][0]])\n",
    "                    #         angles_title = str(['E3el', 'E3az', 'M3el', 'M3az', '1/ED', '1/MD', '1/ED^2', '1/MD^2'])\n",
    "                    \n",
    "                    #         axes[0].set_title('y Truth\\nffi: ' + ffi_num[b] + ' , O' + orbit[b] + '\\nangles: ' + angles_str + '\\n' + angles_title)\n",
    "                    #         im_true = axes[0].imshow(y[b].cpu()[0], cmap=\"gray\", vmin=0, vmax=1)\n",
    "                    #         fig.colorbar(im_true, fraction=0.046, pad=0.04)\n",
    "\n",
    "                    #         axes[1].set_title('y inverse\\nffi: ' + ffi_num[b] + ' , O' + orbit[b] + '\\nangles: ' + angles_str + '\\n' + angles_title)\n",
    "                    #         im_latent = axes[1].imshow(y_inverse[b].cpu()[0], cmap=\"gray\", vmin=0, vmax=1)\n",
    "                    #         fig.colorbar(im_latent, fraction=0.046, pad=0.04)\n",
    "\n",
    "                    #         axes[2].set_title('y Prediction sample\\nffi: ' + ffi_num[b] + ' , O' + orbit[b] + '\\nangles: ' + angles_str + '\\n' + angles_title)\n",
    "                    #         im_pred = axes[2].imshow(samples[b][i].cpu()[0], cmap=\"gray\", vmin=0, vmax=1)\n",
    "                    #         fig.colorbar(im_pred, fraction=0.046, pad=0.04)\n",
    "\n",
    "                    #         fig.savefig(os.path.join(self.images_inverse_train_dir, f\"global_step-{self.global_step}_batch_{b}.pdf\"))\n",
    "                    #         plt.close()\n",
    "\n",
    "\n",
    "\n",
    "        \n",
    "                    # plot loss graphs\n",
    "                    def read_data(file_path):\n",
    "                        data = {'x': [], 'y': []}\n",
    "                        with open(file_path, 'r') as file:\n",
    "                            for line in file:\n",
    "                                x, y = map(float, line.strip().split())\n",
    "                                data['x'].append(x)\n",
    "                                data['y'].append(y)\n",
    "                        return data\n",
    "            \n",
    "                    valid_loss = read_data(os.path.join(self.log_dir, \"checkpoint_NLL_valid.txt\"))\n",
    "                    train_loss = read_data(os.path.join(self.log_dir, \"checkpoint_NLL_train.txt\"))\n",
    "                    plt.plot(valid_loss['x'], valid_loss['y'], label='Validation NLL')\n",
    "                    plt.plot(train_loss['x'], train_loss['y'], label='Training NLL')\n",
    "                    plt.xlabel('Global step')\n",
    "                    plt.ylabel('NLL')\n",
    "                    plt.title('NLL: Training & Validation')\n",
    "                    plt.legend()\n",
    "                    plt.savefig(os.path.join(self.log_dir, \"graph_NLL_valid_train.png\"))\n",
    "                    plt.close()\n",
    "                \n",
    "\n",
    "                # save model\n",
    "                if self.global_step % self.save_gap == 0 and self.global_step > 0:\n",
    "                    save_model(self.graph, self.optim, self.scheduler, self.checkpoints_dir, self.global_step)\n",
    "\n",
    "                self.global_step = self.global_step + 1\n",
    "\n",
    "            if self.scheduler is not None:\n",
    "                self.scheduler.step()\n",
    "            mean_nll = float(mean_nll / float(num_batchs))\n",
    "            with open(os.path.join(self.log_dir, \"epoch_NLL_train.txt\"), \"a\") as f:\n",
    "                currenttime = time.time()\n",
    "                elapsed = currenttime - starttime\n",
    "                f.write(\"{} \\t {:.2f}\\t {:.5f}\".format(epoch, elapsed, mean_nll) + \"\\n\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "class Inferencer(object):\n",
    "\n",
    "    def __init__(self, model, dataset, args, cuda):\n",
    "\n",
    "        # set path and date\n",
    "        self.out_root = args.out_root\n",
    "        if not os.path.exists(self.out_root):\n",
    "            os.makedirs(self.out_root)\n",
    "\n",
    "        # cuda\n",
    "        self.cuda = cuda\n",
    "\n",
    "        # model\n",
    "        self.model = model\n",
    "\n",
    "\n",
    "        # data\n",
    "        self.dataset_name = args.dataset_name\n",
    "        self.batch_size = args.batch_size\n",
    "        self.data_loader = DataLoader(dataset,\n",
    "                                      batch_size=self.batch_size,\n",
    "                                      shuffle=False,\n",
    "                                      drop_last=False)\n",
    "\n",
    "        self.label_scale = args.label_scale\n",
    "        self.label_bias = args.label_bias\n",
    "        self.num_labels = args.num_labels\n",
    "        self.y_bins = args.y_bins\n",
    "        self.x_bins = args.x_bins\n",
    "\n",
    "\n",
    "    def sampled_based_prediction(self, n_samples):\n",
    "        metrics = []\n",
    "        start = time.time()\n",
    "        for i_batch, batch in enumerate(self.data_loader):\n",
    "            print(f\"Batch IDs: {i_batch}\")\n",
    "\n",
    "            x = batch[\"x\"]\n",
    "            y = batch[\"y\"]\n",
    "            ffi_num = batch['ffi_num']\n",
    "            orbit = batch['orbit']\n",
    "\n",
    "            if self.cuda:\n",
    "                x = x.cuda()\n",
    "                y = y.cuda()\n",
    "\n",
    "\n",
    "            sample_list = list()\n",
    "            nll_list = list()\n",
    "            for i in range(0, n_samples):\n",
    "\n",
    "                print(f\"Samples: {i}/{n_samples}\")\n",
    "\n",
    "                # z, nll = self.model(x, y, reverse=False)\n",
    "                # ypred, logdet = self.model(x, z, reverse=True)\n",
    "\n",
    "                y_sample,_ = self.model(x, reverse=True)\n",
    "                _, nll = self.model(x,y_sample)\n",
    "                loss = torch.mean(nll)\n",
    "                sample_list.append(y_sample)\n",
    "                nll_list.append(loss.data.cpu().numpy())\n",
    "\n",
    "            sample = torch.stack(sample_list)\n",
    "            sample_mean = torch.mean(sample, dim=0, keepdim=False)\n",
    "            sample_std = torch.std(sample, dim=0, keepdim=False)\n",
    "            nll = np.mean(nll_list)\n",
    "\n",
    "            y_pred_imgs_mean = sample_mean\n",
    "            y_pred_imgs_std = sample_std\n",
    "            y_true_imgs = y\n",
    "\n",
    "            # save trues and preds using only xcond\n",
    "            output = []\n",
    "            for i in range(0, len(y_true_imgs)):\n",
    "                true_img = y_true_imgs[i]\n",
    "                pred_img = y_pred_imgs_mean[i]\n",
    "                pred_img_std = y_pred_imgs_std[i]\n",
    "\n",
    "                # print(sample.shape)\n",
    "\n",
    "                fig, axes = plt.subplots(1, 3, figsize=(20, 8))\n",
    "                xc = x[i].cpu()[0]\n",
    "\n",
    "                angles_str = str([round(float(elt), 2) for elt in xc])\n",
    "                angles_title = str(['E3el', 'E3az', 'M3el', 'M3az', '1/ED', '1/MD', '1/ED^2', '1/MD^2'])\n",
    "                # print(f\"E-el: {xc[0]}, E-az: {xc[1]}, M-el: {xc[2]}, M-az: {xc[3]}, 1/ED: {xc[4]}, 1/MD: {xc[5]}, 1/MD^2: {xc[6]}, 1/ED^2: {xc[7]}]\")\n",
    "\n",
    "                \n",
    "                axes[0].set_title('Truth\\nffi: ' + ffi_num[i] + ' , O' + orbit[i] + '\\nangles: ' + angles_str + '\\n' + angles_title)#, f\"E-el: {xc[0]}, E-az: {xc[1]}, M-el: {xc[2]}, M-az: {xc[3]}, 1/ED: {xc[4]}, 1/MD: {xc[5]}, 1/MD^2: {xc[6]}, 1/ED^2: {xc[7]}\")\n",
    "                im_true = axes[0].imshow(true_img.cpu()[0], cmap=\"gray\", vmin=0, vmax=1)\n",
    "                fig.colorbar(im_true, fraction=0.046, pad=0.04)\n",
    "\n",
    "                axes[1].set_title('Prediction mean (using only xcond)\\nffi: ' + ffi_num[i] + ' , O' + orbit[i] + '\\nangles: ' + angles_str + '\\n' + angles_title)\n",
    "                im_pred = axes[1].imshow(pred_img.cpu()[0], cmap=\"gray\", vmin=0, vmax=1)\n",
    "                fig.colorbar(im_pred, fraction=0.046, pad=0.04)\n",
    "\n",
    "                # print(pred_img_std.cpu()[0])\n",
    "                axes[2].set_title('Prediction std (using only xcond)\\nffi: ' + ffi_num[i] + ', O' + orbit[i] + '\\nangles: ' + angles_str + '\\n' + angles_title)\n",
    "                im_pred = axes[2].imshow(pred_img_std.cpu()[0], cmap=\"gray\", vmin=0, vmax=1)\n",
    "                fig.colorbar(im_pred, fraction=0.046, pad=0.04)\n",
    "\n",
    "                fig.savefig(os.path.join(self.out_root, f\"batch_xcond-{i_batch}-{i}.pdf\"))\n",
    "                plt.close()\n",
    "\n",
    "            # Save plots using xcond and y\n",
    "            z, nll = self.model(x, y)\n",
    "            y_inverse, _ = self.model(x, y=z, reverse=True)\n",
    "\n",
    "            for i in range(0, len(y)):\n",
    "                true_img = y[i]\n",
    "                pred_img = y_inverse[i]\n",
    "\n",
    "                fig, axes = plt.subplots(1, 2, figsize=(20, 8))\n",
    "\n",
    "                angles_str = str([round(float(elt), 2) for elt in x[i][0]])\n",
    "                angles_title = str(['E3el', 'E3az', 'M3el', 'M3az', '1/ED', '1/MD', '1/ED^2', '1/MD^2'])\n",
    "\n",
    "                axes[0].set_title('y Truth\\nffi: ' + ffi_num[i] + ', O' + orbit[i] + '\\nangles: ' + angles_str + '\\n' + angles_title)\n",
    "                im_true = axes[0].imshow(true_img.cpu()[0], cmap=\"gray\", vmin=0, vmax=1)\n",
    "                fig.colorbar(im_true, fraction=0.046, pad=0.04)\n",
    "\n",
    "                axes[1].set_title('y inverse\\nffi: ' + ffi_num[i] + ', O' + orbit[i] + '\\nangles: ' + angles_str + '\\n' + angles_title)\n",
    "                im_latent = axes[1].imshow(pred_img.cpu()[0], cmap=\"gray\", vmin=0, vmax=1)\n",
    "                fig.colorbar(im_latent, fraction=0.046, pad=0.04)\n",
    "\n",
    "                fig.savefig(os.path.join(self.out_root, f\"batch_xandy-{i_batch}-{i}.pdf\"))\n",
    "                plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 200,
   "metadata": {
    "executionInfo": {
     "elapsed": 15,
     "status": "ok",
     "timestamp": 1692391055505,
     "user": {
      "displayName": "Joseph Lupo",
      "userId": "18192058496751055649"
     },
     "user_tz": 240
    },
    "id": "T9jhxCYOndFo"
   },
   "outputs": [],
   "source": [
    "# # @title Datasets\n",
    "# import os\n",
    "# import numpy as np\n",
    "# import torch\n",
    "# import PIL.Image as Image\n",
    "# from torch.utils import data\n",
    "# import torchvision.transforms as transforms\n",
    "\n",
    "\n",
    "# class HorseDataset(data.Dataset):\n",
    "\n",
    "#     def __init__(self, dir, size, n_c, portion=\"train\"):\n",
    "#         self.dir = dir\n",
    "#         self.names = self.read_names(dir, portion)\n",
    "#         self.n_c = n_c\n",
    "#         self.size = size\n",
    "\n",
    "#     def read_names(self, dir, portion):\n",
    "\n",
    "#         path = os.path.join(dir, \"{}.txt\".format(portion))\n",
    "#         names = list()\n",
    "#         with open(path, \"r\") as f:\n",
    "#             for line in f:\n",
    "#                 line = line.strip()\n",
    "#                 name = {}\n",
    "#                 name[\"img\"] = os.path.join(dir, os.path.join(\"images\", line))\n",
    "#                 name[\"lbl\"] = os.path.join(dir, os.path.join(\"labels\", line))\n",
    "#                 names.append(name)\n",
    "#         return names\n",
    "\n",
    "#     def __len__(self):\n",
    "#         return len(self.names)\n",
    "\n",
    "\n",
    "#     def __getitem__(self, index):\n",
    "\n",
    "#         # path\n",
    "#         name = self.names[index]\n",
    "#         img_path = name[\"img\"]\n",
    "#         lbl_path = name[\"lbl\"]\n",
    "#         transform = transforms.Compose([transforms.Resize(self.size), transforms.ToTensor()])\n",
    "\n",
    "#         # img\n",
    "#         img = Image.open(img_path).convert(\"RGB\")\n",
    "#         img = transform(img)\n",
    "\n",
    "#         # lbl\n",
    "#         lbl = Image.open(lbl_path).convert(\"L\")\n",
    "#         lbl = transform(lbl)\n",
    "#         lbl = torch.round(lbl)\n",
    "#         if self.n_c > 1:\n",
    "#             lbl = lbl.repeat(self.n_c,1,1)\n",
    "\n",
    "#         return {\"x\":img, \"y\":lbl}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 201,
   "metadata": {
    "executionInfo": {
     "elapsed": 14,
     "status": "ok",
     "timestamp": 1692391055506,
     "user": {
      "displayName": "Joseph Lupo",
      "userId": "18192058496751055649"
     },
     "user_tz": 240
    },
    "id": "FR8B9zdYndHt"
   },
   "outputs": [],
   "source": [
    "# @title TESS Dataset\n",
    "import os\n",
    "from PIL import Image\n",
    "import torch\n",
    "import pickle\n",
    "from torch.utils.data import Dataset, DataLoader, random_split\n",
    "import torchvision.transforms as transforms\n",
    "\n",
    "class TESSDataset(Dataset):\n",
    "    def __init__(self):\n",
    "\n",
    "        # self.data = []\n",
    "        # self.labels = []\n",
    "\n",
    "        # get data\n",
    "        angle_folder = \"/pdo/users/jlupoiii/TESS/data/angles/\"\n",
    "        ccd_folder = \"/pdo/users/jlupoiii/TESS/data/ccds_background_subtracted/\"\n",
    "\n",
    "        # data matrices\n",
    "        X = []\n",
    "        Y = []\n",
    "        ffi_nums = []\n",
    "\n",
    "        self.angles_dic = pickle.load(open(angle_folder+'angles_O13_data_dic.pkl', \"rb\"))\n",
    "        # angles_dic = pickle.load(open(angle_folder+'angles_O13_data.pkl', \"rb\"))\n",
    "\n",
    "\n",
    "        # self.ffi_num_to_orbit_dic = pickle.load(open(angle_folder+'ffi_num_to_orbit_dic.pkl', \"rb\"))\n",
    "\n",
    "        for filename in os.listdir(ccd_folder):\n",
    "            if len(filename) < 40 or filename[27] != '3': continue\n",
    "\n",
    "            image_arr = pickle.load(open(ccd_folder+filename, \"rb\"))\n",
    "            ffi_num = filename[18:18+8]\n",
    "            try:\n",
    "                angles = self.angles_dic[ffi_num]\n",
    "                # print('Got ffi number', ffi_num)\n",
    "            except:\n",
    "                # print('Could not find ffi with number:', ffi_num)\n",
    "                continue\n",
    "            # X.append(np.array([angles[10], angles[11], angles[18], angles[19], angles[22], angles[23], angles[24], angles[25]]))\n",
    "            X.append(np.array([angles['1/ED'], angles['1/MD'], angles['1/ED^2'], angles['1/MD^2'], angles['Eel'], angles['Eaz'], angles['Mel'], angles['Maz'], angles['E3el'], angles['E3az'], angles['M3el'], angles['M3az']]))\n",
    "\n",
    "            Y.append(image_arr.flatten())\n",
    "            ffi_nums.append(ffi_num)\n",
    "\n",
    "        # for x in X:\n",
    "        #     print(x)\n",
    "        \n",
    "        # X = np.array(X)\n",
    "        # Y = np.array(Y)\n",
    "        # ffis = np.array(ffis)\n",
    "        # we are calculating Y GIVEN X\n",
    "        self.data = [Image.fromarray(x) for x in X]\n",
    "        self.labels = [Image.fromarray(y) for y in Y]\n",
    "        self.ffi_nums = ffi_nums\n",
    "\n",
    "        # for s in self.labels:\n",
    "        #     print(s.size)\n",
    "        #     # print(np.array(s).reshape((16,16))\n",
    "            \n",
    "        #     plt.imshow(np.array(s).reshape((16,16)))\n",
    "        #     plt.show()\n",
    "        #     plt.close()\n",
    "\n",
    "\n",
    "\n",
    "        # for class_idx, class_name in enumerate(self.classes):\n",
    "        #     # print(class_idx)\n",
    "        #     horse_img_path = os.path.join(self.root_dir, \"horse\", class_name)\n",
    "        #     mask_img_path = os.path.join(self.root_dir, \"mask\", class_name)\n",
    "\n",
    "        #     horse_image = Image.open(horse_img_path).convert('RGB')\n",
    "        #     mask_image = Image.open(mask_img_path).convert('L')\n",
    "\n",
    "        #     self.data.extend([horse_image])\n",
    "        #     self.labels.extend([mask_image])\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        # horse_img_path, mask_img_path = self.data[idx], self.labels[idx]\n",
    "        # label = self.labels[idx]\n",
    "        # horse_image = Image.open(horse_img_path).convert('RGB')\n",
    "        # mask_image = Image.open(mask_img_path).convert('L')\n",
    "\n",
    "        angles_image = self.data[idx]\n",
    "        ffi_image = self.labels[idx]\n",
    "        ffi_num = self.ffi_nums[idx]\n",
    "        # orbit = self.ffi_num_to_orbit_dic[ffi_num]\n",
    "        orbit = self.angles_dic[ffi_num][\"orbit\"]\n",
    "\n",
    "        transform = transforms.Compose([\n",
    "            # transforms.Resize((1, 8)),\n",
    "            transforms.ToTensor(),\n",
    "            lambda s: s.reshape(1, 12),\n",
    "            # transforms.Normalize(mean=[0.456], std=[0.225])\n",
    "        ])\n",
    "        target_transform = transforms.Compose([\n",
    "            lambda s: np.array(s),\n",
    "            lambda s: s.reshape((16,16)),\n",
    "            # transforms.Resize((16, 16)),\n",
    "            transforms.ToTensor()\n",
    "        ])\n",
    "\n",
    "        angles_image = transform(angles_image)\n",
    "        ffi_image = target_transform(ffi_image)\n",
    "        \n",
    "        # return x=angles, y=ffis\n",
    "        # we are calculating X GIVEN Y\n",
    "        return {\"x\":angles_image, \"y\":ffi_image, \"ffi_num\": ffi_num, \"orbit\": orbit}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 202,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 6458,
     "status": "ok",
     "timestamp": 1692391061951,
     "user": {
      "displayName": "Joseph Lupo",
      "userId": "18192058496751055649"
     },
     "user_tz": 240
    },
    "id": "4S1ymwTMm1gO",
    "outputId": "60acbfb8-8096-4873-a1f1-0f1719f8514a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "693\n",
      "torch.Size([1, 12])\n",
      "torch.Size([1, 16, 16])\n",
      "00007795\n",
      "13\n",
      "x: tensor([ 0.8606,  0.7342,  0.7406,  0.5391, -0.2304, -2.0176,  0.3072, -2.8466,\n",
      "         1.8867,  4.3197,  1.4643,  3.4226])\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAesAAAGiCAYAAADHpO4FAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjcuMiwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy8pXeV/AAAACXBIWXMAAA9hAAAPYQGoP6dpAAAri0lEQVR4nO3df3RU5Z3H8c8kgQliEn4tCcFgUpeK/DAoP7IRXeGYNQdplNNjRUohG1tcNVEgXQtpDVGpRLSyUUmhsEXYc0Swewq6SuFgNCJHEElMV04FZEHI4iaB05qRUJJ05u4fytRpBpjkzjD3Yd6vc54/5s6983zDMX7z/T7P3OuyLMsSAABwrLhoBwAAAC6MZA0AgMORrAEAcDiSNQAADkeyBgDA4UjWAAA4HMkaAACHI1kDAOBwJGsAAByOZA0AgMORrAEA6IadO3eqoKBA6enpcrlc2rJly0Wvqa2t1Y033ii3262///u/17p167o1J8kaAIBuaGtrU3Z2tqqrq0M6/+jRo5o2bZqmTJmihoYGzZ8/Xz/60Y+0ffv2kOd08SAPAAB6xuVyafPmzZo+ffp5z1m4cKHefPNN7d+/33/s3nvv1RdffKFt27aFNE+C3UDDzefz6fPPP1dSUpJcLle0wwEAdJNlWfryyy+Vnp6uuLjINXDPnj2rjo4O259jWVaXfON2u+V2u21/tiTt3r1beXl5Acfy8/M1f/78kD/Dccn6888/V0ZGRrTDAADY1NjYqKuuuioin3327FllXX2lmlq8tj/ryiuv1OnTpwOOVVRU6PHHH7f92ZLU1NSk1NTUgGOpqanyeDz685//rD59+lz0MxyXrJOSkiRJN+sOJahXlKMBAHTXX9SpXdrq//95JHR0dKipxaujdVcrOann1bvnS5+yxh1TY2OjkpOT/cfDVVWHi+OS9blWRIJ6KcFFsgYA43y9E+pSLGUmJ8XZStb+z0lODkjW4ZSWlqbm5uaAY83NzUpOTg6pqpYcmKwBAAiV1/LJa2ObtNfyhS+Y88jNzdXWrVsDju3YsUO5ubkhfwZf3QIAGMsny/bortOnT6uhoUENDQ2SvvpqVkNDg44fPy5JKisr05w5c/znP/DAAzpy5Ih+8pOf6MCBA/rlL3+pV199VQsWLAh5TiprAICxfPLJTm3ck6v37dunKVOm+F+XlpZKkgoLC7Vu3Tr93//9nz9xS1JWVpbefPNNLViwQM8//7yuuuoq/fu//7vy8/NDnpNkDQBAN0yePFkXukVJsLuTTZ48WR999FGP5yRZAwCM5bUseW3c28vOtZdSxNasq6urlZmZqcTEROXk5Gjv3r2RmgoAEKOisWYdDRFJ1ps2bVJpaakqKipUX1+v7Oxs5efnq6WlJRLTAQBwWYtIsl6+fLnmzp2roqIijRw5UqtWrdIVV1yhtWvXdjm3vb1dHo8nYAAAEAqfLHltjJitrDs6OlRXVxdwH9S4uDjl5eVp9+7dXc6vrKxUSkqKf3CrUQBAqGiD99CpU6fk9XqD3ge1qampy/llZWVqbW31j8bGxnCHBACA0aK+GzycTzYBAMSWWNkNHvZkPWjQIMXHxwe9D2paWlq4pwMAxDDf18PO9SYIexu8d+/eGjdunGpqavzHfD6fampqunUfVAAA8JWItMFLS0tVWFio8ePHa+LEiaqqqlJbW5uKiooiMR0AIEad29Vt53oTRCRZz5gxQydPntTixYvV1NSksWPHatu2bV02nQEAYIfXks2nboUvlkiK2AazkpISlZSUROrjAQBgzRoAADhD1L+6BQBAT/nkklcuW9ebgGQNADCWz/pq2LneBLTBAQBwOCprAICxvDbb4HauvZRI1gAAY8VKsqYNDgCAw1FZAwCM5bNc8lk2doPbuPZSIlkDAIxFGxwAADgClTUAwFhexclro+70hjGWSCJZAwCMZdlcs7ZYswYAILJYswYAAI5AZQ0AMJbXipPXsrFmbci9wUnWAABj+eSSz0aT2CczsjVtcAAAHI7KGgBgrFjZYEayBgAYy/6aNW1wAAAQBlTWAABjfbXBzMaDPGiDAwAQWT6btxtlNzgAAAgLKmsAgLFiZYMZyRoAYCyf4mLipigkawCAsbyWS14bT86yc+2lRLKG2Vxm/KIhygxpdQLnQ7IGABjLa3M3uJc2OAAAkeWz4uSzscHMZ0jXha9uAQDgcFTWAABj0QYHAMDhfLK3o9sXvlAiijY4AAAOR2UNADCW/ZuimFGzkqwBAMayf7tRM5J12KOsrKzUhAkTlJSUpMGDB2v69Ok6ePBguKcBACBmhD1Zv/vuuyouLtaePXu0Y8cOdXZ26vbbb1dbW1u4pwIAxLhzz7O2M0wQ9jb4tm3bAl6vW7dOgwcPVl1dnf7xH/+xy/nt7e1qb2/3v/Z4POEOCQBwmaINHiatra2SpAEDBgR9v7KyUikpKf6RkZER6ZAAAJeJc9+ztjNMENEofT6f5s+fr0mTJmn06NFBzykrK1Nra6t/NDY2RjIkAACME9Hd4MXFxdq/f7927dp13nPcbrfcbnckwwAAXKZ8lks+OzdFifVHZJaUlOiNN97Qzp07ddVVV0VqGgBADPPZbGXH7PesLcvSww8/rM2bN6u2tlZZWVnhngIAgJgS9mRdXFysDRs26LXXXlNSUpKampokSSkpKerTp0+4pwMAxDD7j8iM0cp65cqVkqTJkycHHH/ppZf0z//8z+GeDgAQw7xyyWvju9J2rr2UItIGBwAA4cO9wQEAxqINDgCAw3llr5XtDV8oEWXGnxQAAMQwKmtEnsuMDRwAzEMbHAAAh+NBHgAAOJxl8/GYVg/Xu6urq5WZmanExETl5ORo7969Fzy/qqpK1157rfr06aOMjAwtWLBAZ8+eDXk+kjUAAN2wadMmlZaWqqKiQvX19crOzlZ+fr5aWlqCnr9hwwYtWrRIFRUV+uSTT/TrX/9amzZt0k9/+tOQ5yRZAwCMda4Nbmd01/LlyzV37lwVFRVp5MiRWrVqla644gqtXbs26Pnvv/++Jk2apO9///vKzMzU7bffrpkzZ160Gv8mkjUAwFjnnrplZ0iSx+MJGO3t7UHn6+joUF1dnfLy8vzH4uLilJeXp927dwe95qabblJdXZ0/OR85ckRbt27VHXfcEfLPSbIGAMS8jIwMpaSk+EdlZWXQ806dOiWv16vU1NSA46mpqf5nYfyt73//+3ryySd18803q1evXrrmmms0efLkbrXB2Q0OADCW1+YjMs9d29jYqOTkZP9xt9ttO7ZzamtrtXTpUv3yl79UTk6ODh8+rHnz5mnJkiUqLy8P6TNI1gAAY32zld3T6yUpOTk5IFmfz6BBgxQfH6/m5uaA483NzUpLSwt6TXl5uWbPnq0f/ehHkqQxY8aora1N999/v372s58pLu7if2zQBgcAIES9e/fWuHHjVFNT4z/m8/lUU1Oj3NzcoNecOXOmS0KOj4+XFPrDr6isAQDG8ilOPht1Z0+uLS0tVWFhocaPH6+JEyeqqqpKbW1tKioqkiTNmTNHQ4cO9a97FxQUaPny5brhhhv8bfDy8nIVFBT4k/bFkKwBAMbyWi55bbTBe3LtjBkzdPLkSS1evFhNTU0aO3astm3b5t90dvz48YBK+rHHHpPL5dJjjz2mEydO6O/+7u9UUFCgp556KuQ5XZbDHkDt8XiUkpKiybpLCa5e0Q4H4cC9wRFtzvrf3GXvL1anavWaWltbQ1oH7olzueLB974r95U9zxXtpzu18pbfRjTWcKCyBgAYK1wbzJyOZA0AMJZl86lbliEP8iBZAwCM5ZVL3h4+jOPc9SYw408KAABiGJU1AMBYPsveurPPkL2HJGsAgLF8Ntes7Vx7KZkRJQAAMYzKGgBgLJ9c8tnYJGbn2kuJZA0AMFY07mAWDbTBAQBwOCprRB63egQQIbGywYxkDQAwlk82bzdqyJq1GX9SAAAQw6isAQDGsmzuBrcMqaxJ1gAAY/HULQAAHC5WNpiZESUAADGMyhoAYCza4AAAOFys3G404m3wp59+Wi6XS/Pnz4/0VAAAXJYiWll/+OGH+tWvfqXrr78+ktMAAGJUrLTBI1ZZnz59WrNmzdKaNWvUv3//SE0DAIhh55K1nWGCiCXr4uJiTZs2TXl5eRc8r729XR6PJ2AAAIC/ikgbfOPGjaqvr9eHH3540XMrKyv1xBNPRCIMAMBljjZ4DzU2NmrevHl6+eWXlZiYeNHzy8rK1Nra6h+NjY3hDgkAcJmKlTZ42Cvruro6tbS06MYbb/Qf83q92rlzp1asWKH29nbFx8f733O73XK73eEOAwCAy0bYk/Vtt92mjz/+OOBYUVGRRowYoYULFwYkagAA7LBk77vSVvhCiaiwJ+ukpCSNHj064Fjfvn01cODALscBALAjVtasuYMZAMBYJOswqq2tvRTTAABwWaKyBgAYi8oaAACHi5VkzfOsAQBwOCprAICxLMsly0Z1bOfaS4lkDQAwFs+zBgAAjkBlDQAwVqxsMCNZAwCMFStr1rTBAQBwOCprAICxaIMDAOBwsdIGJ1kDAIxl2aysTUnWrFkDAOBwVNYAAGNZkizL3vUmIFkDAIzlk0su7mAGAACijcoaAGAsdoMDAOBwPsslVwx8z5o2OAAADkdlDQAwlmXZ3A1uyHZwkjUAwFixsmZNGxwAAIejsgYAGCtWKmuSNQDAWLGyG5xkDQAwVqxsMGPNGgAAh6OyBgAY66vK2s6adRiDiSCSNQDAWLGywYw2OAAADkdlDQAwliV7z6Q2pAtOsgYAmIs2OAAAcAQqawCAuWKkD05lDQAw19dt8J4O9bANXl1drczMTCUmJionJ0d79+694PlffPGFiouLNWTIELndbn3729/W1q1bQ56PyhoAYKxo3MFs06ZNKi0t1apVq5STk6Oqqirl5+fr4MGDGjx4cJfzOzo69E//9E8aPHiw/vM//1NDhw7VsWPH1K9fv5DnjEhlfeLECf3gBz/QwIED1adPH40ZM0b79u2LxFQAAFxSy5cv19y5c1VUVKSRI0dq1apVuuKKK7R27dqg569du1Z//OMftWXLFk2aNEmZmZm69dZblZ2dHfKcYU/Wf/rTnzRp0iT16tVLv/vd7/SHP/xBzz33nPr37x/uqQAAMc5OC/ybO8k9Hk/AaG9vDzpfR0eH6urqlJeX5z8WFxenvLw87d69O+g1r7/+unJzc1VcXKzU1FSNHj1aS5culdfrDfnnDHsbfNmyZcrIyNBLL73kP5aVlRXuaQAA+GrN2c7Xr76+NiMjI+BwRUWFHn/88S6nnzp1Sl6vV6mpqQHHU1NTdeDAgaBTHDlyRG+//bZmzZqlrVu36vDhw3rooYfU2dmpioqKkMIMe7J+/fXXlZ+fr+9973t69913NXToUD300EOaO3du0PPb29sD/oLxeDzhDgkAgAtqbGxUcnKy/7Xb7Q7bZ/t8Pg0ePFirV69WfHy8xo0bpxMnTujZZ58NOVmHvQ1+5MgRrVy5UsOHD9f27dv14IMP6pFHHtH69euDnl9ZWamUlBT/+Nu/bgAAOJ9zG8zsDElKTk4OGOdL1oMGDVJ8fLyam5sDjjc3NystLS3oNUOGDNG3v/1txcfH+49dd911ampqUkdHR0g/Z9iTtc/n04033qilS5fqhhtu0P3336+5c+dq1apVQc8vKytTa2urfzQ2NoY7JADA5coKw+iG3r17a9y4caqpqfEf8/l8qqmpUW5ubtBrJk2apMOHD8vn8/mPHTp0SEOGDFHv3r1DmjfsyXrIkCEaOXJkwLHrrrtOx48fD3q+2+3u8hcNAABOVVpaqjVr1mj9+vX65JNP9OCDD6qtrU1FRUWSpDlz5qisrMx//oMPPqg//vGPmjdvng4dOqQ333xTS5cuVXFxcchzhn3NetKkSTp48GDAsUOHDunqq68O91QAgBgXjXuDz5gxQydPntTixYvV1NSksWPHatu2bf5NZ8ePH1dc3F9r4YyMDG3fvl0LFizQ9ddfr6FDh2revHlauHBhyHOGPVkvWLBAN910k5YuXap77rlHe/fu1erVq7V69epwTwUAQFRuGVpSUqKSkpKg79XW1nY5lpubqz179vR4vrC3wSdMmKDNmzfrlVde0ejRo7VkyRJVVVVp1qxZ4Z4KAICYEJHbjX7nO9/Rd77znUh8NAAAfrHyiEzuDQ4AMFeMPHWLZA0AMJjr62HneufjEZkAADgclTUAwFy0wQEAcLgYSda0wQEAcDgqawCAucL0iEynI1kDAIz1zSdn9fR6E9AGBwDA4aisAQDmipENZiRrAIC5YmTNmjY4AAAOR2UNADCWy/pq2LneBCRrAIC5WLMGAMDhWLMGAABOQGUNADAXbXAAABwuRpI1bXAAAByOyhoAYK4YqaxJ1gAAc7EbHAAAOAGVNQDAWNzBDAAAp4uRNWva4AAAOBzJGgAAh6MNDgAwlks216zDFklkkawBAObiq1sAAMAJqKwBAOaKkd3gJGsAgLliJFnTBgcAwOGorAEAxuIOZgAAOB1tcAAA4ARhT9Zer1fl5eXKyspSnz59dM0112jJkiWyLEP+fAEAmMMKwzBA2Nvgy5Yt08qVK7V+/XqNGjVK+/btU1FRkVJSUvTII4+EezoAQAxjzbqH3n//fd11112aNm2aJCkzM1OvvPKK9u7dG+6pAACICWFvg990002qqanRoUOHJEm///3vtWvXLk2dOjXo+e3t7fJ4PAEDAICQnLvdqJ1hgLBX1osWLZLH49GIESMUHx8vr9erp556SrNmzQp6fmVlpZ544olwhwEAiAXsBu+ZV199VS+//LI2bNig+vp6rV+/Xr/4xS+0fv36oOeXlZWptbXVPxobG8MdEgDgMnVuzdrOMEHYK+tHH31UixYt0r333itJGjNmjI4dO6bKykoVFhZ2Od/tdsvtdoc7DAAALhthT9ZnzpxRXFxgwR4fHy+fzxfuqQAAsS5G2uBhT9YFBQV66qmnNGzYMI0aNUofffSRli9frvvuuy/cUwEAYp3dVnasJusXX3xR5eXleuihh9TS0qL09HT9y7/8ixYvXhzuqQAAiAlhT9ZJSUmqqqpSVVVVuD8aAIBAtMEBAHC4GEnWPMgDAACHo7IGABgrVu4NTmUNAIDDkawBAHA42uAAAHPFyAYzkjUAwFixsmZNsgYAmM2QhGsHa9YAADgclTUAwFysWQMA4GyxsmZNGxwAAIejsgYAmIs2OAAAzkYbHAAAOALJGgBgLisMoweqq6uVmZmpxMRE5eTkaO/evSFdt3HjRrlcLk2fPr1b85GsAQDmikKy3rRpk0pLS1VRUaH6+nplZ2crPz9fLS0tF7zus88+07/+67/qlltu6facJGsAQMzzeDwBo729/bznLl++XHPnzlVRUZFGjhypVatW6YorrtDatWvPe43X69WsWbP0xBNP6Fvf+la34yNZAwCMdW6DmZ0hSRkZGUpJSfGPysrKoPN1dHSorq5OeXl5/mNxcXHKy8vT7t27zxvnk08+qcGDB+uHP/xhj35OdoMDAMwVpq9uNTY2Kjk52X/Y7XYHPf3UqVPyer1KTU0NOJ6amqoDBw4EvWbXrl369a9/rYaGhh6HSbIGAJgrTMk6OTk5IFmHy5dffqnZs2drzZo1GjRoUI8/h2QNAECIBg0apPj4eDU3Nwccb25uVlpaWpfz/+d//kefffaZCgoK/Md8Pp8kKSEhQQcPHtQ111xz0XlZswYAGCtca9ah6t27t8aNG6eamhr/MZ/Pp5qaGuXm5nY5f8SIEfr444/V0NDgH3feeaemTJmihoYGZWRkhDQvlTUAwFxRuN1oaWmpCgsLNX78eE2cOFFVVVVqa2tTUVGRJGnOnDkaOnSoKisrlZiYqNGjRwdc369fP0nqcvxCSNYAAHTDjBkzdPLkSS1evFhNTU0aO3astm3b5t90dvz4ccXFhbdxTbIGABgrWvcGLykpUUlJSdD3amtrL3jtunXruj0fyRoAYK4YeeoWG8wAAHA4KmsAgLlipLImWQMAjOX6eti53gS0wQEAcDgqawCAuWiDAwDgbNH66talRrIGAJgrRipr1qwBAHA4KmsAgNkMqY7t6HZlvXPnThUUFCg9PV0ul0tbtmwJeN+yLC1evFhDhgxRnz59lJeXp08//TRc8QIA4Hepn7oVLd1O1m1tbcrOzlZ1dXXQ95955hm98MILWrVqlT744AP17dtX+fn5Onv2rO1gAQCIRd1ug0+dOlVTp04N+p5lWaqqqtJjjz2mu+66S5L0H//xH0pNTdWWLVt07733drmmvb1d7e3t/tcej6e7IQEAYhUbzLrv6NGjampqUl5env9YSkqKcnJytHv37qDXVFZWKiUlxT9CfRA3AAC0wXugqalJkvzP9DwnNTXV/97fKisrU2trq380NjaGMyQAAIwX9d3gbrdbbrc72mEAAExEG7z70tLSJEnNzc0Bx5ubm/3vAQAQLrTBeyArK0tpaWmqqanxH/N4PPrggw+Um5sbzqkAAIgZ3W6Dnz59WocPH/a/Pnr0qBoaGjRgwAANGzZM8+fP189//nMNHz5cWVlZKi8vV3p6uqZPnx7OuAEAiJk2eLeT9b59+zRlyhT/69LSUklSYWGh1q1bp5/85Cdqa2vT/fffry+++EI333yztm3bpsTExPBFDQCARLI+n8mTJ8uyzv/TuVwuPfnkk3ryySdtBQYAwMXEylO3eJAHAAAOF/WvbgEA0GO0wQEAcDaXZcl1gaXZUK43AW1wAAAcjsoaAGAu2uAAADgbu8EBAIAjUFkDAMxFGxwAAGejDQ4AAByByhoAYC7a4AAAOFustMFJ1gAAc8VIZc2aNQAADkdlDQAwmimtbDtI1gAAc1nWV8PO9QagDQ4AgMNRWQMAjMVucAAAnI7d4AAAwAmorAEAxnL5vhp2rjcByRoAYC7a4AAAwAmorAEAxmI3OAAAThcjN0UhWQMAjBUrlTVr1gAAOByVNQDAXDGyG5xkDQAwFm1wAADgCFTWAABzsRscAABnow0OAAAcgcoaAGAudoMDAOBstMHPY+fOnSooKFB6erpcLpe2bNnif6+zs1MLFy7UmDFj1LdvX6Wnp2vOnDn6/PPPwxkzAAAxpdvJuq2tTdnZ2aquru7y3pkzZ1RfX6/y8nLV19frt7/9rQ4ePKg777wzLMECABDAZ9kfBuh2G3zq1KmaOnVq0PdSUlK0Y8eOgGMrVqzQxIkTdfz4cQ0bNqzLNe3t7Wpvb/e/9ng83Q0JABCrYmTNOuK7wVtbW+VyudSvX7+g71dWViolJcU/MjIyIh0SAOAy4dJf1617NKL9A4Qoosn67NmzWrhwoWbOnKnk5OSg55SVlam1tdU/GhsbIxkSAADGidhu8M7OTt1zzz2yLEsrV64873lut1tutztSYQAALmfcwaznziXqY8eO6e233z5vVQ0AgB18dauHziXqTz/9VG+99ZYGDhwY7ikAAIiq6upqZWZmKjExUTk5Odq7d+95z12zZo1uueUW9e/fX/3791deXt4Fzw+m28n69OnTamhoUENDgyTp6NGjamho0PHjx9XZ2am7775b+/bt08svvyyv16umpiY1NTWpo6Oju1MBAHBhVhhGN23atEmlpaWqqKhQfX29srOzlZ+fr5aWlqDn19bWaubMmXrnnXe0e/duZWRk6Pbbb9eJEydCntNlWd1r2NfW1mrKlCldjhcWFurxxx9XVlZW0OveeecdTZ48+aKf7/F4lJKSosm6SwmuXt0JDQDgAH+xOlWr19Ta2hqxZdBzueKWyRVKSEjs8ef85S9n9V7tE2psbAyI9UL7qXJycjRhwgStWLFCkuTz+ZSRkaGHH35YixYtuuicXq9X/fv314oVKzRnzpyQ4uz2mvXkyZN1ofzezdwPAEDU/e3XhisqKvT44493Oa+jo0N1dXUqKyvzH4uLi1NeXp52794d0lxnzpxRZ2enBgwYEHJ83BscAGAu39fDzvVS0Mo6mFOnTsnr9So1NTXgeGpqqg4cOBDSlAsXLlR6erry8vJCDpNkDQAwlsuy5LLR0T13bXJy8iX55tLTTz+tjRs3qra2VomJobfvSdYAAIRo0KBBio+PV3Nzc8Dx5uZmpaWlXfDaX/ziF3r66af11ltv6frrr+/WvBG/3SgAABFziXeD9+7dW+PGjVNNTY3/mM/nU01NjXJzc8973TPPPKMlS5Zo27ZtGj9+fPcmFZU1AMBkUbiDWWlpqQoLCzV+/HhNnDhRVVVVamtrU1FRkSRpzpw5Gjp0qCorKyVJy5Yt0+LFi7VhwwZlZmaqqalJknTllVfqyiuvDGlOkjUAwFjRuIPZjBkzdPLkSS1evFhNTU0aO3astm3b5t90dvz4ccXF/bVxvXLlSnV0dOjuu+8O+Jzz7TgPhmQNAEA3lZSUqKSkJOh7tbW1Aa8/++wz2/ORrAEA5uJBHgAAOJvL99Wwc70J2A0OAIDDUVkDAMxFGxwAAIfr4ZOzAq43AG1wAAAcjsoaAGCscN0b3OlI1gAAc8XImjVtcAAAHI7KGgBgLkv2nmdtRmFNsgYAmIs1awAAnM6SzTXrsEUSUaxZAwDgcFTWAABzxchucJI1AMBcPkkum9cbgDY4AAAOR2UNADAWu8EBAHC6GFmzpg0OAIDDUVkDAMwVI5U1yRoAYK4YSda0wQEAcDgqawCAuWLke9YkawCAsfjqFgAATseaNQAAcAIqawCAuXyW5LJRHfvMqKxJ1gAAc9EGD27nzp0qKChQenq6XC6XtmzZct5zH3jgAblcLlVVVdkIEQCA2NbtZN3W1qbs7GxVV1df8LzNmzdrz549Sk9P73FwAABcmPXX6ronQ2ZU1t1ug0+dOlVTp0694DknTpzQww8/rO3bt2vatGk9Dg4AgAuKkTZ42NesfT6fZs+erUcffVSjRo266Pnt7e1qb2/3v/Z4POEOCQAAo4X9q1vLli1TQkKCHnnkkZDOr6ysVEpKin9kZGSEOyQAwOXKZ9kfBghrsq6rq9Pzzz+vdevWyeUK7f5vZWVlam1t9Y/GxsZwhgQAuJxZPvvDAGFN1u+9955aWlo0bNgwJSQkKCEhQceOHdOPf/xjZWZmBr3G7XYrOTk5YAAAgL8K65r17NmzlZeXF3AsPz9fs2fPVlFRUTinAgCADWbnc/r0aR0+fNj/+ujRo2poaNCAAQM0bNgwDRw4MOD8Xr16KS0tTddee639aAEA+Cafza9fGbJm3e1kvW/fPk2ZMsX/urS0VJJUWFiodevWhS0wAAAuiso6uMmTJ8vqxg/32WefdXcKAADwDdwbHABgLks2K+uwRRJRJGsAgLlipA3O86wBAHA4KmsAgLl8Pkk2bmziM+OmKCRrAIC5aIMDAAAnoLIGAJgrRiprkjUAwFwxcgcz2uAAADgclTUAwFiW5ZNl4zGXdq69lEjWAABzWZa9VjZr1gAARJhlc83akGTNmjUAAA5HZQ0AMJfPJ7lsrDuzZg0AQITRBgcAAE5AZQ0AMJbl88my0Qbnq1sAAEQabXAAAOAEVNYAAHP5LMl1+VfWJGsAgLksS5Kdr26ZkaxpgwMA4HBU1gAAY1k+S5aNNrhlSGVNsgYAmMvyyV4b3IyvbtEGBwAYy/JZtkdPVFdXKzMzU4mJicrJydHevXsveP5vfvMbjRgxQomJiRozZoy2bt3arflI1gAAdMOmTZtUWlqqiooK1dfXKzs7W/n5+WppaQl6/vvvv6+ZM2fqhz/8oT766CNNnz5d06dP1/79+0Oe02U5rGHf2tqqfv366WbdoQT1inY4AIBu+os6tUtb9cUXXyglJSUic3g8HqWkpNjOFedibWxsVHJysv+42+2W2+0Oek1OTo4mTJigFStWSJJ8Pp8yMjL08MMPa9GiRV3OnzFjhtra2vTGG2/4j/3DP/yDxo4dq1WrVoUWqOUwjY2N525Hw2AwGAyDR2NjY8RyxZ///GcrLS0tLHFeeeWVXY5VVFQEnbe9vd2Kj4+3Nm/eHHB8zpw51p133hn0moyMDOvf/u3fAo4tXrzYuv7660P+eR23wSw9PV2NjY1KSkqSy+W66Pkej0cZGRld/ipyOuK+tEyNWzI3duK+tJwUt2VZ+vLLL5Wenh6xORITE3X06FF1dHTY/izLsrrkm/NV1adOnZLX61VqamrA8dTUVB04cCDoNU1NTUHPb2pqCjlGxyXruLg4XXXVVd2+Ljk5Oer/gfYEcV9apsYtmRs7cV9aTok7Uu3vb0pMTFRiYmLE53ECNpgBABCiQYMGKT4+Xs3NzQHHm5ublZaWFvSatLS0bp0fDMkaAIAQ9e7dW+PGjVNNTY3/mM/nU01NjXJzc4Nek5ubG3C+JO3YseO85wfjuDZ4d7ndblVUVJx3fcGpiPvSMjVuydzYifvSMjVuE5WWlqqwsFDjx4/XxIkTVVVVpba2NhUVFUmS5syZo6FDh6qyslKSNG/ePN1666167rnnNG3aNG3cuFH79u3T6tWrQ57TcV/dAgDA6VasWKFnn31WTU1NGjt2rF544QXl5ORIkiZPnqzMzEytW7fOf/5vfvMbPfbYY/rss880fPhwPfPMM7rjjjtCno9kDQCAw7FmDQCAw5GsAQBwOJI1AAAOR7IGAMDhjE7W3X1EmRNUVlZqwoQJSkpK0uDBgzV9+nQdPHgw2mF129NPPy2Xy6X58+dHO5SLOnHihH7wgx9o4MCB6tOnj8aMGaN9+/ZFO6wL8nq9Ki8vV1ZWlvr06aNrrrlGS5YskRP3g+7cuVMFBQVKT0+Xy+XSli1bAt63LEuLFy/WkCFD1KdPH+Xl5enTTz+NTrDfcKG4Ozs7tXDhQo0ZM0Z9+/ZVenq65syZo88//zx6AX/tYv/e3/TAAw/I5XKpqqrqksWHyDA2WXf3EWVO8e6776q4uFh79uzRjh071NnZqdtvv11tbW3RDi1kH374oX71q1/p+uuvj3YoF/WnP/1JkyZNUq9evfS73/1Of/jDH/Tcc8+pf//+0Q7tgpYtW6aVK1dqxYoV+uSTT7Rs2TI988wzevHFF6MdWhdtbW3Kzs5WdXV10PefeeYZvfDCC1q1apU++OAD9e3bV/n5+Tp79uwljjTQheI+c+aM6uvrVV5ervr6ev32t7/VwYMHdeedd0Yh0kAX+/c+Z/PmzdqzZ09E78+NSyjkR344zMSJE63i4mL/a6/Xa6Wnp1uVlZVRjKr7WlpaLEnWu+++G+1QQvLll19aw4cPt3bs2GHdeuut1rx586Id0gUtXLjQuvnmm6MdRrdNmzbNuu+++wKOffe737VmzZoVpYhCIyngaUQ+n89KS0uznn32Wf+xL774wnK73dYrr7wShQiD+9u4g9m7d68lyTp27NilCSoE54v7f//3f62hQ4da+/fvt66++uouT3yCeYysrDs6OlRXV6e8vDz/sbi4OOXl5Wn37t1RjKz7WltbJUkDBgyIciShKS4u1rRp0wL+7Z3s9ddf1/jx4/W9731PgwcP1g033KA1a9ZEO6yLuummm1RTU6NDhw5Jkn7/+99r165dmjp1apQj656jR4+qqakp4L+XlJQU5eTkGPm76nK51K9fv2iHckE+n0+zZ8/Wo48+qlGjRkU7HISJkbcb7ckjypzI5/Np/vz5mjRpkkaPHh3tcC5q48aNqq+v14cffhjtUEJ25MgRrVy5UqWlpfrpT3+qDz/8UI888oh69+6twsLCaId3XosWLZLH49GIESMUHx8vr9erp556SrNmzYp2aN1y7hGAdh8PGG1nz57VwoULNXPmTEc80epCli1bpoSEBD3yyCPRDgVhZGSyvlwUFxdr//792rVrV7RDuajGxkbNmzdPO3bsMOqRdD6fT+PHj9fSpUslSTfccIP279+vVatWOTpZv/rqq3r55Ze1YcMGjRo1Sg0NDZo/f77S09MdHfflqLOzU/fcc48sy9LKlSujHc4F1dXV6fnnn1d9fX2X5zPDbEa2wXvyiDKnKSkp0RtvvKF33nmnR8/vvtTq6urU0tKiG2+8UQkJCUpISNC7776rF154QQkJCfJ6vdEOMaghQ4Zo5MiRAceuu+46HT9+PEoRhebRRx/VokWLdO+992rMmDGaPXu2FixY4H8wgCnO/T6a+rt6LlEfO3ZMO3bscHxV/d5776mlpUXDhg3z/54eO3ZMP/7xj5WZmRnt8GCDkcm6J48ocwrLslRSUqLNmzfr7bffVlZWVrRDCsltt92mjz/+WA0NDf4xfvx4zZo1Sw0NDYqPj492iEFNmjSpy1fjDh06pKuvvjpKEYXmzJkziosL/PWMj4+Xz+eLUkQ9k5WVpbS0tIDfVY/How8++MDxv6vnEvWnn36qt956SwMHDox2SBc1e/Zs/fd//3fA72l6eroeffRRbd++PdrhwQZj2+AXe0SZUxUXF2vDhg167bXXlJSU5F+3S0lJUZ8+faIc3fklJSV1WVfv27evBg4c6Oj19gULFuimm27S0qVLdc8992jv3r1avXp1tx5NFw0FBQV66qmnNGzYMI0aNUofffSRli9frvvuuy/aoXVx+vRpHT582P/66NGjamho0IABAzRs2DDNnz9fP//5zzV8+HBlZWWpvLxc6enpmj59evSC1oXjHjJkiO6++27V19frjTfekNfr9f+uDhgwQL17945W2Bf99/7bPyp69eqltLQ0XXvttZc6VIRTtLej2/Hiiy9aw4YNs3r37m1NnDjR2rNnT7RDuihJQcdLL70U7dC6zYSvblmWZf3Xf/2XNXr0aMvtdlsjRoywVq9eHe2QLsrj8Vjz5s2zhg0bZiUmJlrf+ta3rJ/97GdWe3t7tEPr4p133gn633RhYaFlWV99fau8vNxKTU213G63ddttt1kHDx6MbtDWheM+evToeX9X33nnHcfGHQxf3bo88IhMAAAczsg1awAAYgnJGgAAhyNZAwDgcCRrAAAcjmQNAIDDkawBAHA4kjUAAA5HsgYAwOFI1gAAOBzJGgAAhyNZAwDgcP8P1C4r9Z/qpKIAAAAASUVORK5CYII=",
      "text/plain": [
       "<Figure size 640x480 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# MAKE DATASET\n",
    "\n",
    "# we are calculating Y GIVEN X\n",
    "tess_dataset = TESSDataset()\n",
    "print(len(tess_dataset))\n",
    "print(tess_dataset[1]['x'].shape)\n",
    "print(tess_dataset[1]['y'].shape)\n",
    "print(tess_dataset[1]['ffi_num'])\n",
    "print(tess_dataset[1]['orbit'])\n",
    "\n",
    "# plt.plot(tess_dataset[1]['x'][0])\n",
    "print('x:', tess_dataset[1]['x'][0])\n",
    "\n",
    "plt.imshow(tess_dataset[1]['y'][0], vmin=0, vmax=1)\n",
    "plt.colorbar()\n",
    "plt.show()\n",
    "plt.close()\n",
    "\n",
    "# # displays all datapoints\n",
    "# for i in range(len(tess_dataset)):\n",
    "#     print('------')\n",
    "#     print('x:', tess_dataset[i]['x'][0])\n",
    "#     plt.imshow(tess_dataset[i]['y'][0], vmin=0, vmax=1)\n",
    "#     plt.colorbar()\n",
    "#     plt.show()\n",
    "#     plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "kjh1WU3Mm1iF",
    "outputId": "46a92afc-dfbc-403e-9731-2b7708c7725c",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of param: 8598596\n",
      "training, validation lengths:  277 70\n",
      "Iteration: 0/277000 \t Elapsed time: 0.18 \t Loss:44.98308\n",
      "Iteration: 1/277000 \t Elapsed time: 0.36 \t Loss:44.63782\n",
      "Iteration: 2/277000 \t Elapsed time: 0.52 \t Loss:44.37938\n",
      "Iteration: 3/277000 \t Elapsed time: 0.68 \t Loss:44.15368\n",
      "Iteration: 4/277000 \t Elapsed time: 0.84 \t Loss:43.94790\n",
      "Iteration: 5/277000 \t Elapsed time: 1.00 \t Loss:43.75607\n",
      "Iteration: 6/277000 \t Elapsed time: 1.17 \t Loss:43.57469\n",
      "Iteration: 7/277000 \t Elapsed time: 1.33 \t Loss:43.40157\n",
      "Iteration: 8/277000 \t Elapsed time: 1.49 \t Loss:43.23518\n",
      "Iteration: 9/277000 \t Elapsed time: 1.66 \t Loss:43.07437\n",
      "Iteration: 10/277000 \t Elapsed time: 1.83 \t Loss:42.91819\n",
      "Iteration: 11/277000 \t Elapsed time: 1.99 \t Loss:42.76605\n",
      "Iteration: 12/277000 \t Elapsed time: 2.17 \t Loss:42.61726\n",
      "Iteration: 13/277000 \t Elapsed time: 2.33 \t Loss:42.47140\n",
      "Iteration: 14/277000 \t Elapsed time: 2.49 \t Loss:42.32810\n",
      "Iteration: 15/277000 \t Elapsed time: 2.65 \t Loss:42.18712\n",
      "Iteration: 16/277000 \t Elapsed time: 2.82 \t Loss:42.04828\n",
      "Iteration: 17/277000 \t Elapsed time: 3.00 \t Loss:41.91131\n",
      "Iteration: 18/277000 \t Elapsed time: 3.16 \t Loss:41.77608\n",
      "Iteration: 19/277000 \t Elapsed time: 3.32 \t Loss:41.64265\n",
      "Iteration: 20/277000 \t Elapsed time: 3.49 \t Loss:41.51084\n",
      "Iteration: 21/277000 \t Elapsed time: 3.67 \t Loss:41.38060\n",
      "Iteration: 22/277000 \t Elapsed time: 3.83 \t Loss:41.25234\n",
      "Iteration: 23/277000 \t Elapsed time: 3.99 \t Loss:41.12560\n",
      "Iteration: 24/277000 \t Elapsed time: 4.15 \t Loss:41.00071\n",
      "Iteration: 25/277000 \t Elapsed time: 4.31 \t Loss:40.87813\n",
      "Iteration: 26/277000 \t Elapsed time: 4.48 \t Loss:40.75746\n",
      "Iteration: 27/277000 \t Elapsed time: 4.65 \t Loss:40.63893\n",
      "Iteration: 28/277000 \t Elapsed time: 4.82 \t Loss:40.52335\n",
      "Iteration: 29/277000 \t Elapsed time: 4.99 \t Loss:40.41010\n",
      "Iteration: 30/277000 \t Elapsed time: 5.15 \t Loss:40.29955\n",
      "Iteration: 31/277000 \t Elapsed time: 5.31 \t Loss:40.19192\n",
      "Iteration: 32/277000 \t Elapsed time: 5.47 \t Loss:40.08714\n",
      "Iteration: 33/277000 \t Elapsed time: 5.63 \t Loss:39.98537\n",
      "Iteration: 34/277000 \t Elapsed time: 5.80 \t Loss:39.88651\n",
      "Iteration: 35/277000 \t Elapsed time: 5.99 \t Loss:39.79064\n",
      "Iteration: 36/277000 \t Elapsed time: 6.20 \t Loss:39.69760\n",
      "Iteration: 37/277000 \t Elapsed time: 6.40 \t Loss:39.60716\n",
      "Iteration: 38/277000 \t Elapsed time: 6.60 \t Loss:39.51901\n",
      "Iteration: 39/277000 \t Elapsed time: 6.77 \t Loss:39.43354\n",
      "Iteration: 40/277000 \t Elapsed time: 6.93 \t Loss:39.35005\n",
      "Iteration: 41/277000 \t Elapsed time: 7.09 \t Loss:39.26842\n",
      "Iteration: 42/277000 \t Elapsed time: 7.27 \t Loss:39.18833\n",
      "Iteration: 43/277000 \t Elapsed time: 7.53 \t Loss:39.11007\n",
      "Iteration: 44/277000 \t Elapsed time: 7.74 \t Loss:39.03318\n",
      "Iteration: 45/277000 \t Elapsed time: 7.91 \t Loss:38.95762\n",
      "Iteration: 46/277000 \t Elapsed time: 8.08 \t Loss:38.88315\n",
      "Iteration: 47/277000 \t Elapsed time: 8.28 \t Loss:38.80981\n",
      "Iteration: 48/277000 \t Elapsed time: 8.45 \t Loss:38.73757\n",
      "Iteration: 49/277000 \t Elapsed time: 8.67 \t Loss:38.66649\n",
      "Iteration: 50/277000 \t Elapsed time: 8.91 \t Loss:38.59603\n",
      "Iteration: 51/277000 \t Elapsed time: 9.07 \t Loss:38.52644\n",
      "Iteration: 52/277000 \t Elapsed time: 9.24 \t Loss:38.45785\n",
      "Iteration: 53/277000 \t Elapsed time: 9.41 \t Loss:38.38979\n",
      "Iteration: 54/277000 \t Elapsed time: 9.58 \t Loss:38.32261\n",
      "Iteration: 55/277000 \t Elapsed time: 9.74 \t Loss:38.25607\n",
      "Iteration: 56/277000 \t Elapsed time: 9.91 \t Loss:38.19007\n",
      "Iteration: 57/277000 \t Elapsed time: 10.10 \t Loss:38.12482\n",
      "Iteration: 58/277000 \t Elapsed time: 10.27 \t Loss:38.06015\n",
      "Iteration: 59/277000 \t Elapsed time: 10.47 \t Loss:37.99588\n",
      "Iteration: 60/277000 \t Elapsed time: 10.64 \t Loss:37.93243\n",
      "Iteration: 61/277000 \t Elapsed time: 10.80 \t Loss:37.86947\n",
      "Iteration: 62/277000 \t Elapsed time: 10.97 \t Loss:37.80691\n",
      "Iteration: 63/277000 \t Elapsed time: 11.15 \t Loss:37.74488\n",
      "Iteration: 64/277000 \t Elapsed time: 11.33 \t Loss:37.68344\n",
      "Iteration: 65/277000 \t Elapsed time: 11.48 \t Loss:37.62232\n",
      "Iteration: 66/277000 \t Elapsed time: 11.69 \t Loss:37.56180\n",
      "Iteration: 67/277000 \t Elapsed time: 11.85 \t Loss:37.50167\n",
      "Iteration: 68/277000 \t Elapsed time: 12.01 \t Loss:37.44193\n",
      "Iteration: 69/277000 \t Elapsed time: 12.17 \t Loss:37.38263\n",
      "Iteration: 70/277000 \t Elapsed time: 12.33 \t Loss:37.32380\n",
      "Iteration: 71/277000 \t Elapsed time: 12.51 \t Loss:37.26529\n",
      "Iteration: 72/277000 \t Elapsed time: 12.67 \t Loss:37.20727\n",
      "Iteration: 73/277000 \t Elapsed time: 12.84 \t Loss:37.14960\n",
      "Iteration: 74/277000 \t Elapsed time: 13.03 \t Loss:37.09231\n",
      "Iteration: 75/277000 \t Elapsed time: 13.19 \t Loss:37.03536\n",
      "Iteration: 76/277000 \t Elapsed time: 13.35 \t Loss:36.97881\n",
      "Iteration: 77/277000 \t Elapsed time: 13.52 \t Loss:36.92255\n",
      "Iteration: 78/277000 \t Elapsed time: 13.67 \t Loss:36.86674\n",
      "Iteration: 79/277000 \t Elapsed time: 13.83 \t Loss:36.81123\n",
      "Iteration: 80/277000 \t Elapsed time: 14.00 \t Loss:36.75603\n",
      "Iteration: 81/277000 \t Elapsed time: 14.17 \t Loss:36.70121\n",
      "Iteration: 82/277000 \t Elapsed time: 14.36 \t Loss:36.64664\n",
      "Iteration: 83/277000 \t Elapsed time: 14.51 \t Loss:36.59245\n",
      "Iteration: 84/277000 \t Elapsed time: 14.68 \t Loss:36.53856\n",
      "Iteration: 85/277000 \t Elapsed time: 14.84 \t Loss:36.48497\n",
      "Iteration: 86/277000 \t Elapsed time: 15.01 \t Loss:36.43168\n",
      "Iteration: 87/277000 \t Elapsed time: 15.19 \t Loss:36.37868\n",
      "Iteration: 88/277000 \t Elapsed time: 15.36 \t Loss:36.32597\n",
      "Iteration: 89/277000 \t Elapsed time: 15.53 \t Loss:36.27353\n",
      "Iteration: 90/277000 \t Elapsed time: 15.69 \t Loss:36.22138\n",
      "Iteration: 91/277000 \t Elapsed time: 15.86 \t Loss:36.16949\n",
      "Iteration: 92/277000 \t Elapsed time: 16.16 \t Loss:36.11792\n",
      "Iteration: 93/277000 \t Elapsed time: 16.34 \t Loss:36.06657\n",
      "Iteration: 94/277000 \t Elapsed time: 16.52 \t Loss:36.01552\n",
      "Iteration: 95/277000 \t Elapsed time: 16.70 \t Loss:35.96471\n",
      "Iteration: 96/277000 \t Elapsed time: 16.98 \t Loss:35.91416\n",
      "Iteration: 97/277000 \t Elapsed time: 17.15 \t Loss:35.86383\n",
      "Iteration: 98/277000 \t Elapsed time: 18.10 \t Loss:35.81378\n",
      "Iteration: 99/277000 \t Elapsed time: 18.30 \t Loss:35.76397\n",
      "Iteration: 100/277000 \t Elapsed time: 18.46 \t Loss:35.71437\n",
      "Iteration: 101/277000 \t Elapsed time: 18.67 \t Loss:35.66502\n",
      "Iteration: 102/277000 \t Elapsed time: 19.10 \t Loss:35.61592\n",
      "Iteration: 103/277000 \t Elapsed time: 19.27 \t Loss:35.56699\n",
      "Iteration: 104/277000 \t Elapsed time: 19.44 \t Loss:35.51831\n",
      "Iteration: 105/277000 \t Elapsed time: 19.64 \t Loss:35.46985\n",
      "Iteration: 106/277000 \t Elapsed time: 19.83 \t Loss:35.42158\n",
      "Iteration: 107/277000 \t Elapsed time: 20.08 \t Loss:35.37355\n",
      "Iteration: 108/277000 \t Elapsed time: 20.25 \t Loss:35.32570\n",
      "Iteration: 109/277000 \t Elapsed time: 20.43 \t Loss:35.27806\n",
      "Iteration: 110/277000 \t Elapsed time: 20.60 \t Loss:35.23065\n",
      "Iteration: 111/277000 \t Elapsed time: 20.78 \t Loss:35.18339\n",
      "Iteration: 112/277000 \t Elapsed time: 20.96 \t Loss:35.13635\n",
      "Iteration: 113/277000 \t Elapsed time: 21.14 \t Loss:35.08949\n",
      "Iteration: 114/277000 \t Elapsed time: 21.31 \t Loss:35.04279\n",
      "Iteration: 115/277000 \t Elapsed time: 21.48 \t Loss:34.99630\n",
      "Iteration: 116/277000 \t Elapsed time: 21.65 \t Loss:34.95000\n",
      "Iteration: 117/277000 \t Elapsed time: 21.83 \t Loss:34.90387\n",
      "Iteration: 118/277000 \t Elapsed time: 22.00 \t Loss:34.85791\n",
      "Iteration: 119/277000 \t Elapsed time: 22.16 \t Loss:34.81213\n",
      "Iteration: 120/277000 \t Elapsed time: 22.33 \t Loss:34.76652\n",
      "Iteration: 121/277000 \t Elapsed time: 22.50 \t Loss:34.72105\n",
      "Iteration: 122/277000 \t Elapsed time: 22.66 \t Loss:34.67579\n",
      "Iteration: 123/277000 \t Elapsed time: 22.86 \t Loss:34.63067\n",
      "Iteration: 124/277000 \t Elapsed time: 23.03 \t Loss:34.58571\n",
      "Iteration: 125/277000 \t Elapsed time: 23.21 \t Loss:34.54093\n",
      "Iteration: 126/277000 \t Elapsed time: 23.37 \t Loss:34.49630\n",
      "Iteration: 127/277000 \t Elapsed time: 23.53 \t Loss:34.45182\n",
      "Iteration: 128/277000 \t Elapsed time: 23.69 \t Loss:34.40749\n",
      "Iteration: 129/277000 \t Elapsed time: 23.86 \t Loss:34.36331\n",
      "Iteration: 130/277000 \t Elapsed time: 24.02 \t Loss:34.31927\n",
      "Iteration: 131/277000 \t Elapsed time: 24.20 \t Loss:34.27538\n",
      "Iteration: 132/277000 \t Elapsed time: 24.38 \t Loss:34.23164\n",
      "Iteration: 133/277000 \t Elapsed time: 24.54 \t Loss:34.18805\n",
      "Iteration: 134/277000 \t Elapsed time: 24.71 \t Loss:34.14458\n",
      "Iteration: 135/277000 \t Elapsed time: 24.87 \t Loss:34.10129\n",
      "Iteration: 136/277000 \t Elapsed time: 25.11 \t Loss:34.05811\n",
      "Iteration: 137/277000 \t Elapsed time: 25.35 \t Loss:34.01507\n",
      "Iteration: 138/277000 \t Elapsed time: 25.54 \t Loss:33.97215\n",
      "Iteration: 139/277000 \t Elapsed time: 25.74 \t Loss:33.92936\n",
      "Iteration: 140/277000 \t Elapsed time: 25.93 \t Loss:33.88674\n",
      "Iteration: 141/277000 \t Elapsed time: 26.12 \t Loss:33.84425\n",
      "Iteration: 142/277000 \t Elapsed time: 26.55 \t Loss:33.80185\n",
      "Iteration: 143/277000 \t Elapsed time: 26.72 \t Loss:33.75960\n",
      "Iteration: 144/277000 \t Elapsed time: 26.89 \t Loss:33.71745\n",
      "Iteration: 145/277000 \t Elapsed time: 27.06 \t Loss:33.67545\n",
      "Iteration: 146/277000 \t Elapsed time: 27.26 \t Loss:33.63356\n",
      "Iteration: 147/277000 \t Elapsed time: 27.47 \t Loss:33.59183\n",
      "Iteration: 148/277000 \t Elapsed time: 27.72 \t Loss:33.55019\n",
      "Iteration: 149/277000 \t Elapsed time: 27.93 \t Loss:33.50873\n",
      "Iteration: 150/277000 \t Elapsed time: 28.11 \t Loss:33.46735\n",
      "Iteration: 151/277000 \t Elapsed time: 28.37 \t Loss:33.42603\n",
      "Iteration: 152/277000 \t Elapsed time: 28.55 \t Loss:33.38480\n",
      "Iteration: 153/277000 \t Elapsed time: 28.72 \t Loss:33.34372\n",
      "Iteration: 154/277000 \t Elapsed time: 28.91 \t Loss:33.30279\n",
      "Iteration: 155/277000 \t Elapsed time: 29.11 \t Loss:33.26199\n",
      "Iteration: 156/277000 \t Elapsed time: 29.28 \t Loss:33.22125\n",
      "Iteration: 157/277000 \t Elapsed time: 29.45 \t Loss:33.18060\n",
      "Iteration: 158/277000 \t Elapsed time: 29.66 \t Loss:33.14010\n",
      "Iteration: 159/277000 \t Elapsed time: 29.84 \t Loss:33.09972\n",
      "Iteration: 160/277000 \t Elapsed time: 30.03 \t Loss:33.05943\n",
      "Iteration: 161/277000 \t Elapsed time: 30.19 \t Loss:33.01922\n",
      "Iteration: 162/277000 \t Elapsed time: 30.42 \t Loss:32.97914\n",
      "Iteration: 163/277000 \t Elapsed time: 30.58 \t Loss:32.93920\n",
      "Iteration: 164/277000 \t Elapsed time: 30.76 \t Loss:32.89934\n",
      "Iteration: 165/277000 \t Elapsed time: 30.94 \t Loss:32.85948\n",
      "Iteration: 166/277000 \t Elapsed time: 31.11 \t Loss:32.81971\n",
      "Iteration: 167/277000 \t Elapsed time: 31.27 \t Loss:32.78011\n",
      "Iteration: 168/277000 \t Elapsed time: 31.44 \t Loss:32.74059\n",
      "Iteration: 169/277000 \t Elapsed time: 31.61 \t Loss:32.70110\n",
      "Iteration: 170/277000 \t Elapsed time: 31.77 \t Loss:32.66174\n",
      "Iteration: 171/277000 \t Elapsed time: 31.94 \t Loss:32.62243\n",
      "Iteration: 172/277000 \t Elapsed time: 32.13 \t Loss:32.58315\n",
      "Iteration: 173/277000 \t Elapsed time: 32.28 \t Loss:32.54396\n",
      "Iteration: 174/277000 \t Elapsed time: 32.45 \t Loss:32.50483\n",
      "Iteration: 175/277000 \t Elapsed time: 32.61 \t Loss:32.46581\n",
      "Iteration: 176/277000 \t Elapsed time: 32.79 \t Loss:32.42693\n",
      "Iteration: 177/277000 \t Elapsed time: 32.96 \t Loss:32.38817\n",
      "Iteration: 178/277000 \t Elapsed time: 33.13 \t Loss:32.34949\n",
      "Iteration: 179/277000 \t Elapsed time: 33.31 \t Loss:32.31099\n",
      "Iteration: 180/277000 \t Elapsed time: 33.47 \t Loss:32.27261\n",
      "Iteration: 181/277000 \t Elapsed time: 33.63 \t Loss:32.23419\n",
      "Iteration: 182/277000 \t Elapsed time: 33.81 \t Loss:32.19594\n",
      "Iteration: 183/277000 \t Elapsed time: 34.18 \t Loss:32.15787\n",
      "Iteration: 184/277000 \t Elapsed time: 34.35 \t Loss:32.11971\n",
      "Iteration: 185/277000 \t Elapsed time: 34.52 \t Loss:32.08179\n",
      "Iteration: 186/277000 \t Elapsed time: 34.70 \t Loss:32.04395\n",
      "Iteration: 187/277000 \t Elapsed time: 34.86 \t Loss:32.00616\n",
      "Iteration: 188/277000 \t Elapsed time: 35.04 \t Loss:31.96852\n",
      "Iteration: 189/277000 \t Elapsed time: 35.20 \t Loss:31.93089\n",
      "Iteration: 190/277000 \t Elapsed time: 35.36 \t Loss:31.89338\n",
      "Iteration: 191/277000 \t Elapsed time: 35.54 \t Loss:31.85589\n",
      "Iteration: 192/277000 \t Elapsed time: 35.70 \t Loss:31.81849\n",
      "Iteration: 193/277000 \t Elapsed time: 35.88 \t Loss:31.78122\n",
      "Iteration: 194/277000 \t Elapsed time: 36.06 \t Loss:31.74403\n",
      "Iteration: 195/277000 \t Elapsed time: 36.23 \t Loss:31.70693\n",
      "Iteration: 196/277000 \t Elapsed time: 36.39 \t Loss:31.66987\n",
      "Iteration: 197/277000 \t Elapsed time: 36.56 \t Loss:31.63288\n",
      "Iteration: 198/277000 \t Elapsed time: 36.74 \t Loss:31.59598\n",
      "Iteration: 199/277000 \t Elapsed time: 36.90 \t Loss:31.55916\n",
      "Iteration: 200/277000 \t Elapsed time: 37.06 \t Loss:31.52247\n",
      "Iteration: 201/277000 \t Elapsed time: 37.24 \t Loss:31.48584\n",
      "Iteration: 202/277000 \t Elapsed time: 37.41 \t Loss:31.44926\n",
      "Iteration: 203/277000 \t Elapsed time: 37.57 \t Loss:31.41275\n",
      "Iteration: 204/277000 \t Elapsed time: 37.75 \t Loss:31.37632\n",
      "Iteration: 205/277000 \t Elapsed time: 37.92 \t Loss:31.33997\n",
      "Iteration: 206/277000 \t Elapsed time: 38.08 \t Loss:31.30371\n",
      "Iteration: 207/277000 \t Elapsed time: 38.26 \t Loss:31.26754\n",
      "Iteration: 208/277000 \t Elapsed time: 38.42 \t Loss:31.23145\n",
      "Iteration: 209/277000 \t Elapsed time: 38.59 \t Loss:31.19542\n",
      "Iteration: 210/277000 \t Elapsed time: 38.77 \t Loss:31.15947\n",
      "Iteration: 211/277000 \t Elapsed time: 38.96 \t Loss:31.12358\n",
      "Iteration: 212/277000 \t Elapsed time: 39.13 \t Loss:31.08778\n",
      "Iteration: 213/277000 \t Elapsed time: 39.30 \t Loss:31.05204\n",
      "Iteration: 214/277000 \t Elapsed time: 39.47 \t Loss:31.01637\n",
      "Iteration: 215/277000 \t Elapsed time: 39.64 \t Loss:30.98078\n",
      "Iteration: 216/277000 \t Elapsed time: 39.80 \t Loss:30.94526\n",
      "Iteration: 217/277000 \t Elapsed time: 39.97 \t Loss:30.90982\n",
      "Iteration: 218/277000 \t Elapsed time: 40.13 \t Loss:30.87445\n",
      "Iteration: 219/277000 \t Elapsed time: 40.29 \t Loss:30.83917\n",
      "Iteration: 220/277000 \t Elapsed time: 40.47 \t Loss:30.80398\n",
      "Iteration: 221/277000 \t Elapsed time: 40.64 \t Loss:30.76883\n",
      "Iteration: 222/277000 \t Elapsed time: 40.81 \t Loss:30.73370\n",
      "Iteration: 223/277000 \t Elapsed time: 40.99 \t Loss:30.69858\n",
      "Iteration: 224/277000 \t Elapsed time: 41.16 \t Loss:30.66356\n",
      "Iteration: 225/277000 \t Elapsed time: 41.32 \t Loss:30.62867\n",
      "Iteration: 226/277000 \t Elapsed time: 41.49 \t Loss:30.59388\n",
      "Iteration: 227/277000 \t Elapsed time: 41.66 \t Loss:30.55914\n",
      "Iteration: 228/277000 \t Elapsed time: 41.82 \t Loss:30.52443\n",
      "Iteration: 229/277000 \t Elapsed time: 41.99 \t Loss:30.48978\n",
      "Iteration: 230/277000 \t Elapsed time: 42.15 \t Loss:30.45524\n",
      "Iteration: 231/277000 \t Elapsed time: 42.32 \t Loss:30.42080\n",
      "Iteration: 232/277000 \t Elapsed time: 42.49 \t Loss:30.38638\n",
      "Iteration: 233/277000 \t Elapsed time: 42.66 \t Loss:30.35199\n",
      "Iteration: 234/277000 \t Elapsed time: 42.82 \t Loss:30.31765\n",
      "Iteration: 235/277000 \t Elapsed time: 42.98 \t Loss:30.28337\n",
      "Iteration: 236/277000 \t Elapsed time: 43.15 \t Loss:30.24921\n",
      "Iteration: 237/277000 \t Elapsed time: 43.31 \t Loss:30.21514\n",
      "Iteration: 238/277000 \t Elapsed time: 43.48 \t Loss:30.18108\n",
      "Iteration: 239/277000 \t Elapsed time: 43.65 \t Loss:30.14701\n",
      "Iteration: 240/277000 \t Elapsed time: 43.81 \t Loss:30.11297\n",
      "Iteration: 241/277000 \t Elapsed time: 44.00 \t Loss:30.07909\n",
      "Iteration: 242/277000 \t Elapsed time: 44.18 \t Loss:30.04531\n",
      "Iteration: 243/277000 \t Elapsed time: 44.35 \t Loss:30.01155\n",
      "Iteration: 244/277000 \t Elapsed time: 44.53 \t Loss:29.97783\n",
      "Iteration: 245/277000 \t Elapsed time: 44.71 \t Loss:29.94422\n",
      "Iteration: 246/277000 \t Elapsed time: 44.90 \t Loss:29.91067\n",
      "Iteration: 247/277000 \t Elapsed time: 45.10 \t Loss:29.87717\n",
      "Iteration: 248/277000 \t Elapsed time: 45.28 \t Loss:29.84374\n",
      "Iteration: 249/277000 \t Elapsed time: 45.45 \t Loss:29.81039\n",
      "Iteration: 250/277000 \t Elapsed time: 45.63 \t Loss:29.77713\n",
      "Iteration: 251/277000 \t Elapsed time: 45.81 \t Loss:29.74391\n",
      "Iteration: 252/277000 \t Elapsed time: 46.00 \t Loss:29.71077\n",
      "Iteration: 253/277000 \t Elapsed time: 46.20 \t Loss:29.67771\n",
      "Iteration: 254/277000 \t Elapsed time: 46.38 \t Loss:29.64469\n",
      "Iteration: 255/277000 \t Elapsed time: 46.56 \t Loss:29.61175\n",
      "Iteration: 256/277000 \t Elapsed time: 46.80 \t Loss:29.57885\n",
      "Iteration: 257/277000 \t Elapsed time: 46.97 \t Loss:29.54605\n",
      "Iteration: 258/277000 \t Elapsed time: 47.15 \t Loss:29.51330\n",
      "Iteration: 259/277000 \t Elapsed time: 47.32 \t Loss:29.48061\n",
      "Iteration: 260/277000 \t Elapsed time: 47.49 \t Loss:29.44797\n",
      "Iteration: 261/277000 \t Elapsed time: 47.71 \t Loss:29.41540\n",
      "Iteration: 262/277000 \t Elapsed time: 47.88 \t Loss:29.38290\n",
      "Iteration: 263/277000 \t Elapsed time: 48.07 \t Loss:29.35050\n",
      "Iteration: 264/277000 \t Elapsed time: 48.24 \t Loss:29.31805\n",
      "Iteration: 265/277000 \t Elapsed time: 48.41 \t Loss:29.28570\n",
      "Iteration: 266/277000 \t Elapsed time: 48.64 \t Loss:29.25351\n",
      "Iteration: 267/277000 \t Elapsed time: 48.81 \t Loss:29.22146\n",
      "Iteration: 268/277000 \t Elapsed time: 48.97 \t Loss:29.18942\n",
      "Iteration: 269/277000 \t Elapsed time: 49.15 \t Loss:29.15729\n",
      "Iteration: 270/277000 \t Elapsed time: 49.31 \t Loss:29.12503\n",
      "Iteration: 271/277000 \t Elapsed time: 49.47 \t Loss:29.09298\n",
      "Iteration: 272/277000 \t Elapsed time: 49.63 \t Loss:29.06123\n",
      "Iteration: 273/277000 \t Elapsed time: 49.79 \t Loss:29.02937\n",
      "Iteration: 274/277000 \t Elapsed time: 49.94 \t Loss:28.99739\n",
      "Iteration: 275/277000 \t Elapsed time: 50.11 \t Loss:28.96569\n",
      "Iteration: 276/277000 \t Elapsed time: 50.28 \t Loss:28.93412\n",
      "Iteration: 277/277000 \t Elapsed time: 50.48 \t Loss:28.90237\n",
      "Iteration: 278/277000 \t Elapsed time: 50.65 \t Loss:28.87077\n",
      "Iteration: 279/277000 \t Elapsed time: 50.81 \t Loss:28.83932\n",
      "Iteration: 280/277000 \t Elapsed time: 50.98 \t Loss:28.80779\n",
      "Iteration: 281/277000 \t Elapsed time: 51.15 \t Loss:28.77631\n",
      "Iteration: 282/277000 \t Elapsed time: 51.36 \t Loss:28.74500\n",
      "Iteration: 283/277000 \t Elapsed time: 51.56 \t Loss:28.71368\n",
      "Iteration: 284/277000 \t Elapsed time: 51.76 \t Loss:28.68242\n",
      "Iteration: 285/277000 \t Elapsed time: 51.95 \t Loss:28.65123\n",
      "Iteration: 286/277000 \t Elapsed time: 52.12 \t Loss:28.62014\n",
      "Iteration: 287/277000 \t Elapsed time: 52.30 \t Loss:28.58907\n",
      "Iteration: 288/277000 \t Elapsed time: 52.48 \t Loss:28.55803\n",
      "Iteration: 289/277000 \t Elapsed time: 52.66 \t Loss:28.52710\n",
      "Iteration: 290/277000 \t Elapsed time: 52.82 \t Loss:28.49623\n",
      "Iteration: 291/277000 \t Elapsed time: 53.02 \t Loss:28.46534\n",
      "Iteration: 292/277000 \t Elapsed time: 53.19 \t Loss:28.43454\n",
      "Iteration: 293/277000 \t Elapsed time: 53.42 \t Loss:28.40380\n",
      "Iteration: 294/277000 \t Elapsed time: 53.58 \t Loss:28.37308\n",
      "Iteration: 295/277000 \t Elapsed time: 53.77 \t Loss:28.34241\n",
      "Iteration: 296/277000 \t Elapsed time: 53.98 \t Loss:28.31182\n",
      "Iteration: 297/277000 \t Elapsed time: 54.15 \t Loss:28.28130\n",
      "Iteration: 298/277000 \t Elapsed time: 54.32 \t Loss:28.25084\n",
      "Iteration: 299/277000 \t Elapsed time: 54.49 \t Loss:28.22044\n",
      "Iteration: 300/277000 \t Elapsed time: 54.65 \t Loss:28.19007\n",
      "Iteration: 301/277000 \t Elapsed time: 54.82 \t Loss:28.15976\n",
      "Iteration: 302/277000 \t Elapsed time: 54.98 \t Loss:28.12949\n",
      "Iteration: 303/277000 \t Elapsed time: 55.16 \t Loss:28.09926\n",
      "Iteration: 304/277000 \t Elapsed time: 55.36 \t Loss:28.06909\n",
      "Iteration: 305/277000 \t Elapsed time: 55.53 \t Loss:28.03897\n",
      "Iteration: 306/277000 \t Elapsed time: 55.76 \t Loss:28.00887\n",
      "Iteration: 307/277000 \t Elapsed time: 56.22 \t Loss:27.97883\n",
      "Iteration: 308/277000 \t Elapsed time: 56.40 \t Loss:27.94885\n",
      "Iteration: 309/277000 \t Elapsed time: 56.57 \t Loss:27.91894\n",
      "Iteration: 310/277000 \t Elapsed time: 56.74 \t Loss:27.88910\n",
      "Iteration: 311/277000 \t Elapsed time: 56.91 \t Loss:27.85931\n",
      "Iteration: 312/277000 \t Elapsed time: 57.08 \t Loss:27.82959\n",
      "Iteration: 313/277000 \t Elapsed time: 57.26 \t Loss:27.79990\n",
      "Iteration: 314/277000 \t Elapsed time: 57.45 \t Loss:27.77025\n",
      "Iteration: 315/277000 \t Elapsed time: 57.62 \t Loss:27.74062\n",
      "Iteration: 316/277000 \t Elapsed time: 57.79 \t Loss:27.71104\n",
      "Iteration: 317/277000 \t Elapsed time: 57.97 \t Loss:27.68153\n",
      "Iteration: 318/277000 \t Elapsed time: 58.15 \t Loss:27.65207\n",
      "Iteration: 319/277000 \t Elapsed time: 58.35 \t Loss:27.62268\n",
      "Iteration: 320/277000 \t Elapsed time: 58.55 \t Loss:27.59335\n",
      "Iteration: 321/277000 \t Elapsed time: 58.77 \t Loss:27.56407\n",
      "Iteration: 322/277000 \t Elapsed time: 59.00 \t Loss:27.53485\n",
      "Iteration: 323/277000 \t Elapsed time: 59.25 \t Loss:27.50568\n",
      "Iteration: 324/277000 \t Elapsed time: 59.44 \t Loss:27.47652\n",
      "Iteration: 325/277000 \t Elapsed time: 59.62 \t Loss:27.44737\n",
      "Iteration: 326/277000 \t Elapsed time: 59.86 \t Loss:27.41825\n",
      "Iteration: 327/277000 \t Elapsed time: 60.03 \t Loss:27.38920\n",
      "Iteration: 328/277000 \t Elapsed time: 60.24 \t Loss:27.36025\n",
      "Iteration: 329/277000 \t Elapsed time: 60.43 \t Loss:27.33139\n",
      "Iteration: 330/277000 \t Elapsed time: 60.62 \t Loss:27.30256\n",
      "Iteration: 331/277000 \t Elapsed time: 60.82 \t Loss:27.27374\n",
      "Iteration: 332/277000 \t Elapsed time: 61.01 \t Loss:27.24497\n",
      "Iteration: 333/277000 \t Elapsed time: 61.20 \t Loss:27.21627\n",
      "Iteration: 334/277000 \t Elapsed time: 61.38 \t Loss:27.18769\n",
      "Iteration: 335/277000 \t Elapsed time: 61.55 \t Loss:27.15926\n",
      "Iteration: 336/277000 \t Elapsed time: 61.77 \t Loss:27.13080\n",
      "Iteration: 337/277000 \t Elapsed time: 61.95 \t Loss:27.10213\n",
      "Iteration: 338/277000 \t Elapsed time: 62.12 \t Loss:27.07332\n",
      "Iteration: 339/277000 \t Elapsed time: 62.29 \t Loss:27.04487\n",
      "Iteration: 340/277000 \t Elapsed time: 62.45 \t Loss:27.01673\n",
      "Iteration: 341/277000 \t Elapsed time: 62.62 \t Loss:26.98842\n",
      "Iteration: 342/277000 \t Elapsed time: 62.78 \t Loss:26.95995\n",
      "Iteration: 343/277000 \t Elapsed time: 62.95 \t Loss:26.93173\n",
      "Iteration: 344/277000 \t Elapsed time: 63.11 \t Loss:26.90363\n",
      "Iteration: 345/277000 \t Elapsed time: 63.28 \t Loss:26.87534\n",
      "Iteration: 346/277000 \t Elapsed time: 63.46 \t Loss:26.84708\n",
      "Iteration: 347/277000 \t Elapsed time: 63.63 \t Loss:26.81912\n",
      "Iteration: 348/277000 \t Elapsed time: 63.81 \t Loss:26.79117\n",
      "Iteration: 349/277000 \t Elapsed time: 63.98 \t Loss:26.76305\n",
      "Iteration: 350/277000 \t Elapsed time: 64.14 \t Loss:26.73509\n",
      "Iteration: 351/277000 \t Elapsed time: 64.31 \t Loss:26.70731\n",
      "Iteration: 352/277000 \t Elapsed time: 64.48 \t Loss:26.67942\n",
      "Iteration: 353/277000 \t Elapsed time: 64.65 \t Loss:26.65149\n",
      "Iteration: 354/277000 \t Elapsed time: 64.82 \t Loss:26.62379\n",
      "Iteration: 355/277000 \t Elapsed time: 64.99 \t Loss:26.59616\n",
      "Iteration: 356/277000 \t Elapsed time: 65.16 \t Loss:26.56846\n",
      "Iteration: 357/277000 \t Elapsed time: 65.38 \t Loss:26.54082\n",
      "Iteration: 358/277000 \t Elapsed time: 65.64 \t Loss:26.51335\n",
      "Iteration: 359/277000 \t Elapsed time: 65.87 \t Loss:26.48591\n",
      "Iteration: 360/277000 \t Elapsed time: 66.12 \t Loss:26.45836\n",
      "Iteration: 361/277000 \t Elapsed time: 66.29 \t Loss:26.43079\n",
      "Iteration: 362/277000 \t Elapsed time: 66.46 \t Loss:26.40326\n",
      "Iteration: 363/277000 \t Elapsed time: 66.65 \t Loss:26.37585\n",
      "Iteration: 364/277000 \t Elapsed time: 66.83 \t Loss:26.34859\n",
      "Iteration: 365/277000 \t Elapsed time: 67.05 \t Loss:26.32138\n",
      "Iteration: 366/277000 \t Elapsed time: 67.27 \t Loss:26.29409\n",
      "Iteration: 367/277000 \t Elapsed time: 67.46 \t Loss:26.26679\n",
      "Iteration: 368/277000 \t Elapsed time: 67.67 \t Loss:26.23963\n",
      "Iteration: 369/277000 \t Elapsed time: 67.84 \t Loss:26.21260\n",
      "Iteration: 370/277000 \t Elapsed time: 68.26 \t Loss:26.18556\n",
      "Iteration: 371/277000 \t Elapsed time: 68.46 \t Loss:26.15848\n",
      "Iteration: 372/277000 \t Elapsed time: 68.66 \t Loss:26.13144\n",
      "Iteration: 373/277000 \t Elapsed time: 68.82 \t Loss:26.10454\n",
      "Iteration: 374/277000 \t Elapsed time: 69.00 \t Loss:26.07769\n",
      "Iteration: 375/277000 \t Elapsed time: 69.20 \t Loss:26.05083\n",
      "Iteration: 376/277000 \t Elapsed time: 69.37 \t Loss:26.02398\n",
      "Iteration: 377/277000 \t Elapsed time: 69.53 \t Loss:25.99718\n",
      "Iteration: 378/277000 \t Elapsed time: 69.72 \t Loss:25.97049\n",
      "Iteration: 379/277000 \t Elapsed time: 69.91 \t Loss:25.94383\n",
      "Iteration: 380/277000 \t Elapsed time: 70.13 \t Loss:25.91717\n",
      "Iteration: 381/277000 \t Elapsed time: 70.31 \t Loss:25.89055\n",
      "Iteration: 382/277000 \t Elapsed time: 70.48 \t Loss:25.86398\n",
      "Iteration: 383/277000 \t Elapsed time: 70.65 \t Loss:25.83746\n",
      "Iteration: 384/277000 \t Elapsed time: 70.82 \t Loss:25.81100\n",
      "Iteration: 385/277000 \t Elapsed time: 70.99 \t Loss:25.78456\n",
      "Iteration: 386/277000 \t Elapsed time: 71.16 \t Loss:25.75817\n",
      "Iteration: 387/277000 \t Elapsed time: 71.34 \t Loss:25.73180\n",
      "Iteration: 388/277000 \t Elapsed time: 71.55 \t Loss:25.70546\n",
      "Iteration: 389/277000 \t Elapsed time: 71.77 \t Loss:25.67919\n",
      "Iteration: 390/277000 \t Elapsed time: 71.94 \t Loss:25.65295\n",
      "Iteration: 391/277000 \t Elapsed time: 72.11 \t Loss:25.62678\n",
      "Iteration: 392/277000 \t Elapsed time: 72.30 \t Loss:25.60064\n",
      "Iteration: 393/277000 \t Elapsed time: 72.47 \t Loss:25.57456\n",
      "Iteration: 394/277000 \t Elapsed time: 72.65 \t Loss:25.54854\n",
      "Iteration: 395/277000 \t Elapsed time: 72.83 \t Loss:25.52262\n",
      "Iteration: 396/277000 \t Elapsed time: 73.04 \t Loss:25.49678\n",
      "Iteration: 397/277000 \t Elapsed time: 73.22 \t Loss:25.47101\n",
      "Iteration: 398/277000 \t Elapsed time: 73.42 \t Loss:25.44491\n",
      "Iteration: 399/277000 \t Elapsed time: 73.59 \t Loss:25.41877\n",
      "Iteration: 400/277000 \t Elapsed time: 73.79 \t Loss:25.39294\n",
      "Iteration: 401/277000 \t Elapsed time: 73.95 \t Loss:25.36730\n",
      "Iteration: 402/277000 \t Elapsed time: 74.13 \t Loss:25.34151\n",
      "Iteration: 403/277000 \t Elapsed time: 74.30 \t Loss:25.31566\n",
      "Iteration: 404/277000 \t Elapsed time: 74.48 \t Loss:25.29011\n",
      "Iteration: 405/277000 \t Elapsed time: 74.74 \t Loss:25.26462\n",
      "Iteration: 406/277000 \t Elapsed time: 74.91 \t Loss:25.23891\n",
      "Iteration: 407/277000 \t Elapsed time: 75.10 \t Loss:25.21347\n",
      "Iteration: 408/277000 \t Elapsed time: 75.27 \t Loss:25.18793\n",
      "Iteration: 409/277000 \t Elapsed time: 75.45 \t Loss:25.16225\n",
      "Iteration: 410/277000 \t Elapsed time: 75.63 \t Loss:25.13681\n",
      "Iteration: 411/277000 \t Elapsed time: 75.85 \t Loss:25.11130\n",
      "Iteration: 412/277000 \t Elapsed time: 76.01 \t Loss:25.08585\n",
      "Iteration: 413/277000 \t Elapsed time: 76.17 \t Loss:25.06052\n",
      "Iteration: 414/277000 \t Elapsed time: 76.63 \t Loss:25.03515\n",
      "Iteration: 415/277000 \t Elapsed time: 76.81 \t Loss:25.00995\n",
      "Iteration: 416/277000 \t Elapsed time: 76.98 \t Loss:24.98472\n",
      "Iteration: 417/277000 \t Elapsed time: 77.21 \t Loss:24.95949\n",
      "Iteration: 418/277000 \t Elapsed time: 77.41 \t Loss:24.93427\n",
      "Iteration: 419/277000 \t Elapsed time: 77.64 \t Loss:24.90911\n",
      "Iteration: 420/277000 \t Elapsed time: 77.82 \t Loss:24.88403\n",
      "Iteration: 421/277000 \t Elapsed time: 77.99 \t Loss:24.85902\n",
      "Iteration: 422/277000 \t Elapsed time: 78.16 \t Loss:24.83404\n",
      "Iteration: 423/277000 \t Elapsed time: 78.34 \t Loss:24.80898\n",
      "Iteration: 424/277000 \t Elapsed time: 78.52 \t Loss:24.78400\n",
      "Iteration: 425/277000 \t Elapsed time: 78.69 \t Loss:24.75909\n",
      "Iteration: 426/277000 \t Elapsed time: 78.90 \t Loss:24.73423\n",
      "Iteration: 427/277000 \t Elapsed time: 79.08 \t Loss:24.70944\n",
      "Iteration: 428/277000 \t Elapsed time: 79.25 \t Loss:24.68461\n",
      "Iteration: 429/277000 \t Elapsed time: 79.44 \t Loss:24.65980\n",
      "Iteration: 430/277000 \t Elapsed time: 79.61 \t Loss:24.63505\n",
      "Iteration: 431/277000 \t Elapsed time: 79.78 \t Loss:24.61034\n",
      "Iteration: 432/277000 \t Elapsed time: 79.97 \t Loss:24.58572\n",
      "Iteration: 433/277000 \t Elapsed time: 80.16 \t Loss:24.56110\n",
      "Iteration: 434/277000 \t Elapsed time: 80.34 \t Loss:24.53650\n",
      "Iteration: 435/277000 \t Elapsed time: 80.52 \t Loss:24.51192\n",
      "Iteration: 436/277000 \t Elapsed time: 80.68 \t Loss:24.48737\n",
      "Iteration: 437/277000 \t Elapsed time: 80.86 \t Loss:24.46290\n",
      "Iteration: 438/277000 \t Elapsed time: 81.06 \t Loss:24.43846\n",
      "Iteration: 439/277000 \t Elapsed time: 81.33 \t Loss:24.41406\n",
      "Iteration: 440/277000 \t Elapsed time: 81.51 \t Loss:24.38969\n",
      "Iteration: 441/277000 \t Elapsed time: 81.69 \t Loss:24.36537\n",
      "Iteration: 442/277000 \t Elapsed time: 81.88 \t Loss:24.34109\n",
      "Iteration: 443/277000 \t Elapsed time: 82.06 \t Loss:24.31688\n",
      "Iteration: 444/277000 \t Elapsed time: 82.24 \t Loss:24.29274\n",
      "Iteration: 445/277000 \t Elapsed time: 82.41 \t Loss:24.26860\n",
      "Iteration: 446/277000 \t Elapsed time: 82.58 \t Loss:24.24437\n",
      "Iteration: 447/277000 \t Elapsed time: 82.77 \t Loss:24.22007\n",
      "Iteration: 448/277000 \t Elapsed time: 82.94 \t Loss:24.19581\n",
      "Iteration: 449/277000 \t Elapsed time: 83.10 \t Loss:24.17170\n",
      "Iteration: 450/277000 \t Elapsed time: 83.27 \t Loss:24.14767\n",
      "Iteration: 451/277000 \t Elapsed time: 83.45 \t Loss:24.12366\n",
      "Iteration: 452/277000 \t Elapsed time: 83.62 \t Loss:24.09964\n",
      "Iteration: 453/277000 \t Elapsed time: 83.78 \t Loss:24.07566\n",
      "Iteration: 454/277000 \t Elapsed time: 83.95 \t Loss:24.05176\n",
      "Iteration: 455/277000 \t Elapsed time: 84.12 \t Loss:24.02787\n",
      "Iteration: 456/277000 \t Elapsed time: 84.29 \t Loss:24.00397\n",
      "Iteration: 457/277000 \t Elapsed time: 84.46 \t Loss:23.98009\n",
      "Iteration: 458/277000 \t Elapsed time: 84.62 \t Loss:23.95629\n",
      "Iteration: 459/277000 \t Elapsed time: 84.79 \t Loss:23.93256\n",
      "Iteration: 460/277000 \t Elapsed time: 84.97 \t Loss:23.90886\n",
      "Iteration: 461/277000 \t Elapsed time: 85.56 \t Loss:23.88513\n",
      "Iteration: 462/277000 \t Elapsed time: 85.92 \t Loss:23.86139\n",
      "Iteration: 463/277000 \t Elapsed time: 86.11 \t Loss:23.83774\n",
      "Iteration: 464/277000 \t Elapsed time: 86.30 \t Loss:23.81416\n",
      "Iteration: 465/277000 \t Elapsed time: 86.49 \t Loss:23.79061\n",
      "Iteration: 466/277000 \t Elapsed time: 86.68 \t Loss:23.76707\n",
      "Iteration: 467/277000 \t Elapsed time: 86.89 \t Loss:23.74353\n",
      "Iteration: 468/277000 \t Elapsed time: 87.06 \t Loss:23.72004\n",
      "Iteration: 469/277000 \t Elapsed time: 87.24 \t Loss:23.69662\n",
      "Iteration: 470/277000 \t Elapsed time: 87.68 \t Loss:23.67328\n",
      "Iteration: 471/277000 \t Elapsed time: 87.89 \t Loss:23.64998\n",
      "Iteration: 472/277000 \t Elapsed time: 88.09 \t Loss:23.62661\n",
      "Iteration: 473/277000 \t Elapsed time: 88.26 \t Loss:23.60313\n",
      "Iteration: 474/277000 \t Elapsed time: 88.45 \t Loss:23.57971\n",
      "Iteration: 475/277000 \t Elapsed time: 88.62 \t Loss:23.55651\n",
      "Iteration: 476/277000 \t Elapsed time: 88.83 \t Loss:23.53334\n",
      "Iteration: 477/277000 \t Elapsed time: 89.04 \t Loss:23.51005\n",
      "Iteration: 478/277000 \t Elapsed time: 89.24 \t Loss:23.48682\n",
      "Iteration: 479/277000 \t Elapsed time: 89.44 \t Loss:23.46371\n",
      "Iteration: 480/277000 \t Elapsed time: 89.60 \t Loss:23.44060\n",
      "Iteration: 481/277000 \t Elapsed time: 89.80 \t Loss:23.41745\n",
      "Iteration: 482/277000 \t Elapsed time: 89.97 \t Loss:23.39438\n",
      "Iteration: 483/277000 \t Elapsed time: 90.19 \t Loss:23.37138\n",
      "Iteration: 484/277000 \t Elapsed time: 91.09 \t Loss:23.34834\n",
      "Iteration: 485/277000 \t Elapsed time: 91.26 \t Loss:23.32534\n",
      "Iteration: 486/277000 \t Elapsed time: 91.44 \t Loss:23.30241\n",
      "Iteration: 487/277000 \t Elapsed time: 91.62 \t Loss:23.27948\n",
      "Iteration: 488/277000 \t Elapsed time: 91.80 \t Loss:23.25657\n",
      "Iteration: 489/277000 \t Elapsed time: 91.98 \t Loss:23.23373\n",
      "Iteration: 490/277000 \t Elapsed time: 92.16 \t Loss:23.21090\n",
      "Iteration: 491/277000 \t Elapsed time: 92.34 \t Loss:23.18810\n",
      "Iteration: 492/277000 \t Elapsed time: 92.51 \t Loss:23.16536\n",
      "Iteration: 493/277000 \t Elapsed time: 92.67 \t Loss:23.14271\n",
      "Iteration: 494/277000 \t Elapsed time: 92.84 \t Loss:23.12015\n",
      "Iteration: 495/277000 \t Elapsed time: 93.00 \t Loss:23.09775\n",
      "Iteration: 496/277000 \t Elapsed time: 93.17 \t Loss:23.07560\n",
      "Iteration: 497/277000 \t Elapsed time: 93.33 \t Loss:23.05351\n",
      "Iteration: 498/277000 \t Elapsed time: 93.50 \t Loss:23.03102\n",
      "Iteration: 499/277000 \t Elapsed time: 93.67 \t Loss:23.00760\n",
      "Iteration: 500/277000 \t Elapsed time: 93.87 \t Loss:22.98415\n",
      "Iteration: 501/277000 \t Elapsed time: 94.04 \t Loss:22.96215\n",
      "Iteration: 502/277000 \t Elapsed time: 94.21 \t Loss:22.94008\n",
      "Iteration: 503/277000 \t Elapsed time: 94.38 \t Loss:22.91676\n",
      "Iteration: 504/277000 \t Elapsed time: 94.55 \t Loss:22.89433\n",
      "Iteration: 505/277000 \t Elapsed time: 94.72 \t Loss:22.87240\n",
      "Iteration: 506/277000 \t Elapsed time: 94.90 \t Loss:22.84949\n",
      "Iteration: 507/277000 \t Elapsed time: 95.06 \t Loss:22.82714\n",
      "Iteration: 508/277000 \t Elapsed time: 95.23 \t Loss:22.80501\n",
      "Iteration: 509/277000 \t Elapsed time: 95.40 \t Loss:22.78241\n",
      "Iteration: 510/277000 \t Elapsed time: 95.59 \t Loss:22.76025\n",
      "Iteration: 511/277000 \t Elapsed time: 95.78 \t Loss:22.73795\n",
      "Iteration: 512/277000 \t Elapsed time: 95.95 \t Loss:22.71566\n",
      "Iteration: 513/277000 \t Elapsed time: 96.12 \t Loss:22.69351\n",
      "Iteration: 514/277000 \t Elapsed time: 96.29 \t Loss:22.67121\n",
      "Iteration: 515/277000 \t Elapsed time: 96.45 \t Loss:22.64917\n",
      "Iteration: 516/277000 \t Elapsed time: 96.62 \t Loss:22.62693\n",
      "Iteration: 517/277000 \t Elapsed time: 96.79 \t Loss:22.60482\n",
      "Iteration: 518/277000 \t Elapsed time: 97.00 \t Loss:22.58284\n",
      "Iteration: 519/277000 \t Elapsed time: 97.22 \t Loss:22.56062\n",
      "Iteration: 520/277000 \t Elapsed time: 97.43 \t Loss:22.53868\n",
      "Iteration: 521/277000 \t Elapsed time: 97.61 \t Loss:22.51665\n",
      "Iteration: 522/277000 \t Elapsed time: 97.78 \t Loss:22.49460\n",
      "Iteration: 523/277000 \t Elapsed time: 97.96 \t Loss:22.47271\n",
      "Iteration: 524/277000 \t Elapsed time: 98.15 \t Loss:22.45074\n",
      "Iteration: 525/277000 \t Elapsed time: 98.38 \t Loss:22.42884\n",
      "Iteration: 526/277000 \t Elapsed time: 98.60 \t Loss:22.40697\n",
      "Iteration: 527/277000 \t Elapsed time: 98.78 \t Loss:22.38509\n",
      "Iteration: 528/277000 \t Elapsed time: 98.95 \t Loss:22.36329\n",
      "Iteration: 529/277000 \t Elapsed time: 99.12 \t Loss:22.34146\n",
      "Iteration: 530/277000 \t Elapsed time: 99.28 \t Loss:22.31969\n",
      "Iteration: 531/277000 \t Elapsed time: 99.45 \t Loss:22.29791\n",
      "Iteration: 532/277000 \t Elapsed time: 99.67 \t Loss:22.27614\n",
      "Iteration: 533/277000 \t Elapsed time: 99.84 \t Loss:22.25445\n",
      "Iteration: 534/277000 \t Elapsed time: 100.01 \t Loss:22.23273\n",
      "Iteration: 535/277000 \t Elapsed time: 100.19 \t Loss:22.21108\n",
      "Iteration: 536/277000 \t Elapsed time: 100.36 \t Loss:22.18946\n",
      "Iteration: 537/277000 \t Elapsed time: 100.57 \t Loss:22.16782\n",
      "Iteration: 538/277000 \t Elapsed time: 100.78 \t Loss:22.14626\n",
      "Iteration: 539/277000 \t Elapsed time: 100.98 \t Loss:22.12470\n",
      "Iteration: 540/277000 \t Elapsed time: 101.15 \t Loss:22.10316\n",
      "Iteration: 541/277000 \t Elapsed time: 101.32 \t Loss:22.08166\n",
      "Iteration: 542/277000 \t Elapsed time: 101.49 \t Loss:22.06017\n",
      "Iteration: 543/277000 \t Elapsed time: 101.65 \t Loss:22.03871\n",
      "Iteration: 544/277000 \t Elapsed time: 101.83 \t Loss:22.01726\n",
      "Iteration: 545/277000 \t Elapsed time: 101.99 \t Loss:21.99583\n",
      "Iteration: 546/277000 \t Elapsed time: 102.18 \t Loss:21.97443\n",
      "Iteration: 547/277000 \t Elapsed time: 102.34 \t Loss:21.95304\n",
      "Iteration: 548/277000 \t Elapsed time: 102.52 \t Loss:21.93167\n",
      "Iteration: 549/277000 \t Elapsed time: 102.69 \t Loss:21.91033\n",
      "Iteration: 550/277000 \t Elapsed time: 102.86 \t Loss:21.88902\n",
      "Iteration: 551/277000 \t Elapsed time: 103.02 \t Loss:21.86774\n",
      "Iteration: 552/277000 \t Elapsed time: 103.19 \t Loss:21.84648\n",
      "Iteration: 553/277000 \t Elapsed time: 103.36 \t Loss:21.82525\n",
      "Iteration: 554/277000 \t Elapsed time: 103.55 \t Loss:21.80407\n",
      "Iteration: 555/277000 \t Elapsed time: 103.72 \t Loss:21.78289\n",
      "Iteration: 556/277000 \t Elapsed time: 103.89 \t Loss:21.76175\n",
      "Iteration: 557/277000 \t Elapsed time: 104.05 \t Loss:21.74062\n",
      "Iteration: 558/277000 \t Elapsed time: 104.22 \t Loss:21.71952\n",
      "Iteration: 559/277000 \t Elapsed time: 104.39 \t Loss:21.69845\n",
      "Iteration: 560/277000 \t Elapsed time: 104.55 \t Loss:21.67739\n",
      "Iteration: 561/277000 \t Elapsed time: 104.72 \t Loss:21.65632\n",
      "Iteration: 562/277000 \t Elapsed time: 104.89 \t Loss:21.63530\n",
      "Iteration: 563/277000 \t Elapsed time: 105.04 \t Loss:21.61430\n",
      "Iteration: 564/277000 \t Elapsed time: 105.20 \t Loss:21.59331\n",
      "Iteration: 565/277000 \t Elapsed time: 105.42 \t Loss:21.57236\n",
      "Iteration: 566/277000 \t Elapsed time: 105.62 \t Loss:21.55144\n",
      "Iteration: 567/277000 \t Elapsed time: 105.81 \t Loss:21.53053\n",
      "Iteration: 568/277000 \t Elapsed time: 105.97 \t Loss:21.50966\n",
      "Iteration: 569/277000 \t Elapsed time: 106.14 \t Loss:21.48882\n",
      "Iteration: 570/277000 \t Elapsed time: 106.31 \t Loss:21.46798\n",
      "Iteration: 571/277000 \t Elapsed time: 106.48 \t Loss:21.44717\n",
      "Iteration: 572/277000 \t Elapsed time: 106.65 \t Loss:21.42637\n",
      "Iteration: 573/277000 \t Elapsed time: 106.83 \t Loss:21.40560\n",
      "Iteration: 574/277000 \t Elapsed time: 107.00 \t Loss:21.38484\n",
      "Iteration: 575/277000 \t Elapsed time: 107.16 \t Loss:21.36411\n",
      "Iteration: 576/277000 \t Elapsed time: 107.33 \t Loss:21.34339\n",
      "Iteration: 577/277000 \t Elapsed time: 107.50 \t Loss:21.32272\n",
      "Iteration: 578/277000 \t Elapsed time: 107.67 \t Loss:21.30205\n",
      "Iteration: 579/277000 \t Elapsed time: 107.89 \t Loss:21.28141\n",
      "Iteration: 580/277000 \t Elapsed time: 108.07 \t Loss:21.26080\n",
      "Iteration: 581/277000 \t Elapsed time: 108.24 \t Loss:21.24021\n",
      "Iteration: 582/277000 \t Elapsed time: 108.41 \t Loss:21.21963\n",
      "Iteration: 583/277000 \t Elapsed time: 108.58 \t Loss:21.19908\n",
      "Iteration: 584/277000 \t Elapsed time: 108.75 \t Loss:21.17855\n",
      "Iteration: 585/277000 \t Elapsed time: 108.92 \t Loss:21.15803\n",
      "Iteration: 586/277000 \t Elapsed time: 109.08 \t Loss:21.13755\n",
      "Iteration: 587/277000 \t Elapsed time: 109.27 \t Loss:21.11707\n",
      "Iteration: 588/277000 \t Elapsed time: 109.44 \t Loss:21.09662\n",
      "Iteration: 589/277000 \t Elapsed time: 109.60 \t Loss:21.07619\n",
      "Iteration: 590/277000 \t Elapsed time: 109.88 \t Loss:21.05579\n",
      "Iteration: 591/277000 \t Elapsed time: 110.09 \t Loss:21.03541\n",
      "Iteration: 592/277000 \t Elapsed time: 110.25 \t Loss:21.01505\n",
      "Iteration: 593/277000 \t Elapsed time: 110.47 \t Loss:20.99469\n",
      "Iteration: 594/277000 \t Elapsed time: 110.63 \t Loss:20.97437\n",
      "Iteration: 595/277000 \t Elapsed time: 110.85 \t Loss:20.95408\n",
      "Iteration: 596/277000 \t Elapsed time: 111.01 \t Loss:20.93381\n",
      "Iteration: 597/277000 \t Elapsed time: 111.19 \t Loss:20.91355\n",
      "Iteration: 598/277000 \t Elapsed time: 111.37 \t Loss:20.89331\n",
      "Iteration: 599/277000 \t Elapsed time: 111.55 \t Loss:20.87311\n",
      "Iteration: 600/277000 \t Elapsed time: 111.75 \t Loss:20.85292\n",
      "Iteration: 601/277000 \t Elapsed time: 111.95 \t Loss:20.83276\n",
      "Iteration: 602/277000 \t Elapsed time: 112.12 \t Loss:20.81262\n",
      "Iteration: 603/277000 \t Elapsed time: 112.29 \t Loss:20.79250\n",
      "Iteration: 604/277000 \t Elapsed time: 112.51 \t Loss:20.77240\n",
      "Iteration: 605/277000 \t Elapsed time: 112.68 \t Loss:20.75231\n",
      "Iteration: 606/277000 \t Elapsed time: 112.86 \t Loss:20.73222\n",
      "Iteration: 607/277000 \t Elapsed time: 113.08 \t Loss:20.71211\n",
      "Iteration: 608/277000 \t Elapsed time: 113.28 \t Loss:20.69201\n",
      "Iteration: 609/277000 \t Elapsed time: 113.73 \t Loss:20.67194\n",
      "Iteration: 610/277000 \t Elapsed time: 113.99 \t Loss:20.65197\n",
      "Iteration: 611/277000 \t Elapsed time: 114.16 \t Loss:20.63203\n",
      "Iteration: 612/277000 \t Elapsed time: 114.33 \t Loss:20.61209\n",
      "Iteration: 613/277000 \t Elapsed time: 114.49 \t Loss:20.59212\n",
      "Iteration: 614/277000 \t Elapsed time: 114.66 \t Loss:20.57217\n",
      "Iteration: 615/277000 \t Elapsed time: 114.86 \t Loss:20.55226\n",
      "Iteration: 616/277000 \t Elapsed time: 115.03 \t Loss:20.53238\n",
      "Iteration: 617/277000 \t Elapsed time: 115.20 \t Loss:20.51253\n",
      "Iteration: 618/277000 \t Elapsed time: 115.38 \t Loss:20.49268\n",
      "Iteration: 619/277000 \t Elapsed time: 115.53 \t Loss:20.47285\n",
      "Iteration: 620/277000 \t Elapsed time: 115.70 \t Loss:20.45304\n",
      "Iteration: 621/277000 \t Elapsed time: 115.89 \t Loss:20.43328\n",
      "Iteration: 622/277000 \t Elapsed time: 116.06 \t Loss:20.41353\n",
      "Iteration: 623/277000 \t Elapsed time: 116.22 \t Loss:20.39379\n",
      "Iteration: 624/277000 \t Elapsed time: 116.39 \t Loss:20.37408\n",
      "Iteration: 625/277000 \t Elapsed time: 116.56 \t Loss:20.35443\n",
      "Iteration: 626/277000 \t Elapsed time: 116.73 \t Loss:20.33483\n",
      "Iteration: 627/277000 \t Elapsed time: 116.90 \t Loss:20.31536\n",
      "Iteration: 628/277000 \t Elapsed time: 117.07 \t Loss:20.29594\n",
      "Iteration: 629/277000 \t Elapsed time: 117.23 \t Loss:20.27656\n",
      "Iteration: 630/277000 \t Elapsed time: 117.40 \t Loss:20.25698\n",
      "Iteration: 631/277000 \t Elapsed time: 117.57 \t Loss:20.23716\n",
      "Iteration: 632/277000 \t Elapsed time: 117.74 \t Loss:20.21722\n",
      "Iteration: 633/277000 \t Elapsed time: 117.92 \t Loss:20.19744\n",
      "Iteration: 634/277000 \t Elapsed time: 118.08 \t Loss:20.17793\n",
      "Iteration: 635/277000 \t Elapsed time: 118.26 \t Loss:20.15852\n",
      "Iteration: 636/277000 \t Elapsed time: 118.45 \t Loss:20.13904\n",
      "Iteration: 637/277000 \t Elapsed time: 118.62 \t Loss:20.11948\n",
      "Iteration: 638/277000 \t Elapsed time: 118.79 \t Loss:20.09994\n",
      "Iteration: 639/277000 \t Elapsed time: 118.96 \t Loss:20.08047\n",
      "Iteration: 640/277000 \t Elapsed time: 119.13 \t Loss:20.06106\n",
      "Iteration: 641/277000 \t Elapsed time: 119.30 \t Loss:20.04169\n",
      "Iteration: 642/277000 \t Elapsed time: 119.47 \t Loss:20.02225\n",
      "Iteration: 643/277000 \t Elapsed time: 119.64 \t Loss:20.00280\n",
      "Iteration: 644/277000 \t Elapsed time: 119.82 \t Loss:19.98346\n",
      "Iteration: 645/277000 \t Elapsed time: 119.99 \t Loss:19.96419\n",
      "Iteration: 646/277000 \t Elapsed time: 120.17 \t Loss:19.94484\n",
      "Iteration: 647/277000 \t Elapsed time: 120.36 \t Loss:19.92547\n",
      "Iteration: 648/277000 \t Elapsed time: 120.55 \t Loss:19.90620\n",
      "Iteration: 649/277000 \t Elapsed time: 120.71 \t Loss:19.88700\n",
      "Iteration: 650/277000 \t Elapsed time: 120.88 \t Loss:19.86775\n",
      "Iteration: 651/277000 \t Elapsed time: 121.05 \t Loss:19.84848\n",
      "Iteration: 652/277000 \t Elapsed time: 121.21 \t Loss:19.82931\n",
      "Iteration: 653/277000 \t Elapsed time: 121.39 \t Loss:19.81018\n",
      "Iteration: 654/277000 \t Elapsed time: 121.55 \t Loss:19.79103\n",
      "Iteration: 655/277000 \t Elapsed time: 121.72 \t Loss:19.77182\n",
      "Iteration: 656/277000 \t Elapsed time: 121.89 \t Loss:19.75269\n",
      "Iteration: 657/277000 \t Elapsed time: 122.05 \t Loss:19.73353\n",
      "Iteration: 658/277000 \t Elapsed time: 122.22 \t Loss:19.71436\n",
      "Iteration: 659/277000 \t Elapsed time: 122.40 \t Loss:19.69521\n",
      "Iteration: 660/277000 \t Elapsed time: 122.57 \t Loss:19.67617\n",
      "Iteration: 661/277000 \t Elapsed time: 122.74 \t Loss:19.65716\n",
      "Iteration: 662/277000 \t Elapsed time: 122.92 \t Loss:19.63813\n",
      "Iteration: 663/277000 \t Elapsed time: 123.10 \t Loss:19.61909\n",
      "Iteration: 664/277000 \t Elapsed time: 123.90 \t Loss:19.60008\n",
      "Iteration: 665/277000 \t Elapsed time: 124.12 \t Loss:19.58109\n",
      "Iteration: 666/277000 \t Elapsed time: 124.36 \t Loss:19.56210\n",
      "Iteration: 667/277000 \t Elapsed time: 124.53 \t Loss:19.54315\n",
      "Iteration: 668/277000 \t Elapsed time: 124.71 \t Loss:19.52424\n",
      "Iteration: 669/277000 \t Elapsed time: 124.89 \t Loss:19.50532\n",
      "Iteration: 670/277000 \t Elapsed time: 125.08 \t Loss:19.48641\n",
      "Iteration: 671/277000 \t Elapsed time: 125.30 \t Loss:19.46752\n",
      "Iteration: 672/277000 \t Elapsed time: 125.56 \t Loss:19.44865\n",
      "Iteration: 673/277000 \t Elapsed time: 125.77 \t Loss:19.42982\n",
      "Iteration: 674/277000 \t Elapsed time: 125.95 \t Loss:19.41100\n",
      "Iteration: 675/277000 \t Elapsed time: 126.12 \t Loss:19.39220\n",
      "Iteration: 676/277000 \t Elapsed time: 126.28 \t Loss:19.37343\n",
      "Iteration: 677/277000 \t Elapsed time: 126.47 \t Loss:19.35469\n",
      "Iteration: 678/277000 \t Elapsed time: 126.66 \t Loss:19.33597\n",
      "Iteration: 679/277000 \t Elapsed time: 126.82 \t Loss:19.31728\n",
      "Iteration: 680/277000 \t Elapsed time: 126.99 \t Loss:19.29852\n",
      "Iteration: 681/277000 \t Elapsed time: 127.18 \t Loss:19.27971\n",
      "Iteration: 682/277000 \t Elapsed time: 127.39 \t Loss:19.26092\n",
      "Iteration: 683/277000 \t Elapsed time: 127.62 \t Loss:19.24226\n",
      "Iteration: 684/277000 \t Elapsed time: 127.84 \t Loss:19.22367\n",
      "Iteration: 685/277000 \t Elapsed time: 128.01 \t Loss:19.20505\n",
      "Iteration: 686/277000 \t Elapsed time: 128.18 \t Loss:19.18637\n",
      "Iteration: 687/277000 \t Elapsed time: 128.36 \t Loss:19.16770\n",
      "Iteration: 688/277000 \t Elapsed time: 128.53 \t Loss:19.14911\n",
      "Iteration: 689/277000 \t Elapsed time: 128.72 \t Loss:19.13056\n",
      "Iteration: 690/277000 \t Elapsed time: 128.90 \t Loss:19.11201\n",
      "Iteration: 691/277000 \t Elapsed time: 129.07 \t Loss:19.09344\n",
      "Iteration: 692/277000 \t Elapsed time: 129.24 \t Loss:19.07489\n",
      "Iteration: 693/277000 \t Elapsed time: 129.40 \t Loss:19.05637\n",
      "Iteration: 694/277000 \t Elapsed time: 129.57 \t Loss:19.03787\n",
      "Iteration: 695/277000 \t Elapsed time: 129.74 \t Loss:19.01938\n",
      "Iteration: 696/277000 \t Elapsed time: 129.91 \t Loss:19.00091\n",
      "Iteration: 697/277000 \t Elapsed time: 130.07 \t Loss:18.98247\n",
      "Iteration: 698/277000 \t Elapsed time: 130.24 \t Loss:18.96405\n",
      "Iteration: 699/277000 \t Elapsed time: 130.45 \t Loss:18.94564\n",
      "Iteration: 700/277000 \t Elapsed time: 130.61 \t Loss:18.92725\n",
      "Iteration: 701/277000 \t Elapsed time: 130.78 \t Loss:18.90885\n",
      "Iteration: 702/277000 \t Elapsed time: 130.94 \t Loss:18.89049\n",
      "Iteration: 703/277000 \t Elapsed time: 131.11 \t Loss:18.87214\n",
      "Iteration: 704/277000 \t Elapsed time: 131.28 \t Loss:18.85380\n",
      "Iteration: 705/277000 \t Elapsed time: 131.45 \t Loss:18.83546\n",
      "Iteration: 706/277000 \t Elapsed time: 131.61 \t Loss:18.81712\n",
      "Iteration: 707/277000 \t Elapsed time: 131.78 \t Loss:18.79881\n",
      "Iteration: 708/277000 \t Elapsed time: 131.94 \t Loss:18.78053\n",
      "Iteration: 709/277000 \t Elapsed time: 132.10 \t Loss:18.76227\n",
      "Iteration: 710/277000 \t Elapsed time: 132.27 \t Loss:18.74405\n",
      "Iteration: 711/277000 \t Elapsed time: 132.43 \t Loss:18.72583\n",
      "Iteration: 712/277000 \t Elapsed time: 132.61 \t Loss:18.70762\n",
      "Iteration: 713/277000 \t Elapsed time: 132.78 \t Loss:18.68943\n",
      "Iteration: 714/277000 \t Elapsed time: 132.94 \t Loss:18.67125\n",
      "Iteration: 715/277000 \t Elapsed time: 133.10 \t Loss:18.65308\n",
      "Iteration: 716/277000 \t Elapsed time: 133.27 \t Loss:18.63492\n",
      "Iteration: 717/277000 \t Elapsed time: 133.44 \t Loss:18.61678\n",
      "Iteration: 718/277000 \t Elapsed time: 133.59 \t Loss:18.59867\n",
      "Iteration: 719/277000 \t Elapsed time: 133.76 \t Loss:18.58057\n",
      "Iteration: 720/277000 \t Elapsed time: 133.92 \t Loss:18.56252\n",
      "Iteration: 721/277000 \t Elapsed time: 134.09 \t Loss:18.54449\n",
      "Iteration: 722/277000 \t Elapsed time: 134.26 \t Loss:18.52653\n",
      "Iteration: 723/277000 \t Elapsed time: 134.45 \t Loss:18.50863\n",
      "Iteration: 724/277000 \t Elapsed time: 134.61 \t Loss:18.49082\n",
      "Iteration: 725/277000 \t Elapsed time: 134.78 \t Loss:18.47309\n",
      "Iteration: 726/277000 \t Elapsed time: 134.95 \t Loss:18.45525\n",
      "Iteration: 727/277000 \t Elapsed time: 135.12 \t Loss:18.43713\n",
      "Iteration: 728/277000 \t Elapsed time: 135.31 \t Loss:18.41873\n",
      "Iteration: 729/277000 \t Elapsed time: 135.52 \t Loss:18.40041\n",
      "Iteration: 730/277000 \t Elapsed time: 135.68 \t Loss:18.38244\n",
      "Iteration: 731/277000 \t Elapsed time: 135.86 \t Loss:18.36474\n",
      "Iteration: 732/277000 \t Elapsed time: 136.05 \t Loss:18.34694\n",
      "Iteration: 733/277000 \t Elapsed time: 136.25 \t Loss:18.32890\n",
      "Iteration: 734/277000 \t Elapsed time: 136.70 \t Loss:18.31083\n",
      "Iteration: 735/277000 \t Elapsed time: 136.87 \t Loss:18.29295\n",
      "Iteration: 736/277000 \t Elapsed time: 137.04 \t Loss:18.27523\n",
      "Iteration: 737/277000 \t Elapsed time: 137.21 \t Loss:18.25740\n",
      "Iteration: 738/277000 \t Elapsed time: 137.42 \t Loss:18.23950\n",
      "Iteration: 739/277000 \t Elapsed time: 137.62 \t Loss:18.22163\n",
      "Iteration: 740/277000 \t Elapsed time: 137.79 \t Loss:18.20388\n",
      "Iteration: 741/277000 \t Elapsed time: 138.00 \t Loss:18.18615\n",
      "Iteration: 742/277000 \t Elapsed time: 138.17 \t Loss:18.16836\n",
      "Iteration: 743/277000 \t Elapsed time: 138.34 \t Loss:18.15056\n",
      "Iteration: 744/277000 \t Elapsed time: 138.55 \t Loss:18.13284\n",
      "Iteration: 745/277000 \t Elapsed time: 138.76 \t Loss:18.11518\n",
      "Iteration: 746/277000 \t Elapsed time: 138.97 \t Loss:18.09749\n",
      "Iteration: 747/277000 \t Elapsed time: 139.16 \t Loss:18.07976\n",
      "Iteration: 748/277000 \t Elapsed time: 139.34 \t Loss:18.06211\n",
      "Iteration: 749/277000 \t Elapsed time: 139.51 \t Loss:18.04452\n",
      "Iteration: 750/277000 \t Elapsed time: 139.71 \t Loss:18.02691\n",
      "Iteration: 751/277000 \t Elapsed time: 139.91 \t Loss:18.00924\n",
      "Iteration: 752/277000 \t Elapsed time: 140.18 \t Loss:17.99161\n",
      "Iteration: 753/277000 \t Elapsed time: 140.37 \t Loss:17.97400\n",
      "Iteration: 754/277000 \t Elapsed time: 140.54 \t Loss:17.95639\n",
      "Iteration: 755/277000 \t Elapsed time: 140.70 \t Loss:17.93878\n",
      "Iteration: 756/277000 \t Elapsed time: 140.87 \t Loss:17.92117\n",
      "Iteration: 757/277000 \t Elapsed time: 141.02 \t Loss:17.90363\n",
      "Iteration: 758/277000 \t Elapsed time: 141.19 \t Loss:17.88612\n",
      "Iteration: 759/277000 \t Elapsed time: 141.35 \t Loss:17.86861\n",
      "Iteration: 760/277000 \t Elapsed time: 141.52 \t Loss:17.85113\n",
      "Iteration: 761/277000 \t Elapsed time: 141.70 \t Loss:17.83364\n",
      "Iteration: 762/277000 \t Elapsed time: 141.85 \t Loss:17.81616\n",
      "Iteration: 763/277000 \t Elapsed time: 142.02 \t Loss:17.79868\n",
      "Iteration: 764/277000 \t Elapsed time: 142.19 \t Loss:17.78122\n",
      "Iteration: 765/277000 \t Elapsed time: 142.36 \t Loss:17.76377\n",
      "Iteration: 766/277000 \t Elapsed time: 142.53 \t Loss:17.74636\n",
      "Iteration: 767/277000 \t Elapsed time: 142.70 \t Loss:17.72897\n",
      "Iteration: 768/277000 \t Elapsed time: 142.92 \t Loss:17.71158\n",
      "Iteration: 769/277000 \t Elapsed time: 143.08 \t Loss:17.69421\n",
      "Iteration: 770/277000 \t Elapsed time: 143.25 \t Loss:17.67686\n",
      "Iteration: 771/277000 \t Elapsed time: 143.41 \t Loss:17.65951\n",
      "Iteration: 772/277000 \t Elapsed time: 143.61 \t Loss:17.64217\n",
      "Iteration: 773/277000 \t Elapsed time: 143.80 \t Loss:17.62485\n",
      "Iteration: 774/277000 \t Elapsed time: 143.97 \t Loss:17.60753\n",
      "Iteration: 775/277000 \t Elapsed time: 144.14 \t Loss:17.59023\n",
      "Iteration: 776/277000 \t Elapsed time: 144.31 \t Loss:17.57295\n",
      "Iteration: 777/277000 \t Elapsed time: 144.48 \t Loss:17.55568\n",
      "Iteration: 778/277000 \t Elapsed time: 144.66 \t Loss:17.53845\n",
      "Iteration: 779/277000 \t Elapsed time: 144.83 \t Loss:17.52121\n",
      "Iteration: 780/277000 \t Elapsed time: 145.01 \t Loss:17.50399\n",
      "Iteration: 781/277000 \t Elapsed time: 145.18 \t Loss:17.48676\n",
      "Iteration: 782/277000 \t Elapsed time: 145.34 \t Loss:17.46958\n",
      "Iteration: 783/277000 \t Elapsed time: 145.51 \t Loss:17.45239\n",
      "Iteration: 784/277000 \t Elapsed time: 145.67 \t Loss:17.43523\n",
      "Iteration: 785/277000 \t Elapsed time: 145.84 \t Loss:17.41808\n",
      "Iteration: 786/277000 \t Elapsed time: 146.02 \t Loss:17.40094\n",
      "Iteration: 787/277000 \t Elapsed time: 146.18 \t Loss:17.38380\n",
      "Iteration: 788/277000 \t Elapsed time: 146.35 \t Loss:17.36669\n",
      "Iteration: 789/277000 \t Elapsed time: 146.51 \t Loss:17.34959\n",
      "Iteration: 790/277000 \t Elapsed time: 146.68 \t Loss:17.33251\n",
      "Iteration: 791/277000 \t Elapsed time: 146.88 \t Loss:17.31545\n",
      "Iteration: 792/277000 \t Elapsed time: 147.05 \t Loss:17.29840\n",
      "Iteration: 793/277000 \t Elapsed time: 147.22 \t Loss:17.28139\n",
      "Iteration: 794/277000 \t Elapsed time: 147.38 \t Loss:17.26437\n",
      "Iteration: 795/277000 \t Elapsed time: 147.79 \t Loss:17.24738\n",
      "Iteration: 796/277000 \t Elapsed time: 147.97 \t Loss:17.23037\n",
      "Iteration: 797/277000 \t Elapsed time: 148.14 \t Loss:17.21340\n",
      "Iteration: 798/277000 \t Elapsed time: 148.30 \t Loss:17.19647\n",
      "Iteration: 799/277000 \t Elapsed time: 148.49 \t Loss:17.17951\n",
      "Iteration: 800/277000 \t Elapsed time: 148.65 \t Loss:17.16252\n",
      "Iteration: 801/277000 \t Elapsed time: 148.82 \t Loss:17.14554\n",
      "Iteration: 802/277000 \t Elapsed time: 148.98 \t Loss:17.12856\n",
      "Iteration: 803/277000 \t Elapsed time: 149.19 \t Loss:17.11157\n",
      "Iteration: 804/277000 \t Elapsed time: 149.38 \t Loss:17.09465\n",
      "Iteration: 805/277000 \t Elapsed time: 149.59 \t Loss:17.07777\n",
      "Iteration: 806/277000 \t Elapsed time: 149.78 \t Loss:17.06092\n",
      "Iteration: 807/277000 \t Elapsed time: 149.95 \t Loss:17.04406\n",
      "Iteration: 808/277000 \t Elapsed time: 150.12 \t Loss:17.02723\n",
      "Iteration: 809/277000 \t Elapsed time: 150.30 \t Loss:17.01038\n",
      "Iteration: 810/277000 \t Elapsed time: 150.49 \t Loss:16.99353\n",
      "Iteration: 811/277000 \t Elapsed time: 150.72 \t Loss:16.97669\n",
      "Iteration: 812/277000 \t Elapsed time: 150.90 \t Loss:16.95989\n",
      "Iteration: 813/277000 \t Elapsed time: 151.07 \t Loss:16.94310\n",
      "Iteration: 814/277000 \t Elapsed time: 151.24 \t Loss:16.92633\n",
      "Iteration: 815/277000 \t Elapsed time: 151.41 \t Loss:16.90959\n",
      "Iteration: 816/277000 \t Elapsed time: 151.63 \t Loss:16.89285\n",
      "Iteration: 817/277000 \t Elapsed time: 151.84 \t Loss:16.87610\n",
      "Iteration: 818/277000 \t Elapsed time: 152.02 \t Loss:16.85939\n",
      "Iteration: 819/277000 \t Elapsed time: 152.20 \t Loss:16.84267\n",
      "Iteration: 820/277000 \t Elapsed time: 152.46 \t Loss:16.82599\n",
      "Iteration: 821/277000 \t Elapsed time: 152.73 \t Loss:16.80933\n",
      "Iteration: 822/277000 \t Elapsed time: 152.91 \t Loss:16.79272\n",
      "Iteration: 823/277000 \t Elapsed time: 153.10 \t Loss:16.77614\n",
      "Iteration: 824/277000 \t Elapsed time: 153.27 \t Loss:16.75965\n",
      "Iteration: 825/277000 \t Elapsed time: 153.44 \t Loss:16.74320\n",
      "Iteration: 826/277000 \t Elapsed time: 153.60 \t Loss:16.72681\n",
      "Iteration: 827/277000 \t Elapsed time: 153.80 \t Loss:16.71031\n",
      "Iteration: 828/277000 \t Elapsed time: 154.00 \t Loss:16.69358\n",
      "Iteration: 829/277000 \t Elapsed time: 154.17 \t Loss:16.67664\n",
      "Iteration: 830/277000 \t Elapsed time: 154.33 \t Loss:16.65973\n",
      "Iteration: 831/277000 \t Elapsed time: 154.54 \t Loss:16.64310\n",
      "Iteration: 832/277000 \t Elapsed time: 154.70 \t Loss:16.62675\n",
      "Iteration: 833/277000 \t Elapsed time: 154.89 \t Loss:16.61036\n",
      "Iteration: 834/277000 \t Elapsed time: 155.05 \t Loss:16.59378\n",
      "Iteration: 835/277000 \t Elapsed time: 155.22 \t Loss:16.57707\n",
      "Iteration: 836/277000 \t Elapsed time: 155.39 \t Loss:16.56049\n",
      "Iteration: 837/277000 \t Elapsed time: 155.55 \t Loss:16.54411\n",
      "Iteration: 838/277000 \t Elapsed time: 155.74 \t Loss:16.52773\n",
      "Iteration: 839/277000 \t Elapsed time: 155.93 \t Loss:16.51122\n",
      "Iteration: 840/277000 \t Elapsed time: 156.11 \t Loss:16.49465\n",
      "Iteration: 841/277000 \t Elapsed time: 156.28 \t Loss:16.47820\n",
      "Iteration: 842/277000 \t Elapsed time: 156.45 \t Loss:16.46185\n",
      "Iteration: 843/277000 \t Elapsed time: 156.62 \t Loss:16.44546\n",
      "Iteration: 844/277000 \t Elapsed time: 156.79 \t Loss:16.42902\n",
      "Iteration: 845/277000 \t Elapsed time: 156.96 \t Loss:16.41258\n",
      "Iteration: 846/277000 \t Elapsed time: 157.12 \t Loss:16.39624\n",
      "Iteration: 847/277000 \t Elapsed time: 157.29 \t Loss:16.37991\n",
      "Iteration: 848/277000 \t Elapsed time: 157.47 \t Loss:16.36358\n",
      "Iteration: 849/277000 \t Elapsed time: 157.63 \t Loss:16.34723\n",
      "Iteration: 850/277000 \t Elapsed time: 157.79 \t Loss:16.33096\n",
      "Iteration: 851/277000 \t Elapsed time: 157.96 \t Loss:16.31473\n",
      "Iteration: 852/277000 \t Elapsed time: 158.13 \t Loss:16.29849\n",
      "Iteration: 853/277000 \t Elapsed time: 158.30 \t Loss:16.28220\n",
      "Iteration: 854/277000 \t Elapsed time: 158.46 \t Loss:16.26593\n",
      "Iteration: 855/277000 \t Elapsed time: 158.64 \t Loss:16.24959\n",
      "Iteration: 856/277000 \t Elapsed time: 158.82 \t Loss:16.23322\n",
      "Iteration: 857/277000 \t Elapsed time: 158.99 \t Loss:16.21685\n",
      "Iteration: 858/277000 \t Elapsed time: 159.15 \t Loss:16.20061\n",
      "Iteration: 859/277000 \t Elapsed time: 159.31 \t Loss:16.18446\n",
      "Iteration: 860/277000 \t Elapsed time: 159.48 \t Loss:16.16832\n",
      "Iteration: 861/277000 \t Elapsed time: 159.65 \t Loss:16.15211\n",
      "Iteration: 862/277000 \t Elapsed time: 159.81 \t Loss:16.13587\n",
      "Iteration: 863/277000 \t Elapsed time: 159.98 \t Loss:16.11964\n",
      "Iteration: 864/277000 \t Elapsed time: 160.15 \t Loss:16.10347\n",
      "Iteration: 865/277000 \t Elapsed time: 160.31 \t Loss:16.08735\n",
      "Iteration: 866/277000 \t Elapsed time: 160.48 \t Loss:16.07123\n",
      "Iteration: 867/277000 \t Elapsed time: 160.65 \t Loss:16.05510\n",
      "Iteration: 868/277000 \t Elapsed time: 160.83 \t Loss:16.03894\n",
      "Iteration: 869/277000 \t Elapsed time: 161.00 \t Loss:16.02283\n",
      "Iteration: 870/277000 \t Elapsed time: 161.16 \t Loss:16.00675\n",
      "Iteration: 871/277000 \t Elapsed time: 161.32 \t Loss:15.99068\n",
      "Iteration: 872/277000 \t Elapsed time: 161.50 \t Loss:15.97461\n",
      "Iteration: 873/277000 \t Elapsed time: 161.66 \t Loss:15.95854\n",
      "Iteration: 874/277000 \t Elapsed time: 161.83 \t Loss:15.94248\n",
      "Iteration: 875/277000 \t Elapsed time: 161.99 \t Loss:15.92644\n",
      "Iteration: 876/277000 \t Elapsed time: 162.17 \t Loss:15.91043\n",
      "Iteration: 877/277000 \t Elapsed time: 162.33 \t Loss:15.89442\n",
      "Iteration: 878/277000 \t Elapsed time: 162.49 \t Loss:15.87842\n",
      "Iteration: 879/277000 \t Elapsed time: 162.66 \t Loss:15.86244\n",
      "Iteration: 880/277000 \t Elapsed time: 162.86 \t Loss:15.84645\n",
      "Iteration: 881/277000 \t Elapsed time: 163.03 \t Loss:15.83049\n",
      "Iteration: 882/277000 \t Elapsed time: 163.22 \t Loss:15.81455\n",
      "Iteration: 883/277000 \t Elapsed time: 163.39 \t Loss:15.79862\n",
      "Iteration: 884/277000 \t Elapsed time: 163.62 \t Loss:15.78268\n",
      "Iteration: 885/277000 \t Elapsed time: 163.84 \t Loss:15.76675\n",
      "Iteration: 886/277000 \t Elapsed time: 164.01 \t Loss:15.75084\n",
      "Iteration: 887/277000 \t Elapsed time: 164.18 \t Loss:15.73495\n",
      "Iteration: 888/277000 \t Elapsed time: 164.34 \t Loss:15.71906\n",
      "Iteration: 889/277000 \t Elapsed time: 164.51 \t Loss:15.70317\n",
      "Iteration: 890/277000 \t Elapsed time: 164.70 \t Loss:15.68731\n",
      "Iteration: 891/277000 \t Elapsed time: 164.86 \t Loss:15.67146\n",
      "Iteration: 892/277000 \t Elapsed time: 165.02 \t Loss:15.65561\n",
      "Iteration: 893/277000 \t Elapsed time: 165.18 \t Loss:15.63978\n",
      "Iteration: 894/277000 \t Elapsed time: 165.42 \t Loss:15.62398\n",
      "Iteration: 895/277000 \t Elapsed time: 165.59 \t Loss:15.60817\n",
      "Iteration: 896/277000 \t Elapsed time: 165.76 \t Loss:15.59238\n",
      "Iteration: 897/277000 \t Elapsed time: 165.93 \t Loss:15.57659\n",
      "Iteration: 898/277000 \t Elapsed time: 166.11 \t Loss:15.56085\n",
      "Iteration: 899/277000 \t Elapsed time: 166.28 \t Loss:15.54508\n",
      "Iteration: 900/277000 \t Elapsed time: 166.48 \t Loss:15.52932\n",
      "Iteration: 901/277000 \t Elapsed time: 166.66 \t Loss:15.51357\n",
      "Iteration: 902/277000 \t Elapsed time: 166.93 \t Loss:15.49783\n",
      "Iteration: 903/277000 \t Elapsed time: 167.10 \t Loss:15.48212\n",
      "Iteration: 904/277000 \t Elapsed time: 167.27 \t Loss:15.46641\n",
      "Iteration: 905/277000 \t Elapsed time: 167.44 \t Loss:15.45073\n",
      "Iteration: 906/277000 \t Elapsed time: 167.61 \t Loss:15.43503\n",
      "Iteration: 907/277000 \t Elapsed time: 168.00 \t Loss:15.41935\n",
      "Iteration: 908/277000 \t Elapsed time: 168.18 \t Loss:15.40370\n",
      "Iteration: 909/277000 \t Elapsed time: 168.34 \t Loss:15.38804\n",
      "Iteration: 910/277000 \t Elapsed time: 168.54 \t Loss:15.37239\n",
      "Iteration: 911/277000 \t Elapsed time: 168.72 \t Loss:15.35677\n",
      "Iteration: 912/277000 \t Elapsed time: 168.90 \t Loss:15.34116\n",
      "Iteration: 913/277000 \t Elapsed time: 169.07 \t Loss:15.32558\n",
      "Iteration: 914/277000 \t Elapsed time: 169.24 \t Loss:15.31001\n",
      "Iteration: 915/277000 \t Elapsed time: 169.44 \t Loss:15.29447\n",
      "Iteration: 916/277000 \t Elapsed time: 169.61 \t Loss:15.27897\n",
      "Iteration: 917/277000 \t Elapsed time: 169.80 \t Loss:15.26356\n",
      "Iteration: 918/277000 \t Elapsed time: 169.99 \t Loss:15.24812\n",
      "Iteration: 919/277000 \t Elapsed time: 170.20 \t Loss:15.23269\n",
      "Iteration: 920/277000 \t Elapsed time: 170.36 \t Loss:15.21701\n",
      "Iteration: 921/277000 \t Elapsed time: 170.53 \t Loss:15.20130\n",
      "Iteration: 922/277000 \t Elapsed time: 170.71 \t Loss:15.18568\n",
      "Iteration: 923/277000 \t Elapsed time: 170.87 \t Loss:15.17030\n",
      "Iteration: 924/277000 \t Elapsed time: 171.04 \t Loss:15.15503\n",
      "Iteration: 925/277000 \t Elapsed time: 171.21 \t Loss:15.13969\n",
      "Iteration: 926/277000 \t Elapsed time: 171.37 \t Loss:15.12439\n",
      "Iteration: 927/277000 \t Elapsed time: 171.54 \t Loss:15.10922\n",
      "Iteration: 928/277000 \t Elapsed time: 171.71 \t Loss:15.09413\n",
      "Iteration: 929/277000 \t Elapsed time: 171.87 \t Loss:15.07872\n",
      "Iteration: 930/277000 \t Elapsed time: 172.05 \t Loss:15.06275\n",
      "Iteration: 931/277000 \t Elapsed time: 172.22 \t Loss:15.04666\n",
      "Iteration: 932/277000 \t Elapsed time: 172.38 \t Loss:15.03111\n",
      "Iteration: 933/277000 \t Elapsed time: 172.55 \t Loss:15.01607\n",
      "Iteration: 934/277000 \t Elapsed time: 172.71 \t Loss:15.00095\n",
      "Iteration: 935/277000 \t Elapsed time: 172.88 \t Loss:14.98535\n",
      "Iteration: 936/277000 \t Elapsed time: 173.04 \t Loss:14.96960\n",
      "Iteration: 937/277000 \t Elapsed time: 173.21 \t Loss:14.95431\n",
      "Iteration: 938/277000 \t Elapsed time: 173.39 \t Loss:14.93923\n",
      "Iteration: 939/277000 \t Elapsed time: 173.59 \t Loss:14.92380\n",
      "Iteration: 940/277000 \t Elapsed time: 173.75 \t Loss:14.90823\n",
      "Iteration: 941/277000 \t Elapsed time: 173.92 \t Loss:14.89297\n",
      "Iteration: 942/277000 \t Elapsed time: 174.09 \t Loss:14.87784\n",
      "Iteration: 943/277000 \t Elapsed time: 174.26 \t Loss:14.86244\n",
      "Iteration: 944/277000 \t Elapsed time: 174.43 \t Loss:14.84698\n",
      "Iteration: 945/277000 \t Elapsed time: 174.59 \t Loss:14.83180\n",
      "Iteration: 946/277000 \t Elapsed time: 174.76 \t Loss:14.81664\n",
      "Iteration: 947/277000 \t Elapsed time: 174.93 \t Loss:14.80131\n",
      "Iteration: 948/277000 \t Elapsed time: 175.09 \t Loss:14.78598\n",
      "Iteration: 949/277000 \t Elapsed time: 175.27 \t Loss:14.77083\n",
      "Iteration: 950/277000 \t Elapsed time: 175.43 \t Loss:14.75565\n",
      "Iteration: 951/277000 \t Elapsed time: 175.61 \t Loss:14.74039\n",
      "Iteration: 952/277000 \t Elapsed time: 175.77 \t Loss:14.72520\n",
      "Iteration: 953/277000 \t Elapsed time: 175.94 \t Loss:14.71009\n",
      "Iteration: 954/277000 \t Elapsed time: 176.13 \t Loss:14.69492\n",
      "Iteration: 955/277000 \t Elapsed time: 176.29 \t Loss:14.67967\n",
      "Iteration: 956/277000 \t Elapsed time: 176.45 \t Loss:14.66446\n",
      "Iteration: 957/277000 \t Elapsed time: 176.62 \t Loss:14.64934\n",
      "Iteration: 958/277000 \t Elapsed time: 176.79 \t Loss:14.63421\n",
      "Iteration: 959/277000 \t Elapsed time: 176.96 \t Loss:14.61909\n",
      "Iteration: 960/277000 \t Elapsed time: 177.13 \t Loss:14.60398\n",
      "Iteration: 961/277000 \t Elapsed time: 177.31 \t Loss:14.58884\n",
      "Iteration: 962/277000 \t Elapsed time: 177.48 \t Loss:14.57371\n",
      "Iteration: 963/277000 \t Elapsed time: 177.64 \t Loss:14.55864\n",
      "Iteration: 964/277000 \t Elapsed time: 177.81 \t Loss:14.54360\n",
      "Iteration: 965/277000 \t Elapsed time: 177.98 \t Loss:14.52852\n",
      "Iteration: 966/277000 \t Elapsed time: 178.15 \t Loss:14.51343\n",
      "Iteration: 967/277000 \t Elapsed time: 178.31 \t Loss:14.49838\n",
      "Iteration: 968/277000 \t Elapsed time: 178.48 \t Loss:14.48337\n",
      "Iteration: 969/277000 \t Elapsed time: 178.67 \t Loss:14.46834\n",
      "Iteration: 970/277000 \t Elapsed time: 178.84 \t Loss:14.45331\n",
      "Iteration: 971/277000 \t Elapsed time: 179.01 \t Loss:14.43831\n",
      "Iteration: 972/277000 \t Elapsed time: 179.18 \t Loss:14.42329\n",
      "Iteration: 973/277000 \t Elapsed time: 179.35 \t Loss:14.40831\n",
      "Iteration: 974/277000 \t Elapsed time: 179.54 \t Loss:14.39334\n",
      "Iteration: 975/277000 \t Elapsed time: 179.71 \t Loss:14.37837\n",
      "Iteration: 976/277000 \t Elapsed time: 179.91 \t Loss:14.36341\n",
      "Iteration: 977/277000 \t Elapsed time: 180.08 \t Loss:14.34846\n",
      "Iteration: 978/277000 \t Elapsed time: 180.25 \t Loss:14.33352\n",
      "Iteration: 979/277000 \t Elapsed time: 180.42 \t Loss:14.31859\n",
      "Iteration: 980/277000 \t Elapsed time: 180.59 \t Loss:14.30368\n",
      "Iteration: 981/277000 \t Elapsed time: 180.77 \t Loss:14.28878\n",
      "Iteration: 982/277000 \t Elapsed time: 180.94 \t Loss:14.27388\n",
      "Iteration: 983/277000 \t Elapsed time: 181.16 \t Loss:14.25899\n",
      "Iteration: 984/277000 \t Elapsed time: 181.34 \t Loss:14.24412\n",
      "Iteration: 985/277000 \t Elapsed time: 181.50 \t Loss:14.22930\n",
      "Iteration: 986/277000 \t Elapsed time: 181.67 \t Loss:14.21445\n",
      "Iteration: 987/277000 \t Elapsed time: 181.84 \t Loss:14.19967\n",
      "Iteration: 988/277000 \t Elapsed time: 182.01 \t Loss:14.18492\n",
      "Iteration: 989/277000 \t Elapsed time: 182.19 \t Loss:14.17019\n",
      "Iteration: 990/277000 \t Elapsed time: 182.35 \t Loss:14.15546\n",
      "Iteration: 991/277000 \t Elapsed time: 182.52 \t Loss:14.14065\n",
      "Iteration: 992/277000 \t Elapsed time: 182.69 \t Loss:14.12576\n",
      "Iteration: 993/277000 \t Elapsed time: 182.86 \t Loss:14.11078\n",
      "Iteration: 994/277000 \t Elapsed time: 183.05 \t Loss:14.09597\n",
      "Iteration: 995/277000 \t Elapsed time: 183.21 \t Loss:14.08128\n",
      "Iteration: 996/277000 \t Elapsed time: 183.39 \t Loss:14.06657\n",
      "Iteration: 997/277000 \t Elapsed time: 183.56 \t Loss:14.05173\n",
      "Iteration: 998/277000 \t Elapsed time: 183.72 \t Loss:14.03685\n",
      "Iteration: 999/277000 \t Elapsed time: 183.88 \t Loss:14.02210\n",
      "Iteration: 1000/277000 \t Elapsed time: 184.05 \t Loss:14.00747\n",
      "Start Validating\n",
      "Finish Validating\n",
      "Iteration: 1001/277000 \t Elapsed time: 285.78 \t Loss:13.99277\n",
      "Iteration: 1002/277000 \t Elapsed time: 285.96 \t Loss:13.97800\n",
      "Iteration: 1003/277000 \t Elapsed time: 286.12 \t Loss:13.96330\n",
      "Iteration: 1004/277000 \t Elapsed time: 286.36 \t Loss:13.94862\n",
      "Iteration: 1005/277000 \t Elapsed time: 286.53 \t Loss:13.93395\n",
      "Iteration: 1006/277000 \t Elapsed time: 286.70 \t Loss:13.91923\n",
      "Iteration: 1007/277000 \t Elapsed time: 286.87 \t Loss:13.90457\n",
      "Iteration: 1008/277000 \t Elapsed time: 287.05 \t Loss:13.88997\n",
      "Iteration: 1009/277000 \t Elapsed time: 287.22 \t Loss:13.87532\n",
      "Iteration: 1010/277000 \t Elapsed time: 287.44 \t Loss:13.86067\n",
      "Iteration: 1011/277000 \t Elapsed time: 287.62 \t Loss:13.84606\n",
      "Iteration: 1012/277000 \t Elapsed time: 287.80 \t Loss:13.83148\n",
      "Iteration: 1013/277000 \t Elapsed time: 287.97 \t Loss:13.81687\n",
      "Iteration: 1014/277000 \t Elapsed time: 288.17 \t Loss:13.80224\n",
      "Iteration: 1015/277000 \t Elapsed time: 288.35 \t Loss:13.78764\n",
      "Iteration: 1016/277000 \t Elapsed time: 288.56 \t Loss:13.77306\n",
      "Iteration: 1017/277000 \t Elapsed time: 288.73 \t Loss:13.75849\n",
      "Iteration: 1018/277000 \t Elapsed time: 288.90 \t Loss:13.74392\n",
      "Iteration: 1019/277000 \t Elapsed time: 289.09 \t Loss:13.72940\n",
      "Iteration: 1020/277000 \t Elapsed time: 289.27 \t Loss:13.71489\n",
      "Iteration: 1021/277000 \t Elapsed time: 289.47 \t Loss:13.70039\n",
      "Iteration: 1022/277000 \t Elapsed time: 289.72 \t Loss:13.68586\n",
      "Iteration: 1023/277000 \t Elapsed time: 289.88 \t Loss:13.67133\n",
      "Iteration: 1024/277000 \t Elapsed time: 290.06 \t Loss:13.65680\n",
      "Iteration: 1025/277000 \t Elapsed time: 290.26 \t Loss:13.64228\n",
      "Iteration: 1026/277000 \t Elapsed time: 290.43 \t Loss:13.62781\n",
      "Iteration: 1027/277000 \t Elapsed time: 290.68 \t Loss:13.61337\n",
      "Iteration: 1028/277000 \t Elapsed time: 290.86 \t Loss:13.59891\n",
      "Iteration: 1029/277000 \t Elapsed time: 291.04 \t Loss:13.58446\n",
      "Iteration: 1030/277000 \t Elapsed time: 291.21 \t Loss:13.56999\n",
      "Iteration: 1031/277000 \t Elapsed time: 291.83 \t Loss:13.55554\n",
      "Iteration: 1032/277000 \t Elapsed time: 292.01 \t Loss:13.54114\n",
      "Iteration: 1033/277000 \t Elapsed time: 292.18 \t Loss:13.52677\n",
      "Iteration: 1034/277000 \t Elapsed time: 292.34 \t Loss:13.51243\n",
      "Iteration: 1035/277000 \t Elapsed time: 292.51 \t Loss:13.49811\n",
      "Iteration: 1036/277000 \t Elapsed time: 292.67 \t Loss:13.48383\n",
      "Iteration: 1037/277000 \t Elapsed time: 292.85 \t Loss:13.46963\n",
      "Iteration: 1038/277000 \t Elapsed time: 293.03 \t Loss:13.45531\n",
      "Iteration: 1039/277000 \t Elapsed time: 293.23 \t Loss:13.44092\n",
      "Iteration: 1040/277000 \t Elapsed time: 293.45 \t Loss:13.42627\n",
      "Iteration: 1041/277000 \t Elapsed time: 293.67 \t Loss:13.41170\n",
      "Iteration: 1042/277000 \t Elapsed time: 293.87 \t Loss:13.39737\n",
      "Iteration: 1043/277000 \t Elapsed time: 294.05 \t Loss:13.38322\n",
      "Iteration: 1044/277000 \t Elapsed time: 294.22 \t Loss:13.36899\n",
      "Iteration: 1045/277000 \t Elapsed time: 294.39 \t Loss:13.35452\n",
      "Iteration: 1046/277000 \t Elapsed time: 294.64 \t Loss:13.34006\n",
      "Iteration: 1047/277000 \t Elapsed time: 294.83 \t Loss:13.32580\n",
      "Iteration: 1048/277000 \t Elapsed time: 295.00 \t Loss:13.31160\n",
      "Iteration: 1049/277000 \t Elapsed time: 295.18 \t Loss:13.29734\n",
      "Iteration: 1050/277000 \t Elapsed time: 295.38 \t Loss:13.28300\n",
      "Iteration: 1051/277000 \t Elapsed time: 295.58 \t Loss:13.26873\n",
      "Iteration: 1052/277000 \t Elapsed time: 295.78 \t Loss:13.25455\n",
      "Iteration: 1053/277000 \t Elapsed time: 296.00 \t Loss:13.24035\n",
      "Iteration: 1054/277000 \t Elapsed time: 296.20 \t Loss:13.22607\n",
      "Iteration: 1055/277000 \t Elapsed time: 296.40 \t Loss:13.21186\n",
      "Iteration: 1056/277000 \t Elapsed time: 296.58 \t Loss:13.19774\n",
      "Iteration: 1057/277000 \t Elapsed time: 296.79 \t Loss:13.18363\n",
      "Iteration: 1058/277000 \t Elapsed time: 296.99 \t Loss:13.16950\n",
      "Iteration: 1059/277000 \t Elapsed time: 297.15 \t Loss:13.15537\n",
      "Iteration: 1060/277000 \t Elapsed time: 297.34 \t Loss:13.14129\n",
      "Iteration: 1061/277000 \t Elapsed time: 297.57 \t Loss:13.12718\n",
      "Iteration: 1062/277000 \t Elapsed time: 297.74 \t Loss:13.11297\n",
      "Iteration: 1063/277000 \t Elapsed time: 298.18 \t Loss:13.09874\n",
      "Iteration: 1064/277000 \t Elapsed time: 298.36 \t Loss:13.08447\n",
      "Iteration: 1065/277000 \t Elapsed time: 298.58 \t Loss:13.07016\n",
      "Iteration: 1066/277000 \t Elapsed time: 298.74 \t Loss:13.05592\n",
      "Iteration: 1067/277000 \t Elapsed time: 298.90 \t Loss:13.04181\n",
      "Iteration: 1068/277000 \t Elapsed time: 299.08 \t Loss:13.02779\n",
      "Iteration: 1069/277000 \t Elapsed time: 299.26 \t Loss:13.01376\n",
      "Iteration: 1070/277000 \t Elapsed time: 299.44 \t Loss:12.99961\n",
      "Iteration: 1071/277000 \t Elapsed time: 299.62 \t Loss:12.98545\n",
      "Iteration: 1072/277000 \t Elapsed time: 299.78 \t Loss:12.97132\n",
      "Iteration: 1073/277000 \t Elapsed time: 299.97 \t Loss:12.95729\n",
      "Iteration: 1074/277000 \t Elapsed time: 300.18 \t Loss:12.94332\n",
      "Iteration: 1075/277000 \t Elapsed time: 300.35 \t Loss:12.92931\n",
      "Iteration: 1076/277000 \t Elapsed time: 300.53 \t Loss:12.91523\n",
      "Iteration: 1077/277000 \t Elapsed time: 300.70 \t Loss:12.90112\n",
      "Iteration: 1078/277000 \t Elapsed time: 300.89 \t Loss:12.88706\n",
      "Iteration: 1079/277000 \t Elapsed time: 301.21 \t Loss:12.87307\n",
      "Iteration: 1080/277000 \t Elapsed time: 301.54 \t Loss:12.85913\n",
      "Iteration: 1081/277000 \t Elapsed time: 301.73 \t Loss:12.84516\n",
      "Iteration: 1082/277000 \t Elapsed time: 301.92 \t Loss:12.83115\n",
      "Iteration: 1083/277000 \t Elapsed time: 302.09 \t Loss:12.81711\n",
      "Iteration: 1084/277000 \t Elapsed time: 302.27 \t Loss:12.80311\n",
      "Iteration: 1085/277000 \t Elapsed time: 302.45 \t Loss:12.78915\n",
      "Iteration: 1086/277000 \t Elapsed time: 302.64 \t Loss:12.77523\n",
      "Iteration: 1087/277000 \t Elapsed time: 302.81 \t Loss:12.76132\n",
      "Iteration: 1088/277000 \t Elapsed time: 302.99 \t Loss:12.74737\n",
      "Iteration: 1089/277000 \t Elapsed time: 303.17 \t Loss:12.73342\n",
      "Iteration: 1090/277000 \t Elapsed time: 303.41 \t Loss:12.71952\n",
      "Iteration: 1091/277000 \t Elapsed time: 303.58 \t Loss:12.70569\n",
      "Iteration: 1092/277000 \t Elapsed time: 303.78 \t Loss:12.69194\n",
      "Iteration: 1093/277000 \t Elapsed time: 303.94 \t Loss:12.67812\n",
      "Iteration: 1094/277000 \t Elapsed time: 304.11 \t Loss:12.66410\n",
      "Iteration: 1095/277000 \t Elapsed time: 304.28 \t Loss:12.65003\n",
      "Iteration: 1096/277000 \t Elapsed time: 304.45 \t Loss:12.63623\n",
      "Iteration: 1097/277000 \t Elapsed time: 304.62 \t Loss:12.62247\n",
      "Iteration: 1098/277000 \t Elapsed time: 304.92 \t Loss:12.60853\n",
      "Iteration: 1099/277000 \t Elapsed time: 305.09 \t Loss:12.59456\n",
      "Iteration: 1100/277000 \t Elapsed time: 305.28 \t Loss:12.58084\n",
      "Iteration: 1101/277000 \t Elapsed time: 305.51 \t Loss:12.56705\n",
      "Iteration: 1102/277000 \t Elapsed time: 305.71 \t Loss:12.55313\n",
      "Iteration: 1103/277000 \t Elapsed time: 305.89 \t Loss:12.53931\n",
      "Iteration: 1104/277000 \t Elapsed time: 306.07 \t Loss:12.52558\n",
      "Iteration: 1105/277000 \t Elapsed time: 306.24 \t Loss:12.51174\n",
      "Iteration: 1106/277000 \t Elapsed time: 306.51 \t Loss:12.49792\n",
      "Iteration: 1107/277000 \t Elapsed time: 306.69 \t Loss:12.48421\n",
      "Iteration: 1108/277000 \t Elapsed time: 306.96 \t Loss:12.47042\n",
      "Iteration: 1109/277000 \t Elapsed time: 307.17 \t Loss:12.45660\n",
      "Iteration: 1110/277000 \t Elapsed time: 307.38 \t Loss:12.44290\n",
      "Iteration: 1111/277000 \t Elapsed time: 307.58 \t Loss:12.42915\n",
      "Iteration: 1112/277000 \t Elapsed time: 307.75 \t Loss:12.41537\n",
      "Iteration: 1113/277000 \t Elapsed time: 307.96 \t Loss:12.40167\n",
      "Iteration: 1114/277000 \t Elapsed time: 308.16 \t Loss:12.38798\n",
      "Iteration: 1115/277000 \t Elapsed time: 308.38 \t Loss:12.37422\n",
      "Iteration: 1116/277000 \t Elapsed time: 308.57 \t Loss:12.36053\n",
      "Iteration: 1117/277000 \t Elapsed time: 308.73 \t Loss:12.34687\n",
      "Iteration: 1118/277000 \t Elapsed time: 308.93 \t Loss:12.33317\n",
      "Iteration: 1119/277000 \t Elapsed time: 309.11 \t Loss:12.31952\n",
      "Iteration: 1120/277000 \t Elapsed time: 309.28 \t Loss:12.30591\n",
      "Iteration: 1121/277000 \t Elapsed time: 309.50 \t Loss:12.29235\n",
      "Iteration: 1122/277000 \t Elapsed time: 309.68 \t Loss:12.27886\n",
      "Iteration: 1123/277000 \t Elapsed time: 309.90 \t Loss:12.26556\n",
      "Iteration: 1124/277000 \t Elapsed time: 310.09 \t Loss:12.25219\n",
      "Iteration: 1125/277000 \t Elapsed time: 310.27 \t Loss:12.23877\n",
      "Iteration: 1126/277000 \t Elapsed time: 310.43 \t Loss:12.22481\n",
      "Iteration: 1127/277000 \t Elapsed time: 310.60 \t Loss:12.21085\n",
      "Iteration: 1128/277000 \t Elapsed time: 310.76 \t Loss:12.19725\n",
      "Iteration: 1129/277000 \t Elapsed time: 310.93 \t Loss:12.18414\n",
      "Iteration: 1130/277000 \t Elapsed time: 311.16 \t Loss:12.17096\n",
      "Iteration: 1131/277000 \t Elapsed time: 311.31 \t Loss:12.15732\n",
      "Iteration: 1132/277000 \t Elapsed time: 311.49 \t Loss:12.14347\n",
      "Iteration: 1133/277000 \t Elapsed time: 311.65 \t Loss:12.12966\n",
      "Iteration: 1134/277000 \t Elapsed time: 311.81 \t Loss:12.11557\n",
      "Iteration: 1135/277000 \t Elapsed time: 311.98 \t Loss:12.10183\n",
      "Iteration: 1136/277000 \t Elapsed time: 312.22 \t Loss:12.08870\n",
      "Iteration: 1137/277000 \t Elapsed time: 312.39 \t Loss:12.07537\n",
      "Iteration: 1138/277000 \t Elapsed time: 312.60 \t Loss:12.06155\n",
      "Iteration: 1139/277000 \t Elapsed time: 312.84 \t Loss:12.04780\n",
      "Iteration: 1140/277000 \t Elapsed time: 313.05 \t Loss:12.03427\n",
      "Iteration: 1141/277000 \t Elapsed time: 313.23 \t Loss:12.02086\n",
      "Iteration: 1142/277000 \t Elapsed time: 313.41 \t Loss:12.00746\n",
      "Iteration: 1143/277000 \t Elapsed time: 313.58 \t Loss:11.99384\n",
      "Iteration: 1144/277000 \t Elapsed time: 313.75 \t Loss:11.98021\n",
      "Iteration: 1145/277000 \t Elapsed time: 314.02 \t Loss:11.96697\n",
      "Iteration: 1146/277000 \t Elapsed time: 314.25 \t Loss:11.95365\n",
      "Iteration: 1147/277000 \t Elapsed time: 314.52 \t Loss:11.93988\n",
      "Iteration: 1148/277000 \t Elapsed time: 314.71 \t Loss:11.92639\n",
      "Iteration: 1149/277000 \t Elapsed time: 314.87 \t Loss:11.91307\n",
      "Iteration: 1150/277000 \t Elapsed time: 315.04 \t Loss:11.89958\n",
      "Iteration: 1151/277000 \t Elapsed time: 315.28 \t Loss:11.88614\n",
      "Iteration: 1152/277000 \t Elapsed time: 315.46 \t Loss:11.87267\n",
      "Iteration: 1153/277000 \t Elapsed time: 315.64 \t Loss:11.85923\n",
      "Iteration: 1154/277000 \t Elapsed time: 315.80 \t Loss:11.84586\n",
      "Iteration: 1155/277000 \t Elapsed time: 315.99 \t Loss:11.83247\n",
      "Iteration: 1156/277000 \t Elapsed time: 316.15 \t Loss:11.81900\n",
      "Iteration: 1157/277000 \t Elapsed time: 316.35 \t Loss:11.80564\n",
      "Iteration: 1158/277000 \t Elapsed time: 316.60 \t Loss:11.79235\n",
      "Iteration: 1159/277000 \t Elapsed time: 316.89 \t Loss:11.77893\n",
      "Iteration: 1160/277000 \t Elapsed time: 317.10 \t Loss:11.76547\n",
      "Iteration: 1161/277000 \t Elapsed time: 317.28 \t Loss:11.75217\n",
      "Iteration: 1162/277000 \t Elapsed time: 317.60 \t Loss:11.73887\n",
      "Iteration: 1163/277000 \t Elapsed time: 317.77 \t Loss:11.72549\n",
      "Iteration: 1164/277000 \t Elapsed time: 317.93 \t Loss:11.71212\n",
      "Iteration: 1165/277000 \t Elapsed time: 318.12 \t Loss:11.69886\n",
      "Iteration: 1166/277000 \t Elapsed time: 318.33 \t Loss:11.68555\n",
      "Iteration: 1167/277000 \t Elapsed time: 318.51 \t Loss:11.67220\n",
      "Iteration: 1168/277000 \t Elapsed time: 318.67 \t Loss:11.65889\n",
      "Iteration: 1169/277000 \t Elapsed time: 318.83 \t Loss:11.64563\n",
      "Iteration: 1170/277000 \t Elapsed time: 319.00 \t Loss:11.63234\n",
      "Iteration: 1171/277000 \t Elapsed time: 319.19 \t Loss:11.61906\n",
      "Iteration: 1172/277000 \t Elapsed time: 319.40 \t Loss:11.60585\n",
      "Iteration: 1173/277000 \t Elapsed time: 319.62 \t Loss:11.59258\n",
      "Iteration: 1174/277000 \t Elapsed time: 319.78 \t Loss:11.57930\n",
      "Iteration: 1175/277000 \t Elapsed time: 319.97 \t Loss:11.56602\n",
      "Iteration: 1176/277000 \t Elapsed time: 320.17 \t Loss:11.55280\n",
      "Iteration: 1177/277000 \t Elapsed time: 320.41 \t Loss:11.53960\n",
      "Iteration: 1178/277000 \t Elapsed time: 320.64 \t Loss:11.52638\n",
      "Iteration: 1179/277000 \t Elapsed time: 320.81 \t Loss:11.51319\n",
      "Iteration: 1180/277000 \t Elapsed time: 320.97 \t Loss:11.50011\n",
      "Iteration: 1181/277000 \t Elapsed time: 321.14 \t Loss:11.48711\n",
      "Iteration: 1182/277000 \t Elapsed time: 321.32 \t Loss:11.47398\n",
      "Iteration: 1183/277000 \t Elapsed time: 321.51 \t Loss:11.46047\n",
      "Iteration: 1184/277000 \t Elapsed time: 321.68 \t Loss:11.44730\n",
      "Iteration: 1185/277000 \t Elapsed time: 321.93 \t Loss:11.43426\n",
      "Iteration: 1186/277000 \t Elapsed time: 322.10 \t Loss:11.42089\n",
      "Iteration: 1187/277000 \t Elapsed time: 322.27 \t Loss:11.40782\n",
      "Iteration: 1188/277000 \t Elapsed time: 322.44 \t Loss:11.39485\n",
      "Iteration: 1189/277000 \t Elapsed time: 322.60 \t Loss:11.38152\n",
      "Iteration: 1190/277000 \t Elapsed time: 322.77 \t Loss:11.36843\n",
      "Iteration: 1191/277000 \t Elapsed time: 322.97 \t Loss:11.35526\n",
      "Iteration: 1192/277000 \t Elapsed time: 323.14 \t Loss:11.34209\n",
      "Iteration: 1193/277000 \t Elapsed time: 323.31 \t Loss:11.32907\n",
      "Iteration: 1194/277000 \t Elapsed time: 323.49 \t Loss:11.31589\n",
      "Iteration: 1195/277000 \t Elapsed time: 323.68 \t Loss:11.30279\n",
      "Iteration: 1196/277000 \t Elapsed time: 323.87 \t Loss:11.28973\n",
      "Iteration: 1197/277000 \t Elapsed time: 324.04 \t Loss:11.27656\n",
      "Iteration: 1198/277000 \t Elapsed time: 324.20 \t Loss:11.26358\n",
      "Iteration: 1199/277000 \t Elapsed time: 324.36 \t Loss:11.25048\n",
      "Iteration: 1200/277000 \t Elapsed time: 324.58 \t Loss:11.23742\n",
      "Iteration: 1201/277000 \t Elapsed time: 324.77 \t Loss:11.22435\n",
      "Iteration: 1202/277000 \t Elapsed time: 325.00 \t Loss:11.21128\n",
      "Iteration: 1203/277000 \t Elapsed time: 325.16 \t Loss:11.19833\n",
      "Iteration: 1204/277000 \t Elapsed time: 325.32 \t Loss:11.18530\n",
      "Iteration: 1205/277000 \t Elapsed time: 325.48 \t Loss:11.17232\n",
      "Iteration: 1206/277000 \t Elapsed time: 325.69 \t Loss:11.15928\n",
      "Iteration: 1207/277000 \t Elapsed time: 325.90 \t Loss:11.14633\n",
      "Iteration: 1208/277000 \t Elapsed time: 326.12 \t Loss:11.13335\n",
      "Iteration: 1209/277000 \t Elapsed time: 326.28 \t Loss:11.12041\n",
      "Iteration: 1210/277000 \t Elapsed time: 326.47 \t Loss:11.10734\n",
      "Iteration: 1211/277000 \t Elapsed time: 326.65 \t Loss:11.09427\n",
      "Iteration: 1212/277000 \t Elapsed time: 326.84 \t Loss:11.08134\n",
      "Iteration: 1213/277000 \t Elapsed time: 327.02 \t Loss:11.06841\n",
      "Iteration: 1214/277000 \t Elapsed time: 327.23 \t Loss:11.05554\n",
      "Iteration: 1215/277000 \t Elapsed time: 327.43 \t Loss:11.04264\n",
      "Iteration: 1216/277000 \t Elapsed time: 327.60 \t Loss:11.02977\n",
      "Iteration: 1217/277000 \t Elapsed time: 327.78 \t Loss:11.01700\n",
      "Iteration: 1218/277000 \t Elapsed time: 328.40 \t Loss:11.00427\n",
      "Iteration: 1219/277000 \t Elapsed time: 328.62 \t Loss:10.99134\n",
      "Iteration: 1220/277000 \t Elapsed time: 328.79 \t Loss:10.97808\n",
      "Iteration: 1221/277000 \t Elapsed time: 328.99 \t Loss:10.96504\n",
      "Iteration: 1222/277000 \t Elapsed time: 329.53 \t Loss:10.95226\n",
      "Iteration: 1223/277000 \t Elapsed time: 329.73 \t Loss:10.93947\n",
      "Iteration: 1224/277000 \t Elapsed time: 329.91 \t Loss:10.92652\n",
      "Iteration: 1225/277000 \t Elapsed time: 330.09 \t Loss:10.91373\n",
      "Iteration: 1226/277000 \t Elapsed time: 330.27 \t Loss:10.90078\n",
      "Iteration: 1227/277000 \t Elapsed time: 330.43 \t Loss:10.88770\n",
      "Iteration: 1228/277000 \t Elapsed time: 330.62 \t Loss:10.87501\n",
      "Iteration: 1229/277000 \t Elapsed time: 330.78 \t Loss:10.86229\n",
      "Iteration: 1230/277000 \t Elapsed time: 330.95 \t Loss:10.84938\n",
      "Iteration: 1231/277000 \t Elapsed time: 331.11 \t Loss:10.83649\n",
      "Iteration: 1232/277000 \t Elapsed time: 331.27 \t Loss:10.82363\n",
      "Iteration: 1233/277000 \t Elapsed time: 331.45 \t Loss:10.81076\n",
      "Iteration: 1234/277000 \t Elapsed time: 331.64 \t Loss:10.79820\n",
      "Iteration: 1235/277000 \t Elapsed time: 331.83 \t Loss:10.78554\n",
      "Iteration: 1236/277000 \t Elapsed time: 332.02 \t Loss:10.77267\n",
      "Iteration: 1237/277000 \t Elapsed time: 332.20 \t Loss:10.75974\n",
      "Iteration: 1238/277000 \t Elapsed time: 332.37 \t Loss:10.74686\n",
      "Iteration: 1239/277000 \t Elapsed time: 332.57 \t Loss:10.73415\n",
      "Iteration: 1240/277000 \t Elapsed time: 332.84 \t Loss:10.72156\n",
      "Iteration: 1241/277000 \t Elapsed time: 333.01 \t Loss:10.70882\n",
      "Iteration: 1242/277000 \t Elapsed time: 333.20 \t Loss:10.69596\n",
      "Iteration: 1243/277000 \t Elapsed time: 333.37 \t Loss:10.68319\n",
      "Iteration: 1244/277000 \t Elapsed time: 333.54 \t Loss:10.67050\n",
      "Iteration: 1245/277000 \t Elapsed time: 333.70 \t Loss:10.65782\n",
      "Iteration: 1246/277000 \t Elapsed time: 333.89 \t Loss:10.64512\n",
      "Iteration: 1247/277000 \t Elapsed time: 334.06 \t Loss:10.63233\n",
      "Iteration: 1248/277000 \t Elapsed time: 334.23 \t Loss:10.61962\n",
      "Iteration: 1249/277000 \t Elapsed time: 334.41 \t Loss:10.60705\n",
      "Iteration: 1250/277000 \t Elapsed time: 334.58 \t Loss:10.59459\n",
      "Iteration: 1251/277000 \t Elapsed time: 334.74 \t Loss:10.58259\n",
      "Iteration: 1252/277000 \t Elapsed time: 334.92 \t Loss:10.57165\n",
      "Iteration: 1253/277000 \t Elapsed time: 335.16 \t Loss:10.55738\n",
      "Iteration: 1254/277000 \t Elapsed time: 335.35 \t Loss:10.54604\n",
      "Iteration: 1255/277000 \t Elapsed time: 335.54 \t Loss:10.53191\n",
      "Iteration: 1256/277000 \t Elapsed time: 335.70 \t Loss:10.51984\n",
      "Iteration: 1257/277000 \t Elapsed time: 335.91 \t Loss:10.50627\n",
      "Iteration: 1258/277000 \t Elapsed time: 336.13 \t Loss:10.49418\n",
      "Iteration: 1259/277000 \t Elapsed time: 336.33 \t Loss:10.48089\n",
      "Iteration: 1260/277000 \t Elapsed time: 336.51 \t Loss:10.46869\n",
      "Iteration: 1261/277000 \t Elapsed time: 336.71 \t Loss:10.45588\n",
      "Iteration: 1262/277000 \t Elapsed time: 336.88 \t Loss:10.44376\n",
      "Iteration: 1263/277000 \t Elapsed time: 337.04 \t Loss:10.43120\n",
      "Iteration: 1264/277000 \t Elapsed time: 337.23 \t Loss:10.41825\n",
      "Iteration: 1265/277000 \t Elapsed time: 337.39 \t Loss:10.40557\n",
      "Iteration: 1266/277000 \t Elapsed time: 337.60 \t Loss:10.39302\n",
      "Iteration: 1267/277000 \t Elapsed time: 337.76 \t Loss:10.38056\n",
      "Iteration: 1268/277000 \t Elapsed time: 337.98 \t Loss:10.36780\n",
      "Iteration: 1269/277000 \t Elapsed time: 338.19 \t Loss:10.35507\n",
      "Iteration: 1270/277000 \t Elapsed time: 338.45 \t Loss:10.34253\n",
      "Iteration: 1271/277000 \t Elapsed time: 338.62 \t Loss:10.32991\n",
      "Iteration: 1272/277000 \t Elapsed time: 338.78 \t Loss:10.31744\n",
      "Iteration: 1273/277000 \t Elapsed time: 338.94 \t Loss:10.30486\n",
      "Iteration: 1274/277000 \t Elapsed time: 339.12 \t Loss:10.29228\n",
      "Iteration: 1275/277000 \t Elapsed time: 339.36 \t Loss:10.27991\n",
      "Iteration: 1276/277000 \t Elapsed time: 339.60 \t Loss:10.26742\n",
      "Iteration: 1277/277000 \t Elapsed time: 339.82 \t Loss:10.25490\n",
      "Iteration: 1278/277000 \t Elapsed time: 339.99 \t Loss:10.24227\n",
      "Iteration: 1279/277000 \t Elapsed time: 340.18 \t Loss:10.23001\n",
      "Iteration: 1280/277000 \t Elapsed time: 340.34 \t Loss:10.21759\n",
      "Iteration: 1281/277000 \t Elapsed time: 340.51 \t Loss:10.20481\n",
      "Iteration: 1282/277000 \t Elapsed time: 340.67 \t Loss:10.19238\n",
      "Iteration: 1283/277000 \t Elapsed time: 340.84 \t Loss:10.18009\n",
      "Iteration: 1284/277000 \t Elapsed time: 341.00 \t Loss:10.16764\n",
      "Iteration: 1285/277000 \t Elapsed time: 341.20 \t Loss:10.15499\n",
      "Iteration: 1286/277000 \t Elapsed time: 341.39 \t Loss:10.14256\n",
      "Iteration: 1287/277000 \t Elapsed time: 341.56 \t Loss:10.13026\n",
      "Iteration: 1288/277000 \t Elapsed time: 341.74 \t Loss:10.11775\n",
      "Iteration: 1289/277000 \t Elapsed time: 341.91 \t Loss:10.10522\n",
      "Iteration: 1290/277000 \t Elapsed time: 342.08 \t Loss:10.09296\n",
      "Iteration: 1291/277000 \t Elapsed time: 342.26 \t Loss:10.08068\n",
      "Iteration: 1292/277000 \t Elapsed time: 342.44 \t Loss:10.06811\n",
      "Iteration: 1293/277000 \t Elapsed time: 342.61 \t Loss:10.05563\n",
      "Iteration: 1294/277000 \t Elapsed time: 342.82 \t Loss:10.04329\n",
      "Iteration: 1295/277000 \t Elapsed time: 343.01 \t Loss:10.03094\n",
      "Iteration: 1296/277000 \t Elapsed time: 343.22 \t Loss:10.01851\n",
      "Iteration: 1297/277000 \t Elapsed time: 343.43 \t Loss:10.00610\n",
      "Iteration: 1298/277000 \t Elapsed time: 343.61 \t Loss:9.99380\n",
      "Iteration: 1299/277000 \t Elapsed time: 343.77 \t Loss:9.98143\n",
      "Iteration: 1300/277000 \t Elapsed time: 343.94 \t Loss:9.96904\n",
      "Iteration: 1301/277000 \t Elapsed time: 344.10 \t Loss:9.95671\n",
      "Iteration: 1302/277000 \t Elapsed time: 344.30 \t Loss:9.94440\n",
      "Iteration: 1303/277000 \t Elapsed time: 344.48 \t Loss:9.93207\n",
      "Iteration: 1304/277000 \t Elapsed time: 344.74 \t Loss:9.91972\n",
      "Iteration: 1305/277000 \t Elapsed time: 344.91 \t Loss:9.90739\n",
      "Iteration: 1306/277000 \t Elapsed time: 345.25 \t Loss:9.89509\n",
      "Iteration: 1307/277000 \t Elapsed time: 345.41 \t Loss:9.88280\n",
      "Iteration: 1308/277000 \t Elapsed time: 345.58 \t Loss:9.87052\n",
      "Iteration: 1309/277000 \t Elapsed time: 345.80 \t Loss:9.85827\n",
      "Iteration: 1310/277000 \t Elapsed time: 346.04 \t Loss:9.84607\n",
      "Iteration: 1311/277000 \t Elapsed time: 346.26 \t Loss:9.83390\n",
      "Iteration: 1312/277000 \t Elapsed time: 346.42 \t Loss:9.82170\n",
      "Iteration: 1313/277000 \t Elapsed time: 346.61 \t Loss:9.80945\n",
      "Iteration: 1314/277000 \t Elapsed time: 346.90 \t Loss:9.79727\n",
      "Iteration: 1315/277000 \t Elapsed time: 347.12 \t Loss:9.78520\n",
      "Iteration: 1316/277000 \t Elapsed time: 347.30 \t Loss:9.77310\n",
      "Iteration: 1317/277000 \t Elapsed time: 347.47 \t Loss:9.76075\n",
      "Iteration: 1318/277000 \t Elapsed time: 347.67 \t Loss:9.74829\n",
      "Iteration: 1319/277000 \t Elapsed time: 347.85 \t Loss:9.73614\n",
      "Iteration: 1320/277000 \t Elapsed time: 348.01 \t Loss:9.72393\n",
      "Iteration: 1321/277000 \t Elapsed time: 348.18 \t Loss:9.71146\n",
      "Iteration: 1322/277000 \t Elapsed time: 348.34 \t Loss:9.69919\n",
      "Iteration: 1323/277000 \t Elapsed time: 348.56 \t Loss:9.68725\n",
      "Iteration: 1324/277000 \t Elapsed time: 348.74 \t Loss:9.67521\n",
      "Iteration: 1325/277000 \t Elapsed time: 348.92 \t Loss:9.66291\n",
      "Iteration: 1326/277000 \t Elapsed time: 349.08 \t Loss:9.65063\n",
      "Iteration: 1327/277000 \t Elapsed time: 349.70 \t Loss:9.63861\n",
      "Iteration: 1328/277000 \t Elapsed time: 349.97 \t Loss:9.62650\n",
      "Iteration: 1329/277000 \t Elapsed time: 350.18 \t Loss:9.61416\n",
      "Iteration: 1330/277000 \t Elapsed time: 350.35 \t Loss:9.60197\n",
      "Iteration: 1331/277000 \t Elapsed time: 350.51 \t Loss:9.58991\n",
      "Iteration: 1332/277000 \t Elapsed time: 350.68 \t Loss:9.57770\n",
      "Iteration: 1333/277000 \t Elapsed time: 350.83 \t Loss:9.56560\n",
      "Iteration: 1334/277000 \t Elapsed time: 350.99 \t Loss:9.55365\n",
      "Iteration: 1335/277000 \t Elapsed time: 351.31 \t Loss:9.54159\n",
      "Iteration: 1336/277000 \t Elapsed time: 351.47 \t Loss:9.52957\n",
      "Iteration: 1337/277000 \t Elapsed time: 351.69 \t Loss:9.51762\n",
      "Iteration: 1338/277000 \t Elapsed time: 351.89 \t Loss:9.50566\n",
      "Iteration: 1339/277000 \t Elapsed time: 352.06 \t Loss:9.49332\n",
      "Iteration: 1340/277000 \t Elapsed time: 352.27 \t Loss:9.48105\n",
      "Iteration: 1341/277000 \t Elapsed time: 352.44 \t Loss:9.46902\n",
      "Iteration: 1342/277000 \t Elapsed time: 352.64 \t Loss:9.45723\n",
      "Iteration: 1343/277000 \t Elapsed time: 352.86 \t Loss:9.44525\n",
      "Iteration: 1344/277000 \t Elapsed time: 353.08 \t Loss:9.43289\n",
      "Iteration: 1345/277000 \t Elapsed time: 353.25 \t Loss:9.42070\n",
      "Iteration: 1346/277000 \t Elapsed time: 353.45 \t Loss:9.40870\n",
      "Iteration: 1347/277000 \t Elapsed time: 353.61 \t Loss:9.39664\n",
      "Iteration: 1348/277000 \t Elapsed time: 353.77 \t Loss:9.38457\n",
      "Iteration: 1349/277000 \t Elapsed time: 353.97 \t Loss:9.37268\n",
      "Iteration: 1350/277000 \t Elapsed time: 354.16 \t Loss:9.36081\n",
      "Iteration: 1351/277000 \t Elapsed time: 354.52 \t Loss:9.34869\n",
      "Iteration: 1352/277000 \t Elapsed time: 354.69 \t Loss:9.33679\n",
      "Iteration: 1353/277000 \t Elapsed time: 354.85 \t Loss:9.32498\n",
      "Iteration: 1354/277000 \t Elapsed time: 355.03 \t Loss:9.31294\n",
      "Iteration: 1355/277000 \t Elapsed time: 355.24 \t Loss:9.30077\n",
      "Iteration: 1356/277000 \t Elapsed time: 355.42 \t Loss:9.28875\n",
      "Iteration: 1357/277000 \t Elapsed time: 355.59 \t Loss:9.27668\n",
      "Iteration: 1358/277000 \t Elapsed time: 355.75 \t Loss:9.26468\n",
      "Iteration: 1359/277000 \t Elapsed time: 355.93 \t Loss:9.25282\n",
      "Iteration: 1360/277000 \t Elapsed time: 356.10 \t Loss:9.24092\n",
      "Iteration: 1361/277000 \t Elapsed time: 356.28 \t Loss:9.22911\n",
      "Iteration: 1362/277000 \t Elapsed time: 356.45 \t Loss:9.21723\n",
      "Iteration: 1363/277000 \t Elapsed time: 356.63 \t Loss:9.20525\n",
      "Iteration: 1364/277000 \t Elapsed time: 356.80 \t Loss:9.19328\n",
      "Iteration: 1365/277000 \t Elapsed time: 356.96 \t Loss:9.18127\n",
      "Iteration: 1366/277000 \t Elapsed time: 357.13 \t Loss:9.16927\n",
      "Iteration: 1367/277000 \t Elapsed time: 357.30 \t Loss:9.15735\n",
      "Iteration: 1368/277000 \t Elapsed time: 357.48 \t Loss:9.14543\n",
      "Iteration: 1369/277000 \t Elapsed time: 357.66 \t Loss:9.13358\n",
      "Iteration: 1370/277000 \t Elapsed time: 357.83 \t Loss:9.12186\n",
      "Iteration: 1371/277000 \t Elapsed time: 358.14 \t Loss:9.11000\n",
      "Iteration: 1372/277000 \t Elapsed time: 358.31 \t Loss:9.09827\n",
      "Iteration: 1373/277000 \t Elapsed time: 358.50 \t Loss:9.08657\n",
      "Iteration: 1374/277000 \t Elapsed time: 358.71 \t Loss:9.07488\n",
      "Iteration: 1375/277000 \t Elapsed time: 359.05 \t Loss:9.06281\n",
      "Iteration: 1376/277000 \t Elapsed time: 359.21 \t Loss:9.05066\n",
      "Iteration: 1377/277000 \t Elapsed time: 359.38 \t Loss:9.03867\n",
      "Iteration: 1378/277000 \t Elapsed time: 359.55 \t Loss:9.02697\n",
      "Iteration: 1379/277000 \t Elapsed time: 359.74 \t Loss:9.01530\n",
      "Iteration: 1380/277000 \t Elapsed time: 359.93 \t Loss:9.00348\n",
      "Iteration: 1381/277000 \t Elapsed time: 360.12 \t Loss:8.99153\n",
      "Iteration: 1382/277000 \t Elapsed time: 360.40 \t Loss:8.97960\n",
      "Iteration: 1383/277000 \t Elapsed time: 360.58 \t Loss:8.96787\n",
      "Iteration: 1384/277000 \t Elapsed time: 360.78 \t Loss:8.95623\n",
      "Iteration: 1385/277000 \t Elapsed time: 360.98 \t Loss:8.94438\n",
      "Iteration: 1386/277000 \t Elapsed time: 361.19 \t Loss:8.93247\n",
      "Iteration: 1387/277000 \t Elapsed time: 361.35 \t Loss:8.92073\n",
      "Iteration: 1388/277000 \t Elapsed time: 361.55 \t Loss:8.90897\n",
      "Iteration: 1389/277000 \t Elapsed time: 361.72 \t Loss:8.89722\n",
      "Iteration: 1390/277000 \t Elapsed time: 361.90 \t Loss:8.88559\n",
      "Iteration: 1391/277000 \t Elapsed time: 362.10 \t Loss:8.87376\n",
      "Iteration: 1392/277000 \t Elapsed time: 362.27 \t Loss:8.86187\n",
      "Iteration: 1393/277000 \t Elapsed time: 362.44 \t Loss:8.85038\n",
      "Iteration: 1394/277000 \t Elapsed time: 362.60 \t Loss:8.83877\n",
      "Iteration: 1395/277000 \t Elapsed time: 362.79 \t Loss:8.82681\n",
      "Iteration: 1396/277000 \t Elapsed time: 362.97 \t Loss:8.81514\n",
      "Iteration: 1397/277000 \t Elapsed time: 363.14 \t Loss:8.80345\n",
      "Iteration: 1398/277000 \t Elapsed time: 363.31 \t Loss:8.79149\n",
      "Iteration: 1399/277000 \t Elapsed time: 363.48 \t Loss:8.77976\n",
      "Iteration: 1400/277000 \t Elapsed time: 363.63 \t Loss:8.76833\n",
      "Iteration: 1401/277000 \t Elapsed time: 363.80 \t Loss:8.75665\n",
      "Iteration: 1402/277000 \t Elapsed time: 363.97 \t Loss:8.74477\n",
      "Iteration: 1403/277000 \t Elapsed time: 364.14 \t Loss:8.73330\n",
      "Iteration: 1404/277000 \t Elapsed time: 364.30 \t Loss:8.72184\n",
      "Iteration: 1405/277000 \t Elapsed time: 364.45 \t Loss:8.71013\n",
      "Iteration: 1406/277000 \t Elapsed time: 364.68 \t Loss:8.69838\n",
      "Iteration: 1407/277000 \t Elapsed time: 364.85 \t Loss:8.68678\n",
      "Iteration: 1408/277000 \t Elapsed time: 365.01 \t Loss:8.67507\n",
      "Iteration: 1409/277000 \t Elapsed time: 365.18 \t Loss:8.66323\n",
      "Iteration: 1410/277000 \t Elapsed time: 365.34 \t Loss:8.65137\n",
      "Iteration: 1411/277000 \t Elapsed time: 365.52 \t Loss:8.63969\n",
      "Iteration: 1412/277000 \t Elapsed time: 365.68 \t Loss:8.62838\n",
      "Iteration: 1413/277000 \t Elapsed time: 365.84 \t Loss:8.61687\n",
      "Iteration: 1414/277000 \t Elapsed time: 366.00 \t Loss:8.60499\n",
      "Iteration: 1415/277000 \t Elapsed time: 366.17 \t Loss:8.59325\n",
      "Iteration: 1416/277000 \t Elapsed time: 366.33 \t Loss:8.58172\n",
      "Iteration: 1417/277000 \t Elapsed time: 366.50 \t Loss:8.57002\n",
      "Iteration: 1418/277000 \t Elapsed time: 366.65 \t Loss:8.55846\n",
      "Iteration: 1419/277000 \t Elapsed time: 366.82 \t Loss:8.54699\n",
      "Iteration: 1420/277000 \t Elapsed time: 366.99 \t Loss:8.53536\n",
      "Iteration: 1421/277000 \t Elapsed time: 367.16 \t Loss:8.52375\n",
      "Iteration: 1422/277000 \t Elapsed time: 367.33 \t Loss:8.51214\n",
      "Iteration: 1423/277000 \t Elapsed time: 367.49 \t Loss:8.50051\n",
      "Iteration: 1424/277000 \t Elapsed time: 367.65 \t Loss:8.48896\n",
      "Iteration: 1425/277000 \t Elapsed time: 367.81 \t Loss:8.47738\n",
      "Iteration: 1426/277000 \t Elapsed time: 367.96 \t Loss:8.46583\n",
      "Iteration: 1427/277000 \t Elapsed time: 368.18 \t Loss:8.45438\n",
      "Iteration: 1428/277000 \t Elapsed time: 368.37 \t Loss:8.44290\n",
      "Iteration: 1429/277000 \t Elapsed time: 368.58 \t Loss:8.43156\n",
      "Iteration: 1430/277000 \t Elapsed time: 368.78 \t Loss:8.42045\n",
      "Iteration: 1431/277000 \t Elapsed time: 368.94 \t Loss:8.40891\n",
      "Iteration: 1432/277000 \t Elapsed time: 369.10 \t Loss:8.39759\n",
      "Iteration: 1433/277000 \t Elapsed time: 369.29 \t Loss:8.38607\n",
      "Iteration: 1434/277000 \t Elapsed time: 369.51 \t Loss:8.37402\n",
      "Iteration: 1435/277000 \t Elapsed time: 369.76 \t Loss:8.36238\n",
      "Iteration: 1436/277000 \t Elapsed time: 369.96 \t Loss:8.35141\n",
      "Iteration: 1437/277000 \t Elapsed time: 370.19 \t Loss:8.34009\n",
      "Iteration: 1438/277000 \t Elapsed time: 370.40 \t Loss:8.32828\n",
      "Iteration: 1439/277000 \t Elapsed time: 370.58 \t Loss:8.31697\n",
      "Iteration: 1440/277000 \t Elapsed time: 370.78 \t Loss:8.30544\n",
      "Iteration: 1441/277000 \t Elapsed time: 370.94 \t Loss:8.29344\n",
      "Iteration: 1442/277000 \t Elapsed time: 371.13 \t Loss:8.28214\n",
      "Iteration: 1443/277000 \t Elapsed time: 371.42 \t Loss:8.27102\n",
      "Iteration: 1444/277000 \t Elapsed time: 371.61 \t Loss:8.25956\n",
      "Iteration: 1445/277000 \t Elapsed time: 371.83 \t Loss:8.24805\n",
      "Iteration: 1446/277000 \t Elapsed time: 372.01 \t Loss:8.23652\n",
      "Iteration: 1447/277000 \t Elapsed time: 372.20 \t Loss:8.22491\n",
      "Iteration: 1448/277000 \t Elapsed time: 372.36 \t Loss:8.21348\n",
      "Iteration: 1449/277000 \t Elapsed time: 372.54 \t Loss:8.20208\n",
      "Iteration: 1450/277000 \t Elapsed time: 372.73 \t Loss:8.19065\n",
      "Iteration: 1451/277000 \t Elapsed time: 372.90 \t Loss:8.17945\n",
      "Iteration: 1452/277000 \t Elapsed time: 373.08 \t Loss:8.16826\n",
      "Iteration: 1453/277000 \t Elapsed time: 373.25 \t Loss:8.15716\n",
      "Iteration: 1454/277000 \t Elapsed time: 373.41 \t Loss:8.14603\n",
      "Iteration: 1455/277000 \t Elapsed time: 373.58 \t Loss:8.13474\n",
      "Iteration: 1456/277000 \t Elapsed time: 373.76 \t Loss:8.12290\n",
      "Iteration: 1457/277000 \t Elapsed time: 373.92 \t Loss:8.11110\n",
      "Iteration: 1458/277000 \t Elapsed time: 374.11 \t Loss:8.09982\n",
      "Iteration: 1459/277000 \t Elapsed time: 374.28 \t Loss:8.08887\n",
      "Iteration: 1460/277000 \t Elapsed time: 374.47 \t Loss:8.07780\n",
      "Iteration: 1461/277000 \t Elapsed time: 374.63 \t Loss:8.06626\n",
      "Iteration: 1462/277000 \t Elapsed time: 374.80 \t Loss:8.05452\n",
      "Iteration: 1463/277000 \t Elapsed time: 374.97 \t Loss:8.04305\n",
      "Iteration: 1464/277000 \t Elapsed time: 375.14 \t Loss:8.03193\n",
      "Iteration: 1465/277000 \t Elapsed time: 375.31 \t Loss:8.02094\n",
      "Iteration: 1466/277000 \t Elapsed time: 375.49 \t Loss:8.00983\n",
      "Iteration: 1467/277000 \t Elapsed time: 375.68 \t Loss:7.99834\n",
      "Iteration: 1468/277000 \t Elapsed time: 376.09 \t Loss:7.98669\n",
      "Iteration: 1469/277000 \t Elapsed time: 376.31 \t Loss:7.97539\n",
      "Iteration: 1470/277000 \t Elapsed time: 376.50 \t Loss:7.96450\n",
      "Iteration: 1471/277000 \t Elapsed time: 376.68 \t Loss:7.95331\n",
      "Iteration: 1472/277000 \t Elapsed time: 376.88 \t Loss:7.94190\n",
      "Iteration: 1473/277000 \t Elapsed time: 377.04 \t Loss:7.93042\n",
      "Iteration: 1474/277000 \t Elapsed time: 377.22 \t Loss:7.91901\n",
      "Iteration: 1475/277000 \t Elapsed time: 377.40 \t Loss:7.90784\n",
      "Iteration: 1476/277000 \t Elapsed time: 377.58 \t Loss:7.89678\n",
      "Iteration: 1477/277000 \t Elapsed time: 377.77 \t Loss:7.88566\n",
      "Iteration: 1478/277000 \t Elapsed time: 377.97 \t Loss:7.87451\n",
      "Iteration: 1479/277000 \t Elapsed time: 378.28 \t Loss:7.86324\n",
      "Iteration: 1480/277000 \t Elapsed time: 378.75 \t Loss:7.85182\n",
      "Iteration: 1481/277000 \t Elapsed time: 378.93 \t Loss:7.84050\n",
      "Iteration: 1482/277000 \t Elapsed time: 379.10 \t Loss:7.82928\n",
      "Iteration: 1483/277000 \t Elapsed time: 379.28 \t Loss:7.81808\n",
      "Iteration: 1484/277000 \t Elapsed time: 379.46 \t Loss:7.80705\n",
      "Iteration: 1485/277000 \t Elapsed time: 379.64 \t Loss:7.79606\n",
      "Iteration: 1486/277000 \t Elapsed time: 379.90 \t Loss:7.78527\n",
      "Iteration: 1487/277000 \t Elapsed time: 380.08 \t Loss:7.77496\n",
      "Iteration: 1488/277000 \t Elapsed time: 380.28 \t Loss:7.76456\n",
      "Iteration: 1489/277000 \t Elapsed time: 380.49 \t Loss:7.75328\n",
      "Iteration: 1490/277000 \t Elapsed time: 380.70 \t Loss:7.74062\n",
      "Iteration: 1491/277000 \t Elapsed time: 380.90 \t Loss:7.72945\n",
      "Iteration: 1492/277000 \t Elapsed time: 381.08 \t Loss:7.71907\n",
      "Iteration: 1493/277000 \t Elapsed time: 381.32 \t Loss:7.70752\n",
      "Iteration: 1494/277000 \t Elapsed time: 381.57 \t Loss:7.69563\n",
      "Iteration: 1495/277000 \t Elapsed time: 381.77 \t Loss:7.68493\n",
      "Iteration: 1496/277000 \t Elapsed time: 381.94 \t Loss:7.67446\n",
      "Iteration: 1497/277000 \t Elapsed time: 382.12 \t Loss:7.66261\n",
      "Iteration: 1498/277000 \t Elapsed time: 382.34 \t Loss:7.65110\n",
      "Iteration: 1499/277000 \t Elapsed time: 382.57 \t Loss:7.64083\n",
      "Iteration: 1500/277000 \t Elapsed time: 382.83 \t Loss:7.62998\n",
      "Iteration: 1501/277000 \t Elapsed time: 383.07 \t Loss:7.61792\n",
      "Iteration: 1502/277000 \t Elapsed time: 383.25 \t Loss:7.60720\n",
      "Iteration: 1503/277000 \t Elapsed time: 383.42 \t Loss:7.59606\n",
      "Iteration: 1504/277000 \t Elapsed time: 383.60 \t Loss:7.58511\n",
      "Iteration: 1505/277000 \t Elapsed time: 383.80 \t Loss:7.57389\n",
      "Iteration: 1506/277000 \t Elapsed time: 383.97 \t Loss:7.56273\n",
      "Iteration: 1507/277000 \t Elapsed time: 384.19 \t Loss:7.55302\n",
      "Iteration: 1508/277000 \t Elapsed time: 384.45 \t Loss:7.54048\n",
      "Iteration: 1509/277000 \t Elapsed time: 384.69 \t Loss:7.53040\n",
      "Iteration: 1510/277000 \t Elapsed time: 384.89 \t Loss:7.51884\n",
      "Iteration: 1511/277000 \t Elapsed time: 385.07 \t Loss:7.50777\n",
      "Iteration: 1512/277000 \t Elapsed time: 385.23 \t Loss:7.49728\n",
      "Iteration: 1513/277000 \t Elapsed time: 385.53 \t Loss:7.48509\n",
      "Iteration: 1514/277000 \t Elapsed time: 385.70 \t Loss:7.47483\n",
      "Iteration: 1515/277000 \t Elapsed time: 385.87 \t Loss:7.46319\n",
      "Iteration: 1516/277000 \t Elapsed time: 386.03 \t Loss:7.45274\n",
      "Iteration: 1517/277000 \t Elapsed time: 386.19 \t Loss:7.44160\n",
      "Iteration: 1518/277000 \t Elapsed time: 386.36 \t Loss:7.43035\n",
      "Iteration: 1519/277000 \t Elapsed time: 386.52 \t Loss:7.42003\n",
      "Iteration: 1520/277000 \t Elapsed time: 386.70 \t Loss:7.40860\n",
      "Iteration: 1521/277000 \t Elapsed time: 386.96 \t Loss:7.39926\n",
      "Iteration: 1522/277000 \t Elapsed time: 387.13 \t Loss:7.39082\n",
      "Iteration: 1523/277000 \t Elapsed time: 387.31 \t Loss:7.38311\n",
      "Iteration: 1524/277000 \t Elapsed time: 387.50 \t Loss:7.36974\n",
      "Iteration: 1525/277000 \t Elapsed time: 387.69 \t Loss:7.35532\n",
      "Iteration: 1526/277000 \t Elapsed time: 387.86 \t Loss:7.34845\n",
      "Iteration: 1527/277000 \t Elapsed time: 388.04 \t Loss:7.33345\n",
      "Iteration: 1528/277000 \t Elapsed time: 388.20 \t Loss:7.32362\n",
      "Iteration: 1529/277000 \t Elapsed time: 388.38 \t Loss:7.31245\n",
      "Iteration: 1530/277000 \t Elapsed time: 388.56 \t Loss:7.30092\n",
      "Iteration: 1531/277000 \t Elapsed time: 388.77 \t Loss:7.28998\n",
      "Iteration: 1532/277000 \t Elapsed time: 388.96 \t Loss:7.27941\n",
      "Iteration: 1533/277000 \t Elapsed time: 389.13 \t Loss:7.26763\n",
      "Iteration: 1534/277000 \t Elapsed time: 389.31 \t Loss:7.25736\n",
      "Iteration: 1535/277000 \t Elapsed time: 389.48 \t Loss:7.24584\n",
      "Iteration: 1536/277000 \t Elapsed time: 389.65 \t Loss:7.23513\n",
      "Iteration: 1537/277000 \t Elapsed time: 389.82 \t Loss:7.22412\n",
      "Iteration: 1538/277000 \t Elapsed time: 389.99 \t Loss:7.21312\n",
      "Iteration: 1539/277000 \t Elapsed time: 390.19 \t Loss:7.20219\n",
      "Iteration: 1540/277000 \t Elapsed time: 390.37 \t Loss:7.19128\n",
      "Iteration: 1541/277000 \t Elapsed time: 390.63 \t Loss:7.18032\n",
      "Iteration: 1542/277000 \t Elapsed time: 390.85 \t Loss:7.16941\n",
      "Iteration: 1543/277000 \t Elapsed time: 391.04 \t Loss:7.15893\n",
      "Iteration: 1544/277000 \t Elapsed time: 391.24 \t Loss:7.14813\n",
      "Iteration: 1545/277000 \t Elapsed time: 391.42 \t Loss:7.13710\n",
      "Iteration: 1546/277000 \t Elapsed time: 391.58 \t Loss:7.12682\n",
      "Iteration: 1547/277000 \t Elapsed time: 391.76 \t Loss:7.11533\n",
      "Iteration: 1548/277000 \t Elapsed time: 391.93 \t Loss:7.10439\n",
      "Iteration: 1549/277000 \t Elapsed time: 392.10 \t Loss:7.09377\n",
      "Iteration: 1550/277000 \t Elapsed time: 392.27 \t Loss:7.08305\n",
      "Iteration: 1551/277000 \t Elapsed time: 392.45 \t Loss:7.07210\n",
      "Iteration: 1552/277000 \t Elapsed time: 392.62 \t Loss:7.06146\n",
      "Iteration: 1553/277000 \t Elapsed time: 392.79 \t Loss:7.05015\n",
      "Iteration: 1554/277000 \t Elapsed time: 392.96 \t Loss:7.03974\n",
      "Iteration: 1555/277000 \t Elapsed time: 393.15 \t Loss:7.02885\n",
      "Iteration: 1556/277000 \t Elapsed time: 393.35 \t Loss:7.01797\n",
      "Iteration: 1557/277000 \t Elapsed time: 393.57 \t Loss:7.00730\n",
      "Iteration: 1558/277000 \t Elapsed time: 393.75 \t Loss:6.99635\n",
      "Iteration: 1559/277000 \t Elapsed time: 393.92 \t Loss:6.98582\n",
      "Iteration: 1560/277000 \t Elapsed time: 394.11 \t Loss:6.97479\n",
      "Iteration: 1561/277000 \t Elapsed time: 394.29 \t Loss:6.96439\n",
      "Iteration: 1562/277000 \t Elapsed time: 394.46 \t Loss:6.95385\n",
      "Iteration: 1563/277000 \t Elapsed time: 394.67 \t Loss:6.94480\n",
      "Iteration: 1564/277000 \t Elapsed time: 394.87 \t Loss:6.93432\n",
      "Iteration: 1565/277000 \t Elapsed time: 395.07 \t Loss:6.92302\n",
      "Iteration: 1566/277000 \t Elapsed time: 395.26 \t Loss:6.91128\n",
      "Iteration: 1567/277000 \t Elapsed time: 395.43 \t Loss:6.90309\n",
      "Iteration: 1568/277000 \t Elapsed time: 395.63 \t Loss:6.89425\n",
      "Iteration: 1569/277000 \t Elapsed time: 395.85 \t Loss:6.88058\n",
      "Iteration: 1570/277000 \t Elapsed time: 396.10 \t Loss:6.87067\n",
      "Iteration: 1571/277000 \t Elapsed time: 396.34 \t Loss:6.86109\n",
      "Iteration: 1572/277000 \t Elapsed time: 396.55 \t Loss:6.84765\n",
      "Iteration: 1573/277000 \t Elapsed time: 396.71 \t Loss:6.84007\n",
      "Iteration: 1574/277000 \t Elapsed time: 396.91 \t Loss:6.83037\n",
      "Iteration: 1575/277000 \t Elapsed time: 397.08 \t Loss:6.81683\n",
      "Iteration: 1576/277000 \t Elapsed time: 397.31 \t Loss:6.80821\n",
      "Iteration: 1577/277000 \t Elapsed time: 397.47 \t Loss:6.79604\n",
      "Iteration: 1578/277000 \t Elapsed time: 397.64 \t Loss:6.78560\n",
      "Iteration: 1579/277000 \t Elapsed time: 397.81 \t Loss:6.77402\n",
      "Iteration: 1580/277000 \t Elapsed time: 398.00 \t Loss:6.76394\n",
      "Iteration: 1581/277000 \t Elapsed time: 398.18 \t Loss:6.75240\n",
      "Iteration: 1582/277000 \t Elapsed time: 398.36 \t Loss:6.74272\n",
      "Iteration: 1583/277000 \t Elapsed time: 398.56 \t Loss:6.73111\n",
      "Iteration: 1584/277000 \t Elapsed time: 398.73 \t Loss:6.72099\n",
      "Iteration: 1585/277000 \t Elapsed time: 398.90 \t Loss:6.70996\n",
      "Iteration: 1586/277000 \t Elapsed time: 399.10 \t Loss:6.69935\n",
      "Iteration: 1587/277000 \t Elapsed time: 399.28 \t Loss:6.68883\n",
      "Iteration: 1588/277000 \t Elapsed time: 399.48 \t Loss:6.67811\n",
      "Iteration: 1589/277000 \t Elapsed time: 399.65 \t Loss:6.66743\n",
      "Iteration: 1590/277000 \t Elapsed time: 399.82 \t Loss:6.65738\n",
      "Iteration: 1591/277000 \t Elapsed time: 399.98 \t Loss:6.64616\n",
      "Iteration: 1592/277000 \t Elapsed time: 400.18 \t Loss:6.63624\n",
      "Iteration: 1593/277000 \t Elapsed time: 400.38 \t Loss:6.62520\n",
      "Iteration: 1594/277000 \t Elapsed time: 400.59 \t Loss:6.61480\n",
      "Iteration: 1595/277000 \t Elapsed time: 400.78 \t Loss:6.60392\n",
      "Iteration: 1596/277000 \t Elapsed time: 400.99 \t Loss:6.59404\n",
      "Iteration: 1597/277000 \t Elapsed time: 401.17 \t Loss:6.58302\n",
      "Iteration: 1598/277000 \t Elapsed time: 401.33 \t Loss:6.57232\n",
      "Iteration: 1599/277000 \t Elapsed time: 401.52 \t Loss:6.56186\n",
      "Iteration: 1600/277000 \t Elapsed time: 401.70 \t Loss:6.55123\n",
      "Iteration: 1601/277000 \t Elapsed time: 401.86 \t Loss:6.54088\n",
      "Iteration: 1602/277000 \t Elapsed time: 402.06 \t Loss:6.53011\n",
      "Iteration: 1603/277000 \t Elapsed time: 402.30 \t Loss:6.52012\n",
      "Iteration: 1604/277000 \t Elapsed time: 402.50 \t Loss:6.50937\n",
      "Iteration: 1605/277000 \t Elapsed time: 402.67 \t Loss:6.49865\n",
      "Iteration: 1606/277000 \t Elapsed time: 402.86 \t Loss:6.48836\n",
      "Iteration: 1607/277000 \t Elapsed time: 403.03 \t Loss:6.47761\n",
      "Iteration: 1608/277000 \t Elapsed time: 403.25 \t Loss:6.46736\n",
      "Iteration: 1609/277000 \t Elapsed time: 403.41 \t Loss:6.45729\n",
      "Iteration: 1610/277000 \t Elapsed time: 403.57 \t Loss:6.44741\n",
      "Iteration: 1611/277000 \t Elapsed time: 403.74 \t Loss:6.43768\n",
      "Iteration: 1612/277000 \t Elapsed time: 403.91 \t Loss:6.42879\n",
      "Iteration: 1613/277000 \t Elapsed time: 404.07 \t Loss:6.41610\n",
      "Iteration: 1614/277000 \t Elapsed time: 404.24 \t Loss:6.40584\n",
      "Iteration: 1615/277000 \t Elapsed time: 404.40 \t Loss:6.39895\n",
      "Iteration: 1616/277000 \t Elapsed time: 404.57 \t Loss:6.38816\n",
      "Iteration: 1617/277000 \t Elapsed time: 404.73 \t Loss:6.37748\n",
      "Iteration: 1618/277000 \t Elapsed time: 404.95 \t Loss:6.36597\n",
      "Iteration: 1619/277000 \t Elapsed time: 405.13 \t Loss:6.35695\n",
      "Iteration: 1620/277000 \t Elapsed time: 405.30 \t Loss:6.34427\n",
      "Iteration: 1621/277000 \t Elapsed time: 405.47 \t Loss:6.33408\n",
      "Iteration: 1622/277000 \t Elapsed time: 405.63 \t Loss:6.32398\n",
      "Iteration: 1623/277000 \t Elapsed time: 405.81 \t Loss:6.31247\n",
      "Iteration: 1624/277000 \t Elapsed time: 406.00 \t Loss:6.30313\n",
      "Iteration: 1625/277000 \t Elapsed time: 406.25 \t Loss:6.29226\n",
      "Iteration: 1626/277000 \t Elapsed time: 406.46 \t Loss:6.28129\n",
      "Iteration: 1627/277000 \t Elapsed time: 406.66 \t Loss:6.27234\n",
      "Iteration: 1628/277000 \t Elapsed time: 406.83 \t Loss:6.26232\n",
      "Iteration: 1629/277000 \t Elapsed time: 407.00 \t Loss:6.24995\n",
      "Iteration: 1630/277000 \t Elapsed time: 407.20 \t Loss:6.24075\n",
      "Iteration: 1631/277000 \t Elapsed time: 407.37 \t Loss:6.23090\n",
      "Iteration: 1632/277000 \t Elapsed time: 407.61 \t Loss:6.21892\n",
      "Iteration: 1633/277000 \t Elapsed time: 407.82 \t Loss:6.21049\n",
      "Iteration: 1634/277000 \t Elapsed time: 408.00 \t Loss:6.19961\n",
      "Iteration: 1635/277000 \t Elapsed time: 408.19 \t Loss:6.18828\n",
      "Iteration: 1636/277000 \t Elapsed time: 408.37 \t Loss:6.17870\n",
      "Iteration: 1637/277000 \t Elapsed time: 408.55 \t Loss:6.16776\n",
      "Iteration: 1638/277000 \t Elapsed time: 408.73 \t Loss:6.15759\n",
      "Iteration: 1639/277000 \t Elapsed time: 408.88 \t Loss:6.14759\n",
      "Iteration: 1640/277000 \t Elapsed time: 409.04 \t Loss:6.13746\n",
      "Iteration: 1641/277000 \t Elapsed time: 409.33 \t Loss:6.12605\n",
      "Iteration: 1642/277000 \t Elapsed time: 409.50 \t Loss:6.11737\n",
      "Iteration: 1643/277000 \t Elapsed time: 410.04 \t Loss:6.10627\n",
      "Iteration: 1644/277000 \t Elapsed time: 410.23 \t Loss:6.09676\n",
      "Iteration: 1645/277000 \t Elapsed time: 410.40 \t Loss:6.08662\n",
      "Iteration: 1646/277000 \t Elapsed time: 410.64 \t Loss:6.07687\n",
      "Iteration: 1647/277000 \t Elapsed time: 410.81 \t Loss:6.06588\n",
      "Iteration: 1648/277000 \t Elapsed time: 411.02 \t Loss:6.05604\n",
      "Iteration: 1649/277000 \t Elapsed time: 411.19 \t Loss:6.04438\n",
      "Iteration: 1650/277000 \t Elapsed time: 411.36 \t Loss:6.03551\n",
      "Iteration: 1651/277000 \t Elapsed time: 411.52 \t Loss:6.02581\n",
      "Iteration: 1652/277000 \t Elapsed time: 411.71 \t Loss:6.01749\n",
      "Iteration: 1653/277000 \t Elapsed time: 411.88 \t Loss:6.01946\n",
      "Iteration: 1654/277000 \t Elapsed time: 412.15 \t Loss:6.00160\n",
      "Iteration: 1655/277000 \t Elapsed time: 412.32 \t Loss:5.99344\n",
      "Iteration: 1656/277000 \t Elapsed time: 412.50 \t Loss:5.98150\n",
      "Iteration: 1657/277000 \t Elapsed time: 412.66 \t Loss:5.96743\n",
      "Iteration: 1658/277000 \t Elapsed time: 412.87 \t Loss:5.95952\n",
      "Iteration: 1659/277000 \t Elapsed time: 413.03 \t Loss:5.94766\n",
      "Iteration: 1660/277000 \t Elapsed time: 413.24 \t Loss:5.93784\n",
      "Iteration: 1661/277000 \t Elapsed time: 413.45 \t Loss:5.92690\n",
      "Iteration: 1662/277000 \t Elapsed time: 413.68 \t Loss:5.91544\n",
      "Iteration: 1663/277000 \t Elapsed time: 413.89 \t Loss:5.90714\n",
      "Iteration: 1664/277000 \t Elapsed time: 414.09 \t Loss:5.89486\n",
      "Iteration: 1665/277000 \t Elapsed time: 414.26 \t Loss:5.88644\n",
      "Iteration: 1666/277000 \t Elapsed time: 414.50 \t Loss:5.87627\n",
      "Iteration: 1667/277000 \t Elapsed time: 414.68 \t Loss:5.86600\n",
      "Iteration: 1668/277000 \t Elapsed time: 414.87 \t Loss:5.85621\n",
      "Iteration: 1669/277000 \t Elapsed time: 415.06 \t Loss:5.84418\n",
      "Iteration: 1670/277000 \t Elapsed time: 415.22 \t Loss:5.83457\n",
      "Iteration: 1671/277000 \t Elapsed time: 415.39 \t Loss:5.82436\n",
      "Iteration: 1672/277000 \t Elapsed time: 415.56 \t Loss:5.81415\n",
      "Iteration: 1673/277000 \t Elapsed time: 415.77 \t Loss:5.80396\n",
      "Iteration: 1674/277000 \t Elapsed time: 415.93 \t Loss:5.79405\n",
      "Iteration: 1675/277000 \t Elapsed time: 416.10 \t Loss:5.78390\n",
      "Iteration: 1676/277000 \t Elapsed time: 416.27 \t Loss:5.77427\n",
      "Iteration: 1677/277000 \t Elapsed time: 416.44 \t Loss:5.76424\n",
      "Iteration: 1678/277000 \t Elapsed time: 416.65 \t Loss:5.75384\n",
      "Iteration: 1679/277000 \t Elapsed time: 416.83 \t Loss:5.74389\n",
      "Iteration: 1680/277000 \t Elapsed time: 417.00 \t Loss:5.73414\n",
      "Iteration: 1681/277000 \t Elapsed time: 417.16 \t Loss:5.72370\n",
      "Iteration: 1682/277000 \t Elapsed time: 417.38 \t Loss:5.71584\n",
      "Iteration: 1683/277000 \t Elapsed time: 417.79 \t Loss:5.70522\n",
      "Iteration: 1684/277000 \t Elapsed time: 418.15 \t Loss:5.69420\n",
      "Iteration: 1685/277000 \t Elapsed time: 418.33 \t Loss:5.68529\n",
      "Iteration: 1686/277000 \t Elapsed time: 418.50 \t Loss:5.67529\n",
      "Iteration: 1687/277000 \t Elapsed time: 418.66 \t Loss:5.66474\n",
      "Iteration: 1688/277000 \t Elapsed time: 418.82 \t Loss:5.65552\n",
      "Iteration: 1689/277000 \t Elapsed time: 419.00 \t Loss:5.64453\n",
      "Iteration: 1690/277000 \t Elapsed time: 419.23 \t Loss:5.63573\n",
      "Iteration: 1691/277000 \t Elapsed time: 419.48 \t Loss:5.62553\n",
      "Iteration: 1692/277000 \t Elapsed time: 419.71 \t Loss:5.62034\n",
      "Iteration: 1693/277000 \t Elapsed time: 419.87 \t Loss:5.61485\n",
      "Iteration: 1694/277000 \t Elapsed time: 420.03 \t Loss:5.61515\n",
      "Iteration: 1695/277000 \t Elapsed time: 420.22 \t Loss:5.59517\n",
      "Iteration: 1696/277000 \t Elapsed time: 420.38 \t Loss:5.58761\n",
      "Iteration: 1697/277000 \t Elapsed time: 420.58 \t Loss:5.57521\n",
      "Iteration: 1698/277000 \t Elapsed time: 420.79 \t Loss:5.56275\n",
      "Iteration: 1699/277000 \t Elapsed time: 421.35 \t Loss:5.55447\n",
      "Iteration: 1700/277000 \t Elapsed time: 421.53 \t Loss:5.54365\n",
      "Iteration: 1701/277000 \t Elapsed time: 421.76 \t Loss:5.53169\n",
      "Iteration: 1702/277000 \t Elapsed time: 421.92 \t Loss:5.52413\n",
      "Iteration: 1703/277000 \t Elapsed time: 422.13 \t Loss:5.51094\n",
      "Iteration: 1704/277000 \t Elapsed time: 422.31 \t Loss:5.50347\n",
      "Iteration: 1705/277000 \t Elapsed time: 422.49 \t Loss:5.49177\n",
      "Iteration: 1706/277000 \t Elapsed time: 422.67 \t Loss:5.48320\n",
      "Iteration: 1707/277000 \t Elapsed time: 422.83 \t Loss:5.47461\n",
      "Iteration: 1708/277000 \t Elapsed time: 423.06 \t Loss:5.46260\n",
      "Iteration: 1709/277000 \t Elapsed time: 423.24 \t Loss:5.45248\n",
      "Iteration: 1710/277000 \t Elapsed time: 423.43 \t Loss:5.44358\n",
      "Iteration: 1711/277000 \t Elapsed time: 423.61 \t Loss:5.43260\n",
      "Iteration: 1712/277000 \t Elapsed time: 423.78 \t Loss:5.42371\n",
      "Iteration: 1713/277000 \t Elapsed time: 423.95 \t Loss:5.41314\n",
      "Iteration: 1714/277000 \t Elapsed time: 424.12 \t Loss:5.40413\n",
      "Iteration: 1715/277000 \t Elapsed time: 424.29 \t Loss:5.39402\n",
      "Iteration: 1716/277000 \t Elapsed time: 424.48 \t Loss:5.38442\n",
      "Iteration: 1717/277000 \t Elapsed time: 424.66 \t Loss:5.37488\n",
      "Iteration: 1718/277000 \t Elapsed time: 424.83 \t Loss:5.36490\n",
      "Iteration: 1719/277000 \t Elapsed time: 424.99 \t Loss:5.35539\n",
      "Iteration: 1720/277000 \t Elapsed time: 425.16 \t Loss:5.34545\n",
      "Iteration: 1721/277000 \t Elapsed time: 425.32 \t Loss:5.33609\n",
      "Iteration: 1722/277000 \t Elapsed time: 425.51 \t Loss:5.32829\n",
      "Iteration: 1723/277000 \t Elapsed time: 425.76 \t Loss:5.31740\n",
      "Iteration: 1724/277000 \t Elapsed time: 426.02 \t Loss:5.30743\n",
      "Iteration: 1725/277000 \t Elapsed time: 426.18 \t Loss:5.29770\n",
      "Iteration: 1726/277000 \t Elapsed time: 426.34 \t Loss:5.28955\n",
      "Iteration: 1727/277000 \t Elapsed time: 426.52 \t Loss:5.27922\n",
      "Iteration: 1728/277000 \t Elapsed time: 426.69 \t Loss:5.26969\n",
      "Iteration: 1729/277000 \t Elapsed time: 426.85 \t Loss:5.25946\n",
      "Iteration: 1730/277000 \t Elapsed time: 427.01 \t Loss:5.25092\n",
      "Iteration: 1731/277000 \t Elapsed time: 427.16 \t Loss:5.24026\n",
      "Iteration: 1732/277000 \t Elapsed time: 427.33 \t Loss:5.23116\n",
      "Iteration: 1733/277000 \t Elapsed time: 427.49 \t Loss:5.22110\n",
      "Iteration: 1734/277000 \t Elapsed time: 427.66 \t Loss:5.21186\n",
      "Iteration: 1735/277000 \t Elapsed time: 427.82 \t Loss:5.20242\n",
      "Iteration: 1736/277000 \t Elapsed time: 427.98 \t Loss:5.19302\n",
      "Iteration: 1737/277000 \t Elapsed time: 428.14 \t Loss:5.18325\n",
      "Iteration: 1738/277000 \t Elapsed time: 428.38 \t Loss:5.17398\n",
      "Iteration: 1739/277000 \t Elapsed time: 428.55 \t Loss:5.16520\n",
      "Iteration: 1740/277000 \t Elapsed time: 428.74 \t Loss:5.15612\n",
      "Iteration: 1741/277000 \t Elapsed time: 428.95 \t Loss:5.14956\n",
      "Iteration: 1742/277000 \t Elapsed time: 429.12 \t Loss:5.14041\n",
      "Iteration: 1743/277000 \t Elapsed time: 429.28 \t Loss:5.13482\n",
      "Iteration: 1744/277000 \t Elapsed time: 429.45 \t Loss:5.12701\n",
      "Iteration: 1745/277000 \t Elapsed time: 429.66 \t Loss:5.11202\n",
      "Iteration: 1746/277000 \t Elapsed time: 429.82 \t Loss:5.10152\n",
      "Iteration: 1747/277000 \t Elapsed time: 429.98 \t Loss:5.09529\n",
      "Iteration: 1748/277000 \t Elapsed time: 430.16 \t Loss:5.08035\n",
      "Iteration: 1749/277000 \t Elapsed time: 430.36 \t Loss:5.07401\n",
      "Iteration: 1750/277000 \t Elapsed time: 430.54 \t Loss:5.06246\n",
      "Iteration: 1751/277000 \t Elapsed time: 430.73 \t Loss:5.05421\n",
      "Iteration: 1752/277000 \t Elapsed time: 430.90 \t Loss:5.04647\n",
      "Iteration: 1753/277000 \t Elapsed time: 431.06 \t Loss:5.03505\n",
      "Iteration: 1754/277000 \t Elapsed time: 431.23 \t Loss:5.02666\n",
      "Iteration: 1755/277000 \t Elapsed time: 431.42 \t Loss:5.01640\n",
      "Iteration: 1756/277000 \t Elapsed time: 431.59 \t Loss:5.00693\n",
      "Iteration: 1757/277000 \t Elapsed time: 431.78 \t Loss:5.00545\n",
      "Iteration: 1758/277000 \t Elapsed time: 432.08 \t Loss:4.99329\n",
      "Iteration: 1759/277000 \t Elapsed time: 432.26 \t Loss:4.97954\n",
      "Iteration: 1760/277000 \t Elapsed time: 432.43 \t Loss:4.97309\n",
      "Iteration: 1761/277000 \t Elapsed time: 432.61 \t Loss:4.96472\n",
      "Iteration: 1762/277000 \t Elapsed time: 432.77 \t Loss:4.95495\n",
      "Iteration: 1763/277000 \t Elapsed time: 432.94 \t Loss:4.94776\n",
      "Iteration: 1764/277000 \t Elapsed time: 433.10 \t Loss:4.93473\n",
      "Iteration: 1765/277000 \t Elapsed time: 433.27 \t Loss:4.93172\n",
      "Iteration: 1766/277000 \t Elapsed time: 433.45 \t Loss:4.92225\n",
      "Iteration: 1767/277000 \t Elapsed time: 433.62 \t Loss:4.94250\n",
      "Iteration: 1768/277000 \t Elapsed time: 433.79 \t Loss:4.93489\n",
      "Iteration: 1769/277000 \t Elapsed time: 433.96 \t Loss:4.89708\n",
      "Iteration: 1770/277000 \t Elapsed time: 434.14 \t Loss:4.90185\n",
      "Iteration: 1771/277000 \t Elapsed time: 434.31 \t Loss:4.88087\n",
      "Iteration: 1772/277000 \t Elapsed time: 434.47 \t Loss:4.87674\n",
      "Iteration: 1773/277000 \t Elapsed time: 434.64 \t Loss:4.86505\n",
      "Iteration: 1774/277000 \t Elapsed time: 434.79 \t Loss:4.85902\n",
      "Iteration: 1775/277000 \t Elapsed time: 434.96 \t Loss:4.84077\n",
      "Iteration: 1776/277000 \t Elapsed time: 435.12 \t Loss:4.83490\n",
      "Iteration: 1777/277000 \t Elapsed time: 435.29 \t Loss:4.82128\n",
      "Iteration: 1778/277000 \t Elapsed time: 435.46 \t Loss:4.81646\n",
      "Iteration: 1779/277000 \t Elapsed time: 435.62 \t Loss:4.80559\n",
      "Iteration: 1780/277000 \t Elapsed time: 435.79 \t Loss:4.79489\n",
      "Iteration: 1781/277000 \t Elapsed time: 436.02 \t Loss:4.78422\n",
      "Iteration: 1782/277000 \t Elapsed time: 436.19 \t Loss:4.77758\n",
      "Iteration: 1783/277000 \t Elapsed time: 436.42 \t Loss:4.76385\n",
      "Iteration: 1784/277000 \t Elapsed time: 436.77 \t Loss:4.75743\n",
      "Iteration: 1785/277000 \t Elapsed time: 437.41 \t Loss:4.74813\n",
      "Iteration: 1786/277000 \t Elapsed time: 437.61 \t Loss:4.73922\n",
      "Iteration: 1787/277000 \t Elapsed time: 437.78 \t Loss:4.73390\n",
      "Iteration: 1788/277000 \t Elapsed time: 437.94 \t Loss:4.71816\n",
      "Iteration: 1789/277000 \t Elapsed time: 438.11 \t Loss:4.71217\n",
      "Iteration: 1790/277000 \t Elapsed time: 438.32 \t Loss:4.70315\n",
      "Iteration: 1791/277000 \t Elapsed time: 438.55 \t Loss:4.69729\n",
      "Iteration: 1792/277000 \t Elapsed time: 438.73 \t Loss:4.69487\n",
      "Iteration: 1793/277000 \t Elapsed time: 438.91 \t Loss:4.68495\n",
      "Iteration: 1794/277000 \t Elapsed time: 439.08 \t Loss:4.66765\n",
      "Iteration: 1795/277000 \t Elapsed time: 439.24 \t Loss:4.65881\n",
      "Iteration: 1796/277000 \t Elapsed time: 439.46 \t Loss:4.65492\n",
      "Iteration: 1797/277000 \t Elapsed time: 439.70 \t Loss:4.64048\n",
      "Iteration: 1798/277000 \t Elapsed time: 439.88 \t Loss:4.63126\n",
      "Iteration: 1799/277000 \t Elapsed time: 440.04 \t Loss:4.62432\n",
      "Iteration: 1800/277000 \t Elapsed time: 440.21 \t Loss:4.61248\n",
      "Iteration: 1801/277000 \t Elapsed time: 440.36 \t Loss:4.60353\n",
      "Iteration: 1802/277000 \t Elapsed time: 440.95 \t Loss:4.59548\n",
      "Iteration: 1803/277000 \t Elapsed time: 441.15 \t Loss:4.58570\n",
      "Iteration: 1804/277000 \t Elapsed time: 441.31 \t Loss:4.57601\n",
      "Iteration: 1805/277000 \t Elapsed time: 441.51 \t Loss:4.56877\n",
      "Iteration: 1806/277000 \t Elapsed time: 441.69 \t Loss:4.55775\n",
      "Iteration: 1807/277000 \t Elapsed time: 441.85 \t Loss:4.54942\n",
      "Iteration: 1808/277000 \t Elapsed time: 442.10 \t Loss:4.54497\n",
      "Iteration: 1809/277000 \t Elapsed time: 442.28 \t Loss:4.53327\n",
      "Iteration: 1810/277000 \t Elapsed time: 442.46 \t Loss:4.52433\n",
      "Iteration: 1811/277000 \t Elapsed time: 442.68 \t Loss:4.51995\n",
      "Iteration: 1812/277000 \t Elapsed time: 442.87 \t Loss:4.50798\n",
      "Iteration: 1813/277000 \t Elapsed time: 443.03 \t Loss:4.50192\n",
      "Iteration: 1814/277000 \t Elapsed time: 443.23 \t Loss:4.49150\n",
      "Iteration: 1815/277000 \t Elapsed time: 443.41 \t Loss:4.47944\n",
      "Iteration: 1816/277000 \t Elapsed time: 443.58 \t Loss:4.47558\n",
      "Iteration: 1817/277000 \t Elapsed time: 443.79 \t Loss:4.46392\n",
      "Iteration: 1818/277000 \t Elapsed time: 443.96 \t Loss:4.45278\n",
      "Iteration: 1819/277000 \t Elapsed time: 444.13 \t Loss:4.44519\n",
      "Iteration: 1820/277000 \t Elapsed time: 444.32 \t Loss:4.43597\n",
      "Iteration: 1821/277000 \t Elapsed time: 444.62 \t Loss:4.42756\n",
      "Iteration: 1822/277000 \t Elapsed time: 444.79 \t Loss:4.41652\n",
      "Iteration: 1823/277000 \t Elapsed time: 444.95 \t Loss:4.40841\n",
      "Iteration: 1824/277000 \t Elapsed time: 445.11 \t Loss:4.39959\n",
      "Iteration: 1825/277000 \t Elapsed time: 445.36 \t Loss:4.39994\n",
      "Iteration: 1826/277000 \t Elapsed time: 445.88 \t Loss:4.38725\n",
      "Iteration: 1827/277000 \t Elapsed time: 446.05 \t Loss:4.37789\n",
      "Iteration: 1828/277000 \t Elapsed time: 446.22 \t Loss:4.36673\n",
      "Iteration: 1829/277000 \t Elapsed time: 446.38 \t Loss:4.36094\n",
      "Iteration: 1830/277000 \t Elapsed time: 446.56 \t Loss:4.35371\n",
      "Iteration: 1831/277000 \t Elapsed time: 446.78 \t Loss:4.34071\n",
      "Iteration: 1832/277000 \t Elapsed time: 446.99 \t Loss:4.33786\n",
      "Iteration: 1833/277000 \t Elapsed time: 447.16 \t Loss:4.32856\n",
      "Iteration: 1834/277000 \t Elapsed time: 447.36 \t Loss:4.32483\n",
      "Iteration: 1835/277000 \t Elapsed time: 447.56 \t Loss:4.30488\n",
      "Iteration: 1836/277000 \t Elapsed time: 447.72 \t Loss:4.29844\n",
      "Iteration: 1837/277000 \t Elapsed time: 447.88 \t Loss:4.29002\n",
      "Iteration: 1838/277000 \t Elapsed time: 448.06 \t Loss:4.28005\n",
      "Iteration: 1839/277000 \t Elapsed time: 448.26 \t Loss:4.27558\n",
      "Iteration: 1840/277000 \t Elapsed time: 448.43 \t Loss:4.26540\n",
      "Iteration: 1841/277000 \t Elapsed time: 448.60 \t Loss:4.25128\n",
      "Iteration: 1842/277000 \t Elapsed time: 448.79 \t Loss:4.24386\n",
      "Iteration: 1843/277000 \t Elapsed time: 449.02 \t Loss:4.23720\n",
      "Iteration: 1844/277000 \t Elapsed time: 449.18 \t Loss:4.23186\n",
      "Iteration: 1845/277000 \t Elapsed time: 449.35 \t Loss:4.21614\n",
      "Iteration: 1846/277000 \t Elapsed time: 449.52 \t Loss:4.21022\n",
      "Iteration: 1847/277000 \t Elapsed time: 449.68 \t Loss:4.20108\n",
      "Iteration: 1848/277000 \t Elapsed time: 449.84 \t Loss:4.19492\n",
      "Iteration: 1849/277000 \t Elapsed time: 450.03 \t Loss:4.18183\n",
      "Iteration: 1850/277000 \t Elapsed time: 450.21 \t Loss:4.17267\n",
      "Iteration: 1851/277000 \t Elapsed time: 450.37 \t Loss:4.16481\n",
      "Iteration: 1852/277000 \t Elapsed time: 450.53 \t Loss:4.15727\n",
      "Iteration: 1853/277000 \t Elapsed time: 450.70 \t Loss:4.15768\n",
      "Iteration: 1854/277000 \t Elapsed time: 450.85 \t Loss:4.14353\n",
      "Iteration: 1855/277000 \t Elapsed time: 451.05 \t Loss:4.13456\n",
      "Iteration: 1856/277000 \t Elapsed time: 451.22 \t Loss:4.13163\n",
      "Iteration: 1857/277000 \t Elapsed time: 451.38 \t Loss:4.12500\n",
      "Iteration: 1858/277000 \t Elapsed time: 451.57 \t Loss:4.10919\n",
      "Iteration: 1859/277000 \t Elapsed time: 451.74 \t Loss:4.11432\n",
      "Iteration: 1860/277000 \t Elapsed time: 451.90 \t Loss:4.12025\n",
      "Iteration: 1861/277000 \t Elapsed time: 452.07 \t Loss:4.14772\n",
      "Iteration: 1862/277000 \t Elapsed time: 452.24 \t Loss:4.15777\n",
      "Iteration: 1863/277000 \t Elapsed time: 452.42 \t Loss:4.08390\n",
      "Iteration: 1864/277000 \t Elapsed time: 452.59 \t Loss:4.10105\n",
      "Iteration: 1865/277000 \t Elapsed time: 452.77 \t Loss:4.07301\n",
      "Iteration: 1866/277000 \t Elapsed time: 452.97 \t Loss:4.08480\n",
      "Iteration: 1867/277000 \t Elapsed time: 453.14 \t Loss:4.06273\n",
      "Iteration: 1868/277000 \t Elapsed time: 453.30 \t Loss:4.04728\n",
      "Iteration: 1869/277000 \t Elapsed time: 453.46 \t Loss:4.04257\n",
      "Iteration: 1870/277000 \t Elapsed time: 453.65 \t Loss:4.03420\n",
      "Iteration: 1871/277000 \t Elapsed time: 453.81 \t Loss:4.01829\n",
      "Iteration: 1872/277000 \t Elapsed time: 453.97 \t Loss:4.01646\n",
      "Iteration: 1873/277000 \t Elapsed time: 454.13 \t Loss:3.99409\n",
      "Iteration: 1874/277000 \t Elapsed time: 454.30 \t Loss:4.01192\n",
      "Iteration: 1875/277000 \t Elapsed time: 454.46 \t Loss:3.97349\n",
      "Iteration: 1876/277000 \t Elapsed time: 454.62 \t Loss:3.97630\n",
      "Iteration: 1877/277000 \t Elapsed time: 454.81 \t Loss:3.95991\n",
      "Iteration: 1878/277000 \t Elapsed time: 454.99 \t Loss:3.95616\n",
      "Iteration: 1879/277000 \t Elapsed time: 455.16 \t Loss:3.95633\n",
      "Iteration: 1880/277000 \t Elapsed time: 455.32 \t Loss:3.92889\n",
      "Iteration: 1881/277000 \t Elapsed time: 455.48 \t Loss:3.94210\n",
      "Iteration: 1882/277000 \t Elapsed time: 455.65 \t Loss:3.93473\n",
      "Iteration: 1883/277000 \t Elapsed time: 455.84 \t Loss:3.91425\n",
      "Iteration: 1884/277000 \t Elapsed time: 456.00 \t Loss:3.92359\n",
      "Iteration: 1885/277000 \t Elapsed time: 456.18 \t Loss:3.90831\n",
      "Iteration: 1886/277000 \t Elapsed time: 456.38 \t Loss:3.88390\n",
      "Iteration: 1887/277000 \t Elapsed time: 456.54 \t Loss:3.88634\n",
      "Iteration: 1888/277000 \t Elapsed time: 456.71 \t Loss:3.87718\n",
      "Iteration: 1889/277000 \t Elapsed time: 456.92 \t Loss:3.86871\n",
      "Iteration: 1890/277000 \t Elapsed time: 457.10 \t Loss:3.84943\n",
      "Iteration: 1891/277000 \t Elapsed time: 457.28 \t Loss:3.85133\n",
      "Iteration: 1892/277000 \t Elapsed time: 457.46 \t Loss:3.84454\n",
      "Iteration: 1893/277000 \t Elapsed time: 457.67 \t Loss:3.84028\n",
      "Iteration: 1894/277000 \t Elapsed time: 457.84 \t Loss:3.82114\n",
      "Iteration: 1895/277000 \t Elapsed time: 458.01 \t Loss:3.83153\n",
      "Iteration: 1896/277000 \t Elapsed time: 458.17 \t Loss:3.80685\n",
      "Iteration: 1897/277000 \t Elapsed time: 458.34 \t Loss:3.80461\n",
      "Iteration: 1898/277000 \t Elapsed time: 458.54 \t Loss:3.79242\n",
      "Iteration: 1899/277000 \t Elapsed time: 458.74 \t Loss:3.77646\n",
      "Iteration: 1900/277000 \t Elapsed time: 459.15 \t Loss:3.77367\n",
      "Iteration: 1901/277000 \t Elapsed time: 459.33 \t Loss:3.76066\n",
      "Iteration: 1902/277000 \t Elapsed time: 459.50 \t Loss:3.75365\n",
      "Iteration: 1903/277000 \t Elapsed time: 459.72 \t Loss:3.74581\n",
      "Iteration: 1904/277000 \t Elapsed time: 459.90 \t Loss:3.73481\n",
      "Iteration: 1905/277000 \t Elapsed time: 460.13 \t Loss:3.74141\n",
      "Iteration: 1906/277000 \t Elapsed time: 460.33 \t Loss:3.72115\n",
      "Iteration: 1907/277000 \t Elapsed time: 460.54 \t Loss:3.71680\n",
      "Iteration: 1908/277000 \t Elapsed time: 460.73 \t Loss:3.70406\n",
      "Iteration: 1909/277000 \t Elapsed time: 460.90 \t Loss:3.69379\n",
      "Iteration: 1910/277000 \t Elapsed time: 461.07 \t Loss:3.68799\n",
      "Iteration: 1911/277000 \t Elapsed time: 461.23 \t Loss:3.67815\n",
      "Iteration: 1912/277000 \t Elapsed time: 461.39 \t Loss:3.67405\n",
      "Iteration: 1913/277000 \t Elapsed time: 461.56 \t Loss:3.67533\n",
      "Iteration: 1914/277000 \t Elapsed time: 461.74 \t Loss:3.66857\n",
      "Iteration: 1915/277000 \t Elapsed time: 461.92 \t Loss:3.66181\n",
      "Iteration: 1916/277000 \t Elapsed time: 462.12 \t Loss:3.65000\n",
      "Iteration: 1917/277000 \t Elapsed time: 462.31 \t Loss:3.63824\n",
      "Iteration: 1918/277000 \t Elapsed time: 462.47 \t Loss:3.63416\n",
      "Iteration: 1919/277000 \t Elapsed time: 462.63 \t Loss:3.61843\n",
      "Iteration: 1920/277000 \t Elapsed time: 462.80 \t Loss:3.61967\n",
      "Iteration: 1921/277000 \t Elapsed time: 463.02 \t Loss:3.61780\n",
      "Iteration: 1922/277000 \t Elapsed time: 463.18 \t Loss:3.62225\n",
      "Iteration: 1923/277000 \t Elapsed time: 463.36 \t Loss:3.62155\n",
      "Iteration: 1924/277000 \t Elapsed time: 463.58 \t Loss:3.59239\n",
      "Iteration: 1925/277000 \t Elapsed time: 463.76 \t Loss:3.57178\n",
      "Iteration: 1926/277000 \t Elapsed time: 463.93 \t Loss:3.57188\n",
      "Iteration: 1927/277000 \t Elapsed time: 464.10 \t Loss:3.57131\n",
      "Iteration: 1928/277000 \t Elapsed time: 464.26 \t Loss:3.54907\n",
      "Iteration: 1929/277000 \t Elapsed time: 464.42 \t Loss:3.54846\n",
      "Iteration: 1930/277000 \t Elapsed time: 464.58 \t Loss:3.54368\n",
      "Iteration: 1931/277000 \t Elapsed time: 464.78 \t Loss:3.52416\n",
      "Iteration: 1932/277000 \t Elapsed time: 464.98 \t Loss:3.51862\n",
      "Iteration: 1933/277000 \t Elapsed time: 465.15 \t Loss:3.50693\n",
      "Iteration: 1934/277000 \t Elapsed time: 465.32 \t Loss:3.49976\n",
      "Iteration: 1935/277000 \t Elapsed time: 465.49 \t Loss:3.49098\n",
      "Iteration: 1936/277000 \t Elapsed time: 465.65 \t Loss:3.48294\n",
      "Iteration: 1937/277000 \t Elapsed time: 465.81 \t Loss:3.47465\n",
      "Iteration: 1938/277000 \t Elapsed time: 465.97 \t Loss:3.46593\n",
      "Iteration: 1939/277000 \t Elapsed time: 466.20 \t Loss:3.45902\n",
      "Iteration: 1940/277000 \t Elapsed time: 466.39 \t Loss:3.45052\n",
      "Iteration: 1941/277000 \t Elapsed time: 466.57 \t Loss:3.44305\n",
      "Iteration: 1942/277000 \t Elapsed time: 466.74 \t Loss:3.44102\n",
      "Iteration: 1943/277000 \t Elapsed time: 466.90 \t Loss:3.43140\n",
      "Iteration: 1944/277000 \t Elapsed time: 467.07 \t Loss:3.42183\n",
      "Iteration: 1945/277000 \t Elapsed time: 467.23 \t Loss:3.41473\n",
      "Iteration: 1946/277000 \t Elapsed time: 467.39 \t Loss:3.41463\n",
      "Iteration: 1947/277000 \t Elapsed time: 467.57 \t Loss:3.39911\n",
      "Iteration: 1948/277000 \t Elapsed time: 467.76 \t Loss:3.39219\n",
      "Iteration: 1949/277000 \t Elapsed time: 467.93 \t Loss:3.38547\n",
      "Iteration: 1950/277000 \t Elapsed time: 468.36 \t Loss:3.38551\n",
      "Iteration: 1951/277000 \t Elapsed time: 468.59 \t Loss:3.37488\n",
      "Iteration: 1952/277000 \t Elapsed time: 468.75 \t Loss:3.37126\n",
      "Iteration: 1953/277000 \t Elapsed time: 468.92 \t Loss:3.35423\n",
      "Iteration: 1954/277000 \t Elapsed time: 469.11 \t Loss:3.34919\n",
      "Iteration: 1955/277000 \t Elapsed time: 469.29 \t Loss:3.34315\n",
      "Iteration: 1956/277000 \t Elapsed time: 469.45 \t Loss:3.32877\n",
      "Iteration: 1957/277000 \t Elapsed time: 469.62 \t Loss:3.33875\n",
      "Iteration: 1958/277000 \t Elapsed time: 469.79 \t Loss:3.31517\n",
      "Iteration: 1959/277000 \t Elapsed time: 469.96 \t Loss:3.31990\n",
      "Iteration: 1960/277000 \t Elapsed time: 470.17 \t Loss:3.31421\n",
      "Iteration: 1961/277000 \t Elapsed time: 470.39 \t Loss:3.29835\n",
      "Iteration: 1962/277000 \t Elapsed time: 470.62 \t Loss:3.28685\n",
      "Iteration: 1963/277000 \t Elapsed time: 470.79 \t Loss:3.27577\n",
      "Iteration: 1964/277000 \t Elapsed time: 470.97 \t Loss:3.26592\n",
      "Iteration: 1965/277000 \t Elapsed time: 471.14 \t Loss:3.27814\n",
      "Iteration: 1966/277000 \t Elapsed time: 471.30 \t Loss:3.27796\n",
      "Iteration: 1967/277000 \t Elapsed time: 471.46 \t Loss:3.24499\n",
      "Iteration: 1968/277000 \t Elapsed time: 471.62 \t Loss:3.25398\n",
      "Iteration: 1969/277000 \t Elapsed time: 471.80 \t Loss:3.24058\n",
      "Iteration: 1970/277000 \t Elapsed time: 471.97 \t Loss:3.22641\n",
      "Iteration: 1971/277000 \t Elapsed time: 472.13 \t Loss:3.22895\n",
      "Iteration: 1972/277000 \t Elapsed time: 472.30 \t Loss:3.20892\n",
      "Iteration: 1973/277000 \t Elapsed time: 472.46 \t Loss:3.20481\n",
      "Iteration: 1974/277000 \t Elapsed time: 472.63 \t Loss:3.19528\n",
      "Iteration: 1975/277000 \t Elapsed time: 472.82 \t Loss:3.19114\n",
      "Iteration: 1976/277000 \t Elapsed time: 473.02 \t Loss:3.18198\n",
      "Iteration: 1977/277000 \t Elapsed time: 473.19 \t Loss:3.17118\n",
      "Iteration: 1978/277000 \t Elapsed time: 473.36 \t Loss:3.16328\n",
      "Iteration: 1979/277000 \t Elapsed time: 473.52 \t Loss:3.15461\n",
      "Iteration: 1980/277000 \t Elapsed time: 473.69 \t Loss:3.14432\n",
      "Iteration: 1981/277000 \t Elapsed time: 473.85 \t Loss:3.15128\n",
      "Iteration: 1982/277000 \t Elapsed time: 474.02 \t Loss:3.13761\n",
      "Iteration: 1983/277000 \t Elapsed time: 474.18 \t Loss:3.12651\n",
      "Iteration: 1984/277000 \t Elapsed time: 474.41 \t Loss:3.11863\n",
      "Iteration: 1985/277000 \t Elapsed time: 474.59 \t Loss:3.11200\n",
      "Iteration: 1986/277000 \t Elapsed time: 474.75 \t Loss:3.11407\n",
      "Iteration: 1987/277000 \t Elapsed time: 474.93 \t Loss:3.10696\n",
      "Iteration: 1988/277000 \t Elapsed time: 475.09 \t Loss:3.10426\n",
      "Iteration: 1989/277000 \t Elapsed time: 475.25 \t Loss:3.08284\n",
      "Iteration: 1990/277000 \t Elapsed time: 475.42 \t Loss:3.07840\n",
      "Iteration: 1991/277000 \t Elapsed time: 475.59 \t Loss:3.07124\n",
      "Iteration: 1992/277000 \t Elapsed time: 475.79 \t Loss:3.05662\n",
      "Iteration: 1993/277000 \t Elapsed time: 475.98 \t Loss:3.06003\n",
      "Iteration: 1994/277000 \t Elapsed time: 476.14 \t Loss:3.05459\n",
      "Iteration: 1995/277000 \t Elapsed time: 476.31 \t Loss:3.03880\n",
      "Iteration: 1996/277000 \t Elapsed time: 476.48 \t Loss:3.03304\n",
      "Iteration: 1997/277000 \t Elapsed time: 476.78 \t Loss:3.02620\n",
      "Iteration: 1998/277000 \t Elapsed time: 476.96 \t Loss:3.01945\n",
      "Iteration: 1999/277000 \t Elapsed time: 477.14 \t Loss:3.01665\n",
      "Iteration: 2000/277000 \t Elapsed time: 477.30 \t Loss:2.99696\n",
      "Start Validating\n",
      "Finish Validating\n",
      "Iteration: 2001/277000 \t Elapsed time: 578.22 \t Loss:2.99310\n",
      "Iteration: 2002/277000 \t Elapsed time: 578.40 \t Loss:2.99020\n",
      "Iteration: 2003/277000 \t Elapsed time: 578.57 \t Loss:2.98037\n",
      "Iteration: 2004/277000 \t Elapsed time: 578.74 \t Loss:2.97530\n",
      "Iteration: 2005/277000 \t Elapsed time: 578.90 \t Loss:2.97254\n",
      "Iteration: 2006/277000 \t Elapsed time: 579.07 \t Loss:2.96455\n",
      "Iteration: 2007/277000 \t Elapsed time: 579.26 \t Loss:2.95342\n",
      "Iteration: 2008/277000 \t Elapsed time: 579.47 \t Loss:2.94110\n",
      "Iteration: 2009/277000 \t Elapsed time: 579.64 \t Loss:2.93365\n",
      "Iteration: 2010/277000 \t Elapsed time: 579.81 \t Loss:2.93374\n",
      "Iteration: 2011/277000 \t Elapsed time: 579.97 \t Loss:2.92175\n",
      "Iteration: 2012/277000 \t Elapsed time: 580.13 \t Loss:2.90677\n",
      "Iteration: 2013/277000 \t Elapsed time: 580.34 \t Loss:2.90166\n",
      "Iteration: 2014/277000 \t Elapsed time: 580.51 \t Loss:2.89185\n",
      "Iteration: 2015/277000 \t Elapsed time: 580.68 \t Loss:2.88979\n",
      "Iteration: 2016/277000 \t Elapsed time: 580.83 \t Loss:2.87924\n",
      "Iteration: 2017/277000 \t Elapsed time: 580.99 \t Loss:2.88025\n",
      "Iteration: 2018/277000 \t Elapsed time: 581.18 \t Loss:2.86420\n",
      "Iteration: 2019/277000 \t Elapsed time: 581.34 \t Loss:2.85974\n",
      "Iteration: 2020/277000 \t Elapsed time: 581.50 \t Loss:2.84612\n",
      "Iteration: 2021/277000 \t Elapsed time: 581.66 \t Loss:2.83839\n",
      "Iteration: 2022/277000 \t Elapsed time: 581.83 \t Loss:2.83486\n",
      "Iteration: 2023/277000 \t Elapsed time: 581.99 \t Loss:2.82500\n",
      "Iteration: 2024/277000 \t Elapsed time: 582.15 \t Loss:2.82791\n",
      "Iteration: 2025/277000 \t Elapsed time: 582.31 \t Loss:2.81117\n",
      "Iteration: 2026/277000 \t Elapsed time: 582.49 \t Loss:2.80656\n",
      "Iteration: 2027/277000 \t Elapsed time: 582.65 \t Loss:2.80394\n",
      "Iteration: 2028/277000 \t Elapsed time: 582.82 \t Loss:2.79457\n",
      "Iteration: 2029/277000 \t Elapsed time: 582.98 \t Loss:2.79216\n",
      "Iteration: 2030/277000 \t Elapsed time: 583.16 \t Loss:2.77630\n",
      "Iteration: 2031/277000 \t Elapsed time: 583.34 \t Loss:2.78658\n",
      "Iteration: 2032/277000 \t Elapsed time: 583.50 \t Loss:2.78809\n",
      "Iteration: 2033/277000 \t Elapsed time: 583.67 \t Loss:2.81490\n",
      "Iteration: 2034/277000 \t Elapsed time: 583.83 \t Loss:2.81117\n",
      "Iteration: 2035/277000 \t Elapsed time: 584.00 \t Loss:2.75481\n",
      "Iteration: 2036/277000 \t Elapsed time: 584.21 \t Loss:2.75381\n",
      "Iteration: 2037/277000 \t Elapsed time: 584.36 \t Loss:2.77803\n",
      "Iteration: 2038/277000 \t Elapsed time: 584.53 \t Loss:2.74252\n",
      "Iteration: 2039/277000 \t Elapsed time: 584.73 \t Loss:2.73029\n",
      "Iteration: 2040/277000 \t Elapsed time: 584.91 \t Loss:2.71200\n",
      "Iteration: 2041/277000 \t Elapsed time: 585.09 \t Loss:2.74171\n",
      "Iteration: 2042/277000 \t Elapsed time: 585.29 \t Loss:2.72888\n",
      "Iteration: 2043/277000 \t Elapsed time: 585.48 \t Loss:2.74807\n",
      "Iteration: 2044/277000 \t Elapsed time: 585.65 \t Loss:2.75815\n",
      "Iteration: 2045/277000 \t Elapsed time: 585.83 \t Loss:2.74391\n",
      "Iteration: 2046/277000 \t Elapsed time: 586.00 \t Loss:2.70434\n",
      "Iteration: 2047/277000 \t Elapsed time: 586.18 \t Loss:2.77084\n",
      "Iteration: 2048/277000 \t Elapsed time: 586.38 \t Loss:2.70168\n",
      "Iteration: 2049/277000 \t Elapsed time: 586.54 \t Loss:2.68077\n",
      "Iteration: 2050/277000 \t Elapsed time: 586.71 \t Loss:2.68375\n",
      "Iteration: 2051/277000 \t Elapsed time: 586.87 \t Loss:2.65797\n",
      "Iteration: 2052/277000 \t Elapsed time: 587.04 \t Loss:2.65777\n",
      "Iteration: 2053/277000 \t Elapsed time: 587.21 \t Loss:2.66063\n",
      "Iteration: 2054/277000 \t Elapsed time: 587.43 \t Loss:2.62932\n",
      "Iteration: 2055/277000 \t Elapsed time: 587.64 \t Loss:2.62759\n",
      "Iteration: 2056/277000 \t Elapsed time: 587.80 \t Loss:2.63277\n",
      "Iteration: 2057/277000 \t Elapsed time: 587.97 \t Loss:2.60094\n",
      "Iteration: 2058/277000 \t Elapsed time: 588.16 \t Loss:2.59347\n",
      "Iteration: 2059/277000 \t Elapsed time: 588.33 \t Loss:2.59723\n",
      "Iteration: 2060/277000 \t Elapsed time: 588.49 \t Loss:2.58304\n",
      "Iteration: 2061/277000 \t Elapsed time: 588.67 \t Loss:2.57496\n",
      "Iteration: 2062/277000 \t Elapsed time: 588.84 \t Loss:2.58423\n",
      "Iteration: 2063/277000 \t Elapsed time: 589.13 \t Loss:2.56705\n",
      "Iteration: 2064/277000 \t Elapsed time: 589.29 \t Loss:2.57448\n",
      "Iteration: 2065/277000 \t Elapsed time: 589.49 \t Loss:2.64019\n",
      "Iteration: 2066/277000 \t Elapsed time: 589.65 \t Loss:2.57277\n",
      "Iteration: 2067/277000 \t Elapsed time: 589.81 \t Loss:2.58791\n",
      "Iteration: 2068/277000 \t Elapsed time: 589.97 \t Loss:2.61028\n",
      "Iteration: 2069/277000 \t Elapsed time: 590.17 \t Loss:2.57346\n",
      "Iteration: 2070/277000 \t Elapsed time: 590.33 \t Loss:2.55503\n",
      "Iteration: 2071/277000 \t Elapsed time: 590.49 \t Loss:2.54588\n",
      "Iteration: 2072/277000 \t Elapsed time: 590.65 \t Loss:2.53641\n",
      "Iteration: 2073/277000 \t Elapsed time: 590.81 \t Loss:2.52025\n",
      "Iteration: 2074/277000 \t Elapsed time: 590.98 \t Loss:2.51041\n",
      "Iteration: 2075/277000 \t Elapsed time: 591.14 \t Loss:2.48572\n",
      "Iteration: 2076/277000 \t Elapsed time: 591.30 \t Loss:2.48995\n",
      "Iteration: 2077/277000 \t Elapsed time: 591.47 \t Loss:2.47701\n",
      "Iteration: 2078/277000 \t Elapsed time: 591.64 \t Loss:2.46794\n",
      "Iteration: 2079/277000 \t Elapsed time: 591.81 \t Loss:2.45674\n",
      "Iteration: 2080/277000 \t Elapsed time: 591.98 \t Loss:2.45105\n",
      "Iteration: 2081/277000 \t Elapsed time: 592.14 \t Loss:2.44485\n",
      "Iteration: 2082/277000 \t Elapsed time: 592.30 \t Loss:2.44100\n",
      "Iteration: 2083/277000 \t Elapsed time: 592.47 \t Loss:2.42920\n",
      "Iteration: 2084/277000 \t Elapsed time: 592.63 \t Loss:2.41685\n",
      "Iteration: 2085/277000 \t Elapsed time: 592.81 \t Loss:2.41187\n",
      "Iteration: 2086/277000 \t Elapsed time: 592.98 \t Loss:2.40942\n",
      "Iteration: 2087/277000 \t Elapsed time: 593.17 \t Loss:2.39495\n",
      "Iteration: 2088/277000 \t Elapsed time: 593.35 \t Loss:2.41010\n",
      "Iteration: 2089/277000 \t Elapsed time: 593.51 \t Loss:2.40259\n",
      "Iteration: 2090/277000 \t Elapsed time: 593.68 \t Loss:2.39276\n",
      "Iteration: 2091/277000 \t Elapsed time: 593.86 \t Loss:2.37198\n",
      "Iteration: 2092/277000 \t Elapsed time: 594.03 \t Loss:2.36653\n",
      "Iteration: 2093/277000 \t Elapsed time: 594.19 \t Loss:2.38157\n",
      "Iteration: 2094/277000 \t Elapsed time: 594.36 \t Loss:2.34874\n",
      "Iteration: 2095/277000 \t Elapsed time: 594.52 \t Loss:2.35020\n",
      "Iteration: 2096/277000 \t Elapsed time: 594.69 \t Loss:2.33907\n",
      "Iteration: 2097/277000 \t Elapsed time: 594.90 \t Loss:2.32959\n",
      "Iteration: 2098/277000 \t Elapsed time: 595.08 \t Loss:2.32493\n",
      "Iteration: 2099/277000 \t Elapsed time: 595.24 \t Loss:2.34293\n",
      "Iteration: 2100/277000 \t Elapsed time: 595.41 \t Loss:2.34315\n",
      "Iteration: 2101/277000 \t Elapsed time: 595.57 \t Loss:2.34226\n",
      "Iteration: 2102/277000 \t Elapsed time: 595.74 \t Loss:2.32610\n",
      "Iteration: 2103/277000 \t Elapsed time: 595.92 \t Loss:2.31367\n",
      "Iteration: 2104/277000 \t Elapsed time: 596.09 \t Loss:2.35385\n",
      "Iteration: 2105/277000 \t Elapsed time: 596.28 \t Loss:2.29296\n",
      "Iteration: 2106/277000 \t Elapsed time: 596.45 \t Loss:2.31851\n",
      "Iteration: 2107/277000 \t Elapsed time: 596.61 \t Loss:2.32712\n",
      "Iteration: 2108/277000 \t Elapsed time: 596.79 \t Loss:2.26020\n",
      "Iteration: 2109/277000 \t Elapsed time: 597.01 \t Loss:2.28668\n",
      "Iteration: 2110/277000 \t Elapsed time: 597.17 \t Loss:2.25872\n",
      "Iteration: 2111/277000 \t Elapsed time: 597.34 \t Loss:2.25674\n",
      "Iteration: 2112/277000 \t Elapsed time: 597.51 \t Loss:2.24110\n",
      "Iteration: 2113/277000 \t Elapsed time: 597.67 \t Loss:2.22875\n",
      "Iteration: 2114/277000 \t Elapsed time: 597.83 \t Loss:2.22562\n",
      "Iteration: 2115/277000 \t Elapsed time: 598.00 \t Loss:2.22838\n",
      "Iteration: 2116/277000 \t Elapsed time: 598.16 \t Loss:2.21990\n",
      "Iteration: 2117/277000 \t Elapsed time: 598.33 \t Loss:2.20519\n",
      "Iteration: 2118/277000 \t Elapsed time: 598.51 \t Loss:2.19685\n",
      "Iteration: 2119/277000 \t Elapsed time: 598.67 \t Loss:2.22951\n",
      "Iteration: 2120/277000 \t Elapsed time: 598.85 \t Loss:2.19827\n",
      "Iteration: 2121/277000 \t Elapsed time: 599.02 \t Loss:2.17976\n",
      "Iteration: 2122/277000 \t Elapsed time: 599.23 \t Loss:2.18485\n",
      "Iteration: 2123/277000 \t Elapsed time: 599.39 \t Loss:2.18935\n",
      "Iteration: 2124/277000 \t Elapsed time: 599.56 \t Loss:2.20035\n",
      "Iteration: 2125/277000 \t Elapsed time: 599.73 \t Loss:2.18595\n",
      "Iteration: 2126/277000 \t Elapsed time: 599.92 \t Loss:2.15027\n",
      "Iteration: 2127/277000 \t Elapsed time: 600.11 \t Loss:2.18031\n",
      "Iteration: 2128/277000 \t Elapsed time: 600.29 \t Loss:2.15113\n",
      "Iteration: 2129/277000 \t Elapsed time: 600.45 \t Loss:2.12514\n",
      "Iteration: 2130/277000 \t Elapsed time: 600.62 \t Loss:2.13134\n",
      "Iteration: 2131/277000 \t Elapsed time: 600.78 \t Loss:2.12377\n",
      "Iteration: 2132/277000 \t Elapsed time: 600.96 \t Loss:2.10453\n",
      "Iteration: 2133/277000 \t Elapsed time: 601.11 \t Loss:2.10388\n",
      "Iteration: 2134/277000 \t Elapsed time: 601.29 \t Loss:2.09478\n",
      "Iteration: 2135/277000 \t Elapsed time: 601.48 \t Loss:2.08447\n",
      "Iteration: 2136/277000 \t Elapsed time: 601.65 \t Loss:2.09534\n",
      "Iteration: 2137/277000 \t Elapsed time: 601.81 \t Loss:2.08490\n",
      "Iteration: 2138/277000 \t Elapsed time: 601.97 \t Loss:2.06674\n",
      "Iteration: 2139/277000 \t Elapsed time: 602.14 \t Loss:2.06728\n",
      "Iteration: 2140/277000 \t Elapsed time: 602.31 \t Loss:2.05664\n",
      "Iteration: 2141/277000 \t Elapsed time: 602.51 \t Loss:2.04618\n",
      "Iteration: 2142/277000 \t Elapsed time: 602.68 \t Loss:2.04032\n",
      "Iteration: 2143/277000 \t Elapsed time: 602.91 \t Loss:2.04099\n",
      "Iteration: 2144/277000 \t Elapsed time: 603.10 \t Loss:2.04619\n",
      "Iteration: 2145/277000 \t Elapsed time: 603.26 \t Loss:2.01945\n",
      "Iteration: 2146/277000 \t Elapsed time: 603.43 \t Loss:2.04991\n",
      "Iteration: 2147/277000 \t Elapsed time: 603.59 \t Loss:2.04509\n",
      "Iteration: 2148/277000 \t Elapsed time: 603.76 \t Loss:2.03619\n",
      "Iteration: 2149/277000 \t Elapsed time: 603.92 \t Loss:2.00025\n",
      "Iteration: 2150/277000 \t Elapsed time: 604.10 \t Loss:1.99404\n",
      "Iteration: 2151/277000 \t Elapsed time: 604.29 \t Loss:1.98691\n",
      "Iteration: 2152/277000 \t Elapsed time: 604.46 \t Loss:1.99655\n",
      "Iteration: 2153/277000 \t Elapsed time: 604.62 \t Loss:1.98601\n",
      "Iteration: 2154/277000 \t Elapsed time: 604.79 \t Loss:1.97814\n",
      "Iteration: 2155/277000 \t Elapsed time: 604.95 \t Loss:1.96409\n",
      "Iteration: 2156/277000 \t Elapsed time: 605.11 \t Loss:1.95951\n",
      "Iteration: 2157/277000 \t Elapsed time: 605.27 \t Loss:1.94347\n",
      "Iteration: 2158/277000 \t Elapsed time: 605.43 \t Loss:1.94611\n",
      "Iteration: 2159/277000 \t Elapsed time: 605.60 \t Loss:1.93608\n",
      "Iteration: 2160/277000 \t Elapsed time: 605.78 \t Loss:1.94835\n",
      "Iteration: 2161/277000 \t Elapsed time: 605.96 \t Loss:1.93136\n",
      "Iteration: 2162/277000 \t Elapsed time: 606.13 \t Loss:1.94769\n",
      "Iteration: 2163/277000 \t Elapsed time: 606.31 \t Loss:1.91684\n",
      "Iteration: 2164/277000 \t Elapsed time: 606.48 \t Loss:1.92326\n",
      "Iteration: 2165/277000 \t Elapsed time: 606.67 \t Loss:1.89178\n",
      "Iteration: 2166/277000 \t Elapsed time: 606.86 \t Loss:1.93043\n",
      "Iteration: 2167/277000 \t Elapsed time: 607.02 \t Loss:1.88632\n",
      "Iteration: 2168/277000 \t Elapsed time: 607.19 \t Loss:1.88539\n",
      "Iteration: 2169/277000 \t Elapsed time: 607.35 \t Loss:1.87205\n",
      "Iteration: 2170/277000 \t Elapsed time: 607.54 \t Loss:1.86740\n",
      "Iteration: 2171/277000 \t Elapsed time: 607.76 \t Loss:1.85722\n",
      "Iteration: 2172/277000 \t Elapsed time: 607.95 \t Loss:1.84969\n",
      "Iteration: 2173/277000 \t Elapsed time: 608.11 \t Loss:1.84949\n",
      "Iteration: 2174/277000 \t Elapsed time: 608.27 \t Loss:1.84306\n",
      "Iteration: 2175/277000 \t Elapsed time: 608.44 \t Loss:1.83670\n",
      "Iteration: 2176/277000 \t Elapsed time: 608.61 \t Loss:1.82095\n",
      "Iteration: 2177/277000 \t Elapsed time: 608.79 \t Loss:1.81693\n",
      "Iteration: 2178/277000 \t Elapsed time: 608.95 \t Loss:1.83181\n",
      "Iteration: 2179/277000 \t Elapsed time: 609.11 \t Loss:1.81415\n",
      "Iteration: 2180/277000 \t Elapsed time: 609.28 \t Loss:1.80939\n",
      "Iteration: 2181/277000 \t Elapsed time: 609.45 \t Loss:1.84137\n",
      "Iteration: 2182/277000 \t Elapsed time: 609.62 \t Loss:1.83584\n",
      "Iteration: 2183/277000 \t Elapsed time: 609.79 \t Loss:1.79030\n",
      "Iteration: 2184/277000 \t Elapsed time: 609.95 \t Loss:1.79233\n",
      "Iteration: 2185/277000 \t Elapsed time: 610.12 \t Loss:1.79814\n",
      "Iteration: 2186/277000 \t Elapsed time: 610.29 \t Loss:1.79373\n",
      "Iteration: 2187/277000 \t Elapsed time: 610.45 \t Loss:1.78692\n",
      "Iteration: 2188/277000 \t Elapsed time: 610.64 \t Loss:1.80030\n",
      "Iteration: 2189/277000 \t Elapsed time: 611.38 \t Loss:1.75932\n",
      "Iteration: 2190/277000 \t Elapsed time: 611.59 \t Loss:1.75848\n",
      "Iteration: 2191/277000 \t Elapsed time: 611.76 \t Loss:1.75166\n",
      "Iteration: 2192/277000 \t Elapsed time: 611.96 \t Loss:1.74370\n",
      "Iteration: 2193/277000 \t Elapsed time: 612.14 \t Loss:1.73387\n",
      "Iteration: 2194/277000 \t Elapsed time: 612.31 \t Loss:1.74106\n",
      "Iteration: 2195/277000 \t Elapsed time: 612.48 \t Loss:1.72697\n",
      "Iteration: 2196/277000 \t Elapsed time: 612.64 \t Loss:1.71848\n",
      "Iteration: 2197/277000 \t Elapsed time: 612.80 \t Loss:1.72231\n",
      "Iteration: 2198/277000 \t Elapsed time: 613.00 \t Loss:1.70853\n",
      "Iteration: 2199/277000 \t Elapsed time: 613.18 \t Loss:1.71382\n",
      "Iteration: 2200/277000 \t Elapsed time: 613.34 \t Loss:1.71976\n",
      "Iteration: 2201/277000 \t Elapsed time: 613.50 \t Loss:1.71631\n",
      "Iteration: 2202/277000 \t Elapsed time: 613.68 \t Loss:1.67924\n",
      "Iteration: 2203/277000 \t Elapsed time: 613.85 \t Loss:1.72457\n",
      "Iteration: 2204/277000 \t Elapsed time: 614.05 \t Loss:1.70481\n",
      "Iteration: 2205/277000 \t Elapsed time: 614.24 \t Loss:1.68609\n",
      "Iteration: 2206/277000 \t Elapsed time: 614.43 \t Loss:1.66297\n",
      "Iteration: 2207/277000 \t Elapsed time: 614.60 \t Loss:1.65427\n",
      "Iteration: 2208/277000 \t Elapsed time: 614.76 \t Loss:1.65929\n",
      "Iteration: 2209/277000 \t Elapsed time: 614.92 \t Loss:1.62842\n",
      "Iteration: 2210/277000 \t Elapsed time: 615.09 \t Loss:1.74847\n",
      "Iteration: 2211/277000 \t Elapsed time: 615.26 \t Loss:1.64686\n",
      "Iteration: 2212/277000 \t Elapsed time: 615.43 \t Loss:1.65742\n",
      "Iteration: 2213/277000 \t Elapsed time: 615.60 \t Loss:1.67603\n",
      "Iteration: 2214/277000 \t Elapsed time: 615.81 \t Loss:1.66116\n",
      "Iteration: 2215/277000 \t Elapsed time: 615.98 \t Loss:1.67256\n",
      "Iteration: 2216/277000 \t Elapsed time: 616.16 \t Loss:1.63053\n",
      "Iteration: 2217/277000 \t Elapsed time: 616.33 \t Loss:1.65194\n",
      "Iteration: 2218/277000 \t Elapsed time: 616.52 \t Loss:1.66251\n",
      "Iteration: 2219/277000 \t Elapsed time: 616.68 \t Loss:1.69279\n",
      "Iteration: 2220/277000 \t Elapsed time: 616.84 \t Loss:1.67030\n",
      "Iteration: 2221/277000 \t Elapsed time: 617.00 \t Loss:1.64258\n",
      "Iteration: 2222/277000 \t Elapsed time: 617.16 \t Loss:1.65193\n",
      "Iteration: 2223/277000 \t Elapsed time: 617.32 \t Loss:1.57216\n",
      "Iteration: 2224/277000 \t Elapsed time: 617.49 \t Loss:1.63058\n",
      "Iteration: 2225/277000 \t Elapsed time: 617.65 \t Loss:1.57962\n",
      "Iteration: 2226/277000 \t Elapsed time: 617.83 \t Loss:1.59432\n",
      "Iteration: 2227/277000 \t Elapsed time: 618.02 \t Loss:1.54253\n",
      "Iteration: 2228/277000 \t Elapsed time: 618.20 \t Loss:1.56218\n",
      "Iteration: 2229/277000 \t Elapsed time: 618.38 \t Loss:1.58586\n",
      "Iteration: 2230/277000 \t Elapsed time: 618.54 \t Loss:1.54942\n",
      "Iteration: 2231/277000 \t Elapsed time: 618.71 \t Loss:1.53096\n",
      "Iteration: 2232/277000 \t Elapsed time: 618.91 \t Loss:1.50732\n",
      "Iteration: 2233/277000 \t Elapsed time: 619.09 \t Loss:1.61570\n",
      "Iteration: 2234/277000 \t Elapsed time: 619.25 \t Loss:1.50795\n",
      "Iteration: 2235/277000 \t Elapsed time: 619.44 \t Loss:1.48321\n",
      "Iteration: 2236/277000 \t Elapsed time: 619.61 \t Loss:1.50866\n",
      "Iteration: 2237/277000 \t Elapsed time: 619.79 \t Loss:1.51966\n",
      "Iteration: 2238/277000 \t Elapsed time: 619.96 \t Loss:1.47561\n",
      "Iteration: 2239/277000 \t Elapsed time: 620.12 \t Loss:1.47970\n",
      "Iteration: 2240/277000 \t Elapsed time: 620.28 \t Loss:1.56778\n",
      "Iteration: 2241/277000 \t Elapsed time: 620.46 \t Loss:1.53761\n",
      "Iteration: 2242/277000 \t Elapsed time: 620.64 \t Loss:1.52500\n",
      "Iteration: 2243/277000 \t Elapsed time: 620.80 \t Loss:1.48430\n",
      "Iteration: 2244/277000 \t Elapsed time: 620.98 \t Loss:1.65477\n",
      "Iteration: 2245/277000 \t Elapsed time: 621.15 \t Loss:1.48905\n",
      "Iteration: 2246/277000 \t Elapsed time: 621.33 \t Loss:1.68199\n",
      "Iteration: 2247/277000 \t Elapsed time: 621.48 \t Loss:1.56097\n",
      "Iteration: 2248/277000 \t Elapsed time: 621.65 \t Loss:1.80304\n",
      "Iteration: 2249/277000 \t Elapsed time: 621.81 \t Loss:1.54744\n",
      "Iteration: 2250/277000 \t Elapsed time: 621.98 \t Loss:1.56629\n",
      "Iteration: 2251/277000 \t Elapsed time: 622.15 \t Loss:1.56999\n",
      "Iteration: 2252/277000 \t Elapsed time: 622.32 \t Loss:1.52693\n",
      "Iteration: 2253/277000 \t Elapsed time: 622.48 \t Loss:1.43194\n",
      "Iteration: 2254/277000 \t Elapsed time: 622.67 \t Loss:1.57673\n",
      "Iteration: 2255/277000 \t Elapsed time: 622.83 \t Loss:1.47463\n",
      "Iteration: 2256/277000 \t Elapsed time: 623.07 \t Loss:1.41732\n",
      "Iteration: 2257/277000 \t Elapsed time: 623.26 \t Loss:1.43373\n",
      "Iteration: 2258/277000 \t Elapsed time: 623.43 \t Loss:1.43375\n",
      "Iteration: 2259/277000 \t Elapsed time: 623.61 \t Loss:1.40948\n",
      "Iteration: 2260/277000 \t Elapsed time: 623.77 \t Loss:1.53246\n",
      "Iteration: 2261/277000 \t Elapsed time: 623.93 \t Loss:1.41619\n",
      "Iteration: 2262/277000 \t Elapsed time: 624.10 \t Loss:1.41830\n",
      "Iteration: 2263/277000 \t Elapsed time: 624.27 \t Loss:1.42927\n",
      "Iteration: 2264/277000 \t Elapsed time: 624.42 \t Loss:1.43191\n",
      "Iteration: 2265/277000 \t Elapsed time: 624.59 \t Loss:1.38532\n",
      "Iteration: 2266/277000 \t Elapsed time: 624.77 \t Loss:1.37072\n",
      "Iteration: 2267/277000 \t Elapsed time: 624.95 \t Loss:1.34718\n",
      "Iteration: 2268/277000 \t Elapsed time: 625.11 \t Loss:1.35929\n",
      "Iteration: 2269/277000 \t Elapsed time: 625.28 \t Loss:1.36626\n",
      "Iteration: 2270/277000 \t Elapsed time: 625.44 \t Loss:1.47938\n",
      "Iteration: 2271/277000 \t Elapsed time: 625.60 \t Loss:1.38010\n",
      "Iteration: 2272/277000 \t Elapsed time: 625.76 \t Loss:1.36322\n",
      "Iteration: 2273/277000 \t Elapsed time: 625.96 \t Loss:1.39104\n",
      "Iteration: 2274/277000 \t Elapsed time: 626.12 \t Loss:1.40572\n",
      "Iteration: 2275/277000 \t Elapsed time: 626.29 \t Loss:1.46663\n",
      "Iteration: 2276/277000 \t Elapsed time: 626.45 \t Loss:1.37349\n",
      "Iteration: 2277/277000 \t Elapsed time: 626.62 \t Loss:1.35894\n",
      "Iteration: 2278/277000 \t Elapsed time: 626.81 \t Loss:1.34145\n",
      "Iteration: 2279/277000 \t Elapsed time: 626.99 \t Loss:1.30694\n",
      "Iteration: 2280/277000 \t Elapsed time: 627.18 \t Loss:1.32250\n",
      "Iteration: 2281/277000 \t Elapsed time: 627.34 \t Loss:1.30766\n",
      "Iteration: 2282/277000 \t Elapsed time: 627.51 \t Loss:1.29160\n",
      "Iteration: 2283/277000 \t Elapsed time: 627.68 \t Loss:1.28793\n",
      "Iteration: 2284/277000 \t Elapsed time: 627.85 \t Loss:1.33535\n",
      "Iteration: 2285/277000 \t Elapsed time: 628.02 \t Loss:1.25219\n",
      "Iteration: 2286/277000 \t Elapsed time: 628.19 \t Loss:1.29462\n",
      "Iteration: 2287/277000 \t Elapsed time: 628.36 \t Loss:1.23862\n",
      "Iteration: 2288/277000 \t Elapsed time: 628.53 \t Loss:1.23788\n",
      "Iteration: 2289/277000 \t Elapsed time: 628.72 \t Loss:1.41829\n",
      "Iteration: 2290/277000 \t Elapsed time: 628.87 \t Loss:1.24995\n",
      "Iteration: 2291/277000 \t Elapsed time: 629.06 \t Loss:1.24516\n",
      "Iteration: 2292/277000 \t Elapsed time: 629.24 \t Loss:1.25288\n",
      "Iteration: 2293/277000 \t Elapsed time: 629.41 \t Loss:1.33672\n",
      "Iteration: 2294/277000 \t Elapsed time: 629.58 \t Loss:1.27104\n",
      "Iteration: 2295/277000 \t Elapsed time: 629.76 \t Loss:1.27411\n",
      "Iteration: 2296/277000 \t Elapsed time: 629.94 \t Loss:1.27063\n",
      "Iteration: 2297/277000 \t Elapsed time: 630.12 \t Loss:1.22746\n",
      "Iteration: 2298/277000 \t Elapsed time: 630.31 \t Loss:1.23975\n",
      "Iteration: 2299/277000 \t Elapsed time: 630.47 \t Loss:1.23583\n",
      "Iteration: 2300/277000 \t Elapsed time: 630.64 \t Loss:1.27867\n",
      "Iteration: 2301/277000 \t Elapsed time: 630.84 \t Loss:1.18451\n",
      "Iteration: 2302/277000 \t Elapsed time: 631.02 \t Loss:1.35009\n",
      "Iteration: 2303/277000 \t Elapsed time: 631.25 \t Loss:1.21125\n",
      "Iteration: 2304/277000 \t Elapsed time: 631.42 \t Loss:1.42525\n",
      "Iteration: 2305/277000 \t Elapsed time: 631.58 \t Loss:1.24892\n",
      "Iteration: 2306/277000 \t Elapsed time: 631.75 \t Loss:1.21046\n",
      "Iteration: 2307/277000 \t Elapsed time: 631.93 \t Loss:1.19768\n",
      "Iteration: 2308/277000 \t Elapsed time: 632.09 \t Loss:1.20613\n",
      "Iteration: 2309/277000 \t Elapsed time: 632.29 \t Loss:1.16208\n",
      "Iteration: 2310/277000 \t Elapsed time: 632.47 \t Loss:1.15370\n",
      "Iteration: 2311/277000 \t Elapsed time: 632.64 \t Loss:1.29827\n",
      "Iteration: 2312/277000 \t Elapsed time: 632.80 \t Loss:1.25278\n",
      "Iteration: 2313/277000 \t Elapsed time: 632.97 \t Loss:1.19363\n",
      "Iteration: 2314/277000 \t Elapsed time: 633.14 \t Loss:1.21792\n",
      "Iteration: 2315/277000 \t Elapsed time: 633.30 \t Loss:1.15362\n",
      "Iteration: 2316/277000 \t Elapsed time: 633.49 \t Loss:1.13421\n",
      "Iteration: 2317/277000 \t Elapsed time: 633.68 \t Loss:1.12897\n",
      "Iteration: 2318/277000 \t Elapsed time: 633.85 \t Loss:1.11067\n",
      "Iteration: 2319/277000 \t Elapsed time: 634.01 \t Loss:1.13936\n",
      "Iteration: 2320/277000 \t Elapsed time: 634.17 \t Loss:1.20542\n",
      "Iteration: 2321/277000 \t Elapsed time: 634.33 \t Loss:1.11065\n",
      "Iteration: 2322/277000 \t Elapsed time: 634.49 \t Loss:1.09349\n",
      "Iteration: 2323/277000 \t Elapsed time: 634.65 \t Loss:1.07778\n",
      "Iteration: 2324/277000 \t Elapsed time: 634.82 \t Loss:1.07295\n",
      "Iteration: 2325/277000 \t Elapsed time: 635.00 \t Loss:1.07641\n",
      "Iteration: 2326/277000 \t Elapsed time: 635.18 \t Loss:1.06837\n",
      "Iteration: 2327/277000 \t Elapsed time: 635.34 \t Loss:1.04683\n",
      "Iteration: 2328/277000 \t Elapsed time: 635.51 \t Loss:1.03737\n",
      "Iteration: 2329/277000 \t Elapsed time: 635.67 \t Loss:1.06278\n",
      "Iteration: 2330/277000 \t Elapsed time: 635.83 \t Loss:1.05905\n",
      "Iteration: 2331/277000 \t Elapsed time: 636.03 \t Loss:1.01892\n",
      "Iteration: 2332/277000 \t Elapsed time: 636.20 \t Loss:1.09831\n",
      "Iteration: 2333/277000 \t Elapsed time: 636.36 \t Loss:1.01064\n",
      "Iteration: 2334/277000 \t Elapsed time: 636.53 \t Loss:0.99846\n",
      "Iteration: 2335/277000 \t Elapsed time: 636.69 \t Loss:0.99087\n",
      "Iteration: 2336/277000 \t Elapsed time: 636.85 \t Loss:0.98376\n",
      "Iteration: 2337/277000 \t Elapsed time: 637.02 \t Loss:0.98169\n",
      "Iteration: 2338/277000 \t Elapsed time: 637.18 \t Loss:1.01260\n",
      "Iteration: 2339/277000 \t Elapsed time: 637.34 \t Loss:0.96991\n",
      "Iteration: 2340/277000 \t Elapsed time: 637.51 \t Loss:0.96187\n",
      "Iteration: 2341/277000 \t Elapsed time: 637.68 \t Loss:0.95907\n",
      "Iteration: 2342/277000 \t Elapsed time: 637.84 \t Loss:0.95976\n",
      "Iteration: 2343/277000 \t Elapsed time: 638.00 \t Loss:0.94712\n",
      "Iteration: 2344/277000 \t Elapsed time: 638.26 \t Loss:0.94035\n",
      "Iteration: 2345/277000 \t Elapsed time: 638.44 \t Loss:0.93230\n",
      "Iteration: 2346/277000 \t Elapsed time: 638.60 \t Loss:0.92487\n",
      "Iteration: 2347/277000 \t Elapsed time: 638.78 \t Loss:0.92076\n",
      "Iteration: 2348/277000 \t Elapsed time: 638.99 \t Loss:0.91535\n",
      "Iteration: 2349/277000 \t Elapsed time: 639.18 \t Loss:0.94198\n",
      "Iteration: 2350/277000 \t Elapsed time: 639.39 \t Loss:0.90340\n",
      "Iteration: 2351/277000 \t Elapsed time: 639.56 \t Loss:0.89961\n",
      "Iteration: 2352/277000 \t Elapsed time: 639.73 \t Loss:0.93318\n",
      "Iteration: 2353/277000 \t Elapsed time: 639.89 \t Loss:0.89700\n",
      "Iteration: 2354/277000 \t Elapsed time: 640.06 \t Loss:0.88313\n",
      "Iteration: 2355/277000 \t Elapsed time: 640.27 \t Loss:0.88009\n",
      "Iteration: 2356/277000 \t Elapsed time: 640.43 \t Loss:0.87550\n",
      "Iteration: 2357/277000 \t Elapsed time: 640.60 \t Loss:0.86603\n",
      "Iteration: 2358/277000 \t Elapsed time: 640.76 \t Loss:0.85991\n",
      "Iteration: 2359/277000 \t Elapsed time: 640.94 \t Loss:0.87821\n",
      "Iteration: 2360/277000 \t Elapsed time: 641.11 \t Loss:0.84988\n",
      "Iteration: 2361/277000 \t Elapsed time: 641.29 \t Loss:0.84272\n",
      "Iteration: 2362/277000 \t Elapsed time: 641.47 \t Loss:0.84008\n",
      "Iteration: 2363/277000 \t Elapsed time: 641.64 \t Loss:0.83229\n",
      "Iteration: 2364/277000 \t Elapsed time: 641.80 \t Loss:0.82731\n",
      "Iteration: 2365/277000 \t Elapsed time: 641.96 \t Loss:0.82136\n",
      "Iteration: 2366/277000 \t Elapsed time: 642.13 \t Loss:0.81420\n",
      "Iteration: 2367/277000 \t Elapsed time: 642.29 \t Loss:0.81226\n",
      "Iteration: 2368/277000 \t Elapsed time: 642.49 \t Loss:0.80191\n",
      "Iteration: 2369/277000 \t Elapsed time: 642.64 \t Loss:0.80086\n",
      "Iteration: 2370/277000 \t Elapsed time: 642.81 \t Loss:0.79441\n",
      "Iteration: 2371/277000 \t Elapsed time: 642.97 \t Loss:0.78715\n",
      "Iteration: 2372/277000 \t Elapsed time: 643.14 \t Loss:0.92148\n",
      "Iteration: 2373/277000 \t Elapsed time: 643.30 \t Loss:0.78773\n",
      "Iteration: 2374/277000 \t Elapsed time: 643.47 \t Loss:0.78791\n",
      "Iteration: 2375/277000 \t Elapsed time: 643.63 \t Loss:0.82924\n",
      "Iteration: 2376/277000 \t Elapsed time: 643.79 \t Loss:0.77243\n",
      "Iteration: 2377/277000 \t Elapsed time: 643.95 \t Loss:0.78342\n",
      "Iteration: 2378/277000 \t Elapsed time: 644.10 \t Loss:0.76875\n",
      "Iteration: 2379/277000 \t Elapsed time: 644.27 \t Loss:0.79116\n",
      "Iteration: 2380/277000 \t Elapsed time: 644.47 \t Loss:0.75382\n",
      "Iteration: 2381/277000 \t Elapsed time: 644.65 \t Loss:0.75222\n",
      "Iteration: 2382/277000 \t Elapsed time: 644.81 \t Loss:0.81955\n",
      "Iteration: 2383/277000 \t Elapsed time: 644.99 \t Loss:0.74163\n",
      "Iteration: 2384/277000 \t Elapsed time: 645.19 \t Loss:0.78615\n",
      "Iteration: 2385/277000 \t Elapsed time: 645.38 \t Loss:0.75766\n",
      "Iteration: 2386/277000 \t Elapsed time: 645.55 \t Loss:0.76041\n",
      "Iteration: 2387/277000 \t Elapsed time: 645.75 \t Loss:0.75038\n",
      "Iteration: 2388/277000 \t Elapsed time: 645.92 \t Loss:0.75574\n",
      "Iteration: 2389/277000 \t Elapsed time: 646.11 \t Loss:0.84022\n",
      "Iteration: 2390/277000 \t Elapsed time: 646.35 \t Loss:0.72940\n",
      "Iteration: 2391/277000 \t Elapsed time: 646.55 \t Loss:0.71252\n",
      "Iteration: 2392/277000 \t Elapsed time: 646.74 \t Loss:0.72739\n",
      "Iteration: 2393/277000 \t Elapsed time: 646.93 \t Loss:0.70574\n",
      "Iteration: 2394/277000 \t Elapsed time: 647.10 \t Loss:0.78218\n",
      "Iteration: 2395/277000 \t Elapsed time: 647.27 \t Loss:0.70857\n",
      "Iteration: 2396/277000 \t Elapsed time: 647.43 \t Loss:0.69278\n",
      "Iteration: 2397/277000 \t Elapsed time: 647.60 \t Loss:0.68056\n",
      "Iteration: 2398/277000 \t Elapsed time: 647.78 \t Loss:0.69422\n",
      "Iteration: 2399/277000 \t Elapsed time: 647.95 \t Loss:0.76652\n",
      "Iteration: 2400/277000 \t Elapsed time: 648.12 \t Loss:0.68885\n",
      "Iteration: 2401/277000 \t Elapsed time: 648.29 \t Loss:0.70932\n",
      "Iteration: 2402/277000 \t Elapsed time: 648.45 \t Loss:0.72266\n",
      "Iteration: 2403/277000 \t Elapsed time: 648.62 \t Loss:0.70635\n",
      "Iteration: 2404/277000 \t Elapsed time: 648.87 \t Loss:0.70551\n",
      "Iteration: 2405/277000 \t Elapsed time: 649.13 \t Loss:0.64697\n",
      "Iteration: 2406/277000 \t Elapsed time: 649.30 \t Loss:0.64877\n",
      "Iteration: 2407/277000 \t Elapsed time: 649.47 \t Loss:0.69821\n",
      "Iteration: 2408/277000 \t Elapsed time: 649.63 \t Loss:0.63918\n",
      "Iteration: 2409/277000 \t Elapsed time: 649.79 \t Loss:0.63566\n",
      "Iteration: 2410/277000 \t Elapsed time: 649.94 \t Loss:0.65999\n",
      "Iteration: 2411/277000 \t Elapsed time: 650.11 \t Loss:0.63204\n",
      "Iteration: 2412/277000 \t Elapsed time: 650.27 \t Loss:0.61100\n",
      "Iteration: 2413/277000 \t Elapsed time: 650.52 \t Loss:0.67152\n",
      "Iteration: 2414/277000 \t Elapsed time: 650.69 \t Loss:0.60962\n",
      "Iteration: 2415/277000 \t Elapsed time: 650.91 \t Loss:0.59352\n",
      "Iteration: 2416/277000 \t Elapsed time: 651.07 \t Loss:0.60052\n",
      "Iteration: 2417/277000 \t Elapsed time: 651.31 \t Loss:0.58476\n",
      "Iteration: 2418/277000 \t Elapsed time: 651.47 \t Loss:0.60789\n",
      "Iteration: 2419/277000 \t Elapsed time: 651.64 \t Loss:0.58418\n",
      "Iteration: 2420/277000 \t Elapsed time: 651.80 \t Loss:0.55989\n",
      "Iteration: 2421/277000 \t Elapsed time: 651.99 \t Loss:0.56190\n",
      "Iteration: 2422/277000 \t Elapsed time: 652.17 \t Loss:0.72304\n",
      "Iteration: 2423/277000 \t Elapsed time: 652.40 \t Loss:0.56629\n",
      "Iteration: 2424/277000 \t Elapsed time: 652.56 \t Loss:0.55743\n",
      "Iteration: 2425/277000 \t Elapsed time: 652.72 \t Loss:0.59675\n",
      "Iteration: 2426/277000 \t Elapsed time: 652.89 \t Loss:0.55749\n",
      "Iteration: 2427/277000 \t Elapsed time: 653.05 \t Loss:0.54209\n",
      "Iteration: 2428/277000 \t Elapsed time: 653.36 \t Loss:0.70758\n",
      "Iteration: 2429/277000 \t Elapsed time: 653.54 \t Loss:0.66098\n",
      "Iteration: 2430/277000 \t Elapsed time: 653.72 \t Loss:0.61967\n",
      "Iteration: 2431/277000 \t Elapsed time: 653.88 \t Loss:0.66005\n",
      "Iteration: 2432/277000 \t Elapsed time: 654.07 \t Loss:0.55117\n",
      "Iteration: 2433/277000 \t Elapsed time: 654.23 \t Loss:0.63839\n",
      "Iteration: 2434/277000 \t Elapsed time: 654.43 \t Loss:0.52427\n",
      "Iteration: 2435/277000 \t Elapsed time: 654.61 \t Loss:0.60160\n",
      "Iteration: 2436/277000 \t Elapsed time: 654.84 \t Loss:0.57559\n",
      "Iteration: 2437/277000 \t Elapsed time: 655.04 \t Loss:0.54438\n",
      "Iteration: 2438/277000 \t Elapsed time: 655.20 \t Loss:0.52084\n",
      "Iteration: 2439/277000 \t Elapsed time: 655.37 \t Loss:0.71976\n",
      "Iteration: 2440/277000 \t Elapsed time: 655.95 \t Loss:0.67783\n",
      "Iteration: 2441/277000 \t Elapsed time: 656.15 \t Loss:0.56300\n",
      "Iteration: 2442/277000 \t Elapsed time: 656.32 \t Loss:0.59649\n",
      "Iteration: 2443/277000 \t Elapsed time: 656.50 \t Loss:0.65237\n",
      "Iteration: 2444/277000 \t Elapsed time: 656.67 \t Loss:0.54065\n",
      "Iteration: 2445/277000 \t Elapsed time: 656.83 \t Loss:0.56358\n",
      "Iteration: 2446/277000 \t Elapsed time: 657.01 \t Loss:0.63141\n",
      "Iteration: 2447/277000 \t Elapsed time: 657.18 \t Loss:0.55050\n",
      "Iteration: 2448/277000 \t Elapsed time: 657.35 \t Loss:0.59009\n",
      "Iteration: 2449/277000 \t Elapsed time: 657.52 \t Loss:0.49012\n",
      "Iteration: 2450/277000 \t Elapsed time: 657.67 \t Loss:0.71529\n",
      "Iteration: 2451/277000 \t Elapsed time: 657.89 \t Loss:0.46762\n",
      "Iteration: 2452/277000 \t Elapsed time: 658.05 \t Loss:0.52667\n",
      "Iteration: 2453/277000 \t Elapsed time: 658.23 \t Loss:0.49103\n",
      "Iteration: 2454/277000 \t Elapsed time: 658.43 \t Loss:0.46277\n",
      "Iteration: 2455/277000 \t Elapsed time: 658.67 \t Loss:0.59832\n",
      "Iteration: 2456/277000 \t Elapsed time: 658.85 \t Loss:0.48656\n",
      "Iteration: 2457/277000 \t Elapsed time: 659.03 \t Loss:0.46826\n",
      "Iteration: 2458/277000 \t Elapsed time: 659.24 \t Loss:0.51546\n",
      "Iteration: 2459/277000 \t Elapsed time: 659.41 \t Loss:0.50740\n",
      "Iteration: 2460/277000 \t Elapsed time: 659.57 \t Loss:0.47998\n",
      "Iteration: 2461/277000 \t Elapsed time: 659.93 \t Loss:0.42093\n",
      "Iteration: 2462/277000 \t Elapsed time: 660.12 \t Loss:0.43828\n",
      "Iteration: 2463/277000 \t Elapsed time: 660.28 \t Loss:0.42235\n",
      "Iteration: 2464/277000 \t Elapsed time: 660.46 \t Loss:0.41225\n",
      "Iteration: 2465/277000 \t Elapsed time: 660.62 \t Loss:0.61578\n",
      "Iteration: 2466/277000 \t Elapsed time: 660.78 \t Loss:0.54987\n",
      "Iteration: 2467/277000 \t Elapsed time: 660.95 \t Loss:0.49700\n",
      "Iteration: 2468/277000 \t Elapsed time: 661.11 \t Loss:0.42142\n",
      "Iteration: 2469/277000 \t Elapsed time: 661.27 \t Loss:0.42418\n",
      "Iteration: 2470/277000 \t Elapsed time: 661.55 \t Loss:0.44143\n",
      "Iteration: 2471/277000 \t Elapsed time: 661.73 \t Loss:0.44047\n",
      "Iteration: 2472/277000 \t Elapsed time: 661.90 \t Loss:0.45728\n",
      "Iteration: 2473/277000 \t Elapsed time: 662.07 \t Loss:0.42008\n",
      "Iteration: 2474/277000 \t Elapsed time: 662.24 \t Loss:0.38452\n",
      "Iteration: 2475/277000 \t Elapsed time: 662.40 \t Loss:0.39278\n",
      "Iteration: 2476/277000 \t Elapsed time: 662.58 \t Loss:0.37416\n",
      "Iteration: 2477/277000 \t Elapsed time: 662.76 \t Loss:0.36034\n",
      "Iteration: 2478/277000 \t Elapsed time: 662.93 \t Loss:0.37322\n",
      "Iteration: 2479/277000 \t Elapsed time: 663.10 \t Loss:0.36137\n",
      "Iteration: 2480/277000 \t Elapsed time: 663.27 \t Loss:0.33685\n",
      "Iteration: 2481/277000 \t Elapsed time: 663.43 \t Loss:0.35139\n",
      "Iteration: 2482/277000 \t Elapsed time: 663.61 \t Loss:0.49828\n",
      "Iteration: 2483/277000 \t Elapsed time: 663.80 \t Loss:0.31505\n",
      "Iteration: 2484/277000 \t Elapsed time: 663.98 \t Loss:0.34221\n",
      "Iteration: 2485/277000 \t Elapsed time: 664.16 \t Loss:0.38250\n",
      "Iteration: 2486/277000 \t Elapsed time: 664.33 \t Loss:0.39607\n",
      "Iteration: 2487/277000 \t Elapsed time: 664.49 \t Loss:0.29785\n",
      "Iteration: 2488/277000 \t Elapsed time: 664.65 \t Loss:0.53176\n",
      "Iteration: 2489/277000 \t Elapsed time: 664.82 \t Loss:0.44072\n",
      "Iteration: 2490/277000 \t Elapsed time: 664.99 \t Loss:0.35439\n",
      "Iteration: 2491/277000 \t Elapsed time: 665.17 \t Loss:0.37989\n",
      "Iteration: 2492/277000 \t Elapsed time: 665.35 \t Loss:0.32101\n",
      "Iteration: 2493/277000 \t Elapsed time: 665.52 \t Loss:0.37072\n",
      "Iteration: 2494/277000 \t Elapsed time: 665.67 \t Loss:0.28012\n",
      "Iteration: 2495/277000 \t Elapsed time: 665.85 \t Loss:0.30172\n",
      "Iteration: 2496/277000 \t Elapsed time: 666.02 \t Loss:0.33216\n",
      "Iteration: 2497/277000 \t Elapsed time: 666.19 \t Loss:0.38904\n",
      "Iteration: 2498/277000 \t Elapsed time: 666.36 \t Loss:0.30287\n",
      "Iteration: 2499/277000 \t Elapsed time: 666.52 \t Loss:0.27177\n",
      "Iteration: 2500/277000 \t Elapsed time: 666.78 \t Loss:0.39125\n",
      "Iteration: 2501/277000 \t Elapsed time: 666.96 \t Loss:0.24113\n",
      "Iteration: 2502/277000 \t Elapsed time: 667.13 \t Loss:0.27500\n",
      "Iteration: 2503/277000 \t Elapsed time: 667.29 \t Loss:0.29559\n",
      "Iteration: 2504/277000 \t Elapsed time: 667.48 \t Loss:0.25470\n",
      "Iteration: 2505/277000 \t Elapsed time: 667.65 \t Loss:0.39036\n",
      "Iteration: 2506/277000 \t Elapsed time: 667.87 \t Loss:0.25209\n",
      "Iteration: 2507/277000 \t Elapsed time: 668.04 \t Loss:0.30752\n",
      "Iteration: 2508/277000 \t Elapsed time: 668.20 \t Loss:0.32226\n",
      "Iteration: 2509/277000 \t Elapsed time: 668.37 \t Loss:0.21330\n",
      "Iteration: 2510/277000 \t Elapsed time: 668.53 \t Loss:0.21682\n",
      "Iteration: 2511/277000 \t Elapsed time: 668.69 \t Loss:0.26709\n",
      "Iteration: 2512/277000 \t Elapsed time: 668.84 \t Loss:0.27894\n",
      "Iteration: 2513/277000 \t Elapsed time: 669.00 \t Loss:0.22362\n",
      "Iteration: 2514/277000 \t Elapsed time: 669.17 \t Loss:0.27088\n",
      "Iteration: 2515/277000 \t Elapsed time: 669.35 \t Loss:0.19298\n",
      "Iteration: 2516/277000 \t Elapsed time: 669.53 \t Loss:0.34018\n",
      "Iteration: 2517/277000 \t Elapsed time: 669.69 \t Loss:0.21761\n",
      "Iteration: 2518/277000 \t Elapsed time: 669.87 \t Loss:0.19927\n",
      "Iteration: 2519/277000 \t Elapsed time: 670.04 \t Loss:0.18627\n",
      "Iteration: 2520/277000 \t Elapsed time: 670.21 \t Loss:0.17984\n",
      "Iteration: 2521/277000 \t Elapsed time: 670.38 \t Loss:0.20462\n",
      "Iteration: 2522/277000 \t Elapsed time: 670.55 \t Loss:0.16765\n",
      "Iteration: 2523/277000 \t Elapsed time: 670.71 \t Loss:0.33387\n",
      "Iteration: 2524/277000 \t Elapsed time: 670.90 \t Loss:0.17974\n",
      "Iteration: 2525/277000 \t Elapsed time: 671.10 \t Loss:0.16273\n",
      "Iteration: 2526/277000 \t Elapsed time: 671.37 \t Loss:0.14125\n",
      "Iteration: 2527/277000 \t Elapsed time: 671.55 \t Loss:0.15055\n",
      "Iteration: 2528/277000 \t Elapsed time: 671.71 \t Loss:0.14566\n",
      "Iteration: 2529/277000 \t Elapsed time: 671.94 \t Loss:0.11658\n",
      "Iteration: 2530/277000 \t Elapsed time: 672.09 \t Loss:0.24316\n",
      "Iteration: 2531/277000 \t Elapsed time: 672.25 \t Loss:0.23380\n",
      "Iteration: 2532/277000 \t Elapsed time: 672.41 \t Loss:0.12327\n",
      "Iteration: 2533/277000 \t Elapsed time: 672.58 \t Loss:0.15335\n",
      "Iteration: 2534/277000 \t Elapsed time: 672.75 \t Loss:0.10912\n",
      "Iteration: 2535/277000 \t Elapsed time: 672.92 \t Loss:0.13764\n",
      "Iteration: 2536/277000 \t Elapsed time: 673.13 \t Loss:0.19680\n",
      "Iteration: 2537/277000 \t Elapsed time: 673.35 \t Loss:0.18420\n",
      "Iteration: 2538/277000 \t Elapsed time: 673.53 \t Loss:0.15898\n",
      "Iteration: 2539/277000 \t Elapsed time: 673.70 \t Loss:0.10050\n",
      "Iteration: 2540/277000 \t Elapsed time: 673.92 \t Loss:0.08643\n",
      "Iteration: 2541/277000 \t Elapsed time: 674.11 \t Loss:0.09334\n",
      "Iteration: 2542/277000 \t Elapsed time: 674.31 \t Loss:0.20192\n",
      "Iteration: 2543/277000 \t Elapsed time: 674.47 \t Loss:0.06824\n",
      "Iteration: 2544/277000 \t Elapsed time: 674.64 \t Loss:0.11130\n",
      "Iteration: 2545/277000 \t Elapsed time: 674.81 \t Loss:0.06936\n",
      "Iteration: 2546/277000 \t Elapsed time: 674.98 \t Loss:0.05078\n",
      "Iteration: 2547/277000 \t Elapsed time: 675.18 \t Loss:0.05717\n",
      "Iteration: 2548/277000 \t Elapsed time: 675.35 \t Loss:0.06142\n",
      "Iteration: 2549/277000 \t Elapsed time: 675.53 \t Loss:0.03329\n",
      "Iteration: 2550/277000 \t Elapsed time: 675.70 \t Loss:0.09299\n",
      "Iteration: 2551/277000 \t Elapsed time: 675.91 \t Loss:0.11200\n",
      "Iteration: 2552/277000 \t Elapsed time: 676.07 \t Loss:0.07685\n",
      "Iteration: 2553/277000 \t Elapsed time: 676.26 \t Loss:0.03840\n",
      "Iteration: 2554/277000 \t Elapsed time: 676.42 \t Loss:0.02342\n",
      "Iteration: 2555/277000 \t Elapsed time: 676.60 \t Loss:0.01708\n",
      "Iteration: 2556/277000 \t Elapsed time: 676.77 \t Loss:0.00463\n",
      "Iteration: 2557/277000 \t Elapsed time: 676.93 \t Loss:-0.00370\n",
      "Iteration: 2558/277000 \t Elapsed time: 677.09 \t Loss:0.00057\n",
      "Iteration: 2559/277000 \t Elapsed time: 677.25 \t Loss:0.00092\n",
      "Iteration: 2560/277000 \t Elapsed time: 677.42 \t Loss:0.01507\n",
      "Iteration: 2561/277000 \t Elapsed time: 677.59 \t Loss:0.01766\n",
      "Iteration: 2562/277000 \t Elapsed time: 677.80 \t Loss:0.09179\n",
      "Iteration: 2563/277000 \t Elapsed time: 677.96 \t Loss:-0.00003\n",
      "Iteration: 2564/277000 \t Elapsed time: 678.14 \t Loss:0.02548\n",
      "Iteration: 2565/277000 \t Elapsed time: 678.30 \t Loss:-0.02564\n",
      "Iteration: 2566/277000 \t Elapsed time: 678.56 \t Loss:0.08148\n",
      "Iteration: 2567/277000 \t Elapsed time: 678.72 \t Loss:-0.03547\n",
      "Iteration: 2568/277000 \t Elapsed time: 678.90 \t Loss:-0.02185\n",
      "Iteration: 2569/277000 \t Elapsed time: 679.08 \t Loss:-0.01395\n",
      "Iteration: 2570/277000 \t Elapsed time: 679.25 \t Loss:-0.01497\n",
      "Iteration: 2571/277000 \t Elapsed time: 679.42 \t Loss:-0.00195\n",
      "Iteration: 2572/277000 \t Elapsed time: 679.59 \t Loss:-0.05494\n",
      "Iteration: 2573/277000 \t Elapsed time: 679.75 \t Loss:-0.03372\n",
      "Iteration: 2574/277000 \t Elapsed time: 679.92 \t Loss:-0.04123\n",
      "Iteration: 2575/277000 \t Elapsed time: 680.09 \t Loss:-0.06095\n",
      "Iteration: 2576/277000 \t Elapsed time: 680.25 \t Loss:0.03890\n",
      "Iteration: 2577/277000 \t Elapsed time: 680.44 \t Loss:-0.08646\n",
      "Iteration: 2578/277000 \t Elapsed time: 680.64 \t Loss:-0.09332\n",
      "Iteration: 2579/277000 \t Elapsed time: 680.81 \t Loss:-0.10126\n",
      "Iteration: 2580/277000 \t Elapsed time: 680.96 \t Loss:-0.09937\n",
      "Iteration: 2581/277000 \t Elapsed time: 681.13 \t Loss:-0.10211\n",
      "Iteration: 2582/277000 \t Elapsed time: 681.29 \t Loss:-0.11847\n",
      "Iteration: 2583/277000 \t Elapsed time: 681.46 \t Loss:-0.12068\n",
      "Iteration: 2584/277000 \t Elapsed time: 681.62 \t Loss:-0.06624\n",
      "Iteration: 2585/277000 \t Elapsed time: 681.79 \t Loss:0.00512\n",
      "Iteration: 2586/277000 \t Elapsed time: 681.96 \t Loss:-0.12395\n",
      "Iteration: 2587/277000 \t Elapsed time: 682.14 \t Loss:-0.12891\n",
      "Iteration: 2588/277000 \t Elapsed time: 682.30 \t Loss:-0.11613\n",
      "Iteration: 2589/277000 \t Elapsed time: 682.47 \t Loss:-0.12744\n",
      "Iteration: 2590/277000 \t Elapsed time: 682.63 \t Loss:-0.14232\n",
      "Iteration: 2591/277000 \t Elapsed time: 682.82 \t Loss:-0.14980\n",
      "Iteration: 2592/277000 \t Elapsed time: 683.02 \t Loss:-0.01267\n",
      "Iteration: 2593/277000 \t Elapsed time: 683.18 \t Loss:-0.04632\n",
      "Iteration: 2594/277000 \t Elapsed time: 683.35 \t Loss:-0.10018\n",
      "Iteration: 2595/277000 \t Elapsed time: 683.52 \t Loss:-0.08848\n",
      "Iteration: 2596/277000 \t Elapsed time: 683.69 \t Loss:-0.15846\n",
      "Iteration: 2597/277000 \t Elapsed time: 683.91 \t Loss:-0.16467\n",
      "Iteration: 2598/277000 \t Elapsed time: 684.08 \t Loss:-0.12651\n",
      "Iteration: 2599/277000 \t Elapsed time: 684.27 \t Loss:-0.14923\n",
      "Iteration: 2600/277000 \t Elapsed time: 684.44 \t Loss:-0.18531\n",
      "Iteration: 2601/277000 \t Elapsed time: 684.62 \t Loss:-0.16063\n",
      "Iteration: 2602/277000 \t Elapsed time: 684.78 \t Loss:-0.19677\n",
      "Iteration: 2603/277000 \t Elapsed time: 684.95 \t Loss:-0.17246\n",
      "Iteration: 2604/277000 \t Elapsed time: 685.12 \t Loss:-0.12669\n",
      "Iteration: 2605/277000 \t Elapsed time: 685.33 \t Loss:-0.19904\n",
      "Iteration: 2606/277000 \t Elapsed time: 685.53 \t Loss:-0.18025\n",
      "Iteration: 2607/277000 \t Elapsed time: 685.69 \t Loss:-0.18604\n",
      "Iteration: 2608/277000 \t Elapsed time: 685.86 \t Loss:-0.22010\n",
      "Iteration: 2609/277000 \t Elapsed time: 686.03 \t Loss:-0.23200\n",
      "Iteration: 2610/277000 \t Elapsed time: 686.20 \t Loss:-0.22941\n",
      "Iteration: 2611/277000 \t Elapsed time: 686.36 \t Loss:-0.23565\n",
      "Iteration: 2612/277000 \t Elapsed time: 686.53 \t Loss:-0.06093\n",
      "Iteration: 2613/277000 \t Elapsed time: 686.76 \t Loss:-0.22680\n",
      "Iteration: 2614/277000 \t Elapsed time: 686.92 \t Loss:-0.24383\n",
      "Iteration: 2615/277000 \t Elapsed time: 687.11 \t Loss:-0.21700\n",
      "Iteration: 2616/277000 \t Elapsed time: 687.27 \t Loss:-0.23122\n",
      "Iteration: 2617/277000 \t Elapsed time: 687.44 \t Loss:-0.25574\n",
      "Iteration: 2618/277000 \t Elapsed time: 687.60 \t Loss:-0.26574\n",
      "Iteration: 2619/277000 \t Elapsed time: 687.77 \t Loss:-0.27404\n",
      "Iteration: 2620/277000 \t Elapsed time: 687.98 \t Loss:-0.25883\n",
      "Iteration: 2621/277000 \t Elapsed time: 688.15 \t Loss:-0.28191\n",
      "Iteration: 2622/277000 \t Elapsed time: 688.33 \t Loss:-0.29082\n",
      "Iteration: 2623/277000 \t Elapsed time: 688.54 \t Loss:-0.29237\n",
      "Iteration: 2624/277000 \t Elapsed time: 688.71 \t Loss:-0.29646\n",
      "Iteration: 2625/277000 \t Elapsed time: 688.88 \t Loss:-0.30033\n",
      "Iteration: 2626/277000 \t Elapsed time: 689.04 \t Loss:-0.03130\n",
      "Iteration: 2627/277000 \t Elapsed time: 689.21 \t Loss:-0.21215\n",
      "Iteration: 2628/277000 \t Elapsed time: 689.40 \t Loss:-0.25560\n",
      "Iteration: 2629/277000 \t Elapsed time: 689.58 \t Loss:-0.24376\n",
      "Iteration: 2630/277000 \t Elapsed time: 689.77 \t Loss:-0.24477\n",
      "Iteration: 2631/277000 \t Elapsed time: 689.96 \t Loss:-0.26528\n",
      "Iteration: 2632/277000 \t Elapsed time: 690.14 \t Loss:-0.29044\n",
      "Iteration: 2633/277000 \t Elapsed time: 690.35 \t Loss:-0.27837\n",
      "Iteration: 2634/277000 \t Elapsed time: 690.51 \t Loss:-0.25459\n",
      "Iteration: 2635/277000 \t Elapsed time: 690.68 \t Loss:-0.30293\n",
      "Iteration: 2636/277000 \t Elapsed time: 690.84 \t Loss:-0.31956\n",
      "Iteration: 2637/277000 \t Elapsed time: 691.01 \t Loss:-0.10158\n",
      "Iteration: 2638/277000 \t Elapsed time: 691.19 \t Loss:-0.12624\n",
      "Iteration: 2639/277000 \t Elapsed time: 691.46 \t Loss:-0.11180\n",
      "Iteration: 2640/277000 \t Elapsed time: 691.62 \t Loss:-0.25350\n",
      "Iteration: 2641/277000 \t Elapsed time: 691.80 \t Loss:-0.28654\n",
      "Iteration: 2642/277000 \t Elapsed time: 691.98 \t Loss:-0.22744\n",
      "Iteration: 2643/277000 \t Elapsed time: 692.15 \t Loss:-0.30036\n",
      "Iteration: 2644/277000 \t Elapsed time: 692.31 \t Loss:-0.30165\n",
      "Iteration: 2645/277000 \t Elapsed time: 692.47 \t Loss:-0.31192\n",
      "Iteration: 2646/277000 \t Elapsed time: 692.64 \t Loss:-0.33612\n",
      "Iteration: 2647/277000 \t Elapsed time: 692.80 \t Loss:-0.25079\n",
      "Iteration: 2648/277000 \t Elapsed time: 693.00 \t Loss:-0.19786\n",
      "Iteration: 2649/277000 \t Elapsed time: 693.19 \t Loss:-0.11125\n",
      "Iteration: 2650/277000 \t Elapsed time: 693.41 \t Loss:-0.17779\n",
      "Iteration: 2651/277000 \t Elapsed time: 693.61 \t Loss:-0.30403\n",
      "Iteration: 2652/277000 \t Elapsed time: 693.78 \t Loss:-0.25492\n",
      "Iteration: 2653/277000 \t Elapsed time: 693.95 \t Loss:-0.26935\n",
      "Iteration: 2654/277000 \t Elapsed time: 694.11 \t Loss:-0.36469\n",
      "Iteration: 2655/277000 \t Elapsed time: 694.33 \t Loss:-0.31998\n",
      "Iteration: 2656/277000 \t Elapsed time: 694.52 \t Loss:-0.17639\n",
      "Iteration: 2657/277000 \t Elapsed time: 694.70 \t Loss:-0.25450\n",
      "Iteration: 2658/277000 \t Elapsed time: 694.88 \t Loss:-0.31083\n",
      "Iteration: 2659/277000 \t Elapsed time: 695.06 \t Loss:-0.33598\n",
      "Iteration: 2660/277000 \t Elapsed time: 695.24 \t Loss:0.11358\n",
      "Iteration: 2661/277000 \t Elapsed time: 695.43 \t Loss:-0.15336\n",
      "Iteration: 2662/277000 \t Elapsed time: 695.59 \t Loss:-0.14394\n",
      "Iteration: 2663/277000 \t Elapsed time: 695.75 \t Loss:-0.20488\n",
      "Iteration: 2664/277000 \t Elapsed time: 695.92 \t Loss:0.04202\n",
      "Iteration: 2665/277000 \t Elapsed time: 696.13 \t Loss:-0.28135\n",
      "Iteration: 2666/277000 \t Elapsed time: 696.31 \t Loss:-0.10213\n",
      "Iteration: 2667/277000 \t Elapsed time: 696.52 \t Loss:-0.10497\n",
      "Iteration: 2668/277000 \t Elapsed time: 696.69 \t Loss:-0.32895\n",
      "Iteration: 2669/277000 \t Elapsed time: 696.91 \t Loss:-0.10941\n",
      "Iteration: 2670/277000 \t Elapsed time: 697.08 \t Loss:-0.32289\n",
      "Iteration: 2671/277000 \t Elapsed time: 697.24 \t Loss:0.05267\n",
      "Iteration: 2672/277000 \t Elapsed time: 697.44 \t Loss:-0.27615\n",
      "Iteration: 2673/277000 \t Elapsed time: 697.62 \t Loss:-0.35325\n",
      "Iteration: 2674/277000 \t Elapsed time: 697.78 \t Loss:-0.34893\n",
      "Iteration: 2675/277000 \t Elapsed time: 697.95 \t Loss:-0.34474\n",
      "Iteration: 2676/277000 \t Elapsed time: 698.15 \t Loss:-0.41442\n",
      "Iteration: 2677/277000 \t Elapsed time: 698.33 \t Loss:-0.36253\n",
      "Iteration: 2678/277000 \t Elapsed time: 698.52 \t Loss:-0.40007\n",
      "Iteration: 2679/277000 \t Elapsed time: 698.68 \t Loss:-0.43767\n",
      "Iteration: 2680/277000 \t Elapsed time: 698.85 \t Loss:-0.40067\n",
      "Iteration: 2681/277000 \t Elapsed time: 699.01 \t Loss:-0.43886\n",
      "Iteration: 2682/277000 \t Elapsed time: 699.19 \t Loss:-0.42923\n",
      "Iteration: 2683/277000 \t Elapsed time: 699.35 \t Loss:-0.46022\n",
      "Iteration: 2684/277000 \t Elapsed time: 699.54 \t Loss:-0.46246\n",
      "Iteration: 2685/277000 \t Elapsed time: 699.70 \t Loss:-0.45812\n",
      "Iteration: 2686/277000 \t Elapsed time: 699.86 \t Loss:-0.48708\n",
      "Iteration: 2687/277000 \t Elapsed time: 700.03 \t Loss:-0.45675\n",
      "Iteration: 2688/277000 \t Elapsed time: 700.19 \t Loss:-0.47884\n",
      "Iteration: 2689/277000 \t Elapsed time: 700.36 \t Loss:-0.05834\n",
      "Iteration: 2690/277000 \t Elapsed time: 700.52 \t Loss:-0.28614\n",
      "Iteration: 2691/277000 \t Elapsed time: 700.70 \t Loss:-0.46116\n",
      "Iteration: 2692/277000 \t Elapsed time: 700.91 \t Loss:-0.27815\n",
      "Iteration: 2693/277000 \t Elapsed time: 701.07 \t Loss:-0.43633\n",
      "Iteration: 2694/277000 \t Elapsed time: 701.23 \t Loss:-0.42472\n",
      "Iteration: 2695/277000 \t Elapsed time: 701.42 \t Loss:-0.46541\n",
      "Iteration: 2696/277000 \t Elapsed time: 701.59 \t Loss:-0.16484\n",
      "Iteration: 2697/277000 \t Elapsed time: 701.76 \t Loss:-0.43432\n",
      "Iteration: 2698/277000 \t Elapsed time: 701.92 \t Loss:-0.50035\n",
      "Iteration: 2699/277000 \t Elapsed time: 702.08 \t Loss:-0.42833\n",
      "Iteration: 2700/277000 \t Elapsed time: 702.28 \t Loss:-0.48696\n",
      "Iteration: 2701/277000 \t Elapsed time: 702.50 \t Loss:-0.43238\n",
      "Iteration: 2702/277000 \t Elapsed time: 702.66 \t Loss:-0.50609\n",
      "Iteration: 2703/277000 \t Elapsed time: 702.81 \t Loss:-0.50899\n",
      "Iteration: 2704/277000 \t Elapsed time: 702.98 \t Loss:-0.49917\n",
      "Iteration: 2705/277000 \t Elapsed time: 703.14 \t Loss:-0.39490\n",
      "Iteration: 2706/277000 \t Elapsed time: 703.31 \t Loss:-0.44921\n",
      "Iteration: 2707/277000 \t Elapsed time: 703.47 \t Loss:-0.51965\n",
      "Iteration: 2708/277000 \t Elapsed time: 703.64 \t Loss:-0.47821\n",
      "Iteration: 2709/277000 \t Elapsed time: 703.82 \t Loss:-0.53781\n",
      "Iteration: 2710/277000 \t Elapsed time: 704.00 \t Loss:-0.51357\n",
      "Iteration: 2711/277000 \t Elapsed time: 704.17 \t Loss:-0.51192\n",
      "Iteration: 2712/277000 \t Elapsed time: 704.33 \t Loss:-0.53046\n",
      "Iteration: 2713/277000 \t Elapsed time: 704.50 \t Loss:-0.55044\n",
      "Iteration: 2714/277000 \t Elapsed time: 704.67 \t Loss:-0.33475\n",
      "Iteration: 2715/277000 \t Elapsed time: 704.84 \t Loss:-0.49427\n",
      "Iteration: 2716/277000 \t Elapsed time: 705.01 \t Loss:-0.51507\n",
      "Iteration: 2717/277000 \t Elapsed time: 705.19 \t Loss:-0.54566\n",
      "Iteration: 2718/277000 \t Elapsed time: 705.35 \t Loss:-0.55054\n",
      "Iteration: 2719/277000 \t Elapsed time: 705.51 \t Loss:-0.57008\n",
      "Iteration: 2720/277000 \t Elapsed time: 705.67 \t Loss:-0.39779\n",
      "Iteration: 2721/277000 \t Elapsed time: 705.83 \t Loss:-0.55025\n",
      "Iteration: 2722/277000 \t Elapsed time: 706.01 \t Loss:-0.57741\n",
      "Iteration: 2723/277000 \t Elapsed time: 706.18 \t Loss:-0.45429\n",
      "Iteration: 2724/277000 \t Elapsed time: 706.35 \t Loss:-0.52819\n",
      "Iteration: 2725/277000 \t Elapsed time: 706.51 \t Loss:-0.18537\n",
      "Iteration: 2726/277000 \t Elapsed time: 706.69 \t Loss:-0.53471\n",
      "Iteration: 2727/277000 \t Elapsed time: 706.87 \t Loss:-0.21656\n",
      "Iteration: 2728/277000 \t Elapsed time: 707.03 \t Loss:-0.53020\n",
      "Iteration: 2729/277000 \t Elapsed time: 707.30 \t Loss:-0.38723\n",
      "Iteration: 2730/277000 \t Elapsed time: 707.47 \t Loss:-0.49710\n",
      "Iteration: 2731/277000 \t Elapsed time: 707.63 \t Loss:-0.51294\n",
      "Iteration: 2732/277000 \t Elapsed time: 707.79 \t Loss:-0.56248\n",
      "Iteration: 2733/277000 \t Elapsed time: 707.96 \t Loss:-0.53764\n",
      "Iteration: 2734/277000 \t Elapsed time: 708.13 \t Loss:-0.24992\n",
      "Iteration: 2735/277000 \t Elapsed time: 708.29 \t Loss:-0.60209\n",
      "Iteration: 2736/277000 \t Elapsed time: 708.45 \t Loss:-0.57985\n",
      "Iteration: 2737/277000 \t Elapsed time: 708.66 \t Loss:-0.55201\n",
      "Iteration: 2738/277000 \t Elapsed time: 708.83 \t Loss:-0.55833\n",
      "Iteration: 2739/277000 \t Elapsed time: 709.00 \t Loss:-0.57440\n",
      "Iteration: 2740/277000 \t Elapsed time: 709.15 \t Loss:-0.59571\n",
      "Iteration: 2741/277000 \t Elapsed time: 709.31 \t Loss:-0.63097\n",
      "Iteration: 2742/277000 \t Elapsed time: 709.47 \t Loss:-0.63453\n",
      "Iteration: 2743/277000 \t Elapsed time: 709.63 \t Loss:-0.62138\n",
      "Iteration: 2744/277000 \t Elapsed time: 709.79 \t Loss:-0.63370\n",
      "Iteration: 2745/277000 \t Elapsed time: 709.95 \t Loss:-0.62466\n",
      "Iteration: 2746/277000 \t Elapsed time: 710.12 \t Loss:-0.64655\n",
      "Iteration: 2747/277000 \t Elapsed time: 710.28 \t Loss:-0.13688\n",
      "Iteration: 2748/277000 \t Elapsed time: 710.45 \t Loss:-0.36202\n",
      "Iteration: 2749/277000 \t Elapsed time: 710.61 \t Loss:-0.58464\n",
      "Iteration: 2750/277000 \t Elapsed time: 710.77 \t Loss:-0.59851\n",
      "Iteration: 2751/277000 \t Elapsed time: 710.95 \t Loss:-0.63930\n",
      "Iteration: 2752/277000 \t Elapsed time: 711.15 \t Loss:-0.63275\n",
      "Iteration: 2753/277000 \t Elapsed time: 711.32 \t Loss:0.02970\n",
      "Iteration: 2754/277000 \t Elapsed time: 711.48 \t Loss:-0.45998\n",
      "Iteration: 2755/277000 \t Elapsed time: 711.67 \t Loss:-0.59892\n",
      "Iteration: 2756/277000 \t Elapsed time: 711.83 \t Loss:-0.31051\n",
      "Iteration: 2757/277000 \t Elapsed time: 712.06 \t Loss:-0.63636\n",
      "Iteration: 2758/277000 \t Elapsed time: 712.24 \t Loss:-0.65295\n",
      "Iteration: 2759/277000 \t Elapsed time: 712.41 \t Loss:-0.44855\n",
      "Iteration: 2760/277000 \t Elapsed time: 712.57 \t Loss:-0.65547\n",
      "Iteration: 2761/277000 \t Elapsed time: 712.74 \t Loss:-0.28263\n",
      "Iteration: 2762/277000 \t Elapsed time: 712.90 \t Loss:-0.41902\n",
      "Iteration: 2763/277000 \t Elapsed time: 713.07 \t Loss:-0.60805\n",
      "Iteration: 2764/277000 \t Elapsed time: 713.25 \t Loss:-0.61512\n",
      "Iteration: 2765/277000 \t Elapsed time: 713.44 \t Loss:-0.67987\n",
      "Iteration: 2766/277000 \t Elapsed time: 713.60 \t Loss:-0.66107\n",
      "Iteration: 2767/277000 \t Elapsed time: 713.77 \t Loss:-0.66849\n",
      "Iteration: 2768/277000 \t Elapsed time: 713.95 \t Loss:-0.29402\n",
      "Iteration: 2769/277000 \t Elapsed time: 714.13 \t Loss:-0.44854\n",
      "Iteration: 2770/277000 \t Elapsed time: 714.31 \t Loss:-0.67837\n",
      "Iteration: 2771/277000 \t Elapsed time: 714.48 \t Loss:-0.49646\n",
      "Iteration: 2772/277000 \t Elapsed time: 714.66 \t Loss:-0.73892\n",
      "Iteration: 2773/277000 \t Elapsed time: 714.84 \t Loss:-0.71875\n",
      "Iteration: 2774/277000 \t Elapsed time: 715.01 \t Loss:-0.71344\n",
      "Iteration: 2775/277000 \t Elapsed time: 715.23 \t Loss:-0.73780\n",
      "Iteration: 2776/277000 \t Elapsed time: 715.42 \t Loss:-0.75495\n",
      "Iteration: 2777/277000 \t Elapsed time: 715.63 \t Loss:-0.28182\n",
      "Iteration: 2778/277000 \t Elapsed time: 715.82 \t Loss:-0.63536\n",
      "Iteration: 2779/277000 \t Elapsed time: 716.10 \t Loss:-0.76327\n",
      "Iteration: 2780/277000 \t Elapsed time: 716.30 \t Loss:-0.62616\n",
      "Iteration: 2781/277000 \t Elapsed time: 716.49 \t Loss:-0.75476\n",
      "Iteration: 2782/277000 \t Elapsed time: 716.66 \t Loss:-0.74818\n",
      "Iteration: 2783/277000 \t Elapsed time: 716.88 \t Loss:-0.77197\n",
      "Iteration: 2784/277000 \t Elapsed time: 717.04 \t Loss:-0.77125\n",
      "Iteration: 2785/277000 \t Elapsed time: 717.22 \t Loss:-0.38353\n",
      "Iteration: 2786/277000 \t Elapsed time: 717.40 \t Loss:-0.76948\n",
      "Iteration: 2787/277000 \t Elapsed time: 717.57 \t Loss:-0.77420\n",
      "Iteration: 2788/277000 \t Elapsed time: 717.75 \t Loss:-0.78482\n",
      "Iteration: 2789/277000 \t Elapsed time: 717.92 \t Loss:-0.80216\n",
      "Iteration: 2790/277000 \t Elapsed time: 718.08 \t Loss:-0.79110\n",
      "Iteration: 2791/277000 \t Elapsed time: 718.26 \t Loss:-0.81925\n",
      "Iteration: 2792/277000 \t Elapsed time: 718.46 \t Loss:-0.80201\n",
      "Iteration: 2793/277000 \t Elapsed time: 718.63 \t Loss:-0.81186\n",
      "Iteration: 2794/277000 \t Elapsed time: 718.80 \t Loss:-0.83577\n",
      "Iteration: 2795/277000 \t Elapsed time: 718.97 \t Loss:-0.27674\n",
      "Iteration: 2796/277000 \t Elapsed time: 719.13 \t Loss:-0.55867\n",
      "Iteration: 2797/277000 \t Elapsed time: 719.30 \t Loss:-0.81411\n",
      "Iteration: 2798/277000 \t Elapsed time: 719.51 \t Loss:-0.80538\n",
      "Iteration: 2799/277000 \t Elapsed time: 719.69 \t Loss:-0.80728\n",
      "Iteration: 2800/277000 \t Elapsed time: 719.87 \t Loss:-0.83293\n",
      "Iteration: 2801/277000 \t Elapsed time: 720.03 \t Loss:-0.82276\n",
      "Iteration: 2802/277000 \t Elapsed time: 720.19 \t Loss:-0.83410\n",
      "Iteration: 2803/277000 \t Elapsed time: 720.36 \t Loss:-0.84542\n",
      "Iteration: 2804/277000 \t Elapsed time: 720.53 \t Loss:-0.72209\n",
      "Iteration: 2805/277000 \t Elapsed time: 720.71 \t Loss:-0.85839\n",
      "Iteration: 2806/277000 \t Elapsed time: 721.46 \t Loss:-0.73715\n",
      "Iteration: 2807/277000 \t Elapsed time: 721.64 \t Loss:-0.84419\n",
      "Iteration: 2808/277000 \t Elapsed time: 721.80 \t Loss:-0.85539\n",
      "Iteration: 2809/277000 \t Elapsed time: 721.97 \t Loss:-0.87345\n",
      "Iteration: 2810/277000 \t Elapsed time: 722.14 \t Loss:-0.86489\n",
      "Iteration: 2811/277000 \t Elapsed time: 722.30 \t Loss:-0.59355\n",
      "Iteration: 2812/277000 \t Elapsed time: 722.48 \t Loss:-0.88992\n",
      "Iteration: 2813/277000 \t Elapsed time: 722.64 \t Loss:-0.50755\n",
      "Iteration: 2814/277000 \t Elapsed time: 722.81 \t Loss:-0.68690\n",
      "Iteration: 2815/277000 \t Elapsed time: 722.97 \t Loss:-0.86976\n",
      "Iteration: 2816/277000 \t Elapsed time: 723.13 \t Loss:-0.87281\n",
      "Iteration: 2817/277000 \t Elapsed time: 723.30 \t Loss:-0.89351\n",
      "Iteration: 2818/277000 \t Elapsed time: 723.48 \t Loss:-0.89723\n",
      "Iteration: 2819/277000 \t Elapsed time: 723.63 \t Loss:-0.89215\n",
      "Iteration: 2820/277000 \t Elapsed time: 723.79 \t Loss:-0.91273\n",
      "Iteration: 2821/277000 \t Elapsed time: 723.99 \t Loss:-0.78927\n",
      "Iteration: 2822/277000 \t Elapsed time: 724.16 \t Loss:-0.76733\n",
      "Iteration: 2823/277000 \t Elapsed time: 724.32 \t Loss:-0.88701\n",
      "Iteration: 2824/277000 \t Elapsed time: 724.48 \t Loss:-0.70087\n",
      "Iteration: 2825/277000 \t Elapsed time: 724.88 \t Loss:-0.76188\n",
      "Iteration: 2826/277000 \t Elapsed time: 725.09 \t Loss:-0.59203\n",
      "Iteration: 2827/277000 \t Elapsed time: 725.27 \t Loss:-0.87198\n",
      "Iteration: 2828/277000 \t Elapsed time: 725.45 \t Loss:-0.87451\n",
      "Iteration: 2829/277000 \t Elapsed time: 725.61 \t Loss:-0.69213\n",
      "Iteration: 2830/277000 \t Elapsed time: 725.77 \t Loss:-0.75184\n",
      "Iteration: 2831/277000 \t Elapsed time: 725.93 \t Loss:-0.85655\n",
      "Iteration: 2832/277000 \t Elapsed time: 726.12 \t Loss:-0.89354\n",
      "Iteration: 2833/277000 \t Elapsed time: 726.31 \t Loss:-0.81120\n",
      "Iteration: 2834/277000 \t Elapsed time: 726.51 \t Loss:-0.90768\n",
      "Iteration: 2835/277000 \t Elapsed time: 726.67 \t Loss:-0.87724\n",
      "Iteration: 2836/277000 \t Elapsed time: 726.87 \t Loss:-0.94857\n",
      "Iteration: 2837/277000 \t Elapsed time: 727.05 \t Loss:-0.93349\n",
      "Iteration: 2838/277000 \t Elapsed time: 727.21 \t Loss:-0.95649\n",
      "Iteration: 2839/277000 \t Elapsed time: 727.41 \t Loss:-0.96410\n",
      "Iteration: 2840/277000 \t Elapsed time: 727.58 \t Loss:-0.96942\n",
      "Iteration: 2841/277000 \t Elapsed time: 727.75 \t Loss:-0.98032\n",
      "Iteration: 2842/277000 \t Elapsed time: 727.92 \t Loss:-0.82858\n",
      "Iteration: 2843/277000 \t Elapsed time: 728.08 \t Loss:-0.91488\n",
      "Iteration: 2844/277000 \t Elapsed time: 728.24 \t Loss:-0.98427\n",
      "Iteration: 2845/277000 \t Elapsed time: 728.40 \t Loss:-0.97226\n",
      "Iteration: 2846/277000 \t Elapsed time: 728.56 \t Loss:-0.69736\n",
      "Iteration: 2847/277000 \t Elapsed time: 728.73 \t Loss:-0.93112\n",
      "Iteration: 2848/277000 \t Elapsed time: 728.89 \t Loss:-0.76883\n",
      "Iteration: 2849/277000 \t Elapsed time: 729.06 \t Loss:-0.97841\n",
      "Iteration: 2850/277000 \t Elapsed time: 729.24 \t Loss:-0.98316\n",
      "Iteration: 2851/277000 \t Elapsed time: 729.42 \t Loss:-0.84295\n",
      "Iteration: 2852/277000 \t Elapsed time: 729.65 \t Loss:-0.86994\n",
      "Iteration: 2853/277000 \t Elapsed time: 729.83 \t Loss:-0.96341\n",
      "Iteration: 2854/277000 \t Elapsed time: 729.99 \t Loss:-0.95869\n",
      "Iteration: 2855/277000 \t Elapsed time: 730.15 \t Loss:-0.87015\n",
      "Iteration: 2856/277000 \t Elapsed time: 730.32 \t Loss:-0.96634\n",
      "Iteration: 2857/277000 \t Elapsed time: 730.48 \t Loss:-0.99022\n",
      "Iteration: 2858/277000 \t Elapsed time: 730.69 \t Loss:-1.02279\n",
      "Iteration: 2859/277000 \t Elapsed time: 730.87 \t Loss:-1.00839\n",
      "Iteration: 2860/277000 \t Elapsed time: 731.09 \t Loss:-0.81579\n",
      "Iteration: 2861/277000 \t Elapsed time: 731.25 \t Loss:-1.01613\n",
      "Iteration: 2862/277000 \t Elapsed time: 731.41 \t Loss:-1.02968\n",
      "Iteration: 2863/277000 \t Elapsed time: 731.58 \t Loss:-1.04006\n",
      "Iteration: 2864/277000 \t Elapsed time: 731.74 \t Loss:-1.03328\n",
      "Iteration: 2865/277000 \t Elapsed time: 731.90 \t Loss:-0.83401\n",
      "Iteration: 2866/277000 \t Elapsed time: 732.06 \t Loss:-1.04176\n",
      "Iteration: 2867/277000 \t Elapsed time: 732.24 \t Loss:-1.03669\n",
      "Iteration: 2868/277000 \t Elapsed time: 732.40 \t Loss:-0.94675\n",
      "Iteration: 2869/277000 \t Elapsed time: 732.56 \t Loss:-0.94604\n",
      "Iteration: 2870/277000 \t Elapsed time: 732.74 \t Loss:-1.03517\n",
      "Iteration: 2871/277000 \t Elapsed time: 732.91 \t Loss:-1.05674\n",
      "Iteration: 2872/277000 \t Elapsed time: 733.07 \t Loss:-0.90346\n",
      "Iteration: 2873/277000 \t Elapsed time: 733.23 \t Loss:-1.04998\n",
      "Iteration: 2874/277000 \t Elapsed time: 733.48 \t Loss:-1.05478\n",
      "Iteration: 2875/277000 \t Elapsed time: 733.67 \t Loss:-1.08417\n",
      "Iteration: 2876/277000 \t Elapsed time: 733.85 \t Loss:-0.73374\n",
      "Iteration: 2877/277000 \t Elapsed time: 734.01 \t Loss:-0.99411\n",
      "Iteration: 2878/277000 \t Elapsed time: 734.18 \t Loss:-1.06151\n",
      "Iteration: 2879/277000 \t Elapsed time: 734.34 \t Loss:-1.06483\n",
      "Iteration: 2880/277000 \t Elapsed time: 734.50 \t Loss:-0.93919\n",
      "Iteration: 2881/277000 \t Elapsed time: 734.67 \t Loss:-1.09929\n",
      "Iteration: 2882/277000 \t Elapsed time: 734.83 \t Loss:-0.72540\n",
      "Iteration: 2883/277000 \t Elapsed time: 735.04 \t Loss:-1.05064\n",
      "Iteration: 2884/277000 \t Elapsed time: 735.30 \t Loss:-1.04085\n",
      "Iteration: 2885/277000 \t Elapsed time: 735.50 \t Loss:-1.10496\n",
      "Iteration: 2886/277000 \t Elapsed time: 735.66 \t Loss:-1.03028\n",
      "Iteration: 2887/277000 \t Elapsed time: 735.82 \t Loss:-1.10112\n",
      "Iteration: 2888/277000 \t Elapsed time: 736.00 \t Loss:-1.11026\n",
      "Iteration: 2889/277000 \t Elapsed time: 736.17 \t Loss:-1.12502\n",
      "Iteration: 2890/277000 \t Elapsed time: 736.35 \t Loss:-1.11927\n",
      "Iteration: 2891/277000 \t Elapsed time: 736.51 \t Loss:-1.13163\n",
      "Iteration: 2892/277000 \t Elapsed time: 736.68 \t Loss:-1.14726\n",
      "Iteration: 2893/277000 \t Elapsed time: 736.87 \t Loss:-1.14438\n",
      "Iteration: 2894/277000 \t Elapsed time: 737.03 \t Loss:-1.15358\n",
      "Iteration: 2895/277000 \t Elapsed time: 737.20 \t Loss:-1.15290\n",
      "Iteration: 2896/277000 \t Elapsed time: 737.37 \t Loss:-1.16961\n",
      "Iteration: 2897/277000 \t Elapsed time: 737.54 \t Loss:-0.80620\n",
      "Iteration: 2898/277000 \t Elapsed time: 737.74 \t Loss:-1.02792\n",
      "Iteration: 2899/277000 \t Elapsed time: 737.99 \t Loss:-1.14689\n",
      "Iteration: 2900/277000 \t Elapsed time: 738.15 \t Loss:-0.67566\n",
      "Iteration: 2901/277000 \t Elapsed time: 738.31 \t Loss:-1.14019\n",
      "Iteration: 2902/277000 \t Elapsed time: 738.48 \t Loss:-0.94847\n",
      "Iteration: 2903/277000 \t Elapsed time: 738.73 \t Loss:-1.12338\n",
      "Iteration: 2904/277000 \t Elapsed time: 738.90 \t Loss:-1.03922\n",
      "Iteration: 2905/277000 \t Elapsed time: 739.10 \t Loss:-1.09861\n",
      "Iteration: 2906/277000 \t Elapsed time: 739.28 \t Loss:-1.10934\n",
      "Iteration: 2907/277000 \t Elapsed time: 739.45 \t Loss:-0.86017\n",
      "Iteration: 2908/277000 \t Elapsed time: 739.61 \t Loss:-0.99357\n",
      "Iteration: 2909/277000 \t Elapsed time: 739.80 \t Loss:-0.66847\n",
      "Iteration: 2910/277000 \t Elapsed time: 739.96 \t Loss:-1.09741\n",
      "Iteration: 2911/277000 \t Elapsed time: 740.13 \t Loss:-0.78612\n",
      "Iteration: 2912/277000 \t Elapsed time: 740.34 \t Loss:-1.12073\n",
      "Iteration: 2913/277000 \t Elapsed time: 740.52 \t Loss:-1.06859\n",
      "Iteration: 2914/277000 \t Elapsed time: 740.68 \t Loss:-1.02704\n",
      "Iteration: 2915/277000 \t Elapsed time: 740.89 \t Loss:-1.15898\n",
      "Iteration: 2916/277000 \t Elapsed time: 741.14 \t Loss:-1.13079\n",
      "Iteration: 2917/277000 \t Elapsed time: 741.33 \t Loss:-1.16382\n",
      "Iteration: 2918/277000 \t Elapsed time: 741.50 \t Loss:-0.92049\n",
      "Iteration: 2919/277000 \t Elapsed time: 741.71 \t Loss:-1.14880\n",
      "Iteration: 2920/277000 \t Elapsed time: 741.87 \t Loss:-1.18537\n",
      "Iteration: 2921/277000 \t Elapsed time: 742.04 \t Loss:-1.15826\n",
      "Iteration: 2922/277000 \t Elapsed time: 742.20 \t Loss:-1.19143\n",
      "Iteration: 2923/277000 \t Elapsed time: 742.36 \t Loss:-1.20783\n",
      "Iteration: 2924/277000 \t Elapsed time: 742.54 \t Loss:-1.18860\n",
      "Iteration: 2925/277000 \t Elapsed time: 742.70 \t Loss:-0.86310\n",
      "Iteration: 2926/277000 \t Elapsed time: 742.86 \t Loss:-1.16905\n",
      "Iteration: 2927/277000 \t Elapsed time: 743.02 \t Loss:-1.18262\n",
      "Iteration: 2928/277000 \t Elapsed time: 743.17 \t Loss:-0.97324\n",
      "Iteration: 2929/277000 \t Elapsed time: 743.34 \t Loss:-1.22198\n",
      "Iteration: 2930/277000 \t Elapsed time: 743.51 \t Loss:-1.24016\n",
      "Iteration: 2931/277000 \t Elapsed time: 743.67 \t Loss:-1.22659\n",
      "Iteration: 2932/277000 \t Elapsed time: 744.17 \t Loss:-1.24347\n",
      "Iteration: 2933/277000 \t Elapsed time: 744.36 \t Loss:-1.14496\n",
      "Iteration: 2934/277000 \t Elapsed time: 744.56 \t Loss:-1.24091\n",
      "Iteration: 2935/277000 \t Elapsed time: 744.73 \t Loss:-1.26043\n",
      "Iteration: 2936/277000 \t Elapsed time: 744.90 \t Loss:-1.26093\n",
      "Iteration: 2937/277000 \t Elapsed time: 745.06 \t Loss:-1.27039\n",
      "Iteration: 2938/277000 \t Elapsed time: 745.23 \t Loss:-1.21123\n",
      "Iteration: 2939/277000 \t Elapsed time: 745.42 \t Loss:-1.27117\n",
      "Iteration: 2940/277000 \t Elapsed time: 745.58 \t Loss:-1.27346\n",
      "Iteration: 2941/277000 \t Elapsed time: 745.77 \t Loss:-0.91409\n",
      "Iteration: 2942/277000 \t Elapsed time: 745.94 \t Loss:-1.27670\n",
      "Iteration: 2943/277000 \t Elapsed time: 746.10 \t Loss:-1.17613\n",
      "Iteration: 2944/277000 \t Elapsed time: 746.27 \t Loss:-1.26781\n",
      "Iteration: 2945/277000 \t Elapsed time: 746.43 \t Loss:-1.27328\n",
      "Iteration: 2946/277000 \t Elapsed time: 746.59 \t Loss:-1.28120\n",
      "Iteration: 2947/277000 \t Elapsed time: 746.76 \t Loss:-1.27085\n",
      "Iteration: 2948/277000 \t Elapsed time: 746.94 \t Loss:-1.28864\n",
      "Iteration: 2949/277000 \t Elapsed time: 747.14 \t Loss:-1.27792\n",
      "Iteration: 2950/277000 \t Elapsed time: 747.30 \t Loss:-1.31510\n",
      "Iteration: 2951/277000 \t Elapsed time: 747.50 \t Loss:-1.26593\n",
      "Iteration: 2952/277000 \t Elapsed time: 747.70 \t Loss:-1.15488\n",
      "Iteration: 2953/277000 \t Elapsed time: 747.86 \t Loss:-0.90129\n",
      "Iteration: 2954/277000 \t Elapsed time: 748.03 \t Loss:-1.24544\n",
      "Iteration: 2955/277000 \t Elapsed time: 748.23 \t Loss:-1.27866\n",
      "Iteration: 2956/277000 \t Elapsed time: 748.40 \t Loss:-1.29431\n",
      "Iteration: 2957/277000 \t Elapsed time: 748.57 \t Loss:-1.31848\n",
      "Iteration: 2958/277000 \t Elapsed time: 748.75 \t Loss:-1.30983\n",
      "Iteration: 2959/277000 \t Elapsed time: 748.98 \t Loss:-1.33154\n",
      "Iteration: 2960/277000 \t Elapsed time: 749.14 \t Loss:-1.33112\n",
      "Iteration: 2961/277000 \t Elapsed time: 749.31 \t Loss:-1.33284\n",
      "Iteration: 2962/277000 \t Elapsed time: 749.52 \t Loss:-1.34998\n",
      "Iteration: 2963/277000 \t Elapsed time: 749.70 \t Loss:-1.35681\n",
      "Iteration: 2964/277000 \t Elapsed time: 749.87 \t Loss:-1.36080\n",
      "Iteration: 2965/277000 \t Elapsed time: 750.05 \t Loss:-0.52690\n",
      "Iteration: 2966/277000 \t Elapsed time: 750.26 \t Loss:-1.25456\n",
      "Iteration: 2967/277000 \t Elapsed time: 750.42 \t Loss:-1.30035\n",
      "Iteration: 2968/277000 \t Elapsed time: 750.58 \t Loss:-1.31546\n",
      "Iteration: 2969/277000 \t Elapsed time: 750.75 \t Loss:-1.23438\n",
      "Iteration: 2970/277000 \t Elapsed time: 750.91 \t Loss:-1.33565\n",
      "Iteration: 2971/277000 \t Elapsed time: 751.08 \t Loss:-1.37197\n",
      "Iteration: 2972/277000 \t Elapsed time: 751.23 \t Loss:-1.32643\n",
      "Iteration: 2973/277000 \t Elapsed time: 751.41 \t Loss:-0.53249\n",
      "Iteration: 2974/277000 \t Elapsed time: 751.57 \t Loss:-1.02216\n",
      "Iteration: 2975/277000 \t Elapsed time: 751.73 \t Loss:-1.31900\n",
      "Iteration: 2976/277000 \t Elapsed time: 751.89 \t Loss:-0.69551\n",
      "Iteration: 2977/277000 \t Elapsed time: 752.05 \t Loss:-1.30100\n",
      "Iteration: 2978/277000 \t Elapsed time: 752.24 \t Loss:-1.26378\n",
      "Iteration: 2979/277000 \t Elapsed time: 752.43 \t Loss:-1.30590\n",
      "Iteration: 2980/277000 \t Elapsed time: 752.59 \t Loss:-1.32663\n",
      "Iteration: 2981/277000 \t Elapsed time: 752.75 \t Loss:-1.29624\n",
      "Iteration: 2982/277000 \t Elapsed time: 752.91 \t Loss:-0.04814\n",
      "Iteration: 2983/277000 \t Elapsed time: 753.09 \t Loss:-1.03543\n",
      "Iteration: 2984/277000 \t Elapsed time: 753.28 \t Loss:-1.02764\n",
      "Iteration: 2985/277000 \t Elapsed time: 753.48 \t Loss:-1.16870\n",
      "Iteration: 2986/277000 \t Elapsed time: 753.66 \t Loss:-1.22622\n",
      "Iteration: 2987/277000 \t Elapsed time: 753.85 \t Loss:-1.15618\n",
      "Iteration: 2988/277000 \t Elapsed time: 754.04 \t Loss:-1.26849\n",
      "Iteration: 2989/277000 \t Elapsed time: 754.23 \t Loss:-1.26370\n",
      "Iteration: 2990/277000 \t Elapsed time: 754.39 \t Loss:-0.88002\n",
      "Iteration: 2991/277000 \t Elapsed time: 754.56 \t Loss:-1.27894\n",
      "Iteration: 2992/277000 \t Elapsed time: 754.72 \t Loss:-1.26082\n",
      "Iteration: 2993/277000 \t Elapsed time: 754.89 \t Loss:-1.27109\n",
      "Iteration: 2994/277000 \t Elapsed time: 755.05 \t Loss:-0.14284\n",
      "Iteration: 2995/277000 \t Elapsed time: 755.22 \t Loss:-1.27968\n",
      "Iteration: 2996/277000 \t Elapsed time: 755.39 \t Loss:-1.19767\n",
      "Iteration: 2997/277000 \t Elapsed time: 755.61 \t Loss:-1.24930\n",
      "Iteration: 2998/277000 \t Elapsed time: 755.80 \t Loss:-1.25001\n",
      "Iteration: 2999/277000 \t Elapsed time: 755.98 \t Loss:-0.96684\n",
      "Iteration: 3000/277000 \t Elapsed time: 756.15 \t Loss:-1.22334\n",
      "Start Validating\n",
      "Finish Validating\n",
      "Iteration: 3001/277000 \t Elapsed time: 857.00 \t Loss:-1.14376\n",
      "Iteration: 3002/277000 \t Elapsed time: 857.27 \t Loss:-1.20311\n",
      "Iteration: 3003/277000 \t Elapsed time: 857.45 \t Loss:-1.28797\n",
      "Iteration: 3004/277000 \t Elapsed time: 857.61 \t Loss:-1.31555\n",
      "Iteration: 3005/277000 \t Elapsed time: 857.78 \t Loss:-0.55250\n",
      "Iteration: 3006/277000 \t Elapsed time: 857.95 \t Loss:-0.70186\n",
      "Iteration: 3007/277000 \t Elapsed time: 858.17 \t Loss:-0.65872\n",
      "Iteration: 3008/277000 \t Elapsed time: 858.34 \t Loss:-1.14095\n",
      "Iteration: 3009/277000 \t Elapsed time: 858.52 \t Loss:-0.75435\n",
      "Iteration: 3010/277000 \t Elapsed time: 859.02 \t Loss:-1.04000\n",
      "Iteration: 3011/277000 \t Elapsed time: 859.20 \t Loss:-1.29111\n",
      "Iteration: 3012/277000 \t Elapsed time: 859.36 \t Loss:-1.16939\n",
      "Iteration: 3013/277000 \t Elapsed time: 859.55 \t Loss:-1.13423\n",
      "Iteration: 3014/277000 \t Elapsed time: 859.73 \t Loss:-1.02104\n",
      "Iteration: 3015/277000 \t Elapsed time: 859.91 \t Loss:-1.30937\n",
      "Iteration: 3016/277000 \t Elapsed time: 860.08 \t Loss:-1.24983\n",
      "Iteration: 3017/277000 \t Elapsed time: 860.23 \t Loss:-1.22973\n",
      "Iteration: 3018/277000 \t Elapsed time: 860.40 \t Loss:-1.09378\n",
      "Iteration: 3019/277000 \t Elapsed time: 860.61 \t Loss:-1.22886\n",
      "Iteration: 3020/277000 \t Elapsed time: 860.85 \t Loss:-1.32330\n",
      "Iteration: 3021/277000 \t Elapsed time: 861.01 \t Loss:-1.32563\n",
      "Iteration: 3022/277000 \t Elapsed time: 861.19 \t Loss:-1.32750\n",
      "Iteration: 3023/277000 \t Elapsed time: 861.39 \t Loss:-1.33876\n",
      "Iteration: 3024/277000 \t Elapsed time: 861.62 \t Loss:-1.36122\n",
      "Iteration: 3025/277000 \t Elapsed time: 861.85 \t Loss:-1.37297\n",
      "Iteration: 3026/277000 \t Elapsed time: 862.10 \t Loss:-1.24219\n",
      "Iteration: 3027/277000 \t Elapsed time: 862.27 \t Loss:-0.80330\n",
      "Iteration: 3028/277000 \t Elapsed time: 862.44 \t Loss:-1.03157\n",
      "Iteration: 3029/277000 \t Elapsed time: 862.60 \t Loss:-0.91919\n",
      "Iteration: 3030/277000 \t Elapsed time: 862.81 \t Loss:-1.13131\n",
      "Iteration: 3031/277000 \t Elapsed time: 862.99 \t Loss:-1.12715\n",
      "Iteration: 3032/277000 \t Elapsed time: 863.17 \t Loss:-1.33109\n",
      "Iteration: 3033/277000 \t Elapsed time: 863.34 \t Loss:-1.24465\n",
      "Iteration: 3034/277000 \t Elapsed time: 863.50 \t Loss:-1.37981\n",
      "Iteration: 3035/277000 \t Elapsed time: 863.68 \t Loss:-1.27905\n",
      "Iteration: 3036/277000 \t Elapsed time: 863.84 \t Loss:-1.39107\n",
      "Iteration: 3037/277000 \t Elapsed time: 864.01 \t Loss:-1.44200\n",
      "Iteration: 3038/277000 \t Elapsed time: 864.19 \t Loss:-1.40915\n",
      "Iteration: 3039/277000 \t Elapsed time: 864.38 \t Loss:-1.19364\n",
      "Iteration: 3040/277000 \t Elapsed time: 864.54 \t Loss:-1.44001\n",
      "Iteration: 3041/277000 \t Elapsed time: 864.70 \t Loss:-1.40451\n",
      "Iteration: 3042/277000 \t Elapsed time: 864.91 \t Loss:-1.46221\n",
      "Iteration: 3043/277000 \t Elapsed time: 865.18 \t Loss:-1.45666\n",
      "Iteration: 3044/277000 \t Elapsed time: 865.37 \t Loss:-1.48820\n",
      "Iteration: 3045/277000 \t Elapsed time: 865.54 \t Loss:-1.08833\n",
      "Iteration: 3046/277000 \t Elapsed time: 865.73 \t Loss:-1.26551\n",
      "Iteration: 3047/277000 \t Elapsed time: 865.93 \t Loss:-1.45596\n",
      "Iteration: 3048/277000 \t Elapsed time: 866.10 \t Loss:-1.25987\n",
      "Iteration: 3049/277000 \t Elapsed time: 866.26 \t Loss:-1.19978\n",
      "Iteration: 3050/277000 \t Elapsed time: 866.44 \t Loss:-1.42055\n",
      "Iteration: 3051/277000 \t Elapsed time: 866.60 \t Loss:-1.09922\n",
      "Iteration: 3052/277000 \t Elapsed time: 866.78 \t Loss:-1.45933\n",
      "Iteration: 3053/277000 \t Elapsed time: 866.98 \t Loss:-1.50291\n",
      "Iteration: 3054/277000 \t Elapsed time: 867.15 \t Loss:-1.47180\n",
      "Iteration: 3055/277000 \t Elapsed time: 867.33 \t Loss:-1.47497\n",
      "Iteration: 3056/277000 \t Elapsed time: 867.49 \t Loss:-1.30516\n",
      "Iteration: 3057/277000 \t Elapsed time: 867.67 \t Loss:-1.48093\n",
      "Iteration: 3058/277000 \t Elapsed time: 867.83 \t Loss:-1.49872\n",
      "Iteration: 3059/277000 \t Elapsed time: 868.01 \t Loss:-1.44192\n",
      "Iteration: 3060/277000 \t Elapsed time: 868.17 \t Loss:-1.52768\n",
      "Iteration: 3061/277000 \t Elapsed time: 868.34 \t Loss:-1.51841\n",
      "Iteration: 3062/277000 \t Elapsed time: 868.51 \t Loss:-1.19197\n",
      "Iteration: 3063/277000 \t Elapsed time: 868.86 \t Loss:-1.54911\n",
      "Iteration: 3064/277000 \t Elapsed time: 869.03 \t Loss:-1.46642\n",
      "Iteration: 3065/277000 \t Elapsed time: 869.22 \t Loss:-1.54570\n",
      "Iteration: 3066/277000 \t Elapsed time: 869.40 \t Loss:-1.55352\n",
      "Iteration: 3067/277000 \t Elapsed time: 869.57 \t Loss:-1.56775\n",
      "Iteration: 3068/277000 \t Elapsed time: 869.73 \t Loss:-1.56582\n",
      "Iteration: 3069/277000 \t Elapsed time: 870.03 \t Loss:-1.57712\n",
      "Iteration: 3070/277000 \t Elapsed time: 870.23 \t Loss:-1.58495\n",
      "Iteration: 3071/277000 \t Elapsed time: 870.39 \t Loss:-1.58466\n",
      "Iteration: 3072/277000 \t Elapsed time: 870.55 \t Loss:-1.58671\n",
      "Iteration: 3073/277000 \t Elapsed time: 870.72 \t Loss:-1.08446\n",
      "Iteration: 3074/277000 \t Elapsed time: 870.87 \t Loss:-1.58901\n",
      "Iteration: 3075/277000 \t Elapsed time: 871.15 \t Loss:-1.60117\n",
      "Iteration: 3076/277000 \t Elapsed time: 871.34 \t Loss:-1.59805\n",
      "Iteration: 3077/277000 \t Elapsed time: 871.51 \t Loss:-1.57713\n",
      "Iteration: 3078/277000 \t Elapsed time: 871.67 \t Loss:-1.58305\n",
      "Iteration: 3079/277000 \t Elapsed time: 871.86 \t Loss:-1.59732\n",
      "Iteration: 3080/277000 \t Elapsed time: 872.03 \t Loss:-1.28543\n",
      "Iteration: 3081/277000 \t Elapsed time: 872.25 \t Loss:-1.59284\n",
      "Iteration: 3082/277000 \t Elapsed time: 872.41 \t Loss:-1.62506\n",
      "Iteration: 3083/277000 \t Elapsed time: 872.58 \t Loss:-1.61876\n",
      "Iteration: 3084/277000 \t Elapsed time: 872.76 \t Loss:-1.60296\n",
      "Iteration: 3085/277000 \t Elapsed time: 872.97 \t Loss:-1.51087\n",
      "Iteration: 3086/277000 \t Elapsed time: 873.14 \t Loss:-1.63424\n",
      "Iteration: 3087/277000 \t Elapsed time: 873.32 \t Loss:-1.63614\n",
      "Iteration: 3088/277000 \t Elapsed time: 873.47 \t Loss:-1.59010\n",
      "Iteration: 3089/277000 \t Elapsed time: 873.67 \t Loss:-1.62084\n",
      "Iteration: 3090/277000 \t Elapsed time: 873.90 \t Loss:-1.64560\n",
      "Iteration: 3091/277000 \t Elapsed time: 874.07 \t Loss:-1.64752\n",
      "Iteration: 3092/277000 \t Elapsed time: 874.25 \t Loss:-1.36041\n",
      "Iteration: 3093/277000 \t Elapsed time: 874.42 \t Loss:-1.65837\n",
      "Iteration: 3094/277000 \t Elapsed time: 874.62 \t Loss:-1.64325\n",
      "Iteration: 3095/277000 \t Elapsed time: 874.81 \t Loss:-1.65406\n",
      "Iteration: 3096/277000 \t Elapsed time: 874.99 \t Loss:-1.62743\n",
      "Iteration: 3097/277000 \t Elapsed time: 875.16 \t Loss:-1.67353\n",
      "Iteration: 3098/277000 \t Elapsed time: 875.33 \t Loss:-1.67338\n",
      "Iteration: 3099/277000 \t Elapsed time: 875.51 \t Loss:-1.44993\n",
      "Iteration: 3100/277000 \t Elapsed time: 875.68 \t Loss:-1.64232\n",
      "Iteration: 3101/277000 \t Elapsed time: 875.85 \t Loss:-1.28166\n",
      "Iteration: 3102/277000 \t Elapsed time: 876.02 \t Loss:-1.47076\n",
      "Iteration: 3103/277000 \t Elapsed time: 876.19 \t Loss:-1.64502\n",
      "Iteration: 3104/277000 \t Elapsed time: 876.39 \t Loss:-1.66828\n",
      "Iteration: 3105/277000 \t Elapsed time: 876.59 \t Loss:-1.67859\n",
      "Iteration: 3106/277000 \t Elapsed time: 876.77 \t Loss:-1.58509\n",
      "Iteration: 3107/277000 \t Elapsed time: 876.94 \t Loss:-1.67835\n",
      "Iteration: 3108/277000 \t Elapsed time: 877.10 \t Loss:-1.69860\n",
      "Iteration: 3109/277000 \t Elapsed time: 877.27 \t Loss:-1.60496\n",
      "Iteration: 3110/277000 \t Elapsed time: 877.46 \t Loss:-1.70678\n",
      "Iteration: 3111/277000 \t Elapsed time: 877.63 \t Loss:-1.70471\n",
      "Iteration: 3112/277000 \t Elapsed time: 877.80 \t Loss:-1.71178\n",
      "Iteration: 3113/277000 \t Elapsed time: 877.99 \t Loss:-1.72000\n",
      "Iteration: 3114/277000 \t Elapsed time: 878.21 \t Loss:-1.72091\n",
      "Iteration: 3115/277000 \t Elapsed time: 878.37 \t Loss:-1.72832\n",
      "Iteration: 3116/277000 \t Elapsed time: 878.54 \t Loss:-1.71711\n",
      "Iteration: 3117/277000 \t Elapsed time: 878.73 \t Loss:-1.73026\n",
      "Iteration: 3118/277000 \t Elapsed time: 878.91 \t Loss:-1.74104\n",
      "Iteration: 3119/277000 \t Elapsed time: 879.11 \t Loss:-1.74508\n",
      "Iteration: 3120/277000 \t Elapsed time: 879.28 \t Loss:-1.74680\n",
      "Iteration: 3121/277000 \t Elapsed time: 879.48 \t Loss:-1.75350\n",
      "Iteration: 3122/277000 \t Elapsed time: 879.74 \t Loss:-1.69021\n",
      "Iteration: 3123/277000 \t Elapsed time: 879.92 \t Loss:-1.74759\n",
      "Iteration: 3124/277000 \t Elapsed time: 880.07 \t Loss:-1.75780\n",
      "Iteration: 3125/277000 \t Elapsed time: 880.30 \t Loss:-1.76510\n",
      "Iteration: 3126/277000 \t Elapsed time: 880.49 \t Loss:-1.76086\n",
      "Iteration: 3127/277000 \t Elapsed time: 880.67 \t Loss:-1.76959\n",
      "Iteration: 3128/277000 \t Elapsed time: 880.87 \t Loss:-0.82865\n",
      "Iteration: 3129/277000 \t Elapsed time: 881.05 \t Loss:-1.65869\n",
      "Iteration: 3130/277000 \t Elapsed time: 881.23 \t Loss:-1.67501\n",
      "Iteration: 3131/277000 \t Elapsed time: 881.56 \t Loss:-1.73473\n",
      "Iteration: 3132/277000 \t Elapsed time: 881.73 \t Loss:-1.67914\n",
      "Iteration: 3133/277000 \t Elapsed time: 881.90 \t Loss:-1.72563\n",
      "Iteration: 3134/277000 \t Elapsed time: 882.07 \t Loss:-1.74522\n",
      "Iteration: 3135/277000 \t Elapsed time: 882.24 \t Loss:-1.75253\n",
      "Iteration: 3136/277000 \t Elapsed time: 882.41 \t Loss:-1.49891\n",
      "Iteration: 3137/277000 \t Elapsed time: 882.59 \t Loss:-1.36481\n",
      "Iteration: 3138/277000 \t Elapsed time: 882.77 \t Loss:-1.48379\n",
      "Iteration: 3139/277000 \t Elapsed time: 882.95 \t Loss:-1.71028\n",
      "Iteration: 3140/277000 \t Elapsed time: 883.13 \t Loss:-1.74034\n",
      "Iteration: 3141/277000 \t Elapsed time: 883.36 \t Loss:-1.75512\n",
      "Iteration: 3142/277000 \t Elapsed time: 883.52 \t Loss:-0.00900\n",
      "Iteration: 3143/277000 \t Elapsed time: 883.69 \t Loss:-1.74911\n",
      "Iteration: 3144/277000 \t Elapsed time: 883.86 \t Loss:-1.74552\n",
      "Iteration: 3145/277000 \t Elapsed time: 884.02 \t Loss:-1.52346\n",
      "Iteration: 3146/277000 \t Elapsed time: 884.20 \t Loss:-1.61029\n",
      "Iteration: 3147/277000 \t Elapsed time: 884.37 \t Loss:-1.68591\n",
      "Iteration: 3148/277000 \t Elapsed time: 884.53 \t Loss:-1.71388\n",
      "Iteration: 3149/277000 \t Elapsed time: 884.69 \t Loss:-1.76633\n",
      "Iteration: 3150/277000 \t Elapsed time: 884.86 \t Loss:-1.77692\n",
      "Iteration: 3151/277000 \t Elapsed time: 885.02 \t Loss:-1.78797\n",
      "Iteration: 3152/277000 \t Elapsed time: 885.26 \t Loss:-1.75515\n",
      "Iteration: 3153/277000 \t Elapsed time: 885.42 \t Loss:-1.81111\n",
      "Iteration: 3154/277000 \t Elapsed time: 885.61 \t Loss:-1.81047\n",
      "Iteration: 3155/277000 \t Elapsed time: 885.78 \t Loss:-1.82068\n",
      "Iteration: 3156/277000 \t Elapsed time: 885.95 \t Loss:-1.81714\n",
      "Iteration: 3157/277000 \t Elapsed time: 886.16 \t Loss:0.25522\n",
      "Iteration: 3158/277000 \t Elapsed time: 886.36 \t Loss:-1.60229\n",
      "Iteration: 3159/277000 \t Elapsed time: 886.56 \t Loss:-1.67653\n",
      "Iteration: 3160/277000 \t Elapsed time: 886.73 \t Loss:-1.66022\n",
      "Iteration: 3161/277000 \t Elapsed time: 886.90 \t Loss:-1.28772\n",
      "Iteration: 3162/277000 \t Elapsed time: 887.06 \t Loss:-1.62697\n",
      "Iteration: 3163/277000 \t Elapsed time: 887.26 \t Loss:-1.70935\n",
      "Iteration: 3164/277000 \t Elapsed time: 887.43 \t Loss:-0.84352\n",
      "Iteration: 3165/277000 \t Elapsed time: 887.60 \t Loss:-1.72894\n",
      "Iteration: 3166/277000 \t Elapsed time: 887.82 \t Loss:-1.60154\n",
      "Iteration: 3167/277000 \t Elapsed time: 888.02 \t Loss:-1.73699\n",
      "Iteration: 3168/277000 \t Elapsed time: 888.20 \t Loss:-1.53975\n",
      "Iteration: 3169/277000 \t Elapsed time: 888.38 \t Loss:-1.78246\n",
      "Iteration: 3170/277000 \t Elapsed time: 888.54 \t Loss:-1.48818\n",
      "Iteration: 3171/277000 \t Elapsed time: 888.69 \t Loss:-1.80168\n",
      "Iteration: 3172/277000 \t Elapsed time: 888.87 \t Loss:-1.79256\n",
      "Iteration: 3173/277000 \t Elapsed time: 889.09 \t Loss:-1.81026\n",
      "Iteration: 3174/277000 \t Elapsed time: 889.30 \t Loss:-1.66301\n",
      "Iteration: 3175/277000 \t Elapsed time: 889.46 \t Loss:-1.81036\n",
      "Iteration: 3176/277000 \t Elapsed time: 889.65 \t Loss:-1.80360\n",
      "Iteration: 3177/277000 \t Elapsed time: 889.82 \t Loss:-1.84622\n",
      "Iteration: 3178/277000 \t Elapsed time: 889.99 \t Loss:-1.21184\n",
      "Iteration: 3179/277000 \t Elapsed time: 890.17 \t Loss:-1.75697\n",
      "Iteration: 3180/277000 \t Elapsed time: 890.39 \t Loss:-1.77367\n",
      "Iteration: 3181/277000 \t Elapsed time: 890.64 \t Loss:-1.66461\n",
      "Iteration: 3182/277000 \t Elapsed time: 890.85 \t Loss:-1.83938\n",
      "Iteration: 3183/277000 \t Elapsed time: 891.02 \t Loss:-1.81629\n",
      "Iteration: 3184/277000 \t Elapsed time: 891.19 \t Loss:-0.54197\n",
      "Iteration: 3185/277000 \t Elapsed time: 891.35 \t Loss:-1.33711\n",
      "Iteration: 3186/277000 \t Elapsed time: 891.52 \t Loss:-1.76399\n",
      "Iteration: 3187/277000 \t Elapsed time: 891.69 \t Loss:-1.51012\n",
      "Iteration: 3188/277000 \t Elapsed time: 891.87 \t Loss:-1.74039\n",
      "Iteration: 3189/277000 \t Elapsed time: 892.09 \t Loss:-1.79996\n",
      "Iteration: 3190/277000 \t Elapsed time: 892.27 \t Loss:-1.02882\n",
      "Iteration: 3191/277000 \t Elapsed time: 892.43 \t Loss:-0.72829\n",
      "Iteration: 3192/277000 \t Elapsed time: 892.59 \t Loss:-1.19436\n",
      "Iteration: 3193/277000 \t Elapsed time: 892.80 \t Loss:-1.66888\n",
      "Iteration: 3194/277000 \t Elapsed time: 892.96 \t Loss:-1.27174\n",
      "Iteration: 3195/277000 \t Elapsed time: 893.13 \t Loss:-1.76364\n",
      "Iteration: 3196/277000 \t Elapsed time: 893.28 \t Loss:-1.45947\n",
      "Iteration: 3197/277000 \t Elapsed time: 893.44 \t Loss:-1.63088\n",
      "Iteration: 3198/277000 \t Elapsed time: 893.77 \t Loss:-1.76198\n",
      "Iteration: 3199/277000 \t Elapsed time: 893.96 \t Loss:-1.80000\n",
      "Iteration: 3200/277000 \t Elapsed time: 894.14 \t Loss:-1.30372\n",
      "Iteration: 3201/277000 \t Elapsed time: 894.33 \t Loss:-1.80394\n",
      "Iteration: 3202/277000 \t Elapsed time: 894.53 \t Loss:-1.40408\n",
      "Iteration: 3203/277000 \t Elapsed time: 894.70 \t Loss:-1.40231\n",
      "Iteration: 3204/277000 \t Elapsed time: 894.94 \t Loss:-1.74399\n",
      "Iteration: 3205/277000 \t Elapsed time: 895.13 \t Loss:-1.69160\n",
      "Iteration: 3206/277000 \t Elapsed time: 895.29 \t Loss:-1.54534\n",
      "Iteration: 3207/277000 \t Elapsed time: 895.46 \t Loss:-1.81223\n",
      "Iteration: 3208/277000 \t Elapsed time: 895.62 \t Loss:-1.83129\n",
      "Iteration: 3209/277000 \t Elapsed time: 895.79 \t Loss:-1.86203\n",
      "Iteration: 3210/277000 \t Elapsed time: 895.95 \t Loss:-1.88728\n",
      "Iteration: 3211/277000 \t Elapsed time: 896.13 \t Loss:-1.88343\n",
      "Iteration: 3212/277000 \t Elapsed time: 896.37 \t Loss:-1.88997\n",
      "Iteration: 3213/277000 \t Elapsed time: 896.53 \t Loss:-1.56660\n",
      "Iteration: 3214/277000 \t Elapsed time: 896.70 \t Loss:-1.90532\n",
      "Iteration: 3215/277000 \t Elapsed time: 896.89 \t Loss:-1.90325\n",
      "Iteration: 3216/277000 \t Elapsed time: 897.06 \t Loss:-1.86099\n",
      "Iteration: 3217/277000 \t Elapsed time: 897.32 \t Loss:-1.80752\n",
      "Iteration: 3218/277000 \t Elapsed time: 897.53 \t Loss:-1.85373\n",
      "Iteration: 3219/277000 \t Elapsed time: 897.70 \t Loss:-1.65699\n",
      "Iteration: 3220/277000 \t Elapsed time: 897.91 \t Loss:-1.41038\n",
      "Iteration: 3221/277000 \t Elapsed time: 898.09 \t Loss:-1.88362\n",
      "Iteration: 3222/277000 \t Elapsed time: 898.26 \t Loss:-1.85936\n",
      "Iteration: 3223/277000 \t Elapsed time: 898.42 \t Loss:-1.90730\n",
      "Iteration: 3224/277000 \t Elapsed time: 898.59 \t Loss:-1.80655\n",
      "Iteration: 3225/277000 \t Elapsed time: 898.92 \t Loss:-1.89537\n",
      "Iteration: 3226/277000 \t Elapsed time: 899.09 \t Loss:-1.90529\n",
      "Iteration: 3227/277000 \t Elapsed time: 899.24 \t Loss:-1.89901\n",
      "Iteration: 3228/277000 \t Elapsed time: 899.40 \t Loss:-1.87290\n",
      "Iteration: 3229/277000 \t Elapsed time: 899.57 \t Loss:-1.16160\n",
      "Iteration: 3230/277000 \t Elapsed time: 899.84 \t Loss:-1.79483\n",
      "Iteration: 3231/277000 \t Elapsed time: 900.01 \t Loss:-1.92761\n",
      "Iteration: 3232/277000 \t Elapsed time: 900.19 \t Loss:-1.45576\n",
      "Iteration: 3233/277000 \t Elapsed time: 900.36 \t Loss:-1.89315\n",
      "Iteration: 3234/277000 \t Elapsed time: 900.52 \t Loss:-1.90889\n",
      "Iteration: 3235/277000 \t Elapsed time: 900.69 \t Loss:-1.88958\n",
      "Iteration: 3236/277000 \t Elapsed time: 900.87 \t Loss:-1.93528\n",
      "Iteration: 3237/277000 \t Elapsed time: 901.04 \t Loss:-1.65782\n",
      "Iteration: 3238/277000 \t Elapsed time: 901.25 \t Loss:-1.72899\n",
      "Iteration: 3239/277000 \t Elapsed time: 901.44 \t Loss:-1.82389\n",
      "Iteration: 3240/277000 \t Elapsed time: 901.60 \t Loss:-1.90833\n",
      "Iteration: 3241/277000 \t Elapsed time: 901.82 \t Loss:-1.52557\n",
      "Iteration: 3242/277000 \t Elapsed time: 901.99 \t Loss:-1.92242\n",
      "Iteration: 3243/277000 \t Elapsed time: 902.15 \t Loss:-1.92126\n",
      "Iteration: 3244/277000 \t Elapsed time: 902.36 \t Loss:-1.96276\n",
      "Iteration: 3245/277000 \t Elapsed time: 902.55 \t Loss:-1.94992\n",
      "Iteration: 3246/277000 \t Elapsed time: 902.73 \t Loss:-1.96152\n",
      "Iteration: 3247/277000 \t Elapsed time: 902.90 \t Loss:-1.90923\n",
      "Iteration: 3248/277000 \t Elapsed time: 903.06 \t Loss:-1.95066\n",
      "Iteration: 3249/277000 \t Elapsed time: 903.25 \t Loss:-1.79008\n",
      "Iteration: 3250/277000 \t Elapsed time: 903.46 \t Loss:-1.98721\n",
      "Iteration: 3251/277000 \t Elapsed time: 903.65 \t Loss:-1.61446\n",
      "Iteration: 3252/277000 \t Elapsed time: 903.82 \t Loss:-1.96947\n",
      "Iteration: 3253/277000 \t Elapsed time: 903.98 \t Loss:-1.99339\n",
      "Iteration: 3254/277000 \t Elapsed time: 904.15 \t Loss:-1.88240\n",
      "Iteration: 3255/277000 \t Elapsed time: 904.43 \t Loss:-2.00329\n",
      "Iteration: 3256/277000 \t Elapsed time: 904.64 \t Loss:-1.88339\n",
      "Iteration: 3257/277000 \t Elapsed time: 904.83 \t Loss:-1.96895\n",
      "Iteration: 3258/277000 \t Elapsed time: 905.02 \t Loss:-2.02385\n",
      "Iteration: 3259/277000 \t Elapsed time: 905.19 \t Loss:-1.82452\n",
      "Iteration: 3260/277000 \t Elapsed time: 905.36 \t Loss:-2.02560\n",
      "Iteration: 3261/277000 \t Elapsed time: 905.51 \t Loss:-1.97127\n",
      "Iteration: 3262/277000 \t Elapsed time: 905.70 \t Loss:-1.39696\n",
      "Iteration: 3263/277000 \t Elapsed time: 905.86 \t Loss:-2.00578\n",
      "Iteration: 3264/277000 \t Elapsed time: 906.03 \t Loss:-1.12275\n",
      "Iteration: 3265/277000 \t Elapsed time: 906.40 \t Loss:-1.91105\n",
      "Iteration: 3266/277000 \t Elapsed time: 906.56 \t Loss:-1.99916\n",
      "Iteration: 3267/277000 \t Elapsed time: 906.75 \t Loss:-1.95511\n",
      "Iteration: 3268/277000 \t Elapsed time: 906.95 \t Loss:-1.97431\n",
      "Iteration: 3269/277000 \t Elapsed time: 907.12 \t Loss:-2.02001\n",
      "Iteration: 3270/277000 \t Elapsed time: 907.28 \t Loss:-2.00132\n",
      "Iteration: 3271/277000 \t Elapsed time: 907.44 \t Loss:-1.50992\n",
      "Iteration: 3272/277000 \t Elapsed time: 907.60 \t Loss:-1.97894\n",
      "Iteration: 3273/277000 \t Elapsed time: 907.78 \t Loss:-1.95443\n",
      "Iteration: 3274/277000 \t Elapsed time: 907.96 \t Loss:-1.88071\n",
      "Iteration: 3275/277000 \t Elapsed time: 908.13 \t Loss:-2.00720\n",
      "Iteration: 3276/277000 \t Elapsed time: 908.29 \t Loss:-2.02228\n",
      "Iteration: 3277/277000 \t Elapsed time: 908.46 \t Loss:-1.53362\n",
      "Iteration: 3278/277000 \t Elapsed time: 908.65 \t Loss:-1.91645\n",
      "Iteration: 3279/277000 \t Elapsed time: 908.85 \t Loss:-1.99481\n",
      "Iteration: 3280/277000 \t Elapsed time: 909.05 \t Loss:-1.99767\n",
      "Iteration: 3281/277000 \t Elapsed time: 909.26 \t Loss:-2.05897\n",
      "Iteration: 3282/277000 \t Elapsed time: 909.44 \t Loss:-2.06633\n",
      "Iteration: 3283/277000 \t Elapsed time: 909.62 \t Loss:-2.07780\n",
      "Iteration: 3284/277000 \t Elapsed time: 909.79 \t Loss:-2.08310\n",
      "Iteration: 3285/277000 \t Elapsed time: 910.02 \t Loss:-1.66701\n",
      "Iteration: 3286/277000 \t Elapsed time: 910.21 \t Loss:-2.07928\n",
      "Iteration: 3287/277000 \t Elapsed time: 910.38 \t Loss:-2.08862\n",
      "Iteration: 3288/277000 \t Elapsed time: 910.57 \t Loss:-2.08659\n",
      "Iteration: 3289/277000 \t Elapsed time: 910.76 \t Loss:-2.08843\n",
      "Iteration: 3290/277000 \t Elapsed time: 910.98 \t Loss:-2.09955\n",
      "Iteration: 3291/277000 \t Elapsed time: 911.19 \t Loss:-2.10582\n",
      "Iteration: 3292/277000 \t Elapsed time: 911.36 \t Loss:-2.05027\n",
      "Iteration: 3293/277000 \t Elapsed time: 911.52 \t Loss:-2.11045\n",
      "Iteration: 3294/277000 \t Elapsed time: 911.69 \t Loss:-2.05690\n",
      "Iteration: 3295/277000 \t Elapsed time: 911.85 \t Loss:-2.07189\n",
      "Iteration: 3296/277000 \t Elapsed time: 912.02 \t Loss:-2.12393\n",
      "Iteration: 3297/277000 \t Elapsed time: 912.18 \t Loss:-1.88458\n",
      "Iteration: 3298/277000 \t Elapsed time: 912.38 \t Loss:-2.12830\n",
      "Iteration: 3299/277000 \t Elapsed time: 912.54 \t Loss:-1.83400\n",
      "Iteration: 3300/277000 \t Elapsed time: 912.70 \t Loss:-2.12515\n",
      "Iteration: 3301/277000 \t Elapsed time: 912.87 \t Loss:-2.13029\n",
      "Iteration: 3302/277000 \t Elapsed time: 913.03 \t Loss:-1.62960\n",
      "Iteration: 3303/277000 \t Elapsed time: 913.22 \t Loss:-2.12205\n",
      "Iteration: 3304/277000 \t Elapsed time: 913.39 \t Loss:-2.12576\n",
      "Iteration: 3305/277000 \t Elapsed time: 913.55 \t Loss:-2.12162\n",
      "Iteration: 3306/277000 \t Elapsed time: 913.72 \t Loss:-2.01427\n",
      "Iteration: 3307/277000 \t Elapsed time: 913.89 \t Loss:-2.13465\n",
      "Iteration: 3308/277000 \t Elapsed time: 914.07 \t Loss:-1.97444\n",
      "Iteration: 3309/277000 \t Elapsed time: 914.24 \t Loss:-1.72724\n",
      "Iteration: 3310/277000 \t Elapsed time: 914.40 \t Loss:-2.10044\n",
      "Iteration: 3311/277000 \t Elapsed time: 914.59 \t Loss:-2.04213\n",
      "Iteration: 3312/277000 \t Elapsed time: 914.75 \t Loss:-2.06567\n",
      "Iteration: 3313/277000 \t Elapsed time: 914.92 \t Loss:-1.98311\n",
      "Iteration: 3314/277000 \t Elapsed time: 915.08 \t Loss:-2.15363\n",
      "Iteration: 3315/277000 \t Elapsed time: 915.24 \t Loss:-1.56321\n",
      "Iteration: 3316/277000 \t Elapsed time: 915.41 \t Loss:-1.92924\n",
      "Iteration: 3317/277000 \t Elapsed time: 915.57 \t Loss:-1.67262\n",
      "Iteration: 3318/277000 \t Elapsed time: 915.75 \t Loss:-2.06849\n",
      "Iteration: 3319/277000 \t Elapsed time: 916.03 \t Loss:-2.16933\n",
      "Iteration: 3320/277000 \t Elapsed time: 916.22 \t Loss:-2.16268\n",
      "Iteration: 3321/277000 \t Elapsed time: 916.43 \t Loss:-2.16194\n",
      "Iteration: 3322/277000 \t Elapsed time: 916.61 \t Loss:-2.16417\n",
      "Iteration: 3323/277000 \t Elapsed time: 916.81 \t Loss:-2.11298\n",
      "Iteration: 3324/277000 \t Elapsed time: 916.99 \t Loss:-2.12936\n",
      "Iteration: 3325/277000 \t Elapsed time: 917.16 \t Loss:-2.15096\n",
      "Iteration: 3326/277000 \t Elapsed time: 917.34 \t Loss:-2.17978\n",
      "Iteration: 3327/277000 \t Elapsed time: 917.51 \t Loss:-2.18684\n",
      "Iteration: 3328/277000 \t Elapsed time: 917.67 \t Loss:-2.19874\n",
      "Iteration: 3329/277000 \t Elapsed time: 917.87 \t Loss:-2.10303\n",
      "Iteration: 3330/277000 \t Elapsed time: 918.04 \t Loss:-2.20078\n",
      "Iteration: 3331/277000 \t Elapsed time: 918.21 \t Loss:-2.20008\n",
      "Iteration: 3332/277000 \t Elapsed time: 918.37 \t Loss:-2.20493\n",
      "Iteration: 3333/277000 \t Elapsed time: 918.59 \t Loss:-2.21059\n",
      "Iteration: 3334/277000 \t Elapsed time: 918.80 \t Loss:-2.04476\n",
      "Iteration: 3335/277000 \t Elapsed time: 919.00 \t Loss:-1.70834\n",
      "Iteration: 3336/277000 \t Elapsed time: 919.21 \t Loss:-1.82610\n",
      "Iteration: 3337/277000 \t Elapsed time: 919.38 \t Loss:-2.06518\n",
      "Iteration: 3338/277000 \t Elapsed time: 919.55 \t Loss:-2.12951\n",
      "Iteration: 3339/277000 \t Elapsed time: 919.72 \t Loss:-1.88665\n",
      "Iteration: 3340/277000 \t Elapsed time: 919.88 \t Loss:-2.08168\n",
      "Iteration: 3341/277000 \t Elapsed time: 920.04 \t Loss:-2.13879\n",
      "Iteration: 3342/277000 \t Elapsed time: 920.22 \t Loss:-1.99570\n",
      "Iteration: 3343/277000 \t Elapsed time: 920.39 \t Loss:-2.10816\n",
      "Iteration: 3344/277000 \t Elapsed time: 920.58 \t Loss:-1.70323\n",
      "Iteration: 3345/277000 \t Elapsed time: 920.75 \t Loss:-2.13969\n",
      "Iteration: 3346/277000 \t Elapsed time: 920.95 \t Loss:-1.76893\n",
      "Iteration: 3347/277000 \t Elapsed time: 921.12 \t Loss:-2.16361\n",
      "Iteration: 3348/277000 \t Elapsed time: 921.29 \t Loss:-2.19887\n",
      "Iteration: 3349/277000 \t Elapsed time: 921.46 \t Loss:-1.55544\n",
      "Iteration: 3350/277000 \t Elapsed time: 921.62 \t Loss:-1.99561\n",
      "Iteration: 3351/277000 \t Elapsed time: 921.80 \t Loss:-2.00546\n",
      "Iteration: 3352/277000 \t Elapsed time: 921.97 \t Loss:-2.10522\n",
      "Iteration: 3353/277000 \t Elapsed time: 922.14 \t Loss:-2.19606\n",
      "Iteration: 3354/277000 \t Elapsed time: 922.37 \t Loss:-2.19177\n",
      "Iteration: 3355/277000 \t Elapsed time: 922.55 \t Loss:-2.20635\n",
      "Iteration: 3356/277000 \t Elapsed time: 922.75 \t Loss:-2.21586\n",
      "Iteration: 3357/277000 \t Elapsed time: 922.92 \t Loss:-2.23525\n",
      "Iteration: 3358/277000 \t Elapsed time: 923.09 \t Loss:-2.15384\n",
      "Iteration: 3359/277000 \t Elapsed time: 923.25 \t Loss:-2.23975\n",
      "Iteration: 3360/277000 \t Elapsed time: 923.43 \t Loss:-1.78961\n",
      "Iteration: 3361/277000 \t Elapsed time: 923.60 \t Loss:-1.74536\n",
      "Iteration: 3362/277000 \t Elapsed time: 923.77 \t Loss:-1.55656\n",
      "Iteration: 3363/277000 \t Elapsed time: 923.94 \t Loss:-2.15821\n",
      "Iteration: 3364/277000 \t Elapsed time: 924.14 \t Loss:-2.01118\n",
      "Iteration: 3365/277000 \t Elapsed time: 924.30 \t Loss:-2.18819\n",
      "Iteration: 3366/277000 \t Elapsed time: 924.47 \t Loss:-2.21079\n",
      "Iteration: 3367/277000 \t Elapsed time: 924.65 \t Loss:-2.09843\n",
      "Iteration: 3368/277000 \t Elapsed time: 924.83 \t Loss:-2.08666\n",
      "Iteration: 3369/277000 \t Elapsed time: 925.16 \t Loss:-2.22088\n",
      "Iteration: 3370/277000 \t Elapsed time: 925.35 \t Loss:-2.22988\n",
      "Iteration: 3371/277000 \t Elapsed time: 925.52 \t Loss:-1.34432\n",
      "Iteration: 3372/277000 \t Elapsed time: 925.75 \t Loss:-2.06297\n",
      "Iteration: 3373/277000 \t Elapsed time: 926.00 \t Loss:-2.14015\n",
      "Iteration: 3374/277000 \t Elapsed time: 926.18 \t Loss:-1.73834\n",
      "Iteration: 3375/277000 \t Elapsed time: 926.35 \t Loss:-1.47164\n",
      "Iteration: 3376/277000 \t Elapsed time: 926.53 \t Loss:-1.75916\n",
      "Iteration: 3377/277000 \t Elapsed time: 926.69 \t Loss:-2.19758\n",
      "Iteration: 3378/277000 \t Elapsed time: 926.86 \t Loss:-2.12388\n",
      "Iteration: 3379/277000 \t Elapsed time: 927.02 \t Loss:-2.22094\n",
      "Iteration: 3380/277000 \t Elapsed time: 927.21 \t Loss:-2.24466\n",
      "Iteration: 3381/277000 \t Elapsed time: 927.37 \t Loss:-2.17883\n",
      "Iteration: 3382/277000 \t Elapsed time: 927.53 \t Loss:-2.19346\n",
      "Iteration: 3383/277000 \t Elapsed time: 927.70 \t Loss:-2.27465\n",
      "Iteration: 3384/277000 \t Elapsed time: 927.86 \t Loss:-2.26517\n",
      "Iteration: 3385/277000 \t Elapsed time: 928.04 \t Loss:-2.00376\n",
      "Iteration: 3386/277000 \t Elapsed time: 928.23 \t Loss:-2.26023\n",
      "Iteration: 3387/277000 \t Elapsed time: 928.40 \t Loss:-2.25093\n",
      "Iteration: 3388/277000 \t Elapsed time: 928.57 \t Loss:-2.28797\n",
      "Iteration: 3389/277000 \t Elapsed time: 928.74 \t Loss:-2.28586\n",
      "Iteration: 3390/277000 \t Elapsed time: 928.94 \t Loss:-2.27780\n",
      "Iteration: 3391/277000 \t Elapsed time: 929.11 \t Loss:-1.41309\n",
      "Iteration: 3392/277000 \t Elapsed time: 929.28 \t Loss:-2.15319\n",
      "Iteration: 3393/277000 \t Elapsed time: 929.45 \t Loss:-1.75503\n",
      "Iteration: 3394/277000 \t Elapsed time: 929.62 \t Loss:-2.20089\n",
      "Iteration: 3395/277000 \t Elapsed time: 929.91 \t Loss:-1.99110\n",
      "Iteration: 3396/277000 \t Elapsed time: 930.10 \t Loss:-1.64842\n",
      "Iteration: 3397/277000 \t Elapsed time: 930.26 \t Loss:-2.11694\n",
      "Iteration: 3398/277000 \t Elapsed time: 930.42 \t Loss:-1.61062\n",
      "Iteration: 3399/277000 \t Elapsed time: 930.59 \t Loss:-2.20610\n",
      "Iteration: 3400/277000 \t Elapsed time: 930.76 \t Loss:-2.09262\n",
      "Iteration: 3401/277000 \t Elapsed time: 930.93 \t Loss:-2.19032\n",
      "Iteration: 3402/277000 \t Elapsed time: 931.13 \t Loss:-2.25074\n",
      "Iteration: 3403/277000 \t Elapsed time: 931.30 \t Loss:-2.28562\n",
      "Iteration: 3404/277000 \t Elapsed time: 931.48 \t Loss:-2.02397\n",
      "Iteration: 3405/277000 \t Elapsed time: 931.66 \t Loss:-2.27668\n",
      "Iteration: 3406/277000 \t Elapsed time: 931.86 \t Loss:-2.32177\n",
      "Iteration: 3407/277000 \t Elapsed time: 932.03 \t Loss:-1.40580\n",
      "Iteration: 3408/277000 \t Elapsed time: 932.20 \t Loss:-2.24265\n",
      "Iteration: 3409/277000 \t Elapsed time: 932.36 \t Loss:-2.13778\n",
      "Iteration: 3410/277000 \t Elapsed time: 932.55 \t Loss:-2.23720\n",
      "Iteration: 3411/277000 \t Elapsed time: 932.78 \t Loss:-2.29604\n",
      "Iteration: 3412/277000 \t Elapsed time: 932.95 \t Loss:-1.88994\n",
      "Iteration: 3413/277000 \t Elapsed time: 933.11 \t Loss:-1.70261\n",
      "Iteration: 3414/277000 \t Elapsed time: 933.27 \t Loss:-2.31053\n",
      "Iteration: 3415/277000 \t Elapsed time: 933.44 \t Loss:-2.14446\n",
      "Iteration: 3416/277000 \t Elapsed time: 933.61 \t Loss:-2.10776\n",
      "Iteration: 3417/277000 \t Elapsed time: 933.76 \t Loss:-2.27838\n",
      "Iteration: 3418/277000 \t Elapsed time: 933.92 \t Loss:-2.27795\n",
      "Iteration: 3419/277000 \t Elapsed time: 934.09 \t Loss:-2.29719\n",
      "Iteration: 3420/277000 \t Elapsed time: 934.27 \t Loss:-1.99957\n",
      "Iteration: 3421/277000 \t Elapsed time: 934.44 \t Loss:-2.29598\n",
      "Iteration: 3422/277000 \t Elapsed time: 934.62 \t Loss:-2.26425\n",
      "Iteration: 3423/277000 \t Elapsed time: 934.78 \t Loss:-2.31699\n",
      "Iteration: 3424/277000 \t Elapsed time: 934.94 \t Loss:-2.34082\n",
      "Iteration: 3425/277000 \t Elapsed time: 935.11 \t Loss:-2.26760\n",
      "Iteration: 3426/277000 \t Elapsed time: 935.31 \t Loss:-2.21849\n",
      "Iteration: 3427/277000 \t Elapsed time: 935.47 \t Loss:-2.32607\n",
      "Iteration: 3428/277000 \t Elapsed time: 935.72 \t Loss:-2.35660\n",
      "Iteration: 3429/277000 \t Elapsed time: 935.92 \t Loss:-1.41256\n",
      "Iteration: 3430/277000 \t Elapsed time: 936.09 \t Loss:-2.30221\n",
      "Iteration: 3431/277000 \t Elapsed time: 936.26 \t Loss:-2.24444\n",
      "Iteration: 3432/277000 \t Elapsed time: 936.42 \t Loss:-2.00991\n",
      "Iteration: 3433/277000 \t Elapsed time: 936.59 \t Loss:-2.35551\n",
      "Iteration: 3434/277000 \t Elapsed time: 936.78 \t Loss:-2.32639\n",
      "Iteration: 3435/277000 \t Elapsed time: 936.95 \t Loss:-2.38003\n",
      "Iteration: 3436/277000 \t Elapsed time: 937.11 \t Loss:-2.37026\n",
      "Iteration: 3437/277000 \t Elapsed time: 937.27 \t Loss:-2.38248\n",
      "Iteration: 3438/277000 \t Elapsed time: 937.43 \t Loss:-2.38917\n",
      "Iteration: 3439/277000 \t Elapsed time: 937.59 \t Loss:-2.39219\n",
      "Iteration: 3440/277000 \t Elapsed time: 937.76 \t Loss:-2.40140\n",
      "Iteration: 3441/277000 \t Elapsed time: 937.92 \t Loss:-2.18817\n",
      "Iteration: 3442/277000 \t Elapsed time: 938.10 \t Loss:-2.36375\n",
      "Iteration: 3443/277000 \t Elapsed time: 938.31 \t Loss:-2.39764\n",
      "Iteration: 3444/277000 \t Elapsed time: 938.52 \t Loss:-2.37888\n",
      "Iteration: 3445/277000 \t Elapsed time: 938.69 \t Loss:-2.38869\n",
      "Iteration: 3446/277000 \t Elapsed time: 938.85 \t Loss:-2.40886\n",
      "Iteration: 3447/277000 \t Elapsed time: 939.02 \t Loss:-2.35041\n",
      "Iteration: 3448/277000 \t Elapsed time: 939.18 \t Loss:-2.41543\n",
      "Iteration: 3449/277000 \t Elapsed time: 939.36 \t Loss:-2.42693\n",
      "Iteration: 3450/277000 \t Elapsed time: 939.53 \t Loss:-2.42576\n",
      "Iteration: 3451/277000 \t Elapsed time: 939.70 \t Loss:-2.43763\n",
      "Iteration: 3452/277000 \t Elapsed time: 939.85 \t Loss:-2.19090\n",
      "Iteration: 3453/277000 \t Elapsed time: 940.01 \t Loss:-2.41045\n",
      "Iteration: 3454/277000 \t Elapsed time: 940.17 \t Loss:-2.44286\n",
      "Iteration: 3455/277000 \t Elapsed time: 940.33 \t Loss:-2.42294\n",
      "Iteration: 3456/277000 \t Elapsed time: 940.49 \t Loss:-2.44726\n",
      "Iteration: 3457/277000 \t Elapsed time: 940.65 \t Loss:-2.43354\n",
      "Iteration: 3458/277000 \t Elapsed time: 940.82 \t Loss:-2.13433\n",
      "Iteration: 3459/277000 \t Elapsed time: 940.98 \t Loss:-2.09556\n",
      "Iteration: 3460/277000 \t Elapsed time: 941.14 \t Loss:-1.88935\n",
      "Iteration: 3461/277000 \t Elapsed time: 941.31 \t Loss:-2.39535\n",
      "Iteration: 3462/277000 \t Elapsed time: 941.47 \t Loss:-2.28234\n",
      "Iteration: 3463/277000 \t Elapsed time: 941.63 \t Loss:-1.61682\n",
      "Iteration: 3464/277000 \t Elapsed time: 941.79 \t Loss:-2.05031\n",
      "Iteration: 3465/277000 \t Elapsed time: 941.96 \t Loss:-2.29578\n",
      "Iteration: 3466/277000 \t Elapsed time: 942.12 \t Loss:-2.36108\n",
      "Iteration: 3467/277000 \t Elapsed time: 942.28 \t Loss:-2.42123\n",
      "Iteration: 3468/277000 \t Elapsed time: 942.43 \t Loss:-2.38832\n",
      "Iteration: 3469/277000 \t Elapsed time: 942.60 \t Loss:-1.37699\n",
      "Iteration: 3470/277000 \t Elapsed time: 942.76 \t Loss:-2.29658\n",
      "Iteration: 3471/277000 \t Elapsed time: 942.95 \t Loss:-2.27310\n",
      "Iteration: 3472/277000 \t Elapsed time: 943.12 \t Loss:-2.26396\n",
      "Iteration: 3473/277000 \t Elapsed time: 943.28 \t Loss:-2.26148\n",
      "Iteration: 3474/277000 \t Elapsed time: 943.45 \t Loss:-2.44112\n",
      "Iteration: 3475/277000 \t Elapsed time: 943.61 \t Loss:-2.13815\n",
      "Iteration: 3476/277000 \t Elapsed time: 943.81 \t Loss:-2.41250\n",
      "Iteration: 3477/277000 \t Elapsed time: 943.98 \t Loss:-2.40608\n",
      "Iteration: 3478/277000 \t Elapsed time: 944.17 \t Loss:-2.43903\n",
      "Iteration: 3479/277000 \t Elapsed time: 944.34 \t Loss:-2.25395\n",
      "Iteration: 3480/277000 \t Elapsed time: 944.52 \t Loss:-2.40206\n",
      "Iteration: 3481/277000 \t Elapsed time: 944.69 \t Loss:-2.16679\n",
      "Iteration: 3482/277000 \t Elapsed time: 944.86 \t Loss:-1.69101\n",
      "Iteration: 3483/277000 \t Elapsed time: 945.02 \t Loss:-2.40433\n",
      "Iteration: 3484/277000 \t Elapsed time: 945.26 \t Loss:-1.75117\n",
      "Iteration: 3485/277000 \t Elapsed time: 945.45 \t Loss:-2.17426\n",
      "Iteration: 3486/277000 \t Elapsed time: 945.62 \t Loss:-2.42213\n",
      "Iteration: 3487/277000 \t Elapsed time: 945.78 \t Loss:-1.92099\n",
      "Iteration: 3488/277000 \t Elapsed time: 945.94 \t Loss:-1.92939\n",
      "Iteration: 3489/277000 \t Elapsed time: 946.11 \t Loss:-2.34763\n",
      "Iteration: 3490/277000 \t Elapsed time: 946.29 \t Loss:-2.43221\n",
      "Iteration: 3491/277000 \t Elapsed time: 946.46 \t Loss:-2.42990\n",
      "Iteration: 3492/277000 \t Elapsed time: 946.65 \t Loss:-2.45621\n",
      "Iteration: 3493/277000 \t Elapsed time: 946.85 \t Loss:-2.47783\n",
      "Iteration: 3494/277000 \t Elapsed time: 947.01 \t Loss:-2.43089\n",
      "Iteration: 3495/277000 \t Elapsed time: 947.17 \t Loss:-1.99943\n",
      "Iteration: 3496/277000 \t Elapsed time: 947.33 \t Loss:-2.39141\n",
      "Iteration: 3497/277000 \t Elapsed time: 947.50 \t Loss:-2.40906\n",
      "Iteration: 3498/277000 \t Elapsed time: 947.65 \t Loss:-1.65734\n",
      "Iteration: 3499/277000 \t Elapsed time: 948.05 \t Loss:-2.23673\n",
      "Iteration: 3500/277000 \t Elapsed time: 948.21 \t Loss:-2.41289\n",
      "Iteration: 3501/277000 \t Elapsed time: 948.37 \t Loss:-2.38860\n",
      "Iteration: 3502/277000 \t Elapsed time: 948.54 \t Loss:-2.44543\n",
      "Iteration: 3503/277000 \t Elapsed time: 948.70 \t Loss:-2.46630\n",
      "Iteration: 3504/277000 \t Elapsed time: 948.87 \t Loss:-2.50110\n",
      "Iteration: 3505/277000 \t Elapsed time: 949.03 \t Loss:-2.48568\n",
      "Iteration: 3506/277000 \t Elapsed time: 949.19 \t Loss:-2.40223\n",
      "Iteration: 3507/277000 \t Elapsed time: 949.35 \t Loss:-2.42609\n",
      "Iteration: 3508/277000 \t Elapsed time: 949.51 \t Loss:-2.49917\n",
      "Iteration: 3509/277000 \t Elapsed time: 949.67 \t Loss:-2.44399\n",
      "Iteration: 3510/277000 \t Elapsed time: 949.83 \t Loss:-2.43352\n",
      "Iteration: 3511/277000 \t Elapsed time: 950.00 \t Loss:-2.45223\n",
      "Iteration: 3512/277000 \t Elapsed time: 950.17 \t Loss:-2.51747\n",
      "Iteration: 3513/277000 \t Elapsed time: 950.35 \t Loss:-1.22358\n",
      "Iteration: 3514/277000 \t Elapsed time: 950.51 \t Loss:-2.33201\n",
      "Iteration: 3515/277000 \t Elapsed time: 950.66 \t Loss:-2.41160\n",
      "Iteration: 3516/277000 \t Elapsed time: 950.83 \t Loss:-2.47481\n",
      "Iteration: 3517/277000 \t Elapsed time: 950.98 \t Loss:-1.51701\n",
      "Iteration: 3518/277000 \t Elapsed time: 951.15 \t Loss:-1.73755\n",
      "Iteration: 3519/277000 \t Elapsed time: 951.31 \t Loss:-2.38062\n",
      "Iteration: 3520/277000 \t Elapsed time: 952.21 \t Loss:-2.30911\n",
      "Iteration: 3521/277000 \t Elapsed time: 952.39 \t Loss:-2.41755\n",
      "Iteration: 3522/277000 \t Elapsed time: 952.55 \t Loss:-1.62947\n",
      "Iteration: 3523/277000 \t Elapsed time: 952.71 \t Loss:-2.35794\n",
      "Iteration: 3524/277000 \t Elapsed time: 952.90 \t Loss:-1.20407\n",
      "Iteration: 3525/277000 \t Elapsed time: 953.09 \t Loss:-2.34634\n",
      "Iteration: 3526/277000 \t Elapsed time: 953.27 \t Loss:-2.44908\n",
      "Iteration: 3527/277000 \t Elapsed time: 953.75 \t Loss:-2.43319\n",
      "Iteration: 3528/277000 \t Elapsed time: 953.97 \t Loss:-2.33834\n",
      "Iteration: 3529/277000 \t Elapsed time: 954.13 \t Loss:-2.41842\n",
      "Iteration: 3530/277000 \t Elapsed time: 954.30 \t Loss:-2.33844\n",
      "Iteration: 3531/277000 \t Elapsed time: 954.47 \t Loss:-2.50580\n",
      "Iteration: 3532/277000 \t Elapsed time: 954.69 \t Loss:-2.07488\n",
      "Iteration: 3533/277000 \t Elapsed time: 954.92 \t Loss:-1.52943\n",
      "Iteration: 3534/277000 \t Elapsed time: 955.09 \t Loss:-2.43148\n",
      "Iteration: 3535/277000 \t Elapsed time: 955.25 \t Loss:-2.45009\n",
      "Iteration: 3536/277000 \t Elapsed time: 955.41 \t Loss:-1.80768\n",
      "Iteration: 3537/277000 \t Elapsed time: 955.58 \t Loss:-2.32242\n",
      "Iteration: 3538/277000 \t Elapsed time: 955.74 \t Loss:-2.49034\n",
      "Iteration: 3539/277000 \t Elapsed time: 955.99 \t Loss:-2.50275\n",
      "Iteration: 3540/277000 \t Elapsed time: 956.15 \t Loss:-2.53352\n",
      "Iteration: 3541/277000 \t Elapsed time: 956.32 \t Loss:-2.40905\n",
      "Iteration: 3542/277000 \t Elapsed time: 956.48 \t Loss:-2.53288\n",
      "Iteration: 3543/277000 \t Elapsed time: 956.65 \t Loss:-2.46125\n",
      "Iteration: 3544/277000 \t Elapsed time: 956.82 \t Loss:-2.56266\n",
      "Iteration: 3545/277000 \t Elapsed time: 956.99 \t Loss:-1.19629\n",
      "Iteration: 3546/277000 \t Elapsed time: 957.18 \t Loss:-1.49237\n",
      "Iteration: 3547/277000 \t Elapsed time: 957.35 \t Loss:-2.18495\n",
      "Iteration: 3548/277000 \t Elapsed time: 957.52 \t Loss:-2.30756\n",
      "Iteration: 3549/277000 \t Elapsed time: 957.68 \t Loss:-2.41535\n",
      "Iteration: 3550/277000 \t Elapsed time: 957.84 \t Loss:-2.33067\n",
      "Iteration: 3551/277000 \t Elapsed time: 958.01 \t Loss:-1.73327\n",
      "Iteration: 3552/277000 \t Elapsed time: 958.16 \t Loss:-2.48915\n",
      "Iteration: 3553/277000 \t Elapsed time: 958.32 \t Loss:-2.04742\n",
      "Iteration: 3554/277000 \t Elapsed time: 958.49 \t Loss:-1.53988\n",
      "Iteration: 3555/277000 \t Elapsed time: 958.66 \t Loss:-2.06720\n",
      "Iteration: 3556/277000 \t Elapsed time: 958.82 \t Loss:-2.24220\n",
      "Iteration: 3557/277000 \t Elapsed time: 958.99 \t Loss:-2.44895\n",
      "Iteration: 3558/277000 \t Elapsed time: 959.15 \t Loss:-2.43908\n",
      "Iteration: 3559/277000 \t Elapsed time: 959.33 \t Loss:-1.74369\n",
      "Iteration: 3560/277000 \t Elapsed time: 959.48 \t Loss:-1.83941\n",
      "Iteration: 3561/277000 \t Elapsed time: 959.64 \t Loss:-2.10032\n",
      "Iteration: 3562/277000 \t Elapsed time: 959.86 \t Loss:-2.30814\n",
      "Iteration: 3563/277000 \t Elapsed time: 960.04 \t Loss:-2.43546\n",
      "Iteration: 3564/277000 \t Elapsed time: 960.32 \t Loss:-2.44090\n",
      "Iteration: 3565/277000 \t Elapsed time: 960.54 \t Loss:-2.42622\n",
      "Iteration: 3566/277000 \t Elapsed time: 960.74 \t Loss:-2.44490\n",
      "Iteration: 3567/277000 \t Elapsed time: 960.94 \t Loss:-2.52028\n",
      "Iteration: 3568/277000 \t Elapsed time: 961.10 \t Loss:-2.51033\n",
      "Iteration: 3569/277000 \t Elapsed time: 961.26 \t Loss:-2.36949\n",
      "Iteration: 3570/277000 \t Elapsed time: 961.53 \t Loss:-2.30558\n",
      "Iteration: 3571/277000 \t Elapsed time: 961.70 \t Loss:-2.54643\n",
      "Iteration: 3572/277000 \t Elapsed time: 961.94 \t Loss:-2.56196\n",
      "Iteration: 3573/277000 \t Elapsed time: 962.12 \t Loss:-2.54941\n",
      "Iteration: 3574/277000 \t Elapsed time: 962.30 \t Loss:-2.57237\n",
      "Iteration: 3575/277000 \t Elapsed time: 962.47 \t Loss:-2.58224\n",
      "Iteration: 3576/277000 \t Elapsed time: 962.64 \t Loss:-2.58191\n",
      "Iteration: 3577/277000 \t Elapsed time: 962.80 \t Loss:-2.53749\n",
      "Iteration: 3578/277000 \t Elapsed time: 962.96 \t Loss:-2.59590\n",
      "Iteration: 3579/277000 \t Elapsed time: 963.15 \t Loss:-2.60881\n",
      "Iteration: 3580/277000 \t Elapsed time: 963.33 \t Loss:-2.61383\n",
      "Iteration: 3581/277000 \t Elapsed time: 963.49 \t Loss:-2.61959\n",
      "Iteration: 3582/277000 \t Elapsed time: 963.65 \t Loss:-1.75762\n",
      "Iteration: 3583/277000 \t Elapsed time: 963.82 \t Loss:-2.44198\n",
      "Iteration: 3584/277000 \t Elapsed time: 963.99 \t Loss:-2.60050\n",
      "Iteration: 3585/277000 \t Elapsed time: 964.16 \t Loss:-2.56506\n",
      "Iteration: 3586/277000 \t Elapsed time: 964.35 \t Loss:-2.23024\n",
      "Iteration: 3587/277000 \t Elapsed time: 964.53 \t Loss:-2.05542\n",
      "Iteration: 3588/277000 \t Elapsed time: 964.69 \t Loss:-2.57089\n",
      "Iteration: 3589/277000 \t Elapsed time: 964.86 \t Loss:-1.92619\n",
      "Iteration: 3590/277000 \t Elapsed time: 965.03 \t Loss:-2.58113\n",
      "Iteration: 3591/277000 \t Elapsed time: 965.26 \t Loss:-2.17630\n",
      "Iteration: 3592/277000 \t Elapsed time: 965.44 \t Loss:-2.30683\n",
      "Iteration: 3593/277000 \t Elapsed time: 965.60 \t Loss:-2.56604\n",
      "Iteration: 3594/277000 \t Elapsed time: 965.78 \t Loss:-2.48233\n",
      "Iteration: 3595/277000 \t Elapsed time: 965.98 \t Loss:-2.57108\n",
      "Iteration: 3596/277000 \t Elapsed time: 966.16 \t Loss:-2.62265\n",
      "Iteration: 3597/277000 \t Elapsed time: 966.34 \t Loss:-2.43848\n",
      "Iteration: 3598/277000 \t Elapsed time: 966.53 \t Loss:-2.57211\n",
      "Iteration: 3599/277000 \t Elapsed time: 966.70 \t Loss:-2.52877\n",
      "Iteration: 3600/277000 \t Elapsed time: 966.86 \t Loss:-2.61555\n",
      "Iteration: 3601/277000 \t Elapsed time: 967.04 \t Loss:-2.00300\n",
      "Iteration: 3602/277000 \t Elapsed time: 967.27 \t Loss:-2.63192\n",
      "Iteration: 3603/277000 \t Elapsed time: 967.44 \t Loss:-2.21160\n",
      "Iteration: 3604/277000 \t Elapsed time: 967.62 \t Loss:-2.35624\n",
      "Iteration: 3605/277000 \t Elapsed time: 967.78 \t Loss:-2.62644\n",
      "Iteration: 3606/277000 \t Elapsed time: 967.95 \t Loss:-2.64546\n",
      "Iteration: 3607/277000 \t Elapsed time: 968.10 \t Loss:-2.06322\n",
      "Iteration: 3608/277000 \t Elapsed time: 968.26 \t Loss:-2.50597\n",
      "Iteration: 3609/277000 \t Elapsed time: 968.43 \t Loss:-2.53202\n",
      "Iteration: 3610/277000 \t Elapsed time: 968.66 \t Loss:-2.36760\n",
      "Iteration: 3611/277000 \t Elapsed time: 969.26 \t Loss:-2.64766\n",
      "Iteration: 3612/277000 \t Elapsed time: 969.45 \t Loss:-2.57141\n",
      "Iteration: 3613/277000 \t Elapsed time: 969.65 \t Loss:-2.66990\n",
      "Iteration: 3614/277000 \t Elapsed time: 969.87 \t Loss:-2.68464\n",
      "Iteration: 3615/277000 \t Elapsed time: 970.06 \t Loss:-2.67275\n",
      "Iteration: 3616/277000 \t Elapsed time: 970.26 \t Loss:-2.02677\n",
      "Iteration: 3617/277000 \t Elapsed time: 970.42 \t Loss:-2.36644\n",
      "Iteration: 3618/277000 \t Elapsed time: 970.59 \t Loss:-2.57917\n",
      "Iteration: 3619/277000 \t Elapsed time: 970.78 \t Loss:-2.43160\n",
      "Iteration: 3620/277000 \t Elapsed time: 970.95 \t Loss:-2.69403\n",
      "Iteration: 3621/277000 \t Elapsed time: 971.11 \t Loss:-2.69047\n",
      "Iteration: 3622/277000 \t Elapsed time: 971.28 \t Loss:-2.55213\n",
      "Iteration: 3623/277000 \t Elapsed time: 971.43 \t Loss:-2.69783\n",
      "Iteration: 3624/277000 \t Elapsed time: 971.62 \t Loss:-2.66233\n",
      "Iteration: 3625/277000 \t Elapsed time: 971.82 \t Loss:-2.70717\n",
      "Iteration: 3626/277000 \t Elapsed time: 971.98 \t Loss:-2.70171\n",
      "Iteration: 3627/277000 \t Elapsed time: 972.15 \t Loss:-1.61549\n",
      "Iteration: 3628/277000 \t Elapsed time: 972.31 \t Loss:-2.67900\n",
      "Iteration: 3629/277000 \t Elapsed time: 972.47 \t Loss:-2.67610\n",
      "Iteration: 3630/277000 \t Elapsed time: 972.65 \t Loss:-2.71890\n",
      "Iteration: 3631/277000 \t Elapsed time: 972.96 \t Loss:-2.14991\n",
      "Iteration: 3632/277000 \t Elapsed time: 973.12 \t Loss:-2.53966\n",
      "Iteration: 3633/277000 \t Elapsed time: 973.31 \t Loss:-2.52687\n",
      "Iteration: 3634/277000 \t Elapsed time: 973.48 \t Loss:-2.70225\n",
      "Iteration: 3635/277000 \t Elapsed time: 973.65 \t Loss:-2.69443\n",
      "Iteration: 3636/277000 \t Elapsed time: 973.85 \t Loss:-2.70120\n",
      "Iteration: 3637/277000 \t Elapsed time: 974.01 \t Loss:-2.70825\n",
      "Iteration: 3638/277000 \t Elapsed time: 974.18 \t Loss:-2.72729\n",
      "Iteration: 3639/277000 \t Elapsed time: 974.38 \t Loss:-2.09050\n",
      "Iteration: 3640/277000 \t Elapsed time: 974.55 \t Loss:-2.73628\n",
      "Iteration: 3641/277000 \t Elapsed time: 974.75 \t Loss:-2.70124\n",
      "Iteration: 3642/277000 \t Elapsed time: 975.06 \t Loss:-2.74221\n",
      "Iteration: 3643/277000 \t Elapsed time: 975.31 \t Loss:-2.72079\n",
      "Iteration: 3644/277000 \t Elapsed time: 975.47 \t Loss:-2.74627\n",
      "Iteration: 3645/277000 \t Elapsed time: 975.66 \t Loss:-2.74993\n",
      "Iteration: 3646/277000 \t Elapsed time: 975.82 \t Loss:-2.53980\n",
      "Iteration: 3647/277000 \t Elapsed time: 976.04 \t Loss:-2.74492\n",
      "Iteration: 3648/277000 \t Elapsed time: 976.21 \t Loss:-2.74283\n",
      "Iteration: 3649/277000 \t Elapsed time: 976.37 \t Loss:-1.85831\n",
      "Iteration: 3650/277000 \t Elapsed time: 976.54 \t Loss:-2.72529\n",
      "Iteration: 3651/277000 \t Elapsed time: 976.70 \t Loss:-2.63364\n",
      "Iteration: 3652/277000 \t Elapsed time: 976.87 \t Loss:-2.59915\n",
      "Iteration: 3653/277000 \t Elapsed time: 977.03 \t Loss:-2.49507\n",
      "Iteration: 3654/277000 \t Elapsed time: 977.23 \t Loss:-2.73329\n",
      "Iteration: 3655/277000 \t Elapsed time: 977.39 \t Loss:-2.73113\n",
      "Iteration: 3656/277000 \t Elapsed time: 977.56 \t Loss:-2.42154\n",
      "Iteration: 3657/277000 \t Elapsed time: 977.74 \t Loss:-2.64072\n",
      "Iteration: 3658/277000 \t Elapsed time: 977.89 \t Loss:-2.74398\n",
      "Iteration: 3659/277000 \t Elapsed time: 978.05 \t Loss:-2.65949\n",
      "Iteration: 3660/277000 \t Elapsed time: 978.22 \t Loss:-2.73731\n",
      "Iteration: 3661/277000 \t Elapsed time: 978.65 \t Loss:-2.77271\n",
      "Iteration: 3662/277000 \t Elapsed time: 978.87 \t Loss:-2.19011\n",
      "Iteration: 3663/277000 \t Elapsed time: 979.05 \t Loss:-2.72108\n",
      "Iteration: 3664/277000 \t Elapsed time: 979.24 \t Loss:-2.72562\n",
      "Iteration: 3665/277000 \t Elapsed time: 979.41 \t Loss:-2.39427\n",
      "Iteration: 3666/277000 \t Elapsed time: 979.59 \t Loss:-2.74712\n",
      "Iteration: 3667/277000 \t Elapsed time: 979.78 \t Loss:-2.77163\n",
      "Iteration: 3668/277000 \t Elapsed time: 979.95 \t Loss:-2.69578\n",
      "Iteration: 3669/277000 \t Elapsed time: 980.12 \t Loss:-2.77538\n",
      "Iteration: 3670/277000 \t Elapsed time: 980.29 \t Loss:-2.71371\n",
      "Iteration: 3671/277000 \t Elapsed time: 980.48 \t Loss:-2.78130\n",
      "Iteration: 3672/277000 \t Elapsed time: 980.64 \t Loss:-2.59639\n",
      "Iteration: 3673/277000 \t Elapsed time: 980.87 \t Loss:-2.76436\n",
      "Iteration: 3674/277000 \t Elapsed time: 981.04 \t Loss:-2.74089\n",
      "Iteration: 3675/277000 \t Elapsed time: 981.21 \t Loss:-2.78644\n",
      "Iteration: 3676/277000 \t Elapsed time: 981.44 \t Loss:-2.68645\n",
      "Iteration: 3677/277000 \t Elapsed time: 981.64 \t Loss:-2.43715\n",
      "Iteration: 3678/277000 \t Elapsed time: 981.81 \t Loss:-2.79122\n",
      "Iteration: 3679/277000 \t Elapsed time: 981.98 \t Loss:-2.81239\n",
      "Iteration: 3680/277000 \t Elapsed time: 982.25 \t Loss:-2.79121\n",
      "Iteration: 3681/277000 \t Elapsed time: 982.42 \t Loss:-2.45154\n",
      "Iteration: 3682/277000 \t Elapsed time: 982.59 \t Loss:-2.71879\n",
      "Iteration: 3683/277000 \t Elapsed time: 982.77 \t Loss:-2.79130\n",
      "Iteration: 3684/277000 \t Elapsed time: 982.97 \t Loss:-2.80245\n",
      "Iteration: 3685/277000 \t Elapsed time: 983.17 \t Loss:-2.79548\n",
      "Iteration: 3686/277000 \t Elapsed time: 983.37 \t Loss:-2.82655\n",
      "Iteration: 3687/277000 \t Elapsed time: 983.53 \t Loss:-2.81245\n",
      "Iteration: 3688/277000 \t Elapsed time: 983.72 \t Loss:-2.60806\n",
      "Iteration: 3689/277000 \t Elapsed time: 983.90 \t Loss:-2.83022\n",
      "Iteration: 3690/277000 \t Elapsed time: 984.10 \t Loss:-2.83516\n",
      "Iteration: 3691/277000 \t Elapsed time: 984.29 \t Loss:-2.83764\n",
      "Iteration: 3692/277000 \t Elapsed time: 984.45 \t Loss:-2.84989\n",
      "Iteration: 3693/277000 \t Elapsed time: 984.61 \t Loss:-2.85503\n",
      "Iteration: 3694/277000 \t Elapsed time: 984.78 \t Loss:-2.85258\n",
      "Iteration: 3695/277000 \t Elapsed time: 984.96 \t Loss:-1.81676\n",
      "Iteration: 3696/277000 \t Elapsed time: 985.13 \t Loss:-1.72221\n",
      "Iteration: 3697/277000 \t Elapsed time: 985.30 \t Loss:-2.78585\n",
      "Iteration: 3698/277000 \t Elapsed time: 985.47 \t Loss:-2.53750\n",
      "Iteration: 3699/277000 \t Elapsed time: 985.63 \t Loss:-2.77058\n",
      "Iteration: 3700/277000 \t Elapsed time: 985.79 \t Loss:-2.76154\n",
      "Iteration: 3701/277000 \t Elapsed time: 985.95 \t Loss:-2.80722\n",
      "Iteration: 3702/277000 \t Elapsed time: 986.12 \t Loss:-2.13872\n",
      "Iteration: 3703/277000 \t Elapsed time: 986.34 \t Loss:-2.19462\n",
      "Iteration: 3704/277000 \t Elapsed time: 986.51 \t Loss:-2.78471\n",
      "Iteration: 3705/277000 \t Elapsed time: 986.68 \t Loss:-2.82532\n",
      "Iteration: 3706/277000 \t Elapsed time: 986.87 \t Loss:-2.82796\n",
      "Iteration: 3707/277000 \t Elapsed time: 987.04 \t Loss:-2.82871\n",
      "Iteration: 3708/277000 \t Elapsed time: 987.20 \t Loss:-2.47655\n",
      "Iteration: 3709/277000 \t Elapsed time: 987.37 \t Loss:-2.64043\n",
      "Iteration: 3710/277000 \t Elapsed time: 987.53 \t Loss:-2.80544\n",
      "Iteration: 3711/277000 \t Elapsed time: 987.74 \t Loss:-2.85269\n",
      "Iteration: 3712/277000 \t Elapsed time: 987.92 \t Loss:-2.79459\n",
      "Iteration: 3713/277000 \t Elapsed time: 988.07 \t Loss:-2.14495\n",
      "Iteration: 3714/277000 \t Elapsed time: 988.23 \t Loss:-2.82692\n",
      "Iteration: 3715/277000 \t Elapsed time: 988.40 \t Loss:-2.65888\n",
      "Iteration: 3716/277000 \t Elapsed time: 988.56 \t Loss:-2.85156\n",
      "Iteration: 3717/277000 \t Elapsed time: 988.72 \t Loss:-2.51090\n",
      "Iteration: 3718/277000 \t Elapsed time: 988.90 \t Loss:-2.77344\n",
      "Iteration: 3719/277000 \t Elapsed time: 989.08 \t Loss:-2.62598\n",
      "Iteration: 3720/277000 \t Elapsed time: 989.25 \t Loss:-2.85829\n",
      "Iteration: 3721/277000 \t Elapsed time: 989.42 \t Loss:-2.87118\n",
      "Iteration: 3722/277000 \t Elapsed time: 989.62 \t Loss:-2.88207\n",
      "Iteration: 3723/277000 \t Elapsed time: 989.84 \t Loss:-2.88043\n",
      "Iteration: 3724/277000 \t Elapsed time: 990.04 \t Loss:-2.02940\n",
      "Iteration: 3725/277000 \t Elapsed time: 990.21 \t Loss:-2.88093\n",
      "Iteration: 3726/277000 \t Elapsed time: 990.37 \t Loss:-2.03081\n",
      "Iteration: 3727/277000 \t Elapsed time: 990.53 \t Loss:-2.80720\n",
      "Iteration: 3728/277000 \t Elapsed time: 990.71 \t Loss:-2.51928\n",
      "Iteration: 3729/277000 \t Elapsed time: 990.87 \t Loss:-2.59922\n",
      "Iteration: 3730/277000 \t Elapsed time: 991.05 \t Loss:-2.69989\n",
      "Iteration: 3731/277000 \t Elapsed time: 991.24 \t Loss:-2.87496\n",
      "Iteration: 3732/277000 \t Elapsed time: 991.41 \t Loss:-1.57142\n",
      "Iteration: 3733/277000 \t Elapsed time: 991.60 \t Loss:-2.78790\n",
      "Iteration: 3734/277000 \t Elapsed time: 991.78 \t Loss:-2.77439\n",
      "Iteration: 3735/277000 \t Elapsed time: 991.94 \t Loss:-2.65876\n",
      "Iteration: 3736/277000 \t Elapsed time: 992.12 \t Loss:-2.82555\n",
      "Iteration: 3737/277000 \t Elapsed time: 992.28 \t Loss:-2.83243\n",
      "Iteration: 3738/277000 \t Elapsed time: 992.45 \t Loss:-2.77076\n",
      "Iteration: 3739/277000 \t Elapsed time: 992.61 \t Loss:-2.83657\n",
      "Iteration: 3740/277000 \t Elapsed time: 992.88 \t Loss:-2.66425\n",
      "Iteration: 3741/277000 \t Elapsed time: 993.05 \t Loss:-2.29713\n",
      "Iteration: 3742/277000 \t Elapsed time: 993.24 \t Loss:-2.39682\n",
      "Iteration: 3743/277000 \t Elapsed time: 993.42 \t Loss:-2.67291\n",
      "Iteration: 3744/277000 \t Elapsed time: 993.61 \t Loss:-2.78000\n",
      "Iteration: 3745/277000 \t Elapsed time: 993.79 \t Loss:-2.61513\n",
      "Iteration: 3746/277000 \t Elapsed time: 993.97 \t Loss:-1.89756\n",
      "Iteration: 3747/277000 \t Elapsed time: 994.14 \t Loss:-2.68250\n",
      "Iteration: 3748/277000 \t Elapsed time: 994.31 \t Loss:-2.39438\n",
      "Iteration: 3749/277000 \t Elapsed time: 994.49 \t Loss:-2.55173\n",
      "Iteration: 3750/277000 \t Elapsed time: 994.72 \t Loss:-2.80493\n",
      "Iteration: 3751/277000 \t Elapsed time: 994.88 \t Loss:-2.82590\n",
      "Iteration: 3752/277000 \t Elapsed time: 995.06 \t Loss:-2.85625\n",
      "Iteration: 3753/277000 \t Elapsed time: 995.24 \t Loss:-2.70466\n",
      "Iteration: 3754/277000 \t Elapsed time: 995.41 \t Loss:-2.87769\n",
      "Iteration: 3755/277000 \t Elapsed time: 995.67 \t Loss:-2.87506\n",
      "Iteration: 3756/277000 \t Elapsed time: 995.85 \t Loss:-1.78952\n",
      "Iteration: 3757/277000 \t Elapsed time: 996.05 \t Loss:-2.22400\n",
      "Iteration: 3758/277000 \t Elapsed time: 996.21 \t Loss:-2.81549\n",
      "Iteration: 3759/277000 \t Elapsed time: 996.38 \t Loss:-2.86454\n",
      "Iteration: 3760/277000 \t Elapsed time: 996.56 \t Loss:-2.73476\n",
      "Iteration: 3761/277000 \t Elapsed time: 996.76 \t Loss:-2.88940\n",
      "Iteration: 3762/277000 \t Elapsed time: 996.94 \t Loss:-2.84699\n",
      "Iteration: 3763/277000 \t Elapsed time: 997.20 \t Loss:-2.90862\n",
      "Iteration: 3764/277000 \t Elapsed time: 997.40 \t Loss:-2.00949\n",
      "Iteration: 3765/277000 \t Elapsed time: 997.56 \t Loss:-2.35667\n",
      "Iteration: 3766/277000 \t Elapsed time: 997.73 \t Loss:-2.62347\n",
      "Iteration: 3767/277000 \t Elapsed time: 997.90 \t Loss:-2.70310\n",
      "Iteration: 3768/277000 \t Elapsed time: 998.06 \t Loss:-2.81312\n",
      "Iteration: 3769/277000 \t Elapsed time: 998.23 \t Loss:-2.66349\n",
      "Iteration: 3770/277000 \t Elapsed time: 998.40 \t Loss:-2.76836\n",
      "Iteration: 3771/277000 \t Elapsed time: 998.57 \t Loss:-2.68968\n",
      "Iteration: 3772/277000 \t Elapsed time: 998.74 \t Loss:-2.87240\n",
      "Iteration: 3773/277000 \t Elapsed time: 998.90 \t Loss:-2.70912\n",
      "Iteration: 3774/277000 \t Elapsed time: 999.07 \t Loss:-2.89172\n",
      "Iteration: 3775/277000 \t Elapsed time: 999.23 \t Loss:-2.73419\n",
      "Iteration: 3776/277000 \t Elapsed time: 999.39 \t Loss:-2.91289\n",
      "Iteration: 3777/277000 \t Elapsed time: 999.59 \t Loss:-2.26651\n",
      "Iteration: 3778/277000 \t Elapsed time: 999.76 \t Loss:-2.88469\n",
      "Iteration: 3779/277000 \t Elapsed time: 999.92 \t Loss:-2.81086\n",
      "Iteration: 3780/277000 \t Elapsed time: 1000.08 \t Loss:-2.45407\n",
      "Iteration: 3781/277000 \t Elapsed time: 1000.25 \t Loss:-2.83110\n",
      "Iteration: 3782/277000 \t Elapsed time: 1000.42 \t Loss:-2.68609\n",
      "Iteration: 3783/277000 \t Elapsed time: 1000.58 \t Loss:-1.74952\n",
      "Iteration: 3784/277000 \t Elapsed time: 1000.74 \t Loss:-2.89325\n",
      "Iteration: 3785/277000 \t Elapsed time: 1000.91 \t Loss:-2.88489\n",
      "Iteration: 3786/277000 \t Elapsed time: 1001.07 \t Loss:-2.91964\n",
      "Iteration: 3787/277000 \t Elapsed time: 1001.26 \t Loss:-2.90404\n",
      "Iteration: 3788/277000 \t Elapsed time: 1001.42 \t Loss:-2.91988\n",
      "Iteration: 3789/277000 \t Elapsed time: 1001.64 \t Loss:-2.94319\n",
      "Iteration: 3790/277000 \t Elapsed time: 1001.80 \t Loss:-2.95000\n",
      "Iteration: 3791/277000 \t Elapsed time: 1001.97 \t Loss:-1.84988\n",
      "Iteration: 3792/277000 \t Elapsed time: 1002.13 \t Loss:-2.72785\n",
      "Iteration: 3793/277000 \t Elapsed time: 1002.32 \t Loss:-2.80962\n",
      "Iteration: 3794/277000 \t Elapsed time: 1002.50 \t Loss:-2.85930\n",
      "Iteration: 3795/277000 \t Elapsed time: 1002.67 \t Loss:-2.88942\n",
      "Iteration: 3796/277000 \t Elapsed time: 1002.86 \t Loss:-1.81975\n",
      "Iteration: 3797/277000 \t Elapsed time: 1003.03 \t Loss:-2.40772\n",
      "Iteration: 3798/277000 \t Elapsed time: 1003.19 \t Loss:-2.83220\n",
      "Iteration: 3799/277000 \t Elapsed time: 1003.35 \t Loss:-2.86370\n",
      "Iteration: 3800/277000 \t Elapsed time: 1003.51 \t Loss:-1.96935\n",
      "Iteration: 3801/277000 \t Elapsed time: 1003.67 \t Loss:-2.29105\n",
      "Iteration: 3802/277000 \t Elapsed time: 1003.85 \t Loss:-2.72471\n",
      "Iteration: 3803/277000 \t Elapsed time: 1004.01 \t Loss:-1.97288\n",
      "Iteration: 3804/277000 \t Elapsed time: 1004.22 \t Loss:-2.14285\n",
      "Iteration: 3805/277000 \t Elapsed time: 1004.43 \t Loss:-2.70441\n",
      "Iteration: 3806/277000 \t Elapsed time: 1004.60 \t Loss:-2.83920\n",
      "Iteration: 3807/277000 \t Elapsed time: 1004.77 \t Loss:-2.90611\n",
      "Iteration: 3808/277000 \t Elapsed time: 1004.95 \t Loss:-2.57689\n",
      "Iteration: 3809/277000 \t Elapsed time: 1005.16 \t Loss:-2.72967\n",
      "Iteration: 3810/277000 \t Elapsed time: 1005.41 \t Loss:-2.91381\n",
      "Iteration: 3811/277000 \t Elapsed time: 1005.58 \t Loss:-2.87634\n",
      "Iteration: 3812/277000 \t Elapsed time: 1005.76 \t Loss:-2.83943\n",
      "Iteration: 3813/277000 \t Elapsed time: 1005.95 \t Loss:-2.90282\n",
      "Iteration: 3814/277000 \t Elapsed time: 1006.15 \t Loss:-2.86567\n",
      "Iteration: 3815/277000 \t Elapsed time: 1006.35 \t Loss:-2.93831\n",
      "Iteration: 3816/277000 \t Elapsed time: 1006.52 \t Loss:-2.77461\n",
      "Iteration: 3817/277000 \t Elapsed time: 1006.68 \t Loss:-2.82648\n",
      "Iteration: 3818/277000 \t Elapsed time: 1006.84 \t Loss:-2.98577\n",
      "Iteration: 3819/277000 \t Elapsed time: 1007.01 \t Loss:-2.94972\n",
      "Iteration: 3820/277000 \t Elapsed time: 1007.18 \t Loss:-2.97602\n",
      "Iteration: 3821/277000 \t Elapsed time: 1007.35 \t Loss:-2.98709\n",
      "Iteration: 3822/277000 \t Elapsed time: 1007.58 \t Loss:-2.99397\n",
      "Iteration: 3823/277000 \t Elapsed time: 1007.74 \t Loss:-1.58587\n",
      "Iteration: 3824/277000 \t Elapsed time: 1007.94 \t Loss:-2.67493\n",
      "Iteration: 3825/277000 \t Elapsed time: 1008.15 \t Loss:-2.85713\n",
      "Iteration: 3826/277000 \t Elapsed time: 1008.31 \t Loss:-2.90624\n",
      "Iteration: 3827/277000 \t Elapsed time: 1008.48 \t Loss:-2.95976\n",
      "Iteration: 3828/277000 \t Elapsed time: 1008.64 \t Loss:-2.98771\n",
      "Iteration: 3829/277000 \t Elapsed time: 1008.80 \t Loss:-2.82728\n",
      "Iteration: 3830/277000 \t Elapsed time: 1008.97 \t Loss:-1.98838\n",
      "Iteration: 3831/277000 \t Elapsed time: 1009.16 \t Loss:-2.76978\n",
      "Iteration: 3832/277000 \t Elapsed time: 1009.35 \t Loss:-2.92786\n",
      "Iteration: 3833/277000 \t Elapsed time: 1009.58 \t Loss:-2.94720\n",
      "Iteration: 3834/277000 \t Elapsed time: 1009.81 \t Loss:-3.00464\n",
      "Iteration: 3835/277000 \t Elapsed time: 1009.98 \t Loss:-2.99172\n",
      "Iteration: 3836/277000 \t Elapsed time: 1010.14 \t Loss:-2.99305\n",
      "Iteration: 3837/277000 \t Elapsed time: 1010.30 \t Loss:-2.99398\n",
      "Iteration: 3838/277000 \t Elapsed time: 1010.48 \t Loss:-3.01984\n",
      "Iteration: 3839/277000 \t Elapsed time: 1010.64 \t Loss:-1.10584\n",
      "Iteration: 3840/277000 \t Elapsed time: 1010.80 \t Loss:-2.10168\n",
      "Iteration: 3841/277000 \t Elapsed time: 1010.98 \t Loss:-1.53950\n",
      "Iteration: 3842/277000 \t Elapsed time: 1011.17 \t Loss:-2.60045\n",
      "Iteration: 3843/277000 \t Elapsed time: 1011.36 \t Loss:-2.85527\n",
      "Iteration: 3844/277000 \t Elapsed time: 1011.57 \t Loss:-2.90995\n",
      "Iteration: 3845/277000 \t Elapsed time: 1011.78 \t Loss:-2.67875\n",
      "Iteration: 3846/277000 \t Elapsed time: 1011.96 \t Loss:-2.42276\n",
      "Iteration: 3847/277000 \t Elapsed time: 1012.13 \t Loss:-2.86004\n",
      "Iteration: 3848/277000 \t Elapsed time: 1012.29 \t Loss:-1.17879\n",
      "Iteration: 3849/277000 \t Elapsed time: 1012.46 \t Loss:-2.63687\n",
      "Iteration: 3850/277000 \t Elapsed time: 1012.62 \t Loss:-2.08810\n",
      "Iteration: 3851/277000 \t Elapsed time: 1012.78 \t Loss:-2.13073\n",
      "Iteration: 3852/277000 \t Elapsed time: 1012.94 \t Loss:-2.69958\n",
      "Iteration: 3853/277000 \t Elapsed time: 1013.12 \t Loss:-2.30886\n",
      "Iteration: 3854/277000 \t Elapsed time: 1013.28 \t Loss:-2.36594\n",
      "Iteration: 3855/277000 \t Elapsed time: 1013.46 \t Loss:-2.89026\n",
      "Iteration: 3856/277000 \t Elapsed time: 1013.66 \t Loss:-2.87905\n",
      "Iteration: 3857/277000 \t Elapsed time: 1013.83 \t Loss:-1.64250\n",
      "Iteration: 3858/277000 \t Elapsed time: 1013.99 \t Loss:-2.89378\n",
      "Iteration: 3859/277000 \t Elapsed time: 1014.15 \t Loss:-2.33330\n",
      "Iteration: 3860/277000 \t Elapsed time: 1014.32 \t Loss:-2.65622\n",
      "Iteration: 3861/277000 \t Elapsed time: 1014.48 \t Loss:-2.79295\n",
      "Iteration: 3862/277000 \t Elapsed time: 1014.64 \t Loss:-2.81226\n",
      "Iteration: 3863/277000 \t Elapsed time: 1014.80 \t Loss:-2.93050\n",
      "Iteration: 3864/277000 \t Elapsed time: 1014.97 \t Loss:-1.98778\n",
      "Iteration: 3865/277000 \t Elapsed time: 1015.13 \t Loss:-2.71653\n",
      "Iteration: 3866/277000 \t Elapsed time: 1015.29 \t Loss:-2.89454\n",
      "Iteration: 3867/277000 \t Elapsed time: 1015.45 \t Loss:-2.90670\n",
      "Iteration: 3868/277000 \t Elapsed time: 1015.61 \t Loss:-2.68303\n",
      "Iteration: 3869/277000 \t Elapsed time: 1015.79 \t Loss:-2.07736\n",
      "Iteration: 3870/277000 \t Elapsed time: 1015.96 \t Loss:-2.48697\n",
      "Iteration: 3871/277000 \t Elapsed time: 1016.13 \t Loss:-2.90136\n",
      "Iteration: 3872/277000 \t Elapsed time: 1016.29 \t Loss:-2.83771\n",
      "Iteration: 3873/277000 \t Elapsed time: 1016.45 \t Loss:-2.49752\n",
      "Iteration: 3874/277000 \t Elapsed time: 1016.61 \t Loss:-2.91422\n",
      "Iteration: 3875/277000 \t Elapsed time: 1016.77 \t Loss:-2.78743\n",
      "Iteration: 3876/277000 \t Elapsed time: 1016.94 \t Loss:-2.95374\n",
      "Iteration: 3877/277000 \t Elapsed time: 1017.11 \t Loss:-2.55890\n",
      "Iteration: 3878/277000 \t Elapsed time: 1017.28 \t Loss:-2.98122\n",
      "Iteration: 3879/277000 \t Elapsed time: 1017.48 \t Loss:-3.00386\n",
      "Iteration: 3880/277000 \t Elapsed time: 1017.65 \t Loss:-2.99618\n",
      "Iteration: 3881/277000 \t Elapsed time: 1017.81 \t Loss:-3.01980\n",
      "Iteration: 3882/277000 \t Elapsed time: 1018.03 \t Loss:-2.53866\n",
      "Iteration: 3883/277000 \t Elapsed time: 1018.20 \t Loss:-1.96649\n",
      "Iteration: 3884/277000 \t Elapsed time: 1018.37 \t Loss:-2.95727\n",
      "Iteration: 3885/277000 \t Elapsed time: 1018.53 \t Loss:-3.00017\n",
      "Iteration: 3886/277000 \t Elapsed time: 1018.69 \t Loss:-2.97466\n",
      "Iteration: 3887/277000 \t Elapsed time: 1018.93 \t Loss:-3.02842\n",
      "Iteration: 3888/277000 \t Elapsed time: 1019.13 \t Loss:-2.27384\n",
      "Iteration: 3889/277000 \t Elapsed time: 1019.31 \t Loss:-2.62721\n",
      "Iteration: 3890/277000 \t Elapsed time: 1019.60 \t Loss:-2.84565\n",
      "Iteration: 3891/277000 \t Elapsed time: 1019.84 \t Loss:-3.01987\n",
      "Iteration: 3892/277000 \t Elapsed time: 1020.01 \t Loss:-3.01320\n",
      "Iteration: 3893/277000 \t Elapsed time: 1020.17 \t Loss:-3.01871\n",
      "Iteration: 3894/277000 \t Elapsed time: 1020.33 \t Loss:-3.04855\n",
      "Iteration: 3895/277000 \t Elapsed time: 1020.55 \t Loss:-2.43205\n",
      "Iteration: 3896/277000 \t Elapsed time: 1020.74 \t Loss:-2.59410\n",
      "Iteration: 3897/277000 \t Elapsed time: 1021.01 \t Loss:-2.98284\n",
      "Iteration: 3898/277000 \t Elapsed time: 1021.23 \t Loss:-2.99144\n",
      "Iteration: 3899/277000 \t Elapsed time: 1021.42 \t Loss:-1.76670\n",
      "Iteration: 3900/277000 \t Elapsed time: 1021.58 \t Loss:-2.93262\n",
      "Iteration: 3901/277000 \t Elapsed time: 1021.79 \t Loss:-2.97739\n",
      "Iteration: 3902/277000 \t Elapsed time: 1021.95 \t Loss:-2.93749\n",
      "Iteration: 3903/277000 \t Elapsed time: 1022.11 \t Loss:-2.75312\n",
      "Iteration: 3904/277000 \t Elapsed time: 1022.33 \t Loss:-3.02376\n",
      "Iteration: 3905/277000 \t Elapsed time: 1022.52 \t Loss:-2.97615\n",
      "Iteration: 3906/277000 \t Elapsed time: 1022.67 \t Loss:-2.19270\n",
      "Iteration: 3907/277000 \t Elapsed time: 1022.83 \t Loss:-2.97459\n",
      "Iteration: 3908/277000 \t Elapsed time: 1023.00 \t Loss:-2.42111\n",
      "Iteration: 3909/277000 \t Elapsed time: 1023.16 \t Loss:-2.99412\n",
      "Iteration: 3910/277000 \t Elapsed time: 1023.32 \t Loss:-2.12879\n",
      "Iteration: 3911/277000 \t Elapsed time: 1023.48 \t Loss:-3.01440\n",
      "Iteration: 3912/277000 \t Elapsed time: 1023.64 \t Loss:-2.89308\n",
      "Iteration: 3913/277000 \t Elapsed time: 1023.80 \t Loss:-3.00301\n",
      "Iteration: 3914/277000 \t Elapsed time: 1023.96 \t Loss:-3.04113\n",
      "Iteration: 3915/277000 \t Elapsed time: 1024.12 \t Loss:-1.95802\n",
      "Iteration: 3916/277000 \t Elapsed time: 1024.29 \t Loss:-2.86952\n",
      "Iteration: 3917/277000 \t Elapsed time: 1024.46 \t Loss:-2.34580\n",
      "Iteration: 3918/277000 \t Elapsed time: 1024.64 \t Loss:-2.59928\n",
      "Iteration: 3919/277000 \t Elapsed time: 1024.84 \t Loss:-3.01288\n",
      "Iteration: 3920/277000 \t Elapsed time: 1025.00 \t Loss:-2.53210\n",
      "Iteration: 3921/277000 \t Elapsed time: 1025.17 \t Loss:-2.33656\n",
      "Iteration: 3922/277000 \t Elapsed time: 1025.33 \t Loss:-2.97024\n",
      "Iteration: 3923/277000 \t Elapsed time: 1025.49 \t Loss:-2.97630\n",
      "Iteration: 3924/277000 \t Elapsed time: 1025.65 \t Loss:-2.93428\n",
      "Iteration: 3925/277000 \t Elapsed time: 1025.82 \t Loss:-2.10110\n",
      "Iteration: 3926/277000 \t Elapsed time: 1025.98 \t Loss:-2.06261\n",
      "Iteration: 3927/277000 \t Elapsed time: 1026.14 \t Loss:-2.39255\n",
      "Iteration: 3928/277000 \t Elapsed time: 1026.31 \t Loss:-2.82428\n",
      "Iteration: 3929/277000 \t Elapsed time: 1026.47 \t Loss:-3.05305\n",
      "Iteration: 3930/277000 \t Elapsed time: 1026.67 \t Loss:-2.93297\n",
      "Iteration: 3931/277000 \t Elapsed time: 1026.84 \t Loss:-3.00676\n",
      "Iteration: 3932/277000 \t Elapsed time: 1027.00 \t Loss:-2.72718\n",
      "Iteration: 3933/277000 \t Elapsed time: 1027.20 \t Loss:-1.61682\n",
      "Iteration: 3934/277000 \t Elapsed time: 1027.37 \t Loss:-3.00280\n",
      "Iteration: 3935/277000 \t Elapsed time: 1027.54 \t Loss:-3.00986\n",
      "Iteration: 3936/277000 \t Elapsed time: 1027.73 \t Loss:-3.04826\n",
      "Iteration: 3937/277000 \t Elapsed time: 1027.89 \t Loss:-3.06907\n",
      "Iteration: 3938/277000 \t Elapsed time: 1028.06 \t Loss:-3.01827\n",
      "Iteration: 3939/277000 \t Elapsed time: 1028.22 \t Loss:-3.07917\n",
      "Iteration: 3940/277000 \t Elapsed time: 1028.40 \t Loss:-3.09263\n",
      "Iteration: 3941/277000 \t Elapsed time: 1028.57 \t Loss:-3.11456\n",
      "Iteration: 3942/277000 \t Elapsed time: 1028.77 \t Loss:-2.12823\n",
      "Iteration: 3943/277000 \t Elapsed time: 1029.13 \t Loss:-2.86614\n",
      "Iteration: 3944/277000 \t Elapsed time: 1029.30 \t Loss:-3.04657\n",
      "Iteration: 3945/277000 \t Elapsed time: 1029.46 \t Loss:-2.68607\n",
      "Iteration: 3946/277000 \t Elapsed time: 1029.62 \t Loss:-2.95250\n",
      "Iteration: 3947/277000 \t Elapsed time: 1029.81 \t Loss:-1.59259\n",
      "Iteration: 3948/277000 \t Elapsed time: 1029.98 \t Loss:-2.70572\n",
      "Iteration: 3949/277000 \t Elapsed time: 1030.17 \t Loss:-2.99365\n",
      "Iteration: 3950/277000 \t Elapsed time: 1030.34 \t Loss:-2.92166\n",
      "Iteration: 3951/277000 \t Elapsed time: 1030.51 \t Loss:-2.66841\n",
      "Iteration: 3952/277000 \t Elapsed time: 1030.69 \t Loss:-3.09856\n",
      "Iteration: 3953/277000 \t Elapsed time: 1030.90 \t Loss:-2.57759\n",
      "Iteration: 3954/277000 \t Elapsed time: 1031.09 \t Loss:-3.06542\n",
      "Iteration: 3955/277000 \t Elapsed time: 1031.29 \t Loss:-3.03191\n",
      "Iteration: 3956/277000 \t Elapsed time: 1031.47 \t Loss:-3.08527\n",
      "Iteration: 3957/277000 \t Elapsed time: 1031.64 \t Loss:-1.67827\n",
      "Iteration: 3958/277000 \t Elapsed time: 1031.81 \t Loss:-3.10312\n",
      "Iteration: 3959/277000 \t Elapsed time: 1031.99 \t Loss:-3.08395\n",
      "Iteration: 3960/277000 \t Elapsed time: 1032.15 \t Loss:-3.09791\n",
      "Iteration: 3961/277000 \t Elapsed time: 1032.31 \t Loss:-3.12366\n",
      "Iteration: 3962/277000 \t Elapsed time: 1032.48 \t Loss:-3.02415\n",
      "Iteration: 3963/277000 \t Elapsed time: 1032.64 \t Loss:-2.74447\n",
      "Iteration: 3964/277000 \t Elapsed time: 1032.80 \t Loss:-2.46513\n",
      "Iteration: 3965/277000 \t Elapsed time: 1032.97 \t Loss:-3.09572\n",
      "Iteration: 3966/277000 \t Elapsed time: 1033.13 \t Loss:-2.65238\n",
      "Iteration: 3967/277000 \t Elapsed time: 1033.30 \t Loss:-2.57788\n",
      "Iteration: 3968/277000 \t Elapsed time: 1033.46 \t Loss:-3.09145\n",
      "Iteration: 3969/277000 \t Elapsed time: 1033.67 \t Loss:-3.01372\n",
      "Iteration: 3970/277000 \t Elapsed time: 1033.83 \t Loss:-2.86330\n",
      "Iteration: 3971/277000 \t Elapsed time: 1034.00 \t Loss:-2.38772\n",
      "Iteration: 3972/277000 \t Elapsed time: 1034.19 \t Loss:-3.11603\n",
      "Iteration: 3973/277000 \t Elapsed time: 1034.35 \t Loss:-3.09572\n",
      "Iteration: 3974/277000 \t Elapsed time: 1034.52 \t Loss:-3.10817\n",
      "Iteration: 3975/277000 \t Elapsed time: 1034.68 \t Loss:-2.98014\n",
      "Iteration: 3976/277000 \t Elapsed time: 1034.84 \t Loss:-3.10037\n",
      "Iteration: 3977/277000 \t Elapsed time: 1035.00 \t Loss:-2.65625\n",
      "Iteration: 3978/277000 \t Elapsed time: 1035.17 \t Loss:-2.85281\n",
      "Iteration: 3979/277000 \t Elapsed time: 1035.34 \t Loss:-2.62427\n",
      "Iteration: 3980/277000 \t Elapsed time: 1035.50 \t Loss:-3.10028\n",
      "Iteration: 3981/277000 \t Elapsed time: 1035.69 \t Loss:-2.97534\n",
      "Iteration: 3982/277000 \t Elapsed time: 1036.28 \t Loss:-3.11095\n",
      "Iteration: 3983/277000 \t Elapsed time: 1036.45 \t Loss:-3.15111\n",
      "Iteration: 3984/277000 \t Elapsed time: 1036.61 \t Loss:-3.11264\n",
      "Iteration: 3985/277000 \t Elapsed time: 1036.77 \t Loss:-3.15626\n",
      "Iteration: 3986/277000 \t Elapsed time: 1036.95 \t Loss:-3.16189\n",
      "Iteration: 3987/277000 \t Elapsed time: 1037.12 \t Loss:-2.84798\n",
      "Iteration: 3988/277000 \t Elapsed time: 1037.32 \t Loss:-3.16959\n",
      "Iteration: 3989/277000 \t Elapsed time: 1037.52 \t Loss:-3.17404\n",
      "Iteration: 3990/277000 \t Elapsed time: 1037.70 \t Loss:-2.11803\n",
      "Iteration: 3991/277000 \t Elapsed time: 1037.86 \t Loss:-3.15375\n",
      "Iteration: 3992/277000 \t Elapsed time: 1038.02 \t Loss:-3.04241\n",
      "Iteration: 3993/277000 \t Elapsed time: 1038.19 \t Loss:-2.59700\n",
      "Iteration: 3994/277000 \t Elapsed time: 1038.38 \t Loss:-3.16690\n",
      "Iteration: 3995/277000 \t Elapsed time: 1038.55 \t Loss:-3.17678\n",
      "Iteration: 3996/277000 \t Elapsed time: 1038.77 \t Loss:-2.97750\n",
      "Iteration: 3997/277000 \t Elapsed time: 1039.13 \t Loss:-3.20593\n",
      "Iteration: 3998/277000 \t Elapsed time: 1039.32 \t Loss:-3.19184\n",
      "Iteration: 3999/277000 \t Elapsed time: 1039.48 \t Loss:-3.21180\n",
      "Iteration: 4000/277000 \t Elapsed time: 1039.64 \t Loss:-2.40410\n",
      "Start Validating\n",
      "Finish Validating\n",
      "Iteration: 4001/277000 \t Elapsed time: 1141.14 \t Loss:-3.08285\n",
      "Iteration: 4002/277000 \t Elapsed time: 1141.31 \t Loss:-3.20316\n",
      "Iteration: 4003/277000 \t Elapsed time: 1141.50 \t Loss:-3.20395\n",
      "Iteration: 4004/277000 \t Elapsed time: 1141.70 \t Loss:-3.21638\n",
      "Iteration: 4005/277000 \t Elapsed time: 1141.87 \t Loss:-3.22264\n",
      "Iteration: 4006/277000 \t Elapsed time: 1142.04 \t Loss:-2.90987\n",
      "Iteration: 4007/277000 \t Elapsed time: 1142.20 \t Loss:-3.19927\n",
      "Iteration: 4008/277000 \t Elapsed time: 1142.39 \t Loss:-3.22119\n",
      "Iteration: 4009/277000 \t Elapsed time: 1142.57 \t Loss:-3.21492\n",
      "Iteration: 4010/277000 \t Elapsed time: 1142.81 \t Loss:-3.23233\n",
      "Iteration: 4011/277000 \t Elapsed time: 1143.07 \t Loss:-3.22575\n",
      "Iteration: 4012/277000 \t Elapsed time: 1143.25 \t Loss:-3.19309\n",
      "Iteration: 4013/277000 \t Elapsed time: 1143.41 \t Loss:-3.22643\n",
      "Iteration: 4014/277000 \t Elapsed time: 1143.58 \t Loss:-3.07443\n",
      "Iteration: 4015/277000 \t Elapsed time: 1143.78 \t Loss:-3.24430\n",
      "Iteration: 4016/277000 \t Elapsed time: 1143.95 \t Loss:-3.24336\n",
      "Iteration: 4017/277000 \t Elapsed time: 1144.12 \t Loss:-3.24665\n",
      "Iteration: 4018/277000 \t Elapsed time: 1144.28 \t Loss:-3.24993\n",
      "Iteration: 4019/277000 \t Elapsed time: 1144.45 \t Loss:-3.25319\n",
      "Iteration: 4020/277000 \t Elapsed time: 1144.61 \t Loss:-3.18865\n",
      "Iteration: 4021/277000 \t Elapsed time: 1144.78 \t Loss:-1.15524\n",
      "Iteration: 4022/277000 \t Elapsed time: 1144.99 \t Loss:-3.20541\n",
      "Iteration: 4023/277000 \t Elapsed time: 1145.33 \t Loss:-2.15851\n",
      "Iteration: 4024/277000 \t Elapsed time: 1145.51 \t Loss:-2.76799\n",
      "Iteration: 4025/277000 \t Elapsed time: 1145.67 \t Loss:-3.11534\n",
      "Iteration: 4026/277000 \t Elapsed time: 1145.85 \t Loss:-3.17438\n",
      "Iteration: 4027/277000 \t Elapsed time: 1146.03 \t Loss:-3.17879\n",
      "Iteration: 4028/277000 \t Elapsed time: 1146.23 \t Loss:-3.19585\n",
      "Iteration: 4029/277000 \t Elapsed time: 1146.46 \t Loss:-3.17508\n",
      "Iteration: 4030/277000 \t Elapsed time: 1146.64 \t Loss:-3.20352\n",
      "Iteration: 4031/277000 \t Elapsed time: 1146.80 \t Loss:-3.21558\n",
      "Iteration: 4032/277000 \t Elapsed time: 1147.00 \t Loss:-3.24798\n",
      "Iteration: 4033/277000 \t Elapsed time: 1147.21 \t Loss:-3.25370\n",
      "Iteration: 4034/277000 \t Elapsed time: 1147.38 \t Loss:-1.85119\n",
      "Iteration: 4035/277000 \t Elapsed time: 1147.55 \t Loss:-3.18051\n",
      "Iteration: 4036/277000 \t Elapsed time: 1147.72 \t Loss:-3.17477\n",
      "Iteration: 4037/277000 \t Elapsed time: 1147.90 \t Loss:-2.94228\n",
      "Iteration: 4038/277000 \t Elapsed time: 1148.07 \t Loss:-3.21655\n",
      "Iteration: 4039/277000 \t Elapsed time: 1148.23 \t Loss:-2.66439\n",
      "Iteration: 4040/277000 \t Elapsed time: 1148.40 \t Loss:-3.23505\n",
      "Iteration: 4041/277000 \t Elapsed time: 1148.58 \t Loss:-2.96757\n",
      "Iteration: 4042/277000 \t Elapsed time: 1148.79 \t Loss:-3.23699\n",
      "Iteration: 4043/277000 \t Elapsed time: 1148.96 \t Loss:-3.26114\n",
      "Iteration: 4044/277000 \t Elapsed time: 1149.13 \t Loss:-3.26288\n",
      "Iteration: 4045/277000 \t Elapsed time: 1149.35 \t Loss:-2.63966\n",
      "Iteration: 4046/277000 \t Elapsed time: 1149.55 \t Loss:-2.57983\n",
      "Iteration: 4047/277000 \t Elapsed time: 1149.72 \t Loss:-2.37674\n",
      "Iteration: 4048/277000 \t Elapsed time: 1149.89 \t Loss:-3.16776\n",
      "Iteration: 4049/277000 \t Elapsed time: 1150.05 \t Loss:-2.86269\n",
      "Iteration: 4050/277000 \t Elapsed time: 1150.22 \t Loss:-3.18537\n",
      "Iteration: 4051/277000 \t Elapsed time: 1150.39 \t Loss:-3.00044\n",
      "Iteration: 4052/277000 \t Elapsed time: 1150.58 \t Loss:-1.66525\n",
      "Iteration: 4053/277000 \t Elapsed time: 1150.75 \t Loss:-3.18538\n",
      "Iteration: 4054/277000 \t Elapsed time: 1150.93 \t Loss:-3.19483\n",
      "Iteration: 4055/277000 \t Elapsed time: 1151.09 \t Loss:-2.50029\n",
      "Iteration: 4056/277000 \t Elapsed time: 1151.28 \t Loss:-3.07219\n",
      "Iteration: 4057/277000 \t Elapsed time: 1151.44 \t Loss:-3.17214\n",
      "Iteration: 4058/277000 \t Elapsed time: 1151.61 \t Loss:-3.20130\n",
      "Iteration: 4059/277000 \t Elapsed time: 1151.80 \t Loss:-3.16673\n",
      "Iteration: 4060/277000 \t Elapsed time: 1151.96 \t Loss:-2.37097\n",
      "Iteration: 4061/277000 \t Elapsed time: 1152.16 \t Loss:-3.23752\n",
      "Iteration: 4062/277000 \t Elapsed time: 1152.34 \t Loss:-3.09941\n",
      "Iteration: 4063/277000 \t Elapsed time: 1152.52 \t Loss:-1.98238\n",
      "Iteration: 4064/277000 \t Elapsed time: 1152.73 \t Loss:-2.92996\n",
      "Iteration: 4065/277000 \t Elapsed time: 1153.22 \t Loss:-3.14361\n",
      "Iteration: 4066/277000 \t Elapsed time: 1153.39 \t Loss:-3.17871\n",
      "Iteration: 4067/277000 \t Elapsed time: 1153.56 \t Loss:-3.19047\n",
      "Iteration: 4068/277000 \t Elapsed time: 1153.72 \t Loss:-3.20304\n",
      "Iteration: 4069/277000 \t Elapsed time: 1153.91 \t Loss:-2.34343\n",
      "Iteration: 4070/277000 \t Elapsed time: 1154.08 \t Loss:-2.17291\n",
      "Iteration: 4071/277000 \t Elapsed time: 1154.24 \t Loss:-3.05457\n",
      "Iteration: 4072/277000 \t Elapsed time: 1154.42 \t Loss:-3.05593\n",
      "Iteration: 4073/277000 \t Elapsed time: 1154.59 \t Loss:-3.12981\n",
      "Iteration: 4074/277000 \t Elapsed time: 1154.80 \t Loss:-3.23630\n",
      "Iteration: 4075/277000 \t Elapsed time: 1154.99 \t Loss:-3.25227\n",
      "Iteration: 4076/277000 \t Elapsed time: 1155.15 \t Loss:-2.82956\n",
      "Iteration: 4077/277000 \t Elapsed time: 1155.31 \t Loss:-3.23186\n",
      "Iteration: 4078/277000 \t Elapsed time: 1155.47 \t Loss:-3.27868\n",
      "Iteration: 4079/277000 \t Elapsed time: 1155.63 \t Loss:-3.14691\n",
      "Iteration: 4080/277000 \t Elapsed time: 1155.87 \t Loss:-3.21534\n",
      "Iteration: 4081/277000 \t Elapsed time: 1156.14 \t Loss:-3.26846\n",
      "Iteration: 4082/277000 \t Elapsed time: 1156.32 \t Loss:-3.27441\n",
      "Iteration: 4083/277000 \t Elapsed time: 1156.49 \t Loss:-3.29076\n",
      "Iteration: 4084/277000 \t Elapsed time: 1156.65 \t Loss:-3.18622\n",
      "Iteration: 4085/277000 \t Elapsed time: 1156.85 \t Loss:-3.26780\n",
      "Iteration: 4086/277000 \t Elapsed time: 1157.01 \t Loss:-3.30529\n",
      "Iteration: 4087/277000 \t Elapsed time: 1157.20 \t Loss:-3.17320\n",
      "Iteration: 4088/277000 \t Elapsed time: 1157.40 \t Loss:-3.32765\n",
      "Iteration: 4089/277000 \t Elapsed time: 1157.57 \t Loss:-3.20970\n",
      "Iteration: 4090/277000 \t Elapsed time: 1157.72 \t Loss:-3.26953\n",
      "Iteration: 4091/277000 \t Elapsed time: 1157.89 \t Loss:-3.32078\n",
      "Iteration: 4092/277000 \t Elapsed time: 1158.06 \t Loss:-3.31944\n",
      "Iteration: 4093/277000 \t Elapsed time: 1158.42 \t Loss:-2.64025\n",
      "Iteration: 4094/277000 \t Elapsed time: 1158.64 \t Loss:-3.30989\n",
      "Iteration: 4095/277000 \t Elapsed time: 1158.89 \t Loss:-3.29992\n",
      "Iteration: 4096/277000 \t Elapsed time: 1159.05 \t Loss:-3.31416\n",
      "Iteration: 4097/277000 \t Elapsed time: 1159.22 \t Loss:-3.32014\n",
      "Iteration: 4098/277000 \t Elapsed time: 1159.38 \t Loss:-3.32610\n",
      "Iteration: 4099/277000 \t Elapsed time: 1159.55 \t Loss:-3.33556\n",
      "Iteration: 4100/277000 \t Elapsed time: 1159.73 \t Loss:-3.33664\n",
      "Iteration: 4101/277000 \t Elapsed time: 1159.91 \t Loss:-2.42214\n",
      "Iteration: 4102/277000 \t Elapsed time: 1160.11 \t Loss:-3.18879\n",
      "Iteration: 4103/277000 \t Elapsed time: 1160.28 \t Loss:-3.24687\n",
      "Iteration: 4104/277000 \t Elapsed time: 1160.47 \t Loss:-3.09470\n",
      "Iteration: 4105/277000 \t Elapsed time: 1160.66 \t Loss:-3.28427\n",
      "Iteration: 4106/277000 \t Elapsed time: 1160.82 \t Loss:-3.33861\n",
      "Iteration: 4107/277000 \t Elapsed time: 1161.01 \t Loss:-3.05172\n",
      "Iteration: 4108/277000 \t Elapsed time: 1161.20 \t Loss:-2.36405\n",
      "Iteration: 4109/277000 \t Elapsed time: 1161.38 \t Loss:-2.91791\n",
      "Iteration: 4110/277000 \t Elapsed time: 1161.54 \t Loss:-3.29483\n",
      "Iteration: 4111/277000 \t Elapsed time: 1161.78 \t Loss:-2.75665\n",
      "Iteration: 4112/277000 \t Elapsed time: 1162.00 \t Loss:-3.17490\n",
      "Iteration: 4113/277000 \t Elapsed time: 1162.17 \t Loss:-3.15377\n",
      "Iteration: 4114/277000 \t Elapsed time: 1162.34 \t Loss:-3.30867\n",
      "Iteration: 4115/277000 \t Elapsed time: 1162.51 \t Loss:-3.31077\n",
      "Iteration: 4116/277000 \t Elapsed time: 1162.69 \t Loss:-3.32903\n",
      "Iteration: 4117/277000 \t Elapsed time: 1162.90 \t Loss:-3.23951\n",
      "Iteration: 4118/277000 \t Elapsed time: 1163.06 \t Loss:-2.70222\n",
      "Iteration: 4119/277000 \t Elapsed time: 1163.23 \t Loss:-3.25393\n",
      "Iteration: 4120/277000 \t Elapsed time: 1163.39 \t Loss:-2.75895\n",
      "Iteration: 4121/277000 \t Elapsed time: 1163.56 \t Loss:-3.32650\n",
      "Iteration: 4122/277000 \t Elapsed time: 1163.72 \t Loss:-1.92832\n",
      "Iteration: 4123/277000 \t Elapsed time: 1163.95 \t Loss:-3.25429\n",
      "Iteration: 4124/277000 \t Elapsed time: 1164.11 \t Loss:-3.26696\n",
      "Iteration: 4125/277000 \t Elapsed time: 1164.28 \t Loss:-3.13772\n",
      "Iteration: 4126/277000 \t Elapsed time: 1164.44 \t Loss:-3.31749\n",
      "Iteration: 4127/277000 \t Elapsed time: 1164.68 \t Loss:-3.27565\n",
      "Iteration: 4128/277000 \t Elapsed time: 1164.86 \t Loss:-3.02751\n",
      "Iteration: 4129/277000 \t Elapsed time: 1165.05 \t Loss:-1.68866\n",
      "Iteration: 4130/277000 \t Elapsed time: 1165.21 \t Loss:-3.24807\n",
      "Iteration: 4131/277000 \t Elapsed time: 1165.40 \t Loss:-3.25257\n",
      "Iteration: 4132/277000 \t Elapsed time: 1165.66 \t Loss:-2.33821\n",
      "Iteration: 4133/277000 \t Elapsed time: 1165.83 \t Loss:-3.25933\n",
      "Iteration: 4134/277000 \t Elapsed time: 1165.99 \t Loss:-3.26138\n",
      "Iteration: 4135/277000 \t Elapsed time: 1166.16 \t Loss:-2.60843\n",
      "Iteration: 4136/277000 \t Elapsed time: 1166.32 \t Loss:-3.30516\n",
      "Iteration: 4137/277000 \t Elapsed time: 1166.50 \t Loss:-2.94589\n",
      "Iteration: 4138/277000 \t Elapsed time: 1166.65 \t Loss:-2.78646\n",
      "Iteration: 4139/277000 \t Elapsed time: 1166.83 \t Loss:-2.63124\n",
      "Iteration: 4140/277000 \t Elapsed time: 1167.00 \t Loss:-3.07356\n",
      "Iteration: 4141/277000 \t Elapsed time: 1167.18 \t Loss:-3.04930\n",
      "Iteration: 4142/277000 \t Elapsed time: 1167.42 \t Loss:-3.30434\n",
      "Iteration: 4143/277000 \t Elapsed time: 1167.59 \t Loss:-3.34535\n",
      "Iteration: 4144/277000 \t Elapsed time: 1167.87 \t Loss:-3.34083\n",
      "Iteration: 4145/277000 \t Elapsed time: 1168.05 \t Loss:-2.11066\n",
      "Iteration: 4146/277000 \t Elapsed time: 1168.26 \t Loss:-3.32242\n",
      "Iteration: 4147/277000 \t Elapsed time: 1168.55 \t Loss:-3.30670\n",
      "Iteration: 4148/277000 \t Elapsed time: 1168.74 \t Loss:-3.27441\n",
      "Iteration: 4149/277000 \t Elapsed time: 1168.91 \t Loss:-2.83973\n",
      "Iteration: 4150/277000 \t Elapsed time: 1169.11 \t Loss:-3.08440\n",
      "Iteration: 4151/277000 \t Elapsed time: 1169.27 \t Loss:-2.93610\n",
      "Iteration: 4152/277000 \t Elapsed time: 1169.43 \t Loss:-3.31209\n",
      "Iteration: 4153/277000 \t Elapsed time: 1169.63 \t Loss:-2.18910\n",
      "Iteration: 4154/277000 \t Elapsed time: 1169.81 \t Loss:-3.35159\n",
      "Iteration: 4155/277000 \t Elapsed time: 1170.00 \t Loss:-3.00267\n",
      "Iteration: 4156/277000 \t Elapsed time: 1170.16 \t Loss:-3.36588\n",
      "Iteration: 4157/277000 \t Elapsed time: 1170.32 \t Loss:-3.32718\n",
      "Iteration: 4158/277000 \t Elapsed time: 1170.47 \t Loss:-3.12225\n",
      "Iteration: 4159/277000 \t Elapsed time: 1170.68 \t Loss:-3.33832\n",
      "Iteration: 4160/277000 \t Elapsed time: 1170.87 \t Loss:-2.80011\n",
      "Iteration: 4161/277000 \t Elapsed time: 1171.06 \t Loss:-3.37464\n",
      "Iteration: 4162/277000 \t Elapsed time: 1171.24 \t Loss:-3.21679\n",
      "Iteration: 4163/277000 \t Elapsed time: 1171.41 \t Loss:-3.37548\n",
      "Iteration: 4164/277000 \t Elapsed time: 1171.57 \t Loss:-3.33883\n",
      "Iteration: 4165/277000 \t Elapsed time: 1171.74 \t Loss:-2.62285\n",
      "Iteration: 4166/277000 \t Elapsed time: 1171.90 \t Loss:-3.34445\n",
      "Iteration: 4167/277000 \t Elapsed time: 1172.09 \t Loss:-2.91400\n",
      "Iteration: 4168/277000 \t Elapsed time: 1172.29 \t Loss:-3.25424\n",
      "Iteration: 4169/277000 \t Elapsed time: 1172.47 \t Loss:-2.26309\n",
      "Iteration: 4170/277000 \t Elapsed time: 1172.66 \t Loss:-3.35115\n",
      "Iteration: 4171/277000 \t Elapsed time: 1172.85 \t Loss:-3.03224\n",
      "Iteration: 4172/277000 \t Elapsed time: 1173.02 \t Loss:-3.33730\n",
      "Iteration: 4173/277000 \t Elapsed time: 1173.18 \t Loss:-3.38386\n",
      "Iteration: 4174/277000 \t Elapsed time: 1173.36 \t Loss:-3.27484\n",
      "Iteration: 4175/277000 \t Elapsed time: 1173.57 \t Loss:-3.38300\n",
      "Iteration: 4176/277000 \t Elapsed time: 1173.74 \t Loss:-3.35537\n",
      "Iteration: 4177/277000 \t Elapsed time: 1173.91 \t Loss:-3.39546\n",
      "Iteration: 4178/277000 \t Elapsed time: 1174.10 \t Loss:-3.41220\n",
      "Iteration: 4179/277000 \t Elapsed time: 1174.29 \t Loss:-2.20214\n",
      "Iteration: 4180/277000 \t Elapsed time: 1174.50 \t Loss:-3.28389\n",
      "Iteration: 4181/277000 \t Elapsed time: 1174.69 \t Loss:-2.59704\n",
      "Iteration: 4182/277000 \t Elapsed time: 1174.91 \t Loss:-3.29723\n",
      "Iteration: 4183/277000 \t Elapsed time: 1175.07 \t Loss:-3.40355\n",
      "Iteration: 4184/277000 \t Elapsed time: 1175.24 \t Loss:-3.39301\n",
      "Iteration: 4185/277000 \t Elapsed time: 1175.39 \t Loss:-3.40817\n",
      "Iteration: 4186/277000 \t Elapsed time: 1175.56 \t Loss:-3.42929\n",
      "Iteration: 4187/277000 \t Elapsed time: 1175.73 \t Loss:-3.42463\n",
      "Iteration: 4188/277000 \t Elapsed time: 1175.94 \t Loss:-3.38534\n",
      "Iteration: 4189/277000 \t Elapsed time: 1176.11 \t Loss:-3.42519\n",
      "Iteration: 4190/277000 \t Elapsed time: 1176.29 \t Loss:-3.44381\n",
      "Iteration: 4191/277000 \t Elapsed time: 1176.49 \t Loss:-3.32999\n",
      "Iteration: 4192/277000 \t Elapsed time: 1176.73 \t Loss:-3.35062\n",
      "Iteration: 4193/277000 \t Elapsed time: 1176.89 \t Loss:-3.36342\n",
      "Iteration: 4194/277000 \t Elapsed time: 1177.05 \t Loss:-3.41790\n",
      "Iteration: 4195/277000 \t Elapsed time: 1177.21 \t Loss:-3.44097\n",
      "Iteration: 4196/277000 \t Elapsed time: 1177.38 \t Loss:-3.44726\n",
      "Iteration: 4197/277000 \t Elapsed time: 1177.55 \t Loss:-3.45377\n",
      "Iteration: 4198/277000 \t Elapsed time: 1177.82 \t Loss:-3.42173\n",
      "Iteration: 4199/277000 \t Elapsed time: 1177.98 \t Loss:-3.45410\n",
      "Iteration: 4200/277000 \t Elapsed time: 1178.15 \t Loss:-3.46816\n",
      "Iteration: 4201/277000 \t Elapsed time: 1178.32 \t Loss:-3.46741\n",
      "Iteration: 4202/277000 \t Elapsed time: 1178.48 \t Loss:-2.66776\n",
      "Iteration: 4203/277000 \t Elapsed time: 1178.66 \t Loss:-3.42540\n",
      "Iteration: 4204/277000 \t Elapsed time: 1178.82 \t Loss:-3.44907\n",
      "Iteration: 4205/277000 \t Elapsed time: 1179.00 \t Loss:-3.45998\n",
      "Iteration: 4206/277000 \t Elapsed time: 1179.17 \t Loss:-3.44525\n",
      "Iteration: 4207/277000 \t Elapsed time: 1179.33 \t Loss:-3.12344\n",
      "Iteration: 4208/277000 \t Elapsed time: 1179.50 \t Loss:-3.43541\n",
      "Iteration: 4209/277000 \t Elapsed time: 1179.68 \t Loss:-3.35293\n",
      "Iteration: 4210/277000 \t Elapsed time: 1179.94 \t Loss:-3.28802\n",
      "Iteration: 4211/277000 \t Elapsed time: 1180.10 \t Loss:-3.37905\n",
      "Iteration: 4212/277000 \t Elapsed time: 1180.30 \t Loss:-3.44764\n",
      "Iteration: 4213/277000 \t Elapsed time: 1180.48 \t Loss:-3.46390\n",
      "Iteration: 4214/277000 \t Elapsed time: 1180.67 \t Loss:-3.40059\n",
      "Iteration: 4215/277000 \t Elapsed time: 1180.85 \t Loss:-3.45876\n",
      "Iteration: 4216/277000 \t Elapsed time: 1181.01 \t Loss:-3.46998\n",
      "Iteration: 4217/277000 \t Elapsed time: 1181.17 \t Loss:-3.47298\n",
      "Iteration: 4218/277000 \t Elapsed time: 1181.33 \t Loss:-3.48479\n",
      "Iteration: 4219/277000 \t Elapsed time: 1181.49 \t Loss:-3.49039\n",
      "Iteration: 4220/277000 \t Elapsed time: 1181.66 \t Loss:-3.48642\n",
      "Iteration: 4221/277000 \t Elapsed time: 1181.83 \t Loss:-3.50948\n",
      "Iteration: 4222/277000 \t Elapsed time: 1182.01 \t Loss:-3.47124\n",
      "Iteration: 4223/277000 \t Elapsed time: 1182.20 \t Loss:-3.33886\n",
      "Iteration: 4224/277000 \t Elapsed time: 1182.39 \t Loss:-3.50616\n",
      "Iteration: 4225/277000 \t Elapsed time: 1182.56 \t Loss:-3.47195\n",
      "Iteration: 4226/277000 \t Elapsed time: 1182.75 \t Loss:-2.35081\n",
      "Iteration: 4227/277000 \t Elapsed time: 1182.92 \t Loss:-3.48505\n",
      "Iteration: 4228/277000 \t Elapsed time: 1183.09 \t Loss:-2.29240\n",
      "Iteration: 4229/277000 \t Elapsed time: 1183.27 \t Loss:-3.44899\n",
      "Iteration: 4230/277000 \t Elapsed time: 1183.43 \t Loss:-3.32473\n",
      "Iteration: 4231/277000 \t Elapsed time: 1183.59 \t Loss:-2.37127\n",
      "Iteration: 4232/277000 \t Elapsed time: 1183.78 \t Loss:-3.40670\n",
      "Iteration: 4233/277000 \t Elapsed time: 1183.95 \t Loss:-3.44004\n",
      "Iteration: 4234/277000 \t Elapsed time: 1184.13 \t Loss:-2.91694\n",
      "Iteration: 4235/277000 \t Elapsed time: 1184.33 \t Loss:-3.20711\n",
      "Iteration: 4236/277000 \t Elapsed time: 1184.50 \t Loss:-3.15516\n",
      "Iteration: 4237/277000 \t Elapsed time: 1184.67 \t Loss:-3.46692\n",
      "Iteration: 4238/277000 \t Elapsed time: 1184.85 \t Loss:-2.41759\n",
      "Iteration: 4239/277000 \t Elapsed time: 1185.02 \t Loss:-3.38972\n",
      "Iteration: 4240/277000 \t Elapsed time: 1185.19 \t Loss:-3.35826\n",
      "Iteration: 4241/277000 \t Elapsed time: 1185.39 \t Loss:-3.46555\n",
      "Iteration: 4242/277000 \t Elapsed time: 1185.56 \t Loss:-3.49760\n",
      "Iteration: 4243/277000 \t Elapsed time: 1185.75 \t Loss:-3.49998\n",
      "Iteration: 4244/277000 \t Elapsed time: 1185.91 \t Loss:-2.90384\n",
      "Iteration: 4245/277000 \t Elapsed time: 1186.08 \t Loss:-3.35988\n",
      "Iteration: 4246/277000 \t Elapsed time: 1186.28 \t Loss:-2.50803\n",
      "Iteration: 4247/277000 \t Elapsed time: 1186.85 \t Loss:-3.50351\n",
      "Iteration: 4248/277000 \t Elapsed time: 1187.01 \t Loss:-3.52102\n",
      "Iteration: 4249/277000 \t Elapsed time: 1187.18 \t Loss:-3.50238\n",
      "Iteration: 4250/277000 \t Elapsed time: 1187.36 \t Loss:-3.50523\n",
      "Iteration: 4251/277000 \t Elapsed time: 1187.55 \t Loss:-3.49338\n",
      "Iteration: 4252/277000 \t Elapsed time: 1187.79 \t Loss:-3.52522\n",
      "Iteration: 4253/277000 \t Elapsed time: 1187.97 \t Loss:-3.47524\n",
      "Iteration: 4254/277000 \t Elapsed time: 1188.17 \t Loss:-3.53576\n",
      "Iteration: 4255/277000 \t Elapsed time: 1188.34 \t Loss:-3.26589\n",
      "Iteration: 4256/277000 \t Elapsed time: 1188.53 \t Loss:-3.54043\n",
      "Iteration: 4257/277000 \t Elapsed time: 1188.73 \t Loss:-3.19387\n",
      "Iteration: 4258/277000 \t Elapsed time: 1188.92 \t Loss:-3.53274\n",
      "Iteration: 4259/277000 \t Elapsed time: 1189.09 \t Loss:-3.53893\n",
      "Iteration: 4260/277000 \t Elapsed time: 1189.30 \t Loss:-3.52225\n",
      "Iteration: 4261/277000 \t Elapsed time: 1189.47 \t Loss:-3.48509\n",
      "Iteration: 4262/277000 \t Elapsed time: 1189.63 \t Loss:-3.53950\n",
      "Iteration: 4263/277000 \t Elapsed time: 1189.89 \t Loss:-3.18051\n",
      "Iteration: 4264/277000 \t Elapsed time: 1190.09 \t Loss:-3.53761\n",
      "Iteration: 4265/277000 \t Elapsed time: 1190.25 \t Loss:-2.93562\n",
      "Iteration: 4266/277000 \t Elapsed time: 1190.44 \t Loss:-2.89814\n",
      "Iteration: 4267/277000 \t Elapsed time: 1190.61 \t Loss:-3.55700\n",
      "Iteration: 4268/277000 \t Elapsed time: 1190.78 \t Loss:-3.54026\n",
      "Iteration: 4269/277000 \t Elapsed time: 1190.95 \t Loss:-3.54448\n",
      "Iteration: 4270/277000 \t Elapsed time: 1191.13 \t Loss:-2.99690\n",
      "Iteration: 4271/277000 \t Elapsed time: 1191.31 \t Loss:-3.54333\n",
      "Iteration: 4272/277000 \t Elapsed time: 1191.51 \t Loss:-3.51941\n",
      "Iteration: 4273/277000 \t Elapsed time: 1191.67 \t Loss:-3.41411\n",
      "Iteration: 4274/277000 \t Elapsed time: 1191.89 \t Loss:-3.50059\n",
      "Iteration: 4275/277000 \t Elapsed time: 1192.06 \t Loss:-3.14929\n",
      "Iteration: 4276/277000 \t Elapsed time: 1192.23 \t Loss:-3.53605\n",
      "Iteration: 4277/277000 \t Elapsed time: 1192.45 \t Loss:-3.54970\n",
      "Iteration: 4278/277000 \t Elapsed time: 1192.62 \t Loss:-3.43649\n",
      "Iteration: 4279/277000 \t Elapsed time: 1192.79 \t Loss:-3.56871\n",
      "Iteration: 4280/277000 \t Elapsed time: 1192.95 \t Loss:-3.57518\n",
      "Iteration: 4281/277000 \t Elapsed time: 1193.13 \t Loss:-3.56403\n",
      "Iteration: 4282/277000 \t Elapsed time: 1193.29 \t Loss:-3.14850\n",
      "Iteration: 4283/277000 \t Elapsed time: 1193.47 \t Loss:-2.89971\n",
      "Iteration: 4284/277000 \t Elapsed time: 1193.64 \t Loss:-3.52826\n",
      "Iteration: 4285/277000 \t Elapsed time: 1193.85 \t Loss:-3.23869\n",
      "Iteration: 4286/277000 \t Elapsed time: 1194.02 \t Loss:-3.57288\n",
      "Iteration: 4287/277000 \t Elapsed time: 1194.19 \t Loss:-3.55566\n",
      "Iteration: 4288/277000 \t Elapsed time: 1194.42 \t Loss:-2.27979\n",
      "Iteration: 4289/277000 \t Elapsed time: 1194.61 \t Loss:-2.38045\n",
      "Iteration: 4290/277000 \t Elapsed time: 1194.77 \t Loss:-2.85514\n",
      "Iteration: 4291/277000 \t Elapsed time: 1194.94 \t Loss:-3.56550\n",
      "Iteration: 4292/277000 \t Elapsed time: 1195.13 \t Loss:-2.72813\n",
      "Iteration: 4293/277000 \t Elapsed time: 1195.32 \t Loss:-3.48429\n",
      "Iteration: 4294/277000 \t Elapsed time: 1195.52 \t Loss:-3.51230\n",
      "Iteration: 4295/277000 \t Elapsed time: 1195.75 \t Loss:-3.48047\n",
      "Iteration: 4296/277000 \t Elapsed time: 1195.92 \t Loss:-3.54103\n",
      "Iteration: 4297/277000 \t Elapsed time: 1196.14 \t Loss:-3.52874\n",
      "Iteration: 4298/277000 \t Elapsed time: 1196.34 \t Loss:-2.17563\n",
      "Iteration: 4299/277000 \t Elapsed time: 1196.57 \t Loss:-1.89713\n",
      "Iteration: 4300/277000 \t Elapsed time: 1196.74 \t Loss:-3.43399\n",
      "Iteration: 4301/277000 \t Elapsed time: 1196.90 \t Loss:-3.04216\n",
      "Iteration: 4302/277000 \t Elapsed time: 1197.07 \t Loss:-2.92483\n",
      "Iteration: 4303/277000 \t Elapsed time: 1197.23 \t Loss:-3.27209\n",
      "Iteration: 4304/277000 \t Elapsed time: 1197.40 \t Loss:-3.05730\n"
     ]
    }
   ],
   "source": [
    "# @title Train\n",
    "import argparse\n",
    "import torch\n",
    "from torchvision import datasets\n",
    "# import Learner\n",
    "# import datasets\n",
    "# import utils\n",
    "# from CGlowModel import CondGlowModel\n",
    "\n",
    "\n",
    "if __name__==\"__main__\":\n",
    "\n",
    "    parser = argparse.ArgumentParser(description='train c-Glow')\n",
    "\n",
    "    # input output path\n",
    "    parser.add_argument(\"-d\", \"--dataset_name\", type=str, default=\"horse\")\n",
    "    parser.add_argument(\"-r\", \"--dataset_root\", type=str, default=\"weizmann_horse_db\")\n",
    "\n",
    "    # log root\n",
    "    parser.add_argument(\"--log_root\", type=str, default=\"\")\n",
    "\n",
    "    # C-Glow parameters\n",
    "    # we are calculating Y GIVEN X\n",
    "    parser.add_argument(\"--x_size\", type=tuple, default=(1,12)) # was (1,8) before adding 4 more\n",
    "    parser.add_argument(\"--y_size\", type=tuple, default=(1,16,16))\n",
    "    parser.add_argument(\"--x_hidden_channels\", type=int, default=128)\n",
    "    parser.add_argument(\"--x_hidden_size\", type=int, default=64)\n",
    "    parser.add_argument(\"--y_hidden_channels\", type=int, default=256)\n",
    "    parser.add_argument(\"-K\", \"--depth_flow\", type=int, default=8)\n",
    "    parser.add_argument(\"-L\", \"--num_levels\", type=int, default=3)\n",
    "    parser.add_argument(\"--learn_top\", type=bool, default=False)\n",
    "\n",
    "    # Dataset preprocess parameters\n",
    "    parser.add_argument(\"--label_scale\", type=float, default=1)\n",
    "    parser.add_argument(\"--label_bias\", type=float, default=0.5)\n",
    "    parser.add_argument(\"--x_bins\", type=float, default=256.0)\n",
    "    parser.add_argument(\"--y_bins\", type=float, default=2.0)\n",
    "\n",
    "\n",
    "    # Optimizer parameters\n",
    "    parser.add_argument(\"--optimizer\", type=str, default=\"adam\")\n",
    "    parser.add_argument(\"--lr\", type=float, default=0.0002)\n",
    "    parser.add_argument(\"--betas\", type=tuple, default=(0.9,0.9999))\n",
    "    parser.add_argument(\"--eps\", type=float, default=1e-8)\n",
    "    parser.add_argument(\"--regularizer\", type=float, default=0.0)\n",
    "    parser.add_argument(\"--num_steps\", type=int, default=0)\n",
    "\n",
    "    # Trainer parameters\n",
    "    parser.add_argument(\"--num_epochs\", type=int, default=1000)\n",
    "    parser.add_argument(\"--batch_size\", type=int, default=2)\n",
    "    parser.add_argument(\"--max_grad_clip\", type=float, default=5)\n",
    "    parser.add_argument(\"--max_grad_norm\", type=float, default=0)\n",
    "    parser.add_argument(\"--checkpoints_gap\", type=int, default=1000) # typically 1000\n",
    "    parser.add_argument(\"--nll_gap\", type=int, default=1)\n",
    "    parser.add_argument(\"--inference_gap\", type=int, default=1000)\n",
    "    parser.add_argument(\"--save_gap\", type=int, default=1000)\n",
    "\n",
    "    # model path\n",
    "    parser.add_argument(\"--model_path\", type=str, default=\"\")#/pdo/users/jlupoiii/TESS/model_conditional_norm_flow/1D-angles-O13-image0to1_angle_rad/checkpoints/checkpoint_51000.pth.tar\") #\"/pdo/users/jlupoiii/TESS/model_conditional_norm_flow/log_20230821_2340/checkpoints/checkpoint_4000.pth.tar\") # \"/pdo/users/jlupoiii/TESS/model_conditional_norm_flow/log_20230821_2340/checkpoints/checkpoint_4000.pth.tar\"\n",
    "    \n",
    "    # args = parser.parse_args()\n",
    "    args = parser.parse_known_args()[0]\n",
    "    cuda = torch.cuda.is_available()\n",
    "\n",
    "\n",
    "    # # HORSE DATASET\n",
    "    # dataset_root = 'weizmann_horse_db/'\n",
    "    # # Define any image transformations you want to apply (resize, normalization, etc.)\n",
    "    # transform = transforms.Compose([\n",
    "    #     transforms.Resize((64, 64)),\n",
    "    #     transforms.ToTensor(),\n",
    "    #     # transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "    # ])\n",
    "    # target_transform = transforms.Compose([\n",
    "    #     transforms.Resize((64, 64)),\n",
    "    #     transforms.ToTensor(),\n",
    "    #     # transforms.Normalize(mean=[0.456], std=[0.225])\n",
    "    # ])\n",
    "    # # Create the dataset\n",
    "    # training_set = WeizmannHorseDataset(root_dir=dataset_root, transform=transform, target_transform=target_transform)\n",
    "    # valid_set = WeizmannHorseDataset(root_dir=dataset_root, transform=transform, target_transform=target_transform)\n",
    "\n",
    "    # create TESS dataset\n",
    "    torch.manual_seed(42)\n",
    "    full_dataset = TESSDataset()\n",
    "    train_ratio = 0.8\n",
    "    num_train_samples = int(train_ratio * len(full_dataset))\n",
    "    num_valid_samples = len(full_dataset) - num_train_samples\n",
    "    training_set, valid_set = random_split(full_dataset, [num_train_samples, num_valid_samples])\n",
    "\n",
    "    model = CondGlowModel(args)\n",
    "    if cuda:\n",
    "        model = model.cuda()\n",
    "\n",
    "    # from utils import count_parameters\n",
    "    print(\"number of param: {}\".format(count_parameters(model)))\n",
    "\n",
    "    # optimizer\n",
    "    optim = torch.optim.Adam(model.parameters(), lr=args.lr,betas=args.betas, weight_decay=args.regularizer)\n",
    "\n",
    "    # scheduler\n",
    "    scheduler = None\n",
    "\n",
    "    if args.model_path != \"\":\n",
    "        state = load_state(args.model_path, cuda)\n",
    "        optim.load_state_dict(state[\"optim\"])\n",
    "        model.load_state_dict(state[\"model\"])\n",
    "        args.steps = state[\"iteration\"] + 1\n",
    "        if scheduler is not None and state.get(\"scheduler\", None) is not None:\n",
    "            scheduler.load_state_dict(state[\"scheduler\"])\n",
    "        del state\n",
    "        print('Realoaded model stored at', args.model_path, 'starting at step', args.steps)\n",
    "\n",
    "    # begin to train\n",
    "    trainer = Trainer(model, optim, scheduler, training_set, valid_set, args, cuda)\n",
    "    trainer.train()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "id": "-pMvfdKknvWb"
   },
   "outputs": [],
   "source": [
    "# plt.imshow(y_sample.cpu()[0][0], vmin=0, vmax=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "id": "E0vt1suwnvYt"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "load to gpu\n",
      "Batch IDs: 0\n",
      "Samples: 0/10\n",
      "Samples: 1/10\n",
      "Samples: 2/10\n",
      "Samples: 3/10\n",
      "Samples: 4/10\n",
      "Samples: 5/10\n",
      "Samples: 6/10\n",
      "Samples: 7/10\n",
      "Samples: 8/10\n",
      "Samples: 9/10\n",
      "Batch IDs: 1\n",
      "Samples: 0/10\n",
      "Samples: 1/10\n",
      "Samples: 2/10\n",
      "Samples: 3/10\n",
      "Samples: 4/10\n",
      "Samples: 5/10\n",
      "Samples: 6/10\n",
      "Samples: 7/10\n",
      "Samples: 8/10\n",
      "Samples: 9/10\n",
      "Batch IDs: 2\n",
      "Samples: 0/10\n",
      "Samples: 1/10\n",
      "Samples: 2/10\n",
      "Samples: 3/10\n",
      "Samples: 4/10\n",
      "Samples: 5/10\n",
      "Samples: 6/10\n",
      "Samples: 7/10\n",
      "Samples: 8/10\n",
      "Samples: 9/10\n",
      "Batch IDs: 3\n",
      "Samples: 0/10\n",
      "Samples: 1/10\n",
      "Samples: 2/10\n",
      "Samples: 3/10\n",
      "Samples: 4/10\n",
      "Samples: 5/10\n",
      "Samples: 6/10\n",
      "Samples: 7/10\n",
      "Samples: 8/10\n",
      "Samples: 9/10\n",
      "Batch IDs: 4\n",
      "Samples: 0/10\n",
      "Samples: 1/10\n",
      "Samples: 2/10\n",
      "Samples: 3/10\n",
      "Samples: 4/10\n",
      "Samples: 5/10\n",
      "Samples: 6/10\n",
      "Samples: 7/10\n",
      "Samples: 8/10\n",
      "Samples: 9/10\n",
      "Batch IDs: 5\n",
      "Samples: 0/10\n",
      "Samples: 1/10\n",
      "Samples: 2/10\n",
      "Samples: 3/10\n",
      "Samples: 4/10\n",
      "Samples: 5/10\n",
      "Samples: 6/10\n",
      "Samples: 7/10\n",
      "Samples: 8/10\n",
      "Samples: 9/10\n",
      "Batch IDs: 6\n",
      "Samples: 0/10\n",
      "Samples: 1/10\n",
      "Samples: 2/10\n",
      "Samples: 3/10\n",
      "Samples: 4/10\n",
      "Samples: 5/10\n",
      "Samples: 6/10\n",
      "Samples: 7/10\n",
      "Samples: 8/10\n",
      "Samples: 9/10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "KeyboardInterrupt\n",
      "\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 2000x800 with 4 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import argparse\n",
    "import torch\n",
    "# import Learner\n",
    "# import datasets\n",
    "# import utils\n",
    "# from CGlowModel import CondGlowModel\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "\n",
    "    parser = argparse.ArgumentParser(description='predict c-Glow')\n",
    "\n",
    "    # model parameters\n",
    "    # parser.add_argument(\"--x_size\", type=tuple, default=(3,64,64))\n",
    "    # parser.add_argument(\"--y_size\", type=tuple, default=(1,64,64))\n",
    "    parser.add_argument(\"--x_size\", type=tuple, default=(1,12)) # was (1,8) before adding 4 more\n",
    "    parser.add_argument(\"--y_size\", type=tuple, default=(1,16,16))\n",
    "    parser.add_argument(\"--x_hidden_channels\", type=int, default=128)\n",
    "    parser.add_argument(\"--x_hidden_size\", type=int, default=64)\n",
    "    parser.add_argument(\"--y_hidden_channels\", type=int, default=256)\n",
    "    parser.add_argument(\"-K\", \"--depth_flow\", type=int, default=8)\n",
    "    parser.add_argument(\"-L\", \"--num_levels\", type=int, default=3)\n",
    "    parser.add_argument(\"--learn_top\", type=bool, default=False)\n",
    "\n",
    "    # dataset\n",
    "    parser.add_argument(\"-d\", \"--dataset_name\", type=str, default=\"horse\")\n",
    "    parser.add_argument(\"-r\", \"--dataset_root\", type=str, default=\"\")\n",
    "    parser.add_argument(\"--label_scale\", type=float, default=1)\n",
    "    parser.add_argument(\"--label_bias\", type=float, default=0.5)\n",
    "    parser.add_argument(\"--num_labels\", type=int, default=2)\n",
    "    parser.add_argument(\"--x_bins\", type=float, default=256.0)\n",
    "    parser.add_argument(\"--y_bins\", type=float, default=2.0)\n",
    "\n",
    "    # output\n",
    "    parser.add_argument(\"-o\", \"--out_root\", type=str, default=\"out_dir_temp\")\n",
    "\n",
    "    # model path\n",
    "    parser.add_argument(\"--model_path\", type=str, default=\"/pdo/users/jlupoiii/TESS/model_conditional_norm_flow/log_20231110_0012/checkpoints/checkpoint_550000.pth.tar\")\n",
    "\n",
    "    \n",
    "    # predictor parameters\n",
    "    parser.add_argument(\"--batch_size\", type=int, default=10)\n",
    "    parser.add_argument(\"--num_samples\", type=int, default=10)\n",
    "\n",
    "\n",
    "    # args = parser.parse_args()\n",
    "    args = parser.parse_known_args()[0]\n",
    "    cuda = torch.cuda.is_available()\n",
    "\n",
    "    # create TESS dataset\n",
    "    torch.manual_seed(42)\n",
    "    full_dataset = TESSDataset()\n",
    "    train_ratio = 0.8\n",
    "    num_train_samples = int(train_ratio * len(full_dataset))\n",
    "    num_valid_samples = len(full_dataset) - num_train_samples\n",
    "    training_set, valid_set = random_split(full_dataset, [num_train_samples, num_valid_samples])\n",
    "\n",
    "\n",
    "    # model\n",
    "    model = CondGlowModel(args)\n",
    "    if cuda:\n",
    "        model = model.cuda()\n",
    "    state = load_state(args.model_path, cuda)\n",
    "    model.load_state_dict(state[\"model\"])\n",
    "    del state\n",
    "\n",
    "    # predictor\n",
    "    predictor = Inferencer(model, valid_set, args, cuda)\n",
    "\n",
    "    # predict\n",
    "    predictor.sampled_based_prediction(args.num_samples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "hKoex5akoLe7"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "authorship_tag": "ABX9TyMbQ56iI/lZFYZCaH1MfPGD",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "pytorch_env",
   "language": "python",
   "name": "pytorch_env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.17"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
