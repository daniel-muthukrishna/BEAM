{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 2242,
     "status": "ok",
     "timestamp": 1692391050462,
     "user": {
      "displayName": "Joseph Lupo",
      "userId": "18192058496751055649"
     },
     "user_tz": 240
    },
    "id": "AoYASP6SlsZe",
    "outputId": "4d279a5f-4afd-4985-f1cf-c7fa74d1dd2a"
   },
   "outputs": [],
   "source": [
    "# # only for colab\n",
    "# from google.colab import drive\n",
    "# import os\n",
    "\n",
    "# drive.mount(\"/content/gdrive\")\n",
    "# os.chdir('/content/gdrive/MyDrive/Projects/TESS/model_conditional_norm_flow/')\n",
    "# ! ls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "executionInfo": {
     "elapsed": 11,
     "status": "ok",
     "timestamp": 1692391050463,
     "user": {
      "displayName": "Joseph Lupo",
      "userId": "18192058496751055649"
     },
     "user_tz": 240
    },
    "id": "wWai4P8Il4oq"
   },
   "outputs": [],
   "source": [
    "# ! rm -r log*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "executionInfo": {
     "elapsed": 9,
     "status": "ok",
     "timestamp": 1692391050463,
     "user": {
      "displayName": "Joseph Lupo",
      "userId": "18192058496751055649"
     },
     "user_tz": 240
    },
    "id": "rkQEZJWdtZpu"
   },
   "outputs": [],
   "source": [
    "# Adapted from here: https://github.com/yolu1055/conditional-glow/tree/master"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "executionInfo": {
     "elapsed": 3075,
     "status": "ok",
     "timestamp": 1692391053531,
     "user": {
      "displayName": "Joseph Lupo",
      "userId": "18192058496751055649"
     },
     "user_tz": 240
    },
    "id": "MdI9aotpm1b3"
   },
   "outputs": [],
   "source": [
    "# @title Utils\n",
    "import os\n",
    "import torch\n",
    "import numpy as np\n",
    "\n",
    "def split_feature(tensor, type=\"split\"):\n",
    "    \"\"\"\n",
    "    type = [\"split\", \"cross\"]\n",
    "    \"\"\"\n",
    "    C = tensor.size(1)\n",
    "    if type == \"split\":\n",
    "        return tensor[:, :C // 2, ...], tensor[:, C // 2:, ...]\n",
    "    elif type == \"cross\":\n",
    "        return tensor[:, 0::2, ...], tensor[:, 1::2, ...]\n",
    "\n",
    "\n",
    "def save_model(model, optim, scheduler, dir, iteration):\n",
    "    path = os.path.join(dir, \"checkpoint_{}.pth.tar\".format(iteration))\n",
    "    state = {}\n",
    "    state[\"iteration\"] = iteration\n",
    "    state[\"modelname\"] = model.__class__.__name__\n",
    "    state[\"model\"] = model.state_dict()\n",
    "    state[\"optim\"] = optim.state_dict()\n",
    "    if scheduler is not None:\n",
    "        state[\"scheduler\"] = scheduler.state_dict()\n",
    "    else:\n",
    "        state[\"scheduler\"] = None\n",
    "\n",
    "    torch.save(state, path)\n",
    "\n",
    "\n",
    "def load_state(path, cuda):\n",
    "    if cuda:\n",
    "        print (\"load to gpu\")\n",
    "        state = torch.load(path)\n",
    "    else:\n",
    "        print (\"load to cpu\")\n",
    "        state = torch.load(path, map_location=lambda storage, loc: storage)\n",
    "\n",
    "    return state\n",
    "\n",
    "\n",
    "def _fast_hist(label_true, label_pred, n_class):\n",
    "    mask = (label_true >= 0) & (label_true < n_class)\n",
    "    hist = np.bincount(\n",
    "        n_class * label_true[mask].astype(int) +\n",
    "        label_pred[mask].astype(int), minlength=n_class ** 2).reshape(n_class, n_class)\n",
    "    return hist\n",
    "\n",
    "def compute_accuracy(label_trues, label_preds, n_class):\n",
    "\n",
    "    hist = np.zeros((n_class, n_class))\n",
    "    for lt, lp in zip(label_trues, label_preds):\n",
    "        hist += _fast_hist(lt.flatten(), lp.flatten(), n_class)\n",
    "    acc = np.diag(hist).sum() / hist.sum()\n",
    "    acc_cls = np.diag(hist) / hist.sum(axis=1)\n",
    "    acc_cls = np.nanmean(acc_cls)\n",
    "    iu = np.diag(hist) / (hist.sum(axis=1) + hist.sum(axis=0) - np.diag(hist))\n",
    "    mean_iu = np.nanmean(iu)\n",
    "    freq = hist.sum(axis=1) / hist.sum()\n",
    "    fwavacc = (freq[freq > 0] * iu[freq > 0]).sum()\n",
    "    return acc, acc_cls, mean_iu, fwavacc\n",
    "\n",
    "\n",
    "def count_parameters(model):\n",
    "    return sum(p.numel() for p in model.parameters() if p.requires_grad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 5,
     "status": "ok",
     "timestamp": 1692391053531,
     "user": {
      "displayName": "Joseph Lupo",
      "userId": "18192058496751055649"
     },
     "user_tz": 240
    },
    "id": "y7lYe37Em1eN"
   },
   "outputs": [],
   "source": [
    "# @title Modules\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "class ActNorm(nn.Module):\n",
    "\n",
    "    def __init__(self, num_channels):\n",
    "        super().__init__()\n",
    "\n",
    "        size = [1, num_channels, 1, 1]\n",
    "\n",
    "        bias = torch.normal(mean=torch.zeros(*size), std=torch.ones(*size)*0.05)\n",
    "        logs = torch.normal(mean=torch.zeros(*size), std=torch.ones(*size)*0.05)\n",
    "        self.register_parameter(\"bias\", nn.Parameter(torch.Tensor(bias), requires_grad=True))\n",
    "        self.register_parameter(\"logs\", nn.Parameter(torch.Tensor(logs), requires_grad=True))\n",
    "\n",
    "\n",
    "    def forward(self, input, logdet=0, reverse=False):\n",
    "        dimentions = input.size(2) * input.size(3)\n",
    "        if reverse == False:\n",
    "            input = input + self.bias\n",
    "            input = input * torch.exp(self.logs)\n",
    "            dlogdet = torch.sum(self.logs) * dimentions\n",
    "            logdet = logdet + dlogdet\n",
    "\n",
    "        if reverse == True:\n",
    "            input = input * torch.exp(-self.logs)\n",
    "            input = input - self.bias\n",
    "            dlogdet = - torch.sum(self.logs) * dimentions\n",
    "            logdet = logdet + dlogdet\n",
    "\n",
    "        return input, logdet\n",
    "\n",
    "\n",
    "class Conv2dZeros(nn.Conv2d):\n",
    "\n",
    "    def __init__(self, in_channel, out_channel, kernel_size=[3,3], stride=[1,1]):\n",
    "        padding = (kernel_size[0] - 1) // 2\n",
    "        super().__init__(in_channels=in_channel, out_channels=out_channel, kernel_size=kernel_size, stride=stride, padding=padding)\n",
    "        self.weight.data.normal_(mean=0.0, std=0.1)\n",
    "\n",
    "\n",
    "class Conv1dZeros(nn.Conv1d):\n",
    "\n",
    "    def __init__(self, in_channel, out_channel, kernel_size=[1], stride=[1]):\n",
    "        padding = (kernel_size[0] - 1) // 2\n",
    "        super().__init__(in_channels=in_channel, out_channels=out_channel, kernel_size=kernel_size, stride=stride, padding=padding)\n",
    "        self.weight.data.normal_(mean=0.0, std=0.1)\n",
    "\n",
    "\n",
    "class Conv2dResize(nn.Conv2d):\n",
    "\n",
    "    def __init__(self, in_size, out_size):\n",
    "\n",
    "        stride = [in_size[1]//out_size[1], in_size[2]//out_size[2]]\n",
    "        kernel_size = Conv2dResize.compute_kernel_size(in_size, out_size, stride)\n",
    "        super().__init__(in_channels=in_size[0], out_channels=out_size[0], kernel_size=kernel_size, stride=stride)\n",
    "        self.weight.data.zero_()\n",
    "\n",
    "\n",
    "    @staticmethod\n",
    "    def compute_kernel_size(in_size, out_size, stride):\n",
    "        k0 = in_size[1] - (out_size[1] - 1) * stride[0]\n",
    "        k1 = in_size[2] - (out_size[2] - 1) * stride[1]\n",
    "        return[k0,k1]\n",
    "\n",
    "\n",
    "class Conv1dResize(nn.Conv1d):\n",
    "\n",
    "    def __init__(self, in_size, out_size):\n",
    "\n",
    "        stride = [in_size[1]//out_size[1]]\n",
    "        kernel_size = Conv1dResize.compute_kernel_size(in_size, out_size, stride)\n",
    "        super().__init__(in_channels=in_size[0], out_channels=out_size[0], kernel_size=kernel_size, stride=stride)\n",
    "        self.weight.data.zero_()\n",
    "\n",
    "\n",
    "    @staticmethod\n",
    "    def compute_kernel_size(in_size, out_size, stride):\n",
    "        k0 = in_size[1] - (out_size[1] - 1) * stride[0]\n",
    "        return[k0]\n",
    "\n",
    "\n",
    "class Conv2dNorm(nn.Conv2d):\n",
    "\n",
    "    def __init__(self, in_channels, out_channels, kernel_size=[3, 3], stride=[1, 1]):\n",
    "\n",
    "        padding = (kernel_size[0] - 1) // 2\n",
    "        super().__init__(in_channels, out_channels, kernel_size, stride, padding)\n",
    "        #initialize weight\n",
    "        self.weight.data.normal_(mean=0.0, std=0.05)\n",
    "\n",
    "\n",
    "class Conv1dNorm(nn.Conv1d):\n",
    "\n",
    "    def __init__(self, in_channels, out_channels, kernel_size=[1], stride=[1]):\n",
    "\n",
    "        padding = (kernel_size[0] - 1) // 2\n",
    "        super().__init__(in_channels, out_channels, kernel_size, stride, padding)\n",
    "        #initialize weight\n",
    "        self.weight.data.normal_(mean=0.0, std=0.05)\n",
    "\n",
    "\n",
    "class CondActNorm(nn.Module):\n",
    "\n",
    "    def __init__(self, x_size, y_channels, x_hidden_channels, x_hidden_size):\n",
    "        super().__init__()\n",
    "\n",
    "        C_x,H_x = x_size\n",
    "\n",
    "        # conditioning network\n",
    "        self.x_Con = nn.Sequential(\n",
    "            Conv1dResize(in_size=[C_x,H_x], out_size=[x_hidden_channels, H_x//1]),\n",
    "            nn.ReLU(),\n",
    "            Conv1dResize(in_size=[x_hidden_channels, H_x//1], out_size=[x_hidden_channels, H_x//2]),\n",
    "            nn.ReLU(),\n",
    "            Conv1dResize(in_size=[x_hidden_channels, H_x//2], out_size=[x_hidden_channels, H_x//2]),\n",
    "            nn.ReLU(),\n",
    "            Conv1dResize(in_size=[x_hidden_channels, H_x//2], out_size=[x_hidden_channels, H_x//2]),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "\n",
    "        self.x_Linear = nn.Sequential(\n",
    "            LinearZeros(x_hidden_channels*H_x//(2), x_hidden_size),\n",
    "            nn.ReLU(),\n",
    "            LinearZeros(x_hidden_size, x_hidden_size),\n",
    "            nn.ReLU(),\n",
    "            LinearZeros(x_hidden_size, 2*y_channels),\n",
    "            nn.Tanh()\n",
    "        )\n",
    "\n",
    "\n",
    "    def forward(self, x, y, logdet=0, reverse=False):\n",
    "\n",
    "        B,C,H = x.size()\n",
    "\n",
    "        # generate weights \n",
    "        x = self.x_Con(x)\n",
    "        x = x.view(B, -1)\n",
    "        x = self.x_Linear(x)\n",
    "        x = x.view(B, -1, 1, 1)\n",
    "\n",
    "\n",
    "\n",
    "        logs, bias = split_feature(x)\n",
    "        dimentions = y.size(2) * y.size(3)\n",
    "\n",
    "\n",
    "        if not reverse:\n",
    "            # center and scale\n",
    "            y = y + bias\n",
    "            y = y * torch.exp(logs)\n",
    "            dlogdet = dimentions * torch.sum(logs, dim=(1,2,3))\n",
    "            logdet = logdet + dlogdet\n",
    "        else:\n",
    "            # scale and center\n",
    "            y = y * torch.exp(-logs)\n",
    "            y = y - bias\n",
    "            dlogdet = - dimentions * torch.sum(logs, dim=(1,2,3))\n",
    "            logdet = logdet + dlogdet\n",
    "\n",
    "        return y, logdet\n",
    "\n",
    "\n",
    "\n",
    "class Cond1x1Conv(nn.Module):\n",
    "\n",
    "    def __init__(self, x_size, x_hidden_channels, x_hidden_size, y_channels):\n",
    "\n",
    "        super().__init__()\n",
    "\n",
    "        C_x,H_x = x_size\n",
    "\n",
    "\n",
    "        # conditioning network\n",
    "        self.x_Con = nn.Sequential(\n",
    "            Conv1dResize(in_size=[C_x,H_x], out_size=[x_hidden_channels, H_x//1]),\n",
    "            nn.ReLU(),\n",
    "            Conv1dResize(in_size=[x_hidden_channels, H_x//1], out_size=[x_hidden_channels, H_x//2]),\n",
    "            nn.ReLU(),\n",
    "            Conv1dResize(in_size=[x_hidden_channels, H_x//2], out_size=[x_hidden_channels, H_x//2]),\n",
    "            nn.ReLU(),\n",
    "            Conv1dResize(in_size=[x_hidden_channels, H_x//2], out_size=[x_hidden_channels, H_x//2]),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "\n",
    "        self.x_Linear = nn.Sequential(\n",
    "            LinearZeros(x_hidden_channels*H_x//(2), x_hidden_size),\n",
    "            nn.ReLU(),\n",
    "            LinearZeros(x_hidden_size, x_hidden_size),\n",
    "            nn.ReLU(),\n",
    "            LinearNorm(x_hidden_size, y_channels*y_channels),\n",
    "            nn.Tanh()\n",
    "        )\n",
    "\n",
    "\n",
    "    def get_weight(self, x, y, reverse):\n",
    "        y_channels = y.size(1)\n",
    "        B,C,H = x.size()\n",
    "\n",
    "        x = self.x_Con(x)\n",
    "        x = x.view(B, -1)\n",
    "        x = self.x_Linear(x)\n",
    "        weight = x.view(B, y_channels, y_channels)\n",
    "\n",
    "        dimensions = y.size(2) * y.size(3)\n",
    "        dlogdet = torch.slogdet(weight)[1] * dimensions\n",
    "\n",
    "\n",
    "        if reverse == False:\n",
    "            weight = weight.view(B, y_channels, y_channels,1,1)\n",
    "\n",
    "        else:\n",
    "            weight = torch.inverse(weight.double()).float().view(B, y_channels, y_channels,1,1)\n",
    "\n",
    "        return weight, dlogdet\n",
    "\n",
    "\n",
    "\n",
    "    def forward(self, x, y, logdet=None, reverse=False):\n",
    "\n",
    "        weight, dlogdet = self.get_weight(x, y, reverse)\n",
    "        B,C,H,W = y.size()\n",
    "        y = y.view(1, B*C, H, W)\n",
    "        B_k, C_i_k, C_o_k, H_k, W_k = weight.size()\n",
    "        assert B == B_k and C == C_i_k and C == C_o_k, \"The input and kernel dimensions are different\"\n",
    "        weight = weight.reshape(B_k*C_i_k,C_o_k,H_k,W_k)\n",
    "\n",
    "        if reverse == False:\n",
    "            z = F.conv2d(y, weight, groups=B)\n",
    "            z = z.view(B,C,H,W)\n",
    "            if logdet is not None:\n",
    "                logdet = logdet + dlogdet\n",
    "\n",
    "            return z, logdet\n",
    "        else:\n",
    "            z = F.conv2d(y, weight, groups=B)\n",
    "            z = z.view(B,C,H,W)\n",
    "\n",
    "            if logdet is not None:\n",
    "                logdet = logdet - dlogdet\n",
    "\n",
    "            return z, logdet\n",
    "\n",
    "\n",
    "class Conv2dNormy(nn.Conv2d):\n",
    "\n",
    "    def __init__(self, in_channels, out_channels,\n",
    "                 kernel_size=[3, 3], stride=[1, 1]):\n",
    "        padding = [(kernel_size[0]-1)//2, (kernel_size[1]-1)//2]\n",
    "        super().__init__(in_channels, out_channels, kernel_size, stride,\n",
    "                         padding, bias=False)\n",
    "\n",
    "        #initialize weight\n",
    "        self.weight.data.normal_(mean=0.0, std=0.05)\n",
    "        self.actnorm = ActNorm(out_channels)\n",
    "\n",
    "\n",
    "    def forward(self, input):\n",
    "        x = super().forward(input)\n",
    "        x,_ = self.actnorm(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "class Conv2dZerosy(nn.Conv2d):\n",
    "\n",
    "    def __init__(self, in_channels, out_channels,\n",
    "                 kernel_size=[3, 3], stride=[1, 1]):\n",
    "\n",
    "        padding = [(kernel_size[0]-1)//2, (kernel_size[1]-1)//2]\n",
    "        super().__init__(in_channels, out_channels, kernel_size, stride, padding)\n",
    "\n",
    "        self.logscale_factor = 3.0\n",
    "        self.register_parameter(\"logs\", nn.Parameter(torch.zeros(out_channels, 1, 1)))\n",
    "        self.register_parameter(\"newbias\", nn.Parameter(torch.zeros(out_channels, 1, 1)))\n",
    "\n",
    "        # init\n",
    "        self.weight.data.zero_()\n",
    "        self.bias.data.zero_()\n",
    "\n",
    "    def forward(self, input):\n",
    "        output = super().forward(input)\n",
    "        output = output + self.newbias\n",
    "        output = output * torch.exp(self.logs * self.logscale_factor)\n",
    "        return output\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "class CondAffineCoupling(nn.Module):\n",
    "\n",
    "    def __init__(self, x_size, y_size, hidden_channels):\n",
    "        super().__init__()\n",
    "\n",
    "        self.resize_x = nn.Sequential(\n",
    "            Conv1dZeros(x_size[0], 32),\n",
    "            nn.ReLU(),\n",
    "            Conv1dZeros(32, 16),\n",
    "            nn.ReLU(),\n",
    "            Conv1dResize((16,x_size[1]), out_size=y_size),\n",
    "            nn.ReLU(),\n",
    "            Conv1dZeros(y_size[0], y_size[0]*y_size[1]),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "\n",
    "        self.f = nn.Sequential(\n",
    "            Conv2dNormy(y_size[0]*2, hidden_channels),\n",
    "            nn.ReLU(),\n",
    "            Conv2dNormy(hidden_channels, hidden_channels, kernel_size=[1, 1]),\n",
    "            nn.ReLU(),\n",
    "            Conv2dZerosy(hidden_channels, 2*y_size[0]),\n",
    "            nn.Tanh()\n",
    "        )\n",
    "\n",
    "\n",
    "    def forward(self, x, y, logdet=0.0, reverse=False):\n",
    "\n",
    "        z1, z2 = split_feature(y, \"split\")\n",
    "        x = self.resize_x(x)\n",
    "\n",
    "        x = x.view(z1.shape)\n",
    "\n",
    "        h = torch.cat((x,z1), dim=1)\n",
    "        h = self.f(h)\n",
    "        shift, scale = split_feature(h, \"cross\")\n",
    "        scale = torch.sigmoid(scale + 2.)\n",
    "        if reverse == False:\n",
    "            z2 = z2 + shift\n",
    "            z2 = z2 * scale\n",
    "            logdet = torch.sum(torch.log(scale), dim=(1, 2, 3)) + logdet\n",
    "\n",
    "        if reverse == True:\n",
    "            z2 = z2 / scale\n",
    "            z2 = z2 - shift\n",
    "            logdet = -torch.sum(torch.log(scale), dim=(1, 2, 3)) + logdet\n",
    "\n",
    "        z = torch.cat((z1, z2), dim=1)\n",
    "\n",
    "        return z, logdet\n",
    "\n",
    "\n",
    "\n",
    "class SqueezeLayer(nn.Module):\n",
    "    def __init__(self, factor):\n",
    "        super().__init__()\n",
    "        self.factor = factor\n",
    "\n",
    "    def forward(self, input, logdet=None, reverse=False):\n",
    "        if not reverse:\n",
    "            output = SqueezeLayer.squeeze2d(input, self.factor)\n",
    "            return output, logdet\n",
    "        else:\n",
    "            output = SqueezeLayer.unsqueeze2d(input, self.factor)\n",
    "            return output, logdet\n",
    "\n",
    "\n",
    "    @staticmethod\n",
    "    def squeeze2d(input, factor=2):\n",
    "        assert factor >= 1 and isinstance(factor, int)\n",
    "        if factor == 1:\n",
    "            return input\n",
    "        B, C, H, W = input.size()\n",
    "        assert H % factor == 0 and W % factor == 0, \"{}\".format((H, W))\n",
    "        x = input.view(B, C, H // factor, factor, W // factor, factor)\n",
    "        x = x.permute(0, 1, 3, 5, 2, 4).contiguous()\n",
    "        x = x.view(B, C * factor * factor, H // factor, W // factor)\n",
    "        return x\n",
    "\n",
    "\n",
    "    @staticmethod\n",
    "    def unsqueeze2d(input, factor=2):\n",
    "        assert factor >= 1 and isinstance(factor, int)\n",
    "        factor2 = factor ** 2\n",
    "        if factor == 1:\n",
    "            return input\n",
    "        B, C, H, W = input.size()\n",
    "        assert C % (factor2) == 0, \"{}\".format(C)\n",
    "        x = input.view(B, C // factor2, factor, factor, H, W)\n",
    "        x = x.permute(0, 1, 4, 2, 5, 3).contiguous()\n",
    "        x = x.view(B, C // (factor2), H * factor, W * factor)\n",
    "        return x\n",
    "\n",
    "\n",
    "class Split2d(nn.Module):\n",
    "    def __init__(self, num_channels):\n",
    "        super().__init__()\n",
    "\n",
    "        self.conv = nn.Sequential(\n",
    "            Conv2dZeros(num_channels // 2, num_channels),\n",
    "            nn.Tanh()\n",
    "        )\n",
    "\n",
    "    def split2d_prior(self, z):\n",
    "        h = self.conv(z)\n",
    "        return split_feature(h, \"cross\")\n",
    "\n",
    "    def forward(self, input, logdet=0., reverse=False, eps_std=None):\n",
    "        if not reverse:\n",
    "            z1, z2 = split_feature(input, \"split\")\n",
    "            mean, logs = self.split2d_prior(z1)\n",
    "            logdet = GaussianDiag.logp(mean, logs, z2) + logdet\n",
    "\n",
    "            return z1, logdet\n",
    "        else:\n",
    "            z1 = input\n",
    "            mean, logs = self.split2d_prior(z1)\n",
    "            z2 = GaussianDiag.sample(mean, logs, eps_std)\n",
    "            z = torch.cat((z1, z2), dim=1)\n",
    "\n",
    "            return z, logdet\n",
    "\n",
    "\n",
    "class GaussianDiag:\n",
    "    Log2PI = float(np.log(2 * np.pi))\n",
    "\n",
    "    @staticmethod\n",
    "    def likelihood(mean, logs, x):\n",
    "        return -0.5 * (logs * 2. + ((x - mean) ** 2.) / torch.exp(logs * 2.) + GaussianDiag.Log2PI)\n",
    "\n",
    "    @staticmethod\n",
    "    def logp(mean, logs, x):\n",
    "        likelihood = GaussianDiag.likelihood(mean, logs, x)\n",
    "        return torch.sum(likelihood, dim=(1, 2, 3))\n",
    "\n",
    "    @staticmethod\n",
    "    def sample(mean, logs, eps_std=None):\n",
    "        eps_std = eps_std or 1\n",
    "        eps = torch.normal(mean=torch.zeros_like(mean),\n",
    "                           std=torch.ones_like(logs) * eps_std)\n",
    "        return mean + torch.exp(logs) * eps\n",
    "\n",
    "    @staticmethod\n",
    "    def batchsample(batchsize, mean, logs, eps_std=None):\n",
    "        eps_std = eps_std or 1\n",
    "        sample = GaussianDiag.sample(mean, logs, eps_std)\n",
    "        for i in range(1, batchsize):\n",
    "            s = GaussianDiag.sample(mean, logs, eps_std)\n",
    "            sample = torch.cat((sample, s), dim=0)\n",
    "        return sample\n",
    "\n",
    "\n",
    "\n",
    "class LinearZeros(nn.Linear):\n",
    "\n",
    "    def __init__(self, in_channels, out_channels):\n",
    "        super().__init__(in_channels, out_channels)\n",
    "        self.weight.data.zero_()\n",
    "        self.bias.data.zero_()\n",
    "\n",
    "    def forward(self, input):\n",
    "        output = super().forward(input)\n",
    "        return output\n",
    "\n",
    "\n",
    "class LinearNorm(nn.Linear):\n",
    "\n",
    "    def __init__(self, in_channels, out_channels):\n",
    "        super().__init__(in_channels, out_channels)\n",
    "        self.weight.data.normal_(mean=0.0, std=0.1)\n",
    "        self.bias.data.normal_(mean=0.0, std=0.1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "executionInfo": {
     "elapsed": 4,
     "status": "ok",
     "timestamp": 1692391053531,
     "user": {
      "displayName": "Joseph Lupo",
      "userId": "18192058496751055649"
     },
     "user_tz": 240
    },
    "id": "6ntChUCZnc8W"
   },
   "outputs": [],
   "source": [
    "# @title Glow Model\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "\n",
    "class CondGlowStep(nn.Module):\n",
    "\n",
    "    def __init__(self, x_size, y_size, x_hidden_channels, x_hidden_size, y_hidden_channels):\n",
    "\n",
    "\n",
    "        super().__init__()\n",
    "\n",
    "        # 1. cond-actnorm\n",
    "        self.actnorm = CondActNorm(x_size=x_size, y_channels=y_size[0], x_hidden_channels=x_hidden_channels, x_hidden_size=x_hidden_size)\n",
    "\n",
    "        # 2. cond-1x1conv\n",
    "        self.invconv = Cond1x1Conv(x_size=x_size, x_hidden_channels=x_hidden_channels, x_hidden_size=x_hidden_size, y_channels=y_size[0])\n",
    "\n",
    "        # 3. cond-affine\n",
    "        self.affine = CondAffineCoupling(x_size=x_size, y_size=[y_size[0] // 2, y_size[1], y_size[2]], hidden_channels=y_hidden_channels)\n",
    "\n",
    "\n",
    "    def forward(self, x, y, logdet=None, reverse=False):\n",
    "        \n",
    "        if reverse is False:\n",
    "            # 1. cond-actnorm\n",
    "            y, logdet = self.actnorm(x, y, logdet, reverse=False)\n",
    "\n",
    "            # 2. cond-1x1conv\n",
    "            y, logdet = self.invconv(x, y, logdet, reverse=False)\n",
    "\n",
    "            # 3. cond-affine\n",
    "            y, logdet = self.affine(x, y, logdet, reverse=False)\n",
    "\n",
    "            # Return\n",
    "            return y, logdet\n",
    "\n",
    "\n",
    "        if reverse is True:\n",
    "            # 3. cond-affine\n",
    "            y, logdet = self.affine(x, y, logdet, reverse=True)\n",
    "\n",
    "            # 2. cond-1x1conv\n",
    "            y, logdet = self.invconv(x, y, logdet, reverse=True)\n",
    "\n",
    "            # 1. cond-actnorm\n",
    "            y, logdet = self.actnorm(x, y, logdet, reverse=True)\n",
    "\n",
    "            # Return\n",
    "            return y, logdet\n",
    "\n",
    "\n",
    "class CondGlow(nn.Module):\n",
    "\n",
    "    def __init__(self, x_size, y_size, x_hidden_channels, x_hidden_size, y_hidden_channels, K, L):\n",
    "\n",
    "\n",
    "        super().__init__()\n",
    "        self.layers = nn.ModuleList()\n",
    "        self.output_shapes = []\n",
    "        self.K = K\n",
    "        self.L = L\n",
    "        C, H, W = y_size\n",
    "\n",
    "        for l in range(0, L):\n",
    "\n",
    "            # 1. Squeeze\n",
    "            C, H, W = C * 4, H // 2, W // 2\n",
    "            y_size = [C,H,W]\n",
    "            self.layers.append(SqueezeLayer(factor=2))\n",
    "            self.output_shapes.append([-1, C, H, W])\n",
    "\n",
    "            # 2. K CGlowStep\n",
    "            for k in range(0, K):\n",
    "\n",
    "                self.layers.append(CondGlowStep(x_size = x_size,\n",
    "                                            y_size = y_size,\n",
    "                                            x_hidden_channels = x_hidden_channels,\n",
    "                                            x_hidden_size = x_hidden_size,\n",
    "                                            y_hidden_channels = y_hidden_channels,\n",
    "                                            )\n",
    "                                   )\n",
    "\n",
    "                self.output_shapes.append([-1, C, H, W])\n",
    "\n",
    "            # 3. Split\n",
    "            if l < L - 1:\n",
    "                self.layers.append(Split2d(num_channels=C))\n",
    "                self.output_shapes.append([-1, C // 2, H, W])\n",
    "                C = C // 2\n",
    "\n",
    "\n",
    "    def forward(self, x, y, logdet=0.0, reverse=False, eps_std=1.0):\n",
    "        if reverse == False:\n",
    "            return self.encode(x, y, logdet)\n",
    "        else:\n",
    "            return self.decode(x, y, logdet, eps_std)\n",
    "\n",
    "    def encode(self, x, y, logdet=0.0):\n",
    "        for layer, shape in zip(self.layers, self.output_shapes):\n",
    "            if isinstance(layer, Split2d) or isinstance(layer, SqueezeLayer):\n",
    "                y, logdet = layer(y, logdet, reverse=False)\n",
    "\n",
    "            else:\n",
    "                y, logdet = layer(x, y, logdet, reverse=False)\n",
    "        return y, logdet\n",
    "\n",
    "    def decode(self, x, y, logdet=0.0, eps_std=1.0):\n",
    "        for layer in reversed(self.layers):\n",
    "            if isinstance(layer, Split2d):\n",
    "                y, logdet = layer(y, logdet=logdet, reverse=True, eps_std=eps_std)\n",
    "\n",
    "            elif isinstance(layer, SqueezeLayer):\n",
    "                y, logdet = layer(y, logdet=logdet, reverse=True)\n",
    "\n",
    "            else:\n",
    "                y, logdet = layer(x, y, logdet=logdet, reverse=True)\n",
    "\n",
    "        return y, logdet\n",
    "\n",
    "\n",
    "class CondGlowModel(nn.Module):\n",
    "    BCE = nn.BCEWithLogitsLoss()\n",
    "    CE = nn.CrossEntropyLoss()\n",
    "\n",
    "    def __init__(self, args):\n",
    "        super().__init__()\n",
    "        self.flow = CondGlow(x_size=args.x_size,\n",
    "                            y_size=args.y_size,\n",
    "                            x_hidden_channels=args.x_hidden_channels,\n",
    "                            x_hidden_size=args.x_hidden_size,\n",
    "                            y_hidden_channels=args.y_hidden_channels,\n",
    "                            K=args.depth_flow,\n",
    "                            L=args.num_levels,\n",
    "                            )\n",
    "\n",
    "        self.learn_top = args.learn_top\n",
    "\n",
    "\n",
    "        self.register_parameter(\"new_mean\",\n",
    "                                nn.Parameter(torch.zeros(\n",
    "                                    [1,\n",
    "                                     self.flow.output_shapes[-1][1],\n",
    "                                     self.flow.output_shapes[-1][2],\n",
    "                                     self.flow.output_shapes[-1][3]])))\n",
    "\n",
    "\n",
    "        self.register_parameter(\"new_logs\",\n",
    "                                nn.Parameter(torch.zeros(\n",
    "                                    [1,\n",
    "                                     self.flow.output_shapes[-1][1],\n",
    "                                     self.flow.output_shapes[-1][2],\n",
    "                                     self.flow.output_shapes[-1][3]])))\n",
    "\n",
    "        self.n_bins = args.y_bins\n",
    "\n",
    "\n",
    "    def prior(self):\n",
    "\n",
    "        if self.learn_top:\n",
    "            return self.new_mean, self.new_logs\n",
    "        else:\n",
    "            return torch.zeros_like(self.new_mean), torch.zeros_like(self.new_logs)\n",
    "\n",
    "\n",
    "    def forward(self, x=0.0, y=None, eps_std=1.0, reverse=False):\n",
    "        if reverse == False:\n",
    "            dimensions = y.size(1)*y.size(2)*y.size(3)\n",
    "            logdet = torch.zeros_like(y[:, 0, 0, 0])\n",
    "            logdet += float(-np.log(self.n_bins) * dimensions)\n",
    "            z, objective = self.flow(x, y, logdet=logdet, reverse=False)\n",
    "            mean, logs = self.prior()\n",
    "            objective += GaussianDiag.logp(mean, logs, z)\n",
    "            nll = -objective / float(np.log(2.) * dimensions)\n",
    "            return z, nll\n",
    "\n",
    "        else:\n",
    "            with torch.no_grad():\n",
    "                mean, logs = self.prior()\n",
    "                if y is None:\n",
    "                    y = GaussianDiag.batchsample(x.size(0), mean, logs, eps_std)\n",
    "                y, logdet = self.flow(x, y, eps_std=eps_std, reverse=True)\n",
    "            return y, logdet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {
    "executionInfo": {
     "elapsed": 1976,
     "status": "ok",
     "timestamp": 1692391055504,
     "user": {
      "displayName": "Joseph Lupo",
      "userId": "18192058496751055649"
     },
     "user_tz": 240
    },
    "id": "7xtTbLLNndDk"
   },
   "outputs": [],
   "source": [
    "# @title Learner\n",
    "import os\n",
    "import torch\n",
    "import datetime\n",
    "import numpy as np\n",
    "from torch.utils.data import DataLoader\n",
    "import time\n",
    "# from datasets import convert_to_img\n",
    "# from datasets import preprocess\n",
    "# from datasets import postprocess\n",
    "from torchvision.utils import save_image\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "class Trainer(object):\n",
    "\n",
    "    def __init__(self, graph, optim, scheduler, trainingset, validset, args, cuda):\n",
    "\n",
    "        # set path and date\n",
    "        date = str(datetime.datetime.now())\n",
    "        date = date[:date.rfind(\":\")].replace(\"-\", \"\")\\\n",
    "                                     .replace(\":\", \"\")\\\n",
    "                                     .replace(\" \", \"_\")\n",
    "        self.log_dir = os.path.join(args.log_root, \"log_\" + date)\n",
    "        if not os.path.exists(self.log_dir):\n",
    "            os.makedirs(self.log_dir)\n",
    "\n",
    "\n",
    "        self.checkpoints_dir = os.path.join(self.log_dir, \"checkpoints\")\n",
    "        if not os.path.exists(self.checkpoints_dir):\n",
    "            os.makedirs(self.checkpoints_dir)\n",
    "\n",
    "        self.images_dir = os.path.join(self.log_dir, \"images\")\n",
    "        if not os.path.exists(self.images_dir):\n",
    "            os.makedirs(self.images_dir)\n",
    "\n",
    "        self.valid_samples_dir = os.path.join(self.log_dir, \"valid_samples\")\n",
    "        if not os.path.exists(self.valid_samples_dir):\n",
    "            os.makedirs(self.valid_samples_dir)\n",
    "\n",
    "\n",
    "\n",
    "        # model\n",
    "        self.graph = graph\n",
    "        self.optim = optim\n",
    "        self.scheduler = scheduler\n",
    "\n",
    "        # gradient bound\n",
    "        self.max_grad_clip = args.max_grad_clip\n",
    "        self.max_grad_norm = args.max_grad_norm\n",
    "\n",
    "        # data\n",
    "        self.dataset_name = args.dataset_name\n",
    "        self.batch_size = args.batch_size\n",
    "        self.trainingset_loader = DataLoader(trainingset,\n",
    "                                      batch_size=self.batch_size,\n",
    "                                      shuffle=True,\n",
    "                                      drop_last=True)\n",
    "\n",
    "        self.validset_loader = DataLoader(validset,\n",
    "                                      batch_size=self.batch_size,\n",
    "                                      shuffle=False,\n",
    "                                      drop_last=False)\n",
    "\n",
    "        self.num_epochs = args.num_epochs\n",
    "        self.global_step = args.num_steps\n",
    "        self.label_scale = args.label_scale\n",
    "        self.label_bias = args.label_bias\n",
    "        self.x_bins = args.x_bins\n",
    "        self.y_bins = args.y_bins\n",
    "\n",
    "\n",
    "        self.num_epochs = args.num_epochs\n",
    "        self.nll_gap = args.nll_gap\n",
    "        self.inference_gap = args.inference_gap\n",
    "        self.checkpoints_gap = args.checkpoints_gap\n",
    "        self.save_gap = args.save_gap\n",
    "\n",
    "        # device\n",
    "        self.cuda = cuda\n",
    "\n",
    "    def validate(self):\n",
    "        print (\"Start Validating\")\n",
    "        self.graph.eval()\n",
    "        mean_loss = list()\n",
    "        samples = list()\n",
    "        with torch.no_grad():\n",
    "            for i_batch, batch in enumerate(self.validset_loader):\n",
    "                x = batch[\"x\"]\n",
    "                y = batch[\"y\"]\n",
    "\n",
    "                if self.cuda:\n",
    "                    x = x.cuda()\n",
    "                    y = y.cuda()\n",
    "\n",
    "\n",
    "                # forward\n",
    "                z, nll = self.graph(x,y)\n",
    "                loss = torch.mean(nll)\n",
    "                mean_loss.append(loss.data.cpu().item())\n",
    "\n",
    "                # true label\n",
    "                y_true = y\n",
    "\n",
    "                # sample\n",
    "                batch_samples = []\n",
    "                for b in range(len(y)):\n",
    "                    row = []\n",
    "                    row.append(y_true[b])\n",
    "                    batch_samples.append(row)\n",
    "                for i in range(0, 5):\n",
    "                    y_sample,_ = self.graph(x, y=None, reverse=True)\n",
    "\n",
    "                    for b in range(len(y)):\n",
    "                        batch_samples[b].append(y_sample[b])\n",
    "\n",
    "                for b in range(len(y)):\n",
    "                    samples.append(batch_samples[b])\n",
    "\n",
    "        # save samples\n",
    "        for i in range(0, len(samples)):\n",
    "            if i < 5:\n",
    "                fig, axes = plt.subplots(1, len(samples[i]), figsize=(40, 7))\n",
    "                for b in range(0,len(samples[i])):\n",
    "                    plottitle = 'y true' if b == 0 else f'(Using only x_cond) sample {b}'\n",
    "                    axes[b].set_title(plottitle)\n",
    "                    im_pred = axes[b].imshow(samples[i][b].cpu()[0], cmap=\"gray\")\n",
    "                    fig.colorbar(im_pred, fraction=0.046, pad=0.04)\n",
    "                fig.savefig(os.path.join(self.valid_samples_dir, f\"global_step{self.global_step}_sample{i}_batch{b}.pdf\"))\n",
    "                plt.close()\n",
    "\n",
    "        # save loss\n",
    "        mean = np.mean(mean_loss)\n",
    "        with open(os.path.join(self.log_dir, \"valid_NLL.txt\"), \"a\") as nll_file:\n",
    "            nll_file.write(str(self.global_step) + \"\\t\" + \"{:.5f}\".format(mean) + \"\\n\")\n",
    "        print (\"Finish Validating\")\n",
    "        self.graph.train()\n",
    "\n",
    "\n",
    "    def train(self):\n",
    "\n",
    "        self.graph.train()\n",
    "\n",
    "        starttime = time.time()\n",
    "\n",
    "        # run\n",
    "        num_batchs = len(self.trainingset_loader)\n",
    "        total_its = self.num_epochs * num_batchs\n",
    "        for epoch in range(self.num_epochs):\n",
    "            mean_nll = 0.0\n",
    "            for _, batch in enumerate(self.trainingset_loader):\n",
    "                self.optim.zero_grad()\n",
    "\n",
    "                x = batch[\"x\"]\n",
    "                y = batch[\"y\"]\n",
    "\n",
    "\n",
    "                if self.cuda:\n",
    "                    x = x.cuda()\n",
    "                    y = y.cuda()\n",
    "\n",
    "                # forward\n",
    "                z, nll = self.graph(x, y)\n",
    "\n",
    "\n",
    "                # loss\n",
    "                loss = torch.mean(nll)\n",
    "                mean_nll = mean_nll + loss.data\n",
    "\n",
    "                # backward\n",
    "                self.graph.zero_grad()\n",
    "                self.optim.zero_grad()\n",
    "                loss.backward()\n",
    "\n",
    "                # operate grad\n",
    "                if self.max_grad_clip > 0:\n",
    "                    torch.nn.utils.clip_grad_value_(self.graph.parameters(), self.max_grad_clip)\n",
    "                if self.max_grad_norm > 0:\n",
    "                    torch.nn.utils.clip_grad_norm_(self.graph.parameters(), self.max_grad_norm)\n",
    "\n",
    "                # step\n",
    "                self.optim.step()\n",
    "\n",
    "                currenttime = time.time()\n",
    "                elapsed = currenttime - starttime\n",
    "                print(\"Iteration: {}/{} \\t Elapsed time: {:.2f} \\t Loss:{:.5f}\".format(self.global_step, total_its, elapsed, loss.data))\n",
    "\n",
    "                if self.global_step % self.nll_gap == 0:\n",
    "                    with open(os.path.join(self.log_dir, \"NLL.txt\"), \"a\") as nll_file:\n",
    "                        nll_file.write(str(self.global_step) + \" \\t \" + \"{:.2f} \\t {:.5f}\".format(elapsed, loss.data) + \"\\n\")\n",
    "\n",
    "\n",
    "\n",
    "                # checkpoint\n",
    "                if self.global_step % self.checkpoints_gap == 0 and self.global_step > 0:\n",
    "                    self.validate()\n",
    "\n",
    "                    # samples\n",
    "                    samples = []\n",
    "                    for b in range(self.batch_size):\n",
    "                        samples.append([])\n",
    "\n",
    "                    for i in range(0,5):\n",
    "                        y_sample,_ = self.graph(x, y=None, reverse=True)\n",
    "                        for b in range(self.batch_size):\n",
    "                            samples[b].append(y_sample[b])\n",
    "\n",
    "                    # inverse image\n",
    "                    y_inverse,_ = self.graph(x, y=z, reverse=True)\n",
    "\n",
    "                    # true label\n",
    "                    y_true = y\n",
    "\n",
    "\n",
    "                    # save images\n",
    "                    output = None\n",
    "                    for b in range(0, self.batch_size):\n",
    "\n",
    "                        fig, axes = plt.subplots(1, 3, figsize=(20, 8))\n",
    "\n",
    "                        if b < 5:\n",
    "                            axes[0].set_title('y Truth')\n",
    "                            im_true = axes[0].imshow(y[b].cpu()[0], cmap=\"gray\")\n",
    "                            fig.colorbar(im_true, fraction=0.046, pad=0.04)\n",
    "\n",
    "                            axes[1].set_title('y inverse')\n",
    "                            im_latent = axes[1].imshow(y_inverse[b].cpu()[0], cmap=\"gray\")\n",
    "                            fig.colorbar(im_latent, fraction=0.046, pad=0.04)\n",
    "\n",
    "                            axes[2].set_title('y Prediction sample')\n",
    "                            im_pred = axes[2].imshow(samples[b][i].cpu()[0], cmap=\"gray\")\n",
    "                            fig.colorbar(im_pred, fraction=0.046, pad=0.04)\n",
    "\n",
    "                            fig.savefig(os.path.join(self.images_dir, f\"global_step-{self.global_step}_batch_{b}.pdf\"))\n",
    "                            plt.close()\n",
    "\n",
    "\n",
    "\n",
    "                # save model\n",
    "                if self.global_step % self.save_gap == 0 and self.global_step > 0:\n",
    "                    save_model(self.graph, self.optim, self.scheduler, self.checkpoints_dir, self.global_step)\n",
    "\n",
    "\n",
    "                self.global_step = self.global_step + 1\n",
    "\n",
    "            if self.scheduler is not None:\n",
    "                self.scheduler.step()\n",
    "            mean_nll = float(mean_nll / float(num_batchs))\n",
    "            with open(os.path.join(self.log_dir, \"Epoch_NLL.txt\"), \"a\") as f:\n",
    "                currenttime = time.time()\n",
    "                elapsed = currenttime - starttime\n",
    "                f.write(\"{} \\t {:.2f}\\t {:.5f}\".format(epoch, elapsed, mean_nll) + \"\\n\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "class Inferencer(object):\n",
    "\n",
    "    def __init__(self, model, dataset, args, cuda):\n",
    "\n",
    "        # set path and date\n",
    "        self.out_root = args.out_root\n",
    "        if not os.path.exists(self.out_root):\n",
    "            os.makedirs(self.out_root)\n",
    "\n",
    "        # cuda\n",
    "        self.cuda = cuda\n",
    "\n",
    "        # model\n",
    "        self.model = model\n",
    "\n",
    "\n",
    "        # data\n",
    "        self.dataset_name = args.dataset_name\n",
    "        self.batch_size = args.batch_size\n",
    "        self.data_loader = DataLoader(dataset,\n",
    "                                      batch_size=self.batch_size,\n",
    "                                      shuffle=False,\n",
    "                                      drop_last=False)\n",
    "\n",
    "        self.label_scale = args.label_scale\n",
    "        self.label_bias = args.label_bias\n",
    "        self.num_labels = args.num_labels\n",
    "        self.y_bins = args.y_bins\n",
    "        self.x_bins = args.x_bins\n",
    "\n",
    "\n",
    "    def sampled_based_prediction(self, n_samples):\n",
    "        metrics = []\n",
    "        start = time.time()\n",
    "        for i_batch, batch in enumerate(self.data_loader):\n",
    "            print(f\"Batch IDs: {i_batch}\")\n",
    "\n",
    "            x = batch[\"x\"]\n",
    "            y = batch[\"y\"]\n",
    "\n",
    "            if self.cuda:\n",
    "                x = x.cuda()\n",
    "                y = y.cuda()\n",
    "\n",
    "\n",
    "            sample_list = list()\n",
    "            nll_list = list()\n",
    "            for i in range(0, n_samples):\n",
    "\n",
    "                print(f\"Samples: {i}/{n_samples}\")\n",
    "\n",
    "                # z, nll = self.model(x, y, reverse=False)\n",
    "                # ypred, logdet = self.model(x, z, reverse=True)\n",
    "\n",
    "                y_sample,_ = self.model(x, reverse=True)\n",
    "                _, nll = self.model(x,y_sample)\n",
    "                loss = torch.mean(nll)\n",
    "                sample_list.append(y_sample)\n",
    "                nll_list.append(loss.data.cpu().numpy())\n",
    "\n",
    "            sample = torch.stack(sample_list)\n",
    "            sample_mean = torch.mean(sample, dim=0, keepdim=False)\n",
    "            sample_std = torch.std(sample, dim=0, keepdim=False)\n",
    "            nll = np.mean(nll_list)\n",
    "\n",
    "            y_pred_imgs_mean = sample_mean\n",
    "            y_pred_imgs_std = sample_std\n",
    "            y_true_imgs = y\n",
    "\n",
    "            # save trues and preds using only xcond\n",
    "            output = []\n",
    "            for i in range(0, len(y_true_imgs)):\n",
    "                true_img = y_true_imgs[i]\n",
    "                pred_img = y_pred_imgs_mean[i]\n",
    "                pred_img_std = y_pred_imgs_std[i]\n",
    "\n",
    "                fig, axes = plt.subplots(1, 3, figsize=(20, 8))\n",
    "                xc = x[i].cpu()[0]\n",
    "                print(f\"E-el: {xc[0]}, E-az: {xc[1]}, M-el: {xc[2]}, M-az: {xc[3]}, 1/ED: {xc[4]}, 1/MD: {xc[5]}, 1/MD^2: {xc[6]}, 1/ED^2: {xc[7]}]\")\n",
    "                axes[0].set_title('Truth')#, f\"E-el: {xc[0]}, E-az: {xc[1]}, M-el: {xc[2]}, M-az: {xc[3]}, 1/ED: {xc[4]}, 1/MD: {xc[5]}, 1/MD^2: {xc[6]}, 1/ED^2: {xc[7]}\")\n",
    "                im_true = axes[0].imshow(true_img.cpu()[0], cmap=\"gray\")\n",
    "                fig.colorbar(im_true, fraction=0.046, pad=0.04)\n",
    "\n",
    "                axes[1].set_title('Prediction mean (using only xcond)')\n",
    "                im_pred = axes[1].imshow(pred_img.cpu()[0], cmap=\"gray\")\n",
    "                fig.colorbar(im_pred, fraction=0.046, pad=0.04)\n",
    "\n",
    "                axes[2].set_title('Prediction std (using only xcond)')\n",
    "                im_pred = axes[2].imshow(pred_img_std.cpu()[0], cmap=\"gray\")\n",
    "                fig.colorbar(im_pred, fraction=0.046, pad=0.04)\n",
    "\n",
    "                fig.savefig(os.path.join(self.out_root, f\"batch_xcond-{i_batch}-{i}.pdf\"))\n",
    "                plt.close()\n",
    "\n",
    "            # Save plots using xcond and y\n",
    "            z, nll = self.model(x, y)\n",
    "            y_inverse, _ = self.model(x, y=z, reverse=True)\n",
    "\n",
    "            for i in range(0, len(y)):\n",
    "                true_img = y[i]\n",
    "                pred_img = y_inverse[i]\n",
    "\n",
    "                fig, axes = plt.subplots(1, 2, figsize=(20, 8))\n",
    "\n",
    "                axes[0].set_title('y Truth')\n",
    "                im_true = axes[0].imshow(true_img.cpu()[0], cmap=\"gray\")\n",
    "                fig.colorbar(im_true, fraction=0.046, pad=0.04)\n",
    "\n",
    "                axes[1].set_title('y inverse')\n",
    "                im_latent = axes[1].imshow(pred_img.cpu()[0], cmap=\"gray\")\n",
    "                fig.colorbar(im_latent, fraction=0.046, pad=0.04)\n",
    "\n",
    "                fig.savefig(os.path.join(self.out_root, f\"batch_xandy-{i_batch}-{i}.pdf\"))\n",
    "                plt.close()\n",
    "\n",
    "            \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "executionInfo": {
     "elapsed": 15,
     "status": "ok",
     "timestamp": 1692391055505,
     "user": {
      "displayName": "Joseph Lupo",
      "userId": "18192058496751055649"
     },
     "user_tz": 240
    },
    "id": "T9jhxCYOndFo"
   },
   "outputs": [],
   "source": [
    "# # @title Datasets\n",
    "# import os\n",
    "# import numpy as np\n",
    "# import torch\n",
    "# import PIL.Image as Image\n",
    "# from torch.utils import data\n",
    "# import torchvision.transforms as transforms\n",
    "\n",
    "\n",
    "# class HorseDataset(data.Dataset):\n",
    "\n",
    "#     def __init__(self, dir, size, n_c, portion=\"train\"):\n",
    "#         self.dir = dir\n",
    "#         self.names = self.read_names(dir, portion)\n",
    "#         self.n_c = n_c\n",
    "#         self.size = size\n",
    "\n",
    "#     def read_names(self, dir, portion):\n",
    "\n",
    "#         path = os.path.join(dir, \"{}.txt\".format(portion))\n",
    "#         names = list()\n",
    "#         with open(path, \"r\") as f:\n",
    "#             for line in f:\n",
    "#                 line = line.strip()\n",
    "#                 name = {}\n",
    "#                 name[\"img\"] = os.path.join(dir, os.path.join(\"images\", line))\n",
    "#                 name[\"lbl\"] = os.path.join(dir, os.path.join(\"labels\", line))\n",
    "#                 names.append(name)\n",
    "#         return names\n",
    "\n",
    "#     def __len__(self):\n",
    "#         return len(self.names)\n",
    "\n",
    "\n",
    "#     def __getitem__(self, index):\n",
    "\n",
    "#         # path\n",
    "#         name = self.names[index]\n",
    "#         img_path = name[\"img\"]\n",
    "#         lbl_path = name[\"lbl\"]\n",
    "#         transform = transforms.Compose([transforms.Resize(self.size), transforms.ToTensor()])\n",
    "\n",
    "#         # img\n",
    "#         img = Image.open(img_path).convert(\"RGB\")\n",
    "#         img = transform(img)\n",
    "\n",
    "#         # lbl\n",
    "#         lbl = Image.open(lbl_path).convert(\"L\")\n",
    "#         lbl = transform(lbl)\n",
    "#         lbl = torch.round(lbl)\n",
    "#         if self.n_c > 1:\n",
    "#             lbl = lbl.repeat(self.n_c,1,1)\n",
    "\n",
    "#         return {\"x\":img, \"y\":lbl}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "executionInfo": {
     "elapsed": 14,
     "status": "ok",
     "timestamp": 1692391055506,
     "user": {
      "displayName": "Joseph Lupo",
      "userId": "18192058496751055649"
     },
     "user_tz": 240
    },
    "id": "FR8B9zdYndHt"
   },
   "outputs": [],
   "source": [
    "# @title TESS Dataset\n",
    "import os\n",
    "from PIL import Image\n",
    "import torch\n",
    "import pickle\n",
    "from torch.utils.data import Dataset, DataLoader, random_split\n",
    "import torchvision.transforms as transforms\n",
    "\n",
    "class TESSDataset(Dataset):\n",
    "    def __init__(self):\n",
    "\n",
    "        # self.data = []\n",
    "        # self.labels = []\n",
    "\n",
    "        # get data\n",
    "        angle_folder = \"/pdo/users/jlupoiii/TESS/data/angles/\"\n",
    "        ccd_folder = \"/pdo/users/jlupoiii/TESS/data/ccds/\"\n",
    "\n",
    "        # data matrices\n",
    "        X = []\n",
    "        Y = []\n",
    "        ffis = []\n",
    "\n",
    "        angles_dic = pickle.load(open(angle_folder+'angles_O13_data.pkl', \"rb\"))\n",
    "\n",
    "        for filename in os.listdir(ccd_folder):\n",
    "            if len(filename) < 40 or filename[27] != '3': continue\n",
    "\n",
    "            image_arr = pickle.load(open(ccd_folder+filename, \"rb\"))\n",
    "            ffi_num = filename[18:18+8]\n",
    "            try:\n",
    "                angles = angles_dic[ffi_num]\n",
    "                # print('Got ffi number', ffi_num)\n",
    "            except:\n",
    "                # print('Could not find ffi with number:', ffi_num)\n",
    "                continue\n",
    "            X.append(np.array([angles[10], angles[11], angles[18], angles[19], angles[22], angles[23], angles[24], angles[25]]))\n",
    "            Y.append(image_arr.flatten())\n",
    "            ffis.append(ffi_num)\n",
    "\n",
    "        # X = np.array(X)\n",
    "        # Y = np.array(Y)\n",
    "        # ffis = np.array(ffis)\n",
    "        # we are calculating Y GIVEN X\n",
    "        self.data = [Image.fromarray(x) for x in X]\n",
    "        self.labels = [Image.fromarray(y) for y in Y]\n",
    "        self.ffis = ffis\n",
    "\n",
    "        # for s in self.labels:\n",
    "        #     print(s.size)\n",
    "        #     # print(np.array(s).reshape((16,16))\n",
    "            \n",
    "        #     plt.imshow(np.array(s).reshape((16,16)))\n",
    "        #     plt.show()\n",
    "        #     plt.close()\n",
    "\n",
    "\n",
    "\n",
    "        # for class_idx, class_name in enumerate(self.classes):\n",
    "        #     # print(class_idx)\n",
    "        #     horse_img_path = os.path.join(self.root_dir, \"horse\", class_name)\n",
    "        #     mask_img_path = os.path.join(self.root_dir, \"mask\", class_name)\n",
    "\n",
    "        #     horse_image = Image.open(horse_img_path).convert('RGB')\n",
    "        #     mask_image = Image.open(mask_img_path).convert('L')\n",
    "\n",
    "        #     self.data.extend([horse_image])\n",
    "        #     self.labels.extend([mask_image])\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        # horse_img_path, mask_img_path = self.data[idx], self.labels[idx]\n",
    "        # label = self.labels[idx]\n",
    "        # horse_image = Image.open(horse_img_path).convert('RGB')\n",
    "        # mask_image = Image.open(mask_img_path).convert('L')\n",
    "\n",
    "        angles_image = self.data[idx]\n",
    "        ffi_image = self.labels[idx]\n",
    "\n",
    "        transform = transforms.Compose([\n",
    "            # transforms.Resize((1, 8)),\n",
    "            transforms.ToTensor(),\n",
    "            lambda s: s.reshape(1, 8),\n",
    "            # transforms.Normalize(mean=[0.456], std=[0.225])\n",
    "        ])\n",
    "        target_transform = transforms.Compose([\n",
    "            lambda s: np.array(s),\n",
    "            lambda s: s.reshape((16,16)),\n",
    "            # transforms.Resize((16, 16)),\n",
    "            transforms.ToTensor()\n",
    "        ])\n",
    "\n",
    "        angles_image = transform(angles_image)\n",
    "        ffi_image = target_transform(ffi_image)\n",
    "        \n",
    "        # return x=angles, y=ffis\n",
    "        # we are calculating X GIVEN Y\n",
    "        return {\"x\":angles_image, \"y\":ffi_image}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 6458,
     "status": "ok",
     "timestamp": 1692391061951,
     "user": {
      "displayName": "Joseph Lupo",
      "userId": "18192058496751055649"
     },
     "user_tz": 240
    },
    "id": "4S1ymwTMm1gO",
    "outputId": "60acbfb8-8096-4873-a1f1-0f1719f8514a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "693\n",
      "torch.Size([1, 8])\n",
      "torch.Size([1, 16, 16])\n",
      "x tensor([1.0840e+02, 2.5450e+02, 8.2900e+01, 2.0540e+02, 1.7007e-02, 1.4184e-02,\n",
      "        2.8923e-04, 2.0120e-04])\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# we are calculating Y GIVEN X\n",
    "tess_dataset = TESSDataset()\n",
    "print(len(tess_dataset))\n",
    "print(tess_dataset[1]['x'].shape)\n",
    "print(tess_dataset[1]['y'].shape)\n",
    "\n",
    "# plt.plot(tess_dataset[1]['x'][0])\n",
    "print('x', tess_dataset[1]['x'][0])\n",
    "plt.show()\n",
    "plt.close()\n",
    "\n",
    "plt.imshow(tess_dataset[1]['y'][0])\n",
    "plt.show()\n",
    "plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "kjh1WU3Mm1iF",
    "outputId": "46a92afc-dfbc-403e-9731-2b7708c7725c",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of param: 6619844\n",
      "Iteration: 0/277000 \t Elapsed time: 0.14 \t Loss:45.74196\n",
      "Iteration: 1/277000 \t Elapsed time: 0.30 \t Loss:45.34182\n",
      "Iteration: 2/277000 \t Elapsed time: 0.43 \t Loss:45.01526\n",
      "Iteration: 3/277000 \t Elapsed time: 0.59 \t Loss:44.73060\n",
      "Iteration: 4/277000 \t Elapsed time: 0.75 \t Loss:44.47426\n",
      "Iteration: 5/277000 \t Elapsed time: 0.89 \t Loss:44.23869\n",
      "Iteration: 6/277000 \t Elapsed time: 1.03 \t Loss:44.01900\n",
      "Iteration: 7/277000 \t Elapsed time: 1.17 \t Loss:43.81193\n",
      "Iteration: 8/277000 \t Elapsed time: 1.32 \t Loss:43.61573\n",
      "Iteration: 9/277000 \t Elapsed time: 1.47 \t Loss:43.42771\n",
      "Iteration: 10/277000 \t Elapsed time: 1.62 \t Loss:43.24767\n",
      "Iteration: 11/277000 \t Elapsed time: 1.85 \t Loss:43.07307\n",
      "Iteration: 12/277000 \t Elapsed time: 2.03 \t Loss:42.90585\n",
      "Iteration: 13/277000 \t Elapsed time: 2.18 \t Loss:42.74066\n",
      "Iteration: 14/277000 \t Elapsed time: 2.34 \t Loss:42.58300\n",
      "Iteration: 15/277000 \t Elapsed time: 2.79 \t Loss:42.42582\n",
      "Iteration: 16/277000 \t Elapsed time: 2.93 \t Loss:42.27273\n",
      "Iteration: 17/277000 \t Elapsed time: 3.61 \t Loss:42.12095\n",
      "Iteration: 18/277000 \t Elapsed time: 3.75 \t Loss:41.97128\n",
      "Iteration: 19/277000 \t Elapsed time: 3.88 \t Loss:41.83033\n",
      "Iteration: 20/277000 \t Elapsed time: 4.04 \t Loss:41.68820\n",
      "Iteration: 21/277000 \t Elapsed time: 4.19 \t Loss:41.55447\n",
      "Iteration: 22/277000 \t Elapsed time: 4.35 \t Loss:41.41182\n",
      "Iteration: 23/277000 \t Elapsed time: 4.51 \t Loss:41.27787\n",
      "Iteration: 24/277000 \t Elapsed time: 4.65 \t Loss:41.13634\n",
      "Iteration: 25/277000 \t Elapsed time: 4.83 \t Loss:41.00417\n",
      "Iteration: 26/277000 \t Elapsed time: 4.97 \t Loss:40.85926\n",
      "Iteration: 27/277000 \t Elapsed time: 5.17 \t Loss:40.73304\n",
      "Iteration: 28/277000 \t Elapsed time: 5.31 \t Loss:40.60873\n",
      "Iteration: 29/277000 \t Elapsed time: 5.46 \t Loss:40.49266\n",
      "Iteration: 30/277000 \t Elapsed time: 5.62 \t Loss:40.35939\n",
      "Iteration: 31/277000 \t Elapsed time: 5.77 \t Loss:40.24734\n",
      "Iteration: 32/277000 \t Elapsed time: 5.97 \t Loss:40.13634\n",
      "Iteration: 33/277000 \t Elapsed time: 6.15 \t Loss:40.02504\n",
      "Iteration: 34/277000 \t Elapsed time: 6.55 \t Loss:39.93150\n",
      "Iteration: 35/277000 \t Elapsed time: 6.73 \t Loss:39.80551\n",
      "Iteration: 36/277000 \t Elapsed time: 6.90 \t Loss:39.70764\n",
      "Iteration: 37/277000 \t Elapsed time: 7.12 \t Loss:39.60973\n",
      "Iteration: 38/277000 \t Elapsed time: 7.67 \t Loss:39.51221\n",
      "Iteration: 39/277000 \t Elapsed time: 7.81 \t Loss:39.42048\n",
      "Iteration: 40/277000 \t Elapsed time: 7.95 \t Loss:39.32846\n",
      "Iteration: 41/277000 \t Elapsed time: 8.16 \t Loss:39.24337\n",
      "Iteration: 42/277000 \t Elapsed time: 8.43 \t Loss:39.15681\n",
      "Iteration: 43/277000 \t Elapsed time: 8.58 \t Loss:39.07550\n",
      "Iteration: 44/277000 \t Elapsed time: 8.74 \t Loss:38.99018\n",
      "Iteration: 45/277000 \t Elapsed time: 8.95 \t Loss:38.91154\n",
      "Iteration: 46/277000 \t Elapsed time: 9.11 \t Loss:38.83376\n",
      "Iteration: 47/277000 \t Elapsed time: 9.26 \t Loss:38.75387\n",
      "Iteration: 48/277000 \t Elapsed time: 9.44 \t Loss:38.67600\n",
      "Iteration: 49/277000 \t Elapsed time: 9.64 \t Loss:38.60188\n",
      "Iteration: 50/277000 \t Elapsed time: 9.86 \t Loss:38.52987\n",
      "Iteration: 51/277000 \t Elapsed time: 10.00 \t Loss:38.45630\n",
      "Iteration: 52/277000 \t Elapsed time: 10.16 \t Loss:38.38149\n",
      "Iteration: 53/277000 \t Elapsed time: 10.34 \t Loss:38.30914\n",
      "Iteration: 54/277000 \t Elapsed time: 10.51 \t Loss:38.23993\n",
      "Iteration: 55/277000 \t Elapsed time: 10.70 \t Loss:38.16732\n",
      "Iteration: 56/277000 \t Elapsed time: 10.89 \t Loss:38.09911\n",
      "Iteration: 57/277000 \t Elapsed time: 11.09 \t Loss:38.02917\n",
      "Iteration: 58/277000 \t Elapsed time: 11.24 \t Loss:37.96124\n",
      "Iteration: 59/277000 \t Elapsed time: 11.38 \t Loss:37.89463\n",
      "Iteration: 60/277000 \t Elapsed time: 11.56 \t Loss:37.82785\n",
      "Iteration: 61/277000 \t Elapsed time: 11.88 \t Loss:37.76145\n",
      "Iteration: 62/277000 \t Elapsed time: 12.05 \t Loss:37.69584\n",
      "Iteration: 63/277000 \t Elapsed time: 12.18 \t Loss:37.63157\n",
      "Iteration: 64/277000 \t Elapsed time: 12.43 \t Loss:37.56706\n",
      "Iteration: 65/277000 \t Elapsed time: 12.57 \t Loss:37.50280\n",
      "Iteration: 66/277000 \t Elapsed time: 13.15 \t Loss:37.43963\n",
      "Iteration: 67/277000 \t Elapsed time: 13.36 \t Loss:37.37667\n",
      "Iteration: 68/277000 \t Elapsed time: 13.50 \t Loss:37.31428\n",
      "Iteration: 69/277000 \t Elapsed time: 13.64 \t Loss:37.25233\n",
      "Iteration: 70/277000 \t Elapsed time: 13.81 \t Loss:37.19088\n",
      "Iteration: 71/277000 \t Elapsed time: 13.96 \t Loss:37.13019\n",
      "Iteration: 72/277000 \t Elapsed time: 14.29 \t Loss:37.06953\n",
      "Iteration: 73/277000 \t Elapsed time: 14.45 \t Loss:37.00974\n",
      "Iteration: 74/277000 \t Elapsed time: 14.63 \t Loss:36.95010\n",
      "Iteration: 75/277000 \t Elapsed time: 14.79 \t Loss:36.89072\n",
      "Iteration: 76/277000 \t Elapsed time: 14.92 \t Loss:36.83191\n",
      "Iteration: 77/277000 \t Elapsed time: 16.25 \t Loss:36.77375\n",
      "Iteration: 78/277000 \t Elapsed time: 16.41 \t Loss:36.71558\n",
      "Iteration: 79/277000 \t Elapsed time: 16.61 \t Loss:36.65878\n",
      "Iteration: 80/277000 \t Elapsed time: 16.78 \t Loss:36.60114\n",
      "Iteration: 81/277000 \t Elapsed time: 16.99 \t Loss:36.54411\n",
      "Iteration: 82/277000 \t Elapsed time: 17.19 \t Loss:36.48740\n",
      "Iteration: 83/277000 \t Elapsed time: 17.41 \t Loss:36.43164\n",
      "Iteration: 84/277000 \t Elapsed time: 17.58 \t Loss:36.37598\n",
      "Iteration: 85/277000 \t Elapsed time: 18.23 \t Loss:36.32049\n",
      "Iteration: 86/277000 \t Elapsed time: 18.37 \t Loss:36.26537\n",
      "Iteration: 87/277000 \t Elapsed time: 18.51 \t Loss:36.21105\n",
      "Iteration: 88/277000 \t Elapsed time: 18.68 \t Loss:36.15622\n",
      "Iteration: 89/277000 \t Elapsed time: 18.85 \t Loss:36.10212\n",
      "Iteration: 90/277000 \t Elapsed time: 19.05 \t Loss:36.04837\n",
      "Iteration: 91/277000 \t Elapsed time: 19.25 \t Loss:35.99475\n",
      "Iteration: 92/277000 \t Elapsed time: 19.47 \t Loss:35.94151\n",
      "Iteration: 93/277000 \t Elapsed time: 19.66 \t Loss:35.88869\n",
      "Iteration: 94/277000 \t Elapsed time: 19.80 \t Loss:35.83662\n",
      "Iteration: 95/277000 \t Elapsed time: 19.97 \t Loss:35.78408\n",
      "Iteration: 96/277000 \t Elapsed time: 20.20 \t Loss:35.73192\n",
      "Iteration: 97/277000 \t Elapsed time: 20.43 \t Loss:35.68021\n",
      "Iteration: 98/277000 \t Elapsed time: 20.61 \t Loss:35.62883\n",
      "Iteration: 99/277000 \t Elapsed time: 20.78 \t Loss:35.57778\n",
      "Iteration: 100/277000 \t Elapsed time: 21.04 \t Loss:35.52654\n",
      "Iteration: 101/277000 \t Elapsed time: 21.21 \t Loss:35.47594\n",
      "Iteration: 102/277000 \t Elapsed time: 21.46 \t Loss:35.42550\n",
      "Iteration: 103/277000 \t Elapsed time: 21.69 \t Loss:35.37531\n",
      "Iteration: 104/277000 \t Elapsed time: 21.85 \t Loss:35.32534\n",
      "Iteration: 105/277000 \t Elapsed time: 22.01 \t Loss:35.27563\n",
      "Iteration: 106/277000 \t Elapsed time: 22.15 \t Loss:35.22659\n",
      "Iteration: 107/277000 \t Elapsed time: 22.29 \t Loss:35.17709\n",
      "Iteration: 108/277000 \t Elapsed time: 22.44 \t Loss:35.12761\n",
      "Iteration: 109/277000 \t Elapsed time: 22.60 \t Loss:35.07861\n",
      "Iteration: 110/277000 \t Elapsed time: 22.75 \t Loss:35.02964\n",
      "Iteration: 111/277000 \t Elapsed time: 22.90 \t Loss:34.98071\n",
      "Iteration: 112/277000 \t Elapsed time: 23.03 \t Loss:34.93246\n",
      "Iteration: 113/277000 \t Elapsed time: 23.17 \t Loss:34.88411\n",
      "Iteration: 114/277000 \t Elapsed time: 23.33 \t Loss:34.83600\n",
      "Iteration: 115/277000 \t Elapsed time: 23.53 \t Loss:34.78848\n",
      "Iteration: 116/277000 \t Elapsed time: 23.70 \t Loss:34.74113\n",
      "Iteration: 117/277000 \t Elapsed time: 23.87 \t Loss:34.69398\n",
      "Iteration: 118/277000 \t Elapsed time: 24.01 \t Loss:34.64730\n",
      "Iteration: 119/277000 \t Elapsed time: 24.16 \t Loss:34.60027\n",
      "Iteration: 120/277000 \t Elapsed time: 24.36 \t Loss:34.55373\n",
      "Iteration: 121/277000 \t Elapsed time: 24.52 \t Loss:34.50742\n",
      "Iteration: 122/277000 \t Elapsed time: 24.66 \t Loss:34.46120\n",
      "Iteration: 123/277000 \t Elapsed time: 24.79 \t Loss:34.41558\n",
      "Iteration: 124/277000 \t Elapsed time: 25.19 \t Loss:34.36944\n",
      "Iteration: 125/277000 \t Elapsed time: 25.34 \t Loss:34.32400\n",
      "Iteration: 126/277000 \t Elapsed time: 25.51 \t Loss:34.27777\n",
      "Iteration: 127/277000 \t Elapsed time: 25.66 \t Loss:34.23235\n",
      "Iteration: 128/277000 \t Elapsed time: 26.12 \t Loss:34.18703\n",
      "Iteration: 129/277000 \t Elapsed time: 26.27 \t Loss:34.14193\n",
      "Iteration: 130/277000 \t Elapsed time: 26.41 \t Loss:34.09708\n",
      "Iteration: 131/277000 \t Elapsed time: 26.55 \t Loss:34.05252\n",
      "Iteration: 132/277000 \t Elapsed time: 26.70 \t Loss:34.00808\n",
      "Iteration: 133/277000 \t Elapsed time: 26.87 \t Loss:33.96420\n",
      "Iteration: 134/277000 \t Elapsed time: 27.02 \t Loss:33.92017\n",
      "Iteration: 135/277000 \t Elapsed time: 27.20 \t Loss:33.87622\n",
      "Iteration: 136/277000 \t Elapsed time: 29.03 \t Loss:33.83254\n",
      "Iteration: 137/277000 \t Elapsed time: 29.19 \t Loss:33.78872\n",
      "Iteration: 138/277000 \t Elapsed time: 29.33 \t Loss:33.74532\n",
      "Iteration: 139/277000 \t Elapsed time: 29.48 \t Loss:33.70249\n",
      "Iteration: 140/277000 \t Elapsed time: 29.69 \t Loss:33.65907\n",
      "Iteration: 141/277000 \t Elapsed time: 29.89 \t Loss:33.61599\n",
      "Iteration: 142/277000 \t Elapsed time: 30.04 \t Loss:33.57310\n",
      "Iteration: 143/277000 \t Elapsed time: 30.19 \t Loss:33.53037\n",
      "Iteration: 144/277000 \t Elapsed time: 30.35 \t Loss:33.48778\n",
      "Iteration: 145/277000 \t Elapsed time: 30.51 \t Loss:33.44541\n",
      "Iteration: 146/277000 \t Elapsed time: 30.69 \t Loss:33.40311\n",
      "Iteration: 147/277000 \t Elapsed time: 30.86 \t Loss:33.36100\n",
      "Iteration: 148/277000 \t Elapsed time: 31.00 \t Loss:33.31919\n",
      "Iteration: 149/277000 \t Elapsed time: 31.26 \t Loss:33.27713\n",
      "Iteration: 150/277000 \t Elapsed time: 31.42 \t Loss:33.23542\n",
      "Iteration: 151/277000 \t Elapsed time: 31.57 \t Loss:33.19396\n",
      "Iteration: 152/277000 \t Elapsed time: 31.80 \t Loss:33.15222\n",
      "Iteration: 153/277000 \t Elapsed time: 32.02 \t Loss:33.11105\n",
      "Iteration: 154/277000 \t Elapsed time: 32.17 \t Loss:33.06969\n",
      "Iteration: 155/277000 \t Elapsed time: 32.38 \t Loss:33.02852\n",
      "Iteration: 156/277000 \t Elapsed time: 32.54 \t Loss:32.98758\n",
      "Iteration: 157/277000 \t Elapsed time: 32.68 \t Loss:32.94669\n",
      "Iteration: 158/277000 \t Elapsed time: 32.84 \t Loss:32.90594\n",
      "Iteration: 159/277000 \t Elapsed time: 33.07 \t Loss:32.86544\n",
      "Iteration: 160/277000 \t Elapsed time: 33.58 \t Loss:32.82490\n",
      "Iteration: 161/277000 \t Elapsed time: 33.77 \t Loss:32.78459\n",
      "Iteration: 162/277000 \t Elapsed time: 33.91 \t Loss:32.74420\n",
      "Iteration: 163/277000 \t Elapsed time: 34.11 \t Loss:32.70399\n",
      "Iteration: 164/277000 \t Elapsed time: 34.25 \t Loss:32.66418\n",
      "Iteration: 165/277000 \t Elapsed time: 34.42 \t Loss:32.62413\n",
      "Iteration: 166/277000 \t Elapsed time: 34.62 \t Loss:32.58427\n",
      "Iteration: 167/277000 \t Elapsed time: 34.95 \t Loss:32.54448\n",
      "Iteration: 168/277000 \t Elapsed time: 35.10 \t Loss:32.50486\n",
      "Iteration: 169/277000 \t Elapsed time: 35.24 \t Loss:32.46532\n",
      "Iteration: 170/277000 \t Elapsed time: 35.44 \t Loss:32.42563\n",
      "Iteration: 171/277000 \t Elapsed time: 35.67 \t Loss:32.38609\n",
      "Iteration: 172/277000 \t Elapsed time: 35.81 \t Loss:32.34654\n",
      "Iteration: 173/277000 \t Elapsed time: 35.97 \t Loss:32.30709\n",
      "Iteration: 174/277000 \t Elapsed time: 36.17 \t Loss:32.26776\n",
      "Iteration: 175/277000 \t Elapsed time: 36.34 \t Loss:32.22871\n",
      "Iteration: 176/277000 \t Elapsed time: 36.66 \t Loss:32.18973\n",
      "Iteration: 177/277000 \t Elapsed time: 36.88 \t Loss:32.15099\n",
      "Iteration: 178/277000 \t Elapsed time: 37.10 \t Loss:32.11256\n",
      "Iteration: 179/277000 \t Elapsed time: 37.43 \t Loss:32.07396\n",
      "Iteration: 180/277000 \t Elapsed time: 37.58 \t Loss:32.03528\n",
      "Iteration: 181/277000 \t Elapsed time: 37.79 \t Loss:31.99707\n",
      "Iteration: 182/277000 \t Elapsed time: 37.94 \t Loss:31.95882\n",
      "Iteration: 183/277000 \t Elapsed time: 38.09 \t Loss:31.92076\n",
      "Iteration: 184/277000 \t Elapsed time: 38.25 \t Loss:31.88277\n",
      "Iteration: 185/277000 \t Elapsed time: 38.39 \t Loss:31.84475\n",
      "Iteration: 186/277000 \t Elapsed time: 38.63 \t Loss:31.80691\n",
      "Iteration: 187/277000 \t Elapsed time: 38.85 \t Loss:31.76937\n",
      "Iteration: 188/277000 \t Elapsed time: 39.00 \t Loss:31.73151\n",
      "Iteration: 189/277000 \t Elapsed time: 39.17 \t Loss:31.69425\n",
      "Iteration: 190/277000 \t Elapsed time: 39.32 \t Loss:31.65655\n",
      "Iteration: 191/277000 \t Elapsed time: 39.64 \t Loss:31.61915\n",
      "Iteration: 192/277000 \t Elapsed time: 39.81 \t Loss:31.58187\n",
      "Iteration: 193/277000 \t Elapsed time: 40.00 \t Loss:31.54476\n",
      "Iteration: 194/277000 \t Elapsed time: 40.17 \t Loss:31.50756\n",
      "Iteration: 195/277000 \t Elapsed time: 40.34 \t Loss:31.47053\n",
      "Iteration: 196/277000 \t Elapsed time: 40.51 \t Loss:31.43365\n",
      "Iteration: 197/277000 \t Elapsed time: 40.66 \t Loss:31.39691\n",
      "Iteration: 198/277000 \t Elapsed time: 40.83 \t Loss:31.36003\n",
      "Iteration: 199/277000 \t Elapsed time: 41.02 \t Loss:31.32335\n",
      "Iteration: 200/277000 \t Elapsed time: 41.25 \t Loss:31.28684\n",
      "Iteration: 201/277000 \t Elapsed time: 41.43 \t Loss:31.25030\n",
      "Iteration: 202/277000 \t Elapsed time: 41.58 \t Loss:31.21396\n",
      "Iteration: 203/277000 \t Elapsed time: 41.85 \t Loss:31.17761\n",
      "Iteration: 204/277000 \t Elapsed time: 42.09 \t Loss:31.14143\n",
      "Iteration: 205/277000 \t Elapsed time: 42.25 \t Loss:31.10537\n",
      "Iteration: 206/277000 \t Elapsed time: 42.43 \t Loss:31.06950\n",
      "Iteration: 207/277000 \t Elapsed time: 42.61 \t Loss:31.03317\n",
      "Iteration: 208/277000 \t Elapsed time: 43.18 \t Loss:30.99731\n",
      "Iteration: 209/277000 \t Elapsed time: 43.35 \t Loss:30.96165\n",
      "Iteration: 210/277000 \t Elapsed time: 43.50 \t Loss:30.92584\n",
      "Iteration: 211/277000 \t Elapsed time: 43.73 \t Loss:30.89023\n",
      "Iteration: 212/277000 \t Elapsed time: 43.93 \t Loss:30.85445\n",
      "Iteration: 213/277000 \t Elapsed time: 44.13 \t Loss:30.81912\n",
      "Iteration: 214/277000 \t Elapsed time: 44.32 \t Loss:30.78349\n",
      "Iteration: 215/277000 \t Elapsed time: 44.47 \t Loss:30.74819\n",
      "Iteration: 216/277000 \t Elapsed time: 44.72 \t Loss:30.71279\n",
      "Iteration: 217/277000 \t Elapsed time: 44.90 \t Loss:30.67767\n",
      "Iteration: 218/277000 \t Elapsed time: 45.09 \t Loss:30.64253\n",
      "Iteration: 219/277000 \t Elapsed time: 45.25 \t Loss:30.60750\n",
      "Iteration: 220/277000 \t Elapsed time: 45.45 \t Loss:30.57251\n",
      "Iteration: 221/277000 \t Elapsed time: 45.62 \t Loss:30.53756\n",
      "Iteration: 222/277000 \t Elapsed time: 45.78 \t Loss:30.50269\n",
      "Iteration: 223/277000 \t Elapsed time: 46.01 \t Loss:30.46790\n",
      "Iteration: 224/277000 \t Elapsed time: 46.24 \t Loss:30.43324\n",
      "Iteration: 225/277000 \t Elapsed time: 46.40 \t Loss:30.39866\n",
      "Iteration: 226/277000 \t Elapsed time: 46.54 \t Loss:30.36404\n",
      "Iteration: 227/277000 \t Elapsed time: 46.93 \t Loss:30.32953\n",
      "Iteration: 228/277000 \t Elapsed time: 47.09 \t Loss:30.29514\n",
      "Iteration: 229/277000 \t Elapsed time: 47.24 \t Loss:30.26088\n",
      "Iteration: 230/277000 \t Elapsed time: 47.41 \t Loss:30.22683\n",
      "Iteration: 231/277000 \t Elapsed time: 47.57 \t Loss:30.19238\n",
      "Iteration: 232/277000 \t Elapsed time: 47.74 \t Loss:30.15849\n",
      "Iteration: 233/277000 \t Elapsed time: 47.92 \t Loss:30.12431\n",
      "Iteration: 234/277000 \t Elapsed time: 48.09 \t Loss:30.09025\n",
      "Iteration: 235/277000 \t Elapsed time: 48.22 \t Loss:30.05638\n",
      "Iteration: 236/277000 \t Elapsed time: 48.38 \t Loss:30.02279\n",
      "Iteration: 237/277000 \t Elapsed time: 48.54 \t Loss:29.98882\n",
      "Iteration: 238/277000 \t Elapsed time: 48.70 \t Loss:29.95512\n",
      "Iteration: 239/277000 \t Elapsed time: 48.84 \t Loss:29.92147\n",
      "Iteration: 240/277000 \t Elapsed time: 49.00 \t Loss:29.88790\n",
      "Iteration: 241/277000 \t Elapsed time: 49.15 \t Loss:29.85439\n",
      "Iteration: 242/277000 \t Elapsed time: 49.29 \t Loss:29.82094\n",
      "Iteration: 243/277000 \t Elapsed time: 49.43 \t Loss:29.78763\n",
      "Iteration: 244/277000 \t Elapsed time: 49.57 \t Loss:29.75427\n",
      "Iteration: 245/277000 \t Elapsed time: 49.72 \t Loss:29.72111\n",
      "Iteration: 246/277000 \t Elapsed time: 49.88 \t Loss:29.68792\n",
      "Iteration: 247/277000 \t Elapsed time: 50.03 \t Loss:29.65477\n",
      "Iteration: 248/277000 \t Elapsed time: 50.17 \t Loss:29.62175\n",
      "Iteration: 249/277000 \t Elapsed time: 50.33 \t Loss:29.58879\n",
      "Iteration: 250/277000 \t Elapsed time: 50.50 \t Loss:29.55586\n",
      "Iteration: 251/277000 \t Elapsed time: 50.65 \t Loss:29.52300\n",
      "Iteration: 252/277000 \t Elapsed time: 50.82 \t Loss:29.49023\n",
      "Iteration: 253/277000 \t Elapsed time: 50.97 \t Loss:29.45749\n",
      "Iteration: 254/277000 \t Elapsed time: 51.25 \t Loss:29.42493\n",
      "Iteration: 255/277000 \t Elapsed time: 54.73 \t Loss:29.39227\n",
      "Iteration: 256/277000 \t Elapsed time: 54.88 \t Loss:29.35961\n",
      "Iteration: 257/277000 \t Elapsed time: 55.04 \t Loss:29.32707\n",
      "Iteration: 258/277000 \t Elapsed time: 55.20 \t Loss:29.29453\n",
      "Iteration: 259/277000 \t Elapsed time: 55.37 \t Loss:29.26233\n",
      "Iteration: 260/277000 \t Elapsed time: 55.89 \t Loss:29.22983\n",
      "Iteration: 261/277000 \t Elapsed time: 56.05 \t Loss:29.19752\n",
      "Iteration: 262/277000 \t Elapsed time: 56.30 \t Loss:29.16532\n",
      "Iteration: 263/277000 \t Elapsed time: 56.44 \t Loss:29.13328\n",
      "Iteration: 264/277000 \t Elapsed time: 56.60 \t Loss:29.10121\n",
      "Iteration: 265/277000 \t Elapsed time: 56.78 \t Loss:29.06925\n",
      "Iteration: 266/277000 \t Elapsed time: 56.95 \t Loss:29.03745\n",
      "Iteration: 267/277000 \t Elapsed time: 57.26 \t Loss:29.00552\n",
      "Iteration: 268/277000 \t Elapsed time: 57.48 \t Loss:28.97360\n",
      "Iteration: 269/277000 \t Elapsed time: 57.63 \t Loss:28.94179\n",
      "Iteration: 270/277000 \t Elapsed time: 57.80 \t Loss:28.91003\n",
      "Iteration: 271/277000 \t Elapsed time: 57.98 \t Loss:28.87842\n",
      "Iteration: 272/277000 \t Elapsed time: 58.12 \t Loss:28.84705\n",
      "Iteration: 273/277000 \t Elapsed time: 58.27 \t Loss:28.81530\n",
      "Iteration: 274/277000 \t Elapsed time: 58.47 \t Loss:28.78392\n",
      "Iteration: 275/277000 \t Elapsed time: 58.67 \t Loss:28.75251\n",
      "Iteration: 276/277000 \t Elapsed time: 58.85 \t Loss:28.72115\n",
      "Iteration: 277/277000 \t Elapsed time: 59.06 \t Loss:28.68979\n",
      "Iteration: 278/277000 \t Elapsed time: 59.21 \t Loss:28.65852\n",
      "Iteration: 279/277000 \t Elapsed time: 59.40 \t Loss:28.62729\n",
      "Iteration: 280/277000 \t Elapsed time: 59.59 \t Loss:28.59614\n",
      "Iteration: 281/277000 \t Elapsed time: 59.82 \t Loss:28.56517\n",
      "Iteration: 282/277000 \t Elapsed time: 59.99 \t Loss:28.53397\n",
      "Iteration: 283/277000 \t Elapsed time: 60.15 \t Loss:28.50298\n",
      "Iteration: 284/277000 \t Elapsed time: 60.32 \t Loss:28.47211\n",
      "Iteration: 285/277000 \t Elapsed time: 60.47 \t Loss:28.44134\n",
      "Iteration: 286/277000 \t Elapsed time: 60.62 \t Loss:28.41042\n",
      "Iteration: 287/277000 \t Elapsed time: 60.77 \t Loss:28.37968\n",
      "Iteration: 288/277000 \t Elapsed time: 60.91 \t Loss:28.34894\n",
      "Iteration: 289/277000 \t Elapsed time: 61.12 \t Loss:28.31833\n",
      "Iteration: 290/277000 \t Elapsed time: 61.47 \t Loss:28.28770\n",
      "Iteration: 291/277000 \t Elapsed time: 61.61 \t Loss:28.25714\n",
      "Iteration: 292/277000 \t Elapsed time: 61.75 \t Loss:28.22659\n",
      "Iteration: 293/277000 \t Elapsed time: 61.92 \t Loss:28.19611\n",
      "Iteration: 294/277000 \t Elapsed time: 62.39 \t Loss:28.16581\n",
      "Iteration: 295/277000 \t Elapsed time: 62.60 \t Loss:28.13532\n",
      "Iteration: 296/277000 \t Elapsed time: 62.81 \t Loss:28.10521\n",
      "Iteration: 297/277000 \t Elapsed time: 62.97 \t Loss:28.07503\n",
      "Iteration: 298/277000 \t Elapsed time: 63.15 \t Loss:28.04494\n",
      "Iteration: 299/277000 \t Elapsed time: 63.30 \t Loss:28.01467\n",
      "Iteration: 300/277000 \t Elapsed time: 63.51 \t Loss:27.98445\n",
      "Iteration: 301/277000 \t Elapsed time: 63.64 \t Loss:27.95449\n",
      "Iteration: 302/277000 \t Elapsed time: 63.85 \t Loss:27.92448\n",
      "Iteration: 303/277000 \t Elapsed time: 64.01 \t Loss:27.89454\n",
      "Iteration: 304/277000 \t Elapsed time: 64.17 \t Loss:27.86458\n",
      "Iteration: 305/277000 \t Elapsed time: 64.33 \t Loss:27.83485\n",
      "Iteration: 306/277000 \t Elapsed time: 64.61 \t Loss:27.80505\n",
      "Iteration: 307/277000 \t Elapsed time: 64.96 \t Loss:27.77533\n",
      "Iteration: 308/277000 \t Elapsed time: 65.11 \t Loss:27.74577\n",
      "Iteration: 309/277000 \t Elapsed time: 65.28 \t Loss:27.71605\n",
      "Iteration: 310/277000 \t Elapsed time: 65.44 \t Loss:27.68618\n",
      "Iteration: 311/277000 \t Elapsed time: 65.70 \t Loss:27.65663\n",
      "Iteration: 312/277000 \t Elapsed time: 65.95 \t Loss:27.62743\n",
      "Iteration: 313/277000 \t Elapsed time: 66.15 \t Loss:27.59776\n",
      "Iteration: 314/277000 \t Elapsed time: 66.36 \t Loss:27.56817\n",
      "Iteration: 315/277000 \t Elapsed time: 66.53 \t Loss:27.53906\n",
      "Iteration: 316/277000 \t Elapsed time: 66.73 \t Loss:27.50958\n",
      "Iteration: 317/277000 \t Elapsed time: 67.01 \t Loss:27.48025\n",
      "Iteration: 318/277000 \t Elapsed time: 67.87 \t Loss:27.45111\n",
      "Iteration: 319/277000 \t Elapsed time: 68.13 \t Loss:27.42188\n",
      "Iteration: 320/277000 \t Elapsed time: 68.26 \t Loss:27.39277\n",
      "Iteration: 321/277000 \t Elapsed time: 68.52 \t Loss:27.36366\n",
      "Iteration: 322/277000 \t Elapsed time: 68.73 \t Loss:27.33458\n",
      "Iteration: 323/277000 \t Elapsed time: 68.89 \t Loss:27.30560\n",
      "Iteration: 324/277000 \t Elapsed time: 69.06 \t Loss:27.27671\n",
      "Iteration: 325/277000 \t Elapsed time: 69.26 \t Loss:27.24775\n",
      "Iteration: 326/277000 \t Elapsed time: 69.48 \t Loss:27.21882\n",
      "Iteration: 327/277000 \t Elapsed time: 69.68 \t Loss:27.18996\n",
      "Iteration: 328/277000 \t Elapsed time: 69.85 \t Loss:27.16118\n",
      "Iteration: 329/277000 \t Elapsed time: 70.88 \t Loss:27.13237\n",
      "Iteration: 330/277000 \t Elapsed time: 71.11 \t Loss:27.10352\n",
      "Iteration: 331/277000 \t Elapsed time: 71.26 \t Loss:27.07477\n",
      "Iteration: 332/277000 \t Elapsed time: 71.43 \t Loss:27.04573\n",
      "Iteration: 333/277000 \t Elapsed time: 71.61 \t Loss:27.01660\n",
      "Iteration: 334/277000 \t Elapsed time: 71.81 \t Loss:26.98709\n",
      "Iteration: 335/277000 \t Elapsed time: 71.95 \t Loss:26.95795\n",
      "Iteration: 336/277000 \t Elapsed time: 72.09 \t Loss:26.92919\n",
      "Iteration: 337/277000 \t Elapsed time: 72.32 \t Loss:26.90067\n",
      "Iteration: 338/277000 \t Elapsed time: 72.48 \t Loss:26.87233\n",
      "Iteration: 339/277000 \t Elapsed time: 72.63 \t Loss:26.84388\n",
      "Iteration: 340/277000 \t Elapsed time: 72.80 \t Loss:26.81549\n",
      "Iteration: 341/277000 \t Elapsed time: 73.07 \t Loss:26.78720\n",
      "Iteration: 342/277000 \t Elapsed time: 73.21 \t Loss:26.75896\n",
      "Iteration: 343/277000 \t Elapsed time: 73.36 \t Loss:26.73083\n",
      "Iteration: 344/277000 \t Elapsed time: 73.53 \t Loss:26.70263\n",
      "Iteration: 345/277000 \t Elapsed time: 73.69 \t Loss:26.67450\n",
      "Iteration: 346/277000 \t Elapsed time: 73.90 \t Loss:26.64639\n",
      "Iteration: 347/277000 \t Elapsed time: 74.06 \t Loss:26.61846\n",
      "Iteration: 348/277000 \t Elapsed time: 74.20 \t Loss:26.59043\n",
      "Iteration: 349/277000 \t Elapsed time: 74.34 \t Loss:26.56245\n",
      "Iteration: 350/277000 \t Elapsed time: 74.49 \t Loss:26.53455\n",
      "Iteration: 351/277000 \t Elapsed time: 74.64 \t Loss:26.50672\n",
      "Iteration: 352/277000 \t Elapsed time: 74.78 \t Loss:26.47885\n",
      "Iteration: 353/277000 \t Elapsed time: 75.00 \t Loss:26.45111\n",
      "Iteration: 354/277000 \t Elapsed time: 75.14 \t Loss:26.42334\n",
      "Iteration: 355/277000 \t Elapsed time: 75.30 \t Loss:26.39562\n",
      "Iteration: 356/277000 \t Elapsed time: 75.47 \t Loss:26.36798\n",
      "Iteration: 357/277000 \t Elapsed time: 75.62 \t Loss:26.34050\n",
      "Iteration: 358/277000 \t Elapsed time: 75.76 \t Loss:26.31288\n",
      "Iteration: 359/277000 \t Elapsed time: 75.91 \t Loss:26.28530\n",
      "Iteration: 360/277000 \t Elapsed time: 76.08 \t Loss:26.25780\n",
      "Iteration: 361/277000 \t Elapsed time: 76.26 \t Loss:26.23034\n",
      "Iteration: 362/277000 \t Elapsed time: 77.36 \t Loss:26.20291\n",
      "Iteration: 363/277000 \t Elapsed time: 77.53 \t Loss:26.17554\n",
      "Iteration: 364/277000 \t Elapsed time: 77.67 \t Loss:26.14822\n",
      "Iteration: 365/277000 \t Elapsed time: 77.82 \t Loss:26.12090\n",
      "Iteration: 366/277000 \t Elapsed time: 77.99 \t Loss:26.09367\n",
      "Iteration: 367/277000 \t Elapsed time: 78.20 \t Loss:26.06642\n",
      "Iteration: 368/277000 \t Elapsed time: 78.35 \t Loss:26.03927\n",
      "Iteration: 369/277000 \t Elapsed time: 78.51 \t Loss:26.01209\n",
      "Iteration: 370/277000 \t Elapsed time: 78.67 \t Loss:25.98501\n",
      "Iteration: 371/277000 \t Elapsed time: 78.83 \t Loss:25.95787\n",
      "Iteration: 372/277000 \t Elapsed time: 78.98 \t Loss:25.93082\n",
      "Iteration: 373/277000 \t Elapsed time: 79.13 \t Loss:25.90386\n",
      "Iteration: 374/277000 \t Elapsed time: 79.32 \t Loss:25.87693\n",
      "Iteration: 375/277000 \t Elapsed time: 79.47 \t Loss:25.85004\n",
      "Iteration: 376/277000 \t Elapsed time: 79.69 \t Loss:25.82311\n",
      "Iteration: 377/277000 \t Elapsed time: 79.87 \t Loss:25.79629\n",
      "Iteration: 378/277000 \t Elapsed time: 80.07 \t Loss:25.76950\n",
      "Iteration: 379/277000 \t Elapsed time: 80.24 \t Loss:25.74272\n",
      "Iteration: 380/277000 \t Elapsed time: 80.41 \t Loss:25.71605\n",
      "Iteration: 381/277000 \t Elapsed time: 80.55 \t Loss:25.68941\n",
      "Iteration: 382/277000 \t Elapsed time: 80.76 \t Loss:25.66285\n",
      "Iteration: 383/277000 \t Elapsed time: 80.93 \t Loss:25.63633\n",
      "Iteration: 384/277000 \t Elapsed time: 81.21 \t Loss:25.60980\n",
      "Iteration: 385/277000 \t Elapsed time: 81.35 \t Loss:25.58320\n",
      "Iteration: 386/277000 \t Elapsed time: 81.50 \t Loss:25.55678\n",
      "Iteration: 387/277000 \t Elapsed time: 81.64 \t Loss:25.53011\n",
      "Iteration: 388/277000 \t Elapsed time: 81.78 \t Loss:25.50354\n",
      "Iteration: 389/277000 \t Elapsed time: 82.31 \t Loss:25.47710\n",
      "Iteration: 390/277000 \t Elapsed time: 82.57 \t Loss:25.45082\n",
      "Iteration: 391/277000 \t Elapsed time: 82.71 \t Loss:25.42450\n",
      "Iteration: 392/277000 \t Elapsed time: 82.93 \t Loss:25.39808\n",
      "Iteration: 393/277000 \t Elapsed time: 83.07 \t Loss:25.37183\n",
      "Iteration: 394/277000 \t Elapsed time: 83.30 \t Loss:25.34564\n",
      "Iteration: 395/277000 \t Elapsed time: 83.45 \t Loss:25.31940\n",
      "Iteration: 396/277000 \t Elapsed time: 83.59 \t Loss:25.29323\n",
      "Iteration: 397/277000 \t Elapsed time: 83.75 \t Loss:25.26716\n",
      "Iteration: 398/277000 \t Elapsed time: 83.96 \t Loss:25.24109\n",
      "Iteration: 399/277000 \t Elapsed time: 84.13 \t Loss:25.21496\n",
      "Iteration: 400/277000 \t Elapsed time: 84.31 \t Loss:25.18903\n",
      "Iteration: 401/277000 \t Elapsed time: 84.47 \t Loss:25.16303\n",
      "Iteration: 402/277000 \t Elapsed time: 84.66 \t Loss:25.13702\n",
      "Iteration: 403/277000 \t Elapsed time: 84.83 \t Loss:25.11107\n",
      "Iteration: 404/277000 \t Elapsed time: 84.99 \t Loss:25.08524\n",
      "Iteration: 405/277000 \t Elapsed time: 85.15 \t Loss:25.05937\n",
      "Iteration: 406/277000 \t Elapsed time: 85.34 \t Loss:25.03354\n",
      "Iteration: 407/277000 \t Elapsed time: 85.48 \t Loss:25.00784\n",
      "Iteration: 408/277000 \t Elapsed time: 85.72 \t Loss:24.98214\n",
      "Iteration: 409/277000 \t Elapsed time: 86.01 \t Loss:24.95646\n",
      "Iteration: 410/277000 \t Elapsed time: 86.29 \t Loss:24.93088\n",
      "Iteration: 411/277000 \t Elapsed time: 86.43 \t Loss:24.90532\n",
      "Iteration: 412/277000 \t Elapsed time: 86.60 \t Loss:24.87967\n",
      "Iteration: 413/277000 \t Elapsed time: 87.02 \t Loss:24.85394\n",
      "Iteration: 414/277000 \t Elapsed time: 87.22 \t Loss:24.82841\n",
      "Iteration: 415/277000 \t Elapsed time: 87.37 \t Loss:24.80287\n",
      "Iteration: 416/277000 \t Elapsed time: 87.58 \t Loss:24.77735\n",
      "Iteration: 417/277000 \t Elapsed time: 87.75 \t Loss:24.75198\n",
      "Iteration: 418/277000 \t Elapsed time: 87.91 \t Loss:24.72654\n",
      "Iteration: 419/277000 \t Elapsed time: 88.11 \t Loss:24.70111\n",
      "Iteration: 420/277000 \t Elapsed time: 88.27 \t Loss:24.67566\n",
      "Iteration: 421/277000 \t Elapsed time: 88.49 \t Loss:24.65034\n",
      "Iteration: 422/277000 \t Elapsed time: 88.65 \t Loss:24.62506\n",
      "Iteration: 423/277000 \t Elapsed time: 88.79 \t Loss:24.59974\n",
      "Iteration: 424/277000 \t Elapsed time: 89.03 \t Loss:24.57439\n",
      "Iteration: 425/277000 \t Elapsed time: 89.18 \t Loss:24.54929\n",
      "Iteration: 426/277000 \t Elapsed time: 89.40 \t Loss:24.52412\n",
      "Iteration: 427/277000 \t Elapsed time: 89.55 \t Loss:24.49902\n",
      "Iteration: 428/277000 \t Elapsed time: 89.73 \t Loss:24.47385\n",
      "Iteration: 429/277000 \t Elapsed time: 89.91 \t Loss:24.44881\n",
      "Iteration: 430/277000 \t Elapsed time: 90.06 \t Loss:24.42383\n",
      "Iteration: 431/277000 \t Elapsed time: 90.32 \t Loss:24.39886\n",
      "Iteration: 432/277000 \t Elapsed time: 90.48 \t Loss:24.37395\n",
      "Iteration: 433/277000 \t Elapsed time: 90.63 \t Loss:24.34894\n",
      "Iteration: 434/277000 \t Elapsed time: 90.90 \t Loss:24.32406\n",
      "Iteration: 435/277000 \t Elapsed time: 91.11 \t Loss:24.29916\n",
      "Iteration: 436/277000 \t Elapsed time: 91.73 \t Loss:24.27431\n",
      "Iteration: 437/277000 \t Elapsed time: 91.88 \t Loss:24.24955\n",
      "Iteration: 438/277000 \t Elapsed time: 92.05 \t Loss:24.22480\n",
      "Iteration: 439/277000 \t Elapsed time: 92.24 \t Loss:24.20005\n",
      "Iteration: 440/277000 \t Elapsed time: 92.50 \t Loss:24.17560\n",
      "Iteration: 441/277000 \t Elapsed time: 92.65 \t Loss:24.15094\n",
      "Iteration: 442/277000 \t Elapsed time: 92.82 \t Loss:24.12645\n",
      "Iteration: 443/277000 \t Elapsed time: 92.96 \t Loss:24.10178\n",
      "Iteration: 444/277000 \t Elapsed time: 93.21 \t Loss:24.07701\n",
      "Iteration: 445/277000 \t Elapsed time: 93.37 \t Loss:24.05239\n",
      "Iteration: 446/277000 \t Elapsed time: 93.57 \t Loss:24.02797\n",
      "Iteration: 447/277000 \t Elapsed time: 94.03 \t Loss:24.00367\n",
      "Iteration: 448/277000 \t Elapsed time: 94.19 \t Loss:23.97901\n",
      "Iteration: 449/277000 \t Elapsed time: 94.34 \t Loss:23.95448\n",
      "Iteration: 450/277000 \t Elapsed time: 94.50 \t Loss:23.93008\n",
      "Iteration: 451/277000 \t Elapsed time: 94.70 \t Loss:23.90583\n",
      "Iteration: 452/277000 \t Elapsed time: 94.84 \t Loss:23.88144\n",
      "Iteration: 453/277000 \t Elapsed time: 95.02 \t Loss:23.85708\n",
      "Iteration: 454/277000 \t Elapsed time: 95.20 \t Loss:23.83285\n",
      "Iteration: 455/277000 \t Elapsed time: 95.43 \t Loss:23.80870\n",
      "Iteration: 456/277000 \t Elapsed time: 95.59 \t Loss:23.78438\n",
      "Iteration: 457/277000 \t Elapsed time: 95.77 \t Loss:23.76017\n",
      "Iteration: 458/277000 \t Elapsed time: 95.95 \t Loss:23.73606\n",
      "Iteration: 459/277000 \t Elapsed time: 96.15 \t Loss:23.71198\n",
      "Iteration: 460/277000 \t Elapsed time: 96.29 \t Loss:23.68787\n",
      "Iteration: 461/277000 \t Elapsed time: 96.90 \t Loss:23.66379\n",
      "Iteration: 462/277000 \t Elapsed time: 97.06 \t Loss:23.63978\n",
      "Iteration: 463/277000 \t Elapsed time: 97.20 \t Loss:23.61579\n",
      "Iteration: 464/277000 \t Elapsed time: 97.36 \t Loss:23.59179\n",
      "Iteration: 465/277000 \t Elapsed time: 97.51 \t Loss:23.56787\n",
      "Iteration: 466/277000 \t Elapsed time: 97.65 \t Loss:23.54392\n",
      "Iteration: 467/277000 \t Elapsed time: 97.84 \t Loss:23.52007\n",
      "Iteration: 468/277000 \t Elapsed time: 98.02 \t Loss:23.49620\n",
      "Iteration: 469/277000 \t Elapsed time: 98.18 \t Loss:23.47237\n",
      "Iteration: 470/277000 \t Elapsed time: 98.33 \t Loss:23.44858\n",
      "Iteration: 471/277000 \t Elapsed time: 98.49 \t Loss:23.42487\n",
      "Iteration: 472/277000 \t Elapsed time: 98.65 \t Loss:23.40113\n",
      "Iteration: 473/277000 \t Elapsed time: 98.83 \t Loss:23.37739\n",
      "Iteration: 474/277000 \t Elapsed time: 98.97 \t Loss:23.35373\n",
      "Iteration: 475/277000 \t Elapsed time: 99.11 \t Loss:23.33019\n",
      "Iteration: 476/277000 \t Elapsed time: 99.26 \t Loss:23.30653\n",
      "Iteration: 477/277000 \t Elapsed time: 99.40 \t Loss:23.28294\n",
      "Iteration: 478/277000 \t Elapsed time: 99.58 \t Loss:23.25945\n",
      "Iteration: 479/277000 \t Elapsed time: 99.74 \t Loss:23.23600\n",
      "Iteration: 480/277000 \t Elapsed time: 99.90 \t Loss:23.21263\n",
      "Iteration: 481/277000 \t Elapsed time: 100.06 \t Loss:23.18915\n",
      "Iteration: 482/277000 \t Elapsed time: 100.21 \t Loss:23.16555\n",
      "Iteration: 483/277000 \t Elapsed time: 100.36 \t Loss:23.14196\n",
      "Iteration: 484/277000 \t Elapsed time: 100.53 \t Loss:23.11854\n",
      "Iteration: 485/277000 \t Elapsed time: 100.70 \t Loss:23.09534\n",
      "Iteration: 486/277000 \t Elapsed time: 100.86 \t Loss:23.07212\n",
      "Iteration: 487/277000 \t Elapsed time: 101.02 \t Loss:23.04877\n",
      "Iteration: 488/277000 \t Elapsed time: 101.19 \t Loss:23.02531\n",
      "Iteration: 489/277000 \t Elapsed time: 101.36 \t Loss:23.00205\n",
      "Iteration: 490/277000 \t Elapsed time: 101.51 \t Loss:22.97894\n",
      "Iteration: 491/277000 \t Elapsed time: 101.68 \t Loss:22.95569\n",
      "Iteration: 492/277000 \t Elapsed time: 101.83 \t Loss:22.93251\n",
      "Iteration: 493/277000 \t Elapsed time: 101.99 \t Loss:22.90937\n",
      "Iteration: 494/277000 \t Elapsed time: 102.14 \t Loss:22.88634\n",
      "Iteration: 495/277000 \t Elapsed time: 102.28 \t Loss:22.86331\n",
      "Iteration: 496/277000 \t Elapsed time: 102.43 \t Loss:22.84017\n",
      "Iteration: 497/277000 \t Elapsed time: 102.63 \t Loss:22.81703\n",
      "Iteration: 498/277000 \t Elapsed time: 102.78 \t Loss:22.79401\n",
      "Iteration: 499/277000 \t Elapsed time: 102.93 \t Loss:22.77113\n",
      "Iteration: 500/277000 \t Elapsed time: 103.07 \t Loss:22.74820\n",
      "Iteration: 501/277000 \t Elapsed time: 103.57 \t Loss:22.72530\n",
      "Iteration: 502/277000 \t Elapsed time: 103.73 \t Loss:22.70233\n",
      "Iteration: 503/277000 \t Elapsed time: 103.89 \t Loss:22.67947\n",
      "Iteration: 504/277000 \t Elapsed time: 104.11 \t Loss:22.65670\n",
      "Iteration: 505/277000 \t Elapsed time: 104.25 \t Loss:22.63391\n",
      "Iteration: 506/277000 \t Elapsed time: 104.53 \t Loss:22.61116\n",
      "Iteration: 507/277000 \t Elapsed time: 104.68 \t Loss:22.58834\n",
      "Iteration: 508/277000 \t Elapsed time: 104.85 \t Loss:22.56566\n",
      "Iteration: 509/277000 \t Elapsed time: 105.13 \t Loss:22.54298\n",
      "Iteration: 510/277000 \t Elapsed time: 105.28 \t Loss:22.52032\n",
      "Iteration: 511/277000 \t Elapsed time: 105.49 \t Loss:22.49763\n",
      "Iteration: 512/277000 \t Elapsed time: 105.64 \t Loss:22.47505\n",
      "Iteration: 513/277000 \t Elapsed time: 105.98 \t Loss:22.45241\n",
      "Iteration: 514/277000 \t Elapsed time: 106.17 \t Loss:22.42977\n",
      "Iteration: 515/277000 \t Elapsed time: 106.34 \t Loss:22.40722\n",
      "Iteration: 516/277000 \t Elapsed time: 106.53 \t Loss:22.38476\n",
      "Iteration: 517/277000 \t Elapsed time: 106.76 \t Loss:22.36234\n",
      "Iteration: 518/277000 \t Elapsed time: 106.93 \t Loss:22.33986\n",
      "Iteration: 519/277000 \t Elapsed time: 109.25 \t Loss:22.31740\n",
      "Iteration: 520/277000 \t Elapsed time: 109.39 \t Loss:22.29500\n",
      "Iteration: 521/277000 \t Elapsed time: 109.56 \t Loss:22.27267\n",
      "Iteration: 522/277000 \t Elapsed time: 109.73 \t Loss:22.25037\n",
      "Iteration: 523/277000 \t Elapsed time: 109.88 \t Loss:22.22812\n",
      "Iteration: 524/277000 \t Elapsed time: 110.04 \t Loss:22.20591\n",
      "Iteration: 525/277000 \t Elapsed time: 110.19 \t Loss:22.18371\n",
      "Iteration: 526/277000 \t Elapsed time: 110.46 \t Loss:22.16142\n",
      "Iteration: 527/277000 \t Elapsed time: 110.62 \t Loss:22.13913\n",
      "Iteration: 528/277000 \t Elapsed time: 110.81 \t Loss:22.11685\n",
      "Iteration: 529/277000 \t Elapsed time: 111.60 \t Loss:22.09470\n",
      "Iteration: 530/277000 \t Elapsed time: 111.73 \t Loss:22.07253\n",
      "Iteration: 531/277000 \t Elapsed time: 111.92 \t Loss:22.05034\n",
      "Iteration: 532/277000 \t Elapsed time: 112.10 \t Loss:22.02820\n",
      "Iteration: 533/277000 \t Elapsed time: 112.26 \t Loss:22.00607\n",
      "Iteration: 534/277000 \t Elapsed time: 112.42 \t Loss:21.98405\n",
      "Iteration: 535/277000 \t Elapsed time: 112.56 \t Loss:21.96209\n",
      "Iteration: 536/277000 \t Elapsed time: 112.73 \t Loss:21.94008\n",
      "Iteration: 537/277000 \t Elapsed time: 112.89 \t Loss:21.91811\n",
      "Iteration: 538/277000 \t Elapsed time: 113.06 \t Loss:21.89609\n",
      "Iteration: 539/277000 \t Elapsed time: 113.29 \t Loss:21.87423\n",
      "Iteration: 540/277000 \t Elapsed time: 113.47 \t Loss:21.85231\n",
      "Iteration: 541/277000 \t Elapsed time: 113.61 \t Loss:21.83038\n",
      "Iteration: 542/277000 \t Elapsed time: 113.77 \t Loss:21.80855\n",
      "Iteration: 543/277000 \t Elapsed time: 113.91 \t Loss:21.78674\n",
      "Iteration: 544/277000 \t Elapsed time: 114.14 \t Loss:21.76496\n",
      "Iteration: 545/277000 \t Elapsed time: 114.40 \t Loss:21.74320\n",
      "Iteration: 546/277000 \t Elapsed time: 114.57 \t Loss:21.72139\n",
      "Iteration: 547/277000 \t Elapsed time: 114.76 \t Loss:21.69965\n",
      "Iteration: 548/277000 \t Elapsed time: 114.98 \t Loss:21.67799\n",
      "Iteration: 549/277000 \t Elapsed time: 115.22 \t Loss:21.65633\n",
      "Iteration: 550/277000 \t Elapsed time: 115.48 \t Loss:21.63469\n",
      "Iteration: 551/277000 \t Elapsed time: 115.66 \t Loss:21.61305\n",
      "Iteration: 552/277000 \t Elapsed time: 115.92 \t Loss:21.59148\n",
      "Iteration: 553/277000 \t Elapsed time: 116.10 \t Loss:21.56989\n",
      "Iteration: 554/277000 \t Elapsed time: 117.42 \t Loss:21.54841\n",
      "Iteration: 555/277000 \t Elapsed time: 117.61 \t Loss:21.52684\n",
      "Iteration: 556/277000 \t Elapsed time: 117.85 \t Loss:21.50540\n",
      "Iteration: 557/277000 \t Elapsed time: 118.09 \t Loss:21.48387\n",
      "Iteration: 558/277000 \t Elapsed time: 118.31 \t Loss:21.46241\n",
      "Iteration: 559/277000 \t Elapsed time: 118.58 \t Loss:21.44099\n",
      "Iteration: 560/277000 \t Elapsed time: 118.76 \t Loss:21.41962\n",
      "Iteration: 561/277000 \t Elapsed time: 118.98 \t Loss:21.39823\n",
      "Iteration: 562/277000 \t Elapsed time: 119.19 \t Loss:21.37689\n",
      "Iteration: 563/277000 \t Elapsed time: 119.37 \t Loss:21.35553\n",
      "Iteration: 564/277000 \t Elapsed time: 119.63 \t Loss:21.33426\n",
      "Iteration: 565/277000 \t Elapsed time: 119.80 \t Loss:21.31303\n",
      "Iteration: 566/277000 \t Elapsed time: 119.95 \t Loss:21.29181\n",
      "Iteration: 567/277000 \t Elapsed time: 120.11 \t Loss:21.27068\n",
      "Iteration: 568/277000 \t Elapsed time: 120.28 \t Loss:21.24955\n",
      "Iteration: 569/277000 \t Elapsed time: 120.47 \t Loss:21.22840\n",
      "Iteration: 570/277000 \t Elapsed time: 120.71 \t Loss:21.20724\n",
      "Iteration: 571/277000 \t Elapsed time: 120.89 \t Loss:21.18594\n",
      "Iteration: 572/277000 \t Elapsed time: 121.17 \t Loss:21.16479\n",
      "Iteration: 573/277000 \t Elapsed time: 121.34 \t Loss:21.14365\n",
      "Iteration: 574/277000 \t Elapsed time: 122.52 \t Loss:21.12262\n",
      "Iteration: 575/277000 \t Elapsed time: 122.75 \t Loss:21.10157\n",
      "Iteration: 576/277000 \t Elapsed time: 122.97 \t Loss:21.08051\n",
      "Iteration: 577/277000 \t Elapsed time: 123.14 \t Loss:21.05946\n",
      "Iteration: 578/277000 \t Elapsed time: 123.30 \t Loss:21.03848\n",
      "Iteration: 579/277000 \t Elapsed time: 123.44 \t Loss:21.01759\n",
      "Iteration: 580/277000 \t Elapsed time: 123.62 \t Loss:20.99664\n",
      "Iteration: 581/277000 \t Elapsed time: 123.78 \t Loss:20.97566\n",
      "Iteration: 582/277000 \t Elapsed time: 123.94 \t Loss:20.95468\n",
      "Iteration: 583/277000 \t Elapsed time: 124.16 \t Loss:20.93390\n",
      "Iteration: 584/277000 \t Elapsed time: 124.33 \t Loss:20.91308\n",
      "Iteration: 585/277000 \t Elapsed time: 124.53 \t Loss:20.89233\n",
      "Iteration: 586/277000 \t Elapsed time: 124.75 \t Loss:20.87154\n",
      "Iteration: 587/277000 \t Elapsed time: 124.94 \t Loss:20.85069\n",
      "Iteration: 588/277000 \t Elapsed time: 125.11 \t Loss:20.82986\n",
      "Iteration: 589/277000 \t Elapsed time: 125.26 \t Loss:20.80922\n",
      "Iteration: 590/277000 \t Elapsed time: 125.43 \t Loss:20.78853\n",
      "Iteration: 591/277000 \t Elapsed time: 125.61 \t Loss:20.76787\n",
      "Iteration: 592/277000 \t Elapsed time: 125.75 \t Loss:20.74711\n",
      "Iteration: 593/277000 \t Elapsed time: 126.02 \t Loss:20.72647\n",
      "Iteration: 594/277000 \t Elapsed time: 126.16 \t Loss:20.70587\n",
      "Iteration: 595/277000 \t Elapsed time: 128.19 \t Loss:20.68537\n",
      "Iteration: 596/277000 \t Elapsed time: 128.40 \t Loss:20.66480\n",
      "Iteration: 597/277000 \t Elapsed time: 129.39 \t Loss:20.64425\n",
      "Iteration: 598/277000 \t Elapsed time: 129.60 \t Loss:20.62371\n",
      "Iteration: 599/277000 \t Elapsed time: 129.76 \t Loss:20.60315\n",
      "Iteration: 600/277000 \t Elapsed time: 129.92 \t Loss:20.58266\n",
      "Iteration: 601/277000 \t Elapsed time: 130.07 \t Loss:20.56219\n",
      "Iteration: 602/277000 \t Elapsed time: 130.21 \t Loss:20.54169\n",
      "Iteration: 603/277000 \t Elapsed time: 130.35 \t Loss:20.52134\n",
      "Iteration: 604/277000 \t Elapsed time: 130.53 \t Loss:20.50095\n",
      "Iteration: 605/277000 \t Elapsed time: 130.82 \t Loss:20.48050\n",
      "Iteration: 606/277000 \t Elapsed time: 130.99 \t Loss:20.46014\n",
      "Iteration: 607/277000 \t Elapsed time: 131.39 \t Loss:20.43986\n",
      "Iteration: 608/277000 \t Elapsed time: 131.54 \t Loss:20.41955\n",
      "Iteration: 609/277000 \t Elapsed time: 131.69 \t Loss:20.39924\n",
      "Iteration: 610/277000 \t Elapsed time: 131.83 \t Loss:20.37893\n",
      "Iteration: 611/277000 \t Elapsed time: 131.98 \t Loss:20.35876\n",
      "Iteration: 612/277000 \t Elapsed time: 132.13 \t Loss:20.33857\n",
      "Iteration: 613/277000 \t Elapsed time: 132.33 \t Loss:20.31837\n",
      "Iteration: 614/277000 \t Elapsed time: 132.49 \t Loss:20.29821\n",
      "Iteration: 615/277000 \t Elapsed time: 132.71 \t Loss:20.27819\n",
      "Iteration: 616/277000 \t Elapsed time: 132.88 \t Loss:20.25817\n",
      "Iteration: 617/277000 \t Elapsed time: 133.04 \t Loss:20.23817\n",
      "Iteration: 618/277000 \t Elapsed time: 133.20 \t Loss:20.21818\n",
      "Iteration: 619/277000 \t Elapsed time: 133.35 \t Loss:20.19821\n",
      "Iteration: 620/277000 \t Elapsed time: 133.51 \t Loss:20.17801\n",
      "Iteration: 621/277000 \t Elapsed time: 133.67 \t Loss:20.15775\n",
      "Iteration: 622/277000 \t Elapsed time: 133.81 \t Loss:20.13745\n",
      "Iteration: 623/277000 \t Elapsed time: 133.98 \t Loss:20.11736\n",
      "Iteration: 624/277000 \t Elapsed time: 134.16 \t Loss:20.09748\n",
      "Iteration: 625/277000 \t Elapsed time: 134.31 \t Loss:20.07759\n",
      "Iteration: 626/277000 \t Elapsed time: 134.54 \t Loss:20.05754\n",
      "Iteration: 627/277000 \t Elapsed time: 134.83 \t Loss:20.03748\n",
      "Iteration: 628/277000 \t Elapsed time: 134.99 \t Loss:20.01762\n",
      "Iteration: 629/277000 \t Elapsed time: 135.15 \t Loss:19.99783\n",
      "Iteration: 630/277000 \t Elapsed time: 135.41 \t Loss:19.97795\n",
      "Iteration: 631/277000 \t Elapsed time: 135.57 \t Loss:19.95797\n",
      "Iteration: 632/277000 \t Elapsed time: 135.79 \t Loss:19.93818\n",
      "Iteration: 633/277000 \t Elapsed time: 135.93 \t Loss:19.91847\n",
      "Iteration: 634/277000 \t Elapsed time: 136.08 \t Loss:19.89865\n",
      "Iteration: 635/277000 \t Elapsed time: 136.84 \t Loss:19.87884\n",
      "Iteration: 636/277000 \t Elapsed time: 137.03 \t Loss:19.85914\n",
      "Iteration: 637/277000 \t Elapsed time: 137.26 \t Loss:19.83947\n",
      "Iteration: 638/277000 \t Elapsed time: 137.42 \t Loss:19.81974\n",
      "Iteration: 639/277000 \t Elapsed time: 137.66 \t Loss:19.80006\n",
      "Iteration: 640/277000 \t Elapsed time: 137.83 \t Loss:19.78041\n",
      "Iteration: 641/277000 \t Elapsed time: 138.00 \t Loss:19.76077\n",
      "Iteration: 642/277000 \t Elapsed time: 138.13 \t Loss:19.74113\n",
      "Iteration: 643/277000 \t Elapsed time: 138.28 \t Loss:19.72155\n",
      "Iteration: 644/277000 \t Elapsed time: 138.42 \t Loss:19.70202\n",
      "Iteration: 645/277000 \t Elapsed time: 138.81 \t Loss:19.68244\n",
      "Iteration: 646/277000 \t Elapsed time: 138.97 \t Loss:19.66290\n",
      "Iteration: 647/277000 \t Elapsed time: 139.33 \t Loss:19.64341\n",
      "Iteration: 648/277000 \t Elapsed time: 139.53 \t Loss:19.62392\n",
      "Iteration: 649/277000 \t Elapsed time: 139.76 \t Loss:19.60445\n",
      "Iteration: 650/277000 \t Elapsed time: 139.91 \t Loss:19.58496\n",
      "Iteration: 651/277000 \t Elapsed time: 140.09 \t Loss:19.56552\n",
      "Iteration: 652/277000 \t Elapsed time: 140.27 \t Loss:19.54615\n",
      "Iteration: 653/277000 \t Elapsed time: 140.47 \t Loss:19.52676\n",
      "Iteration: 654/277000 \t Elapsed time: 140.78 \t Loss:19.50737\n",
      "Iteration: 655/277000 \t Elapsed time: 142.27 \t Loss:19.48798\n",
      "Iteration: 656/277000 \t Elapsed time: 142.45 \t Loss:19.46866\n",
      "Iteration: 657/277000 \t Elapsed time: 142.67 \t Loss:19.44932\n",
      "Iteration: 658/277000 \t Elapsed time: 142.84 \t Loss:19.43004\n",
      "Iteration: 659/277000 \t Elapsed time: 143.02 \t Loss:19.41081\n",
      "Iteration: 660/277000 \t Elapsed time: 143.19 \t Loss:19.39160\n",
      "Iteration: 661/277000 \t Elapsed time: 143.47 \t Loss:19.37246\n",
      "Iteration: 662/277000 \t Elapsed time: 143.61 \t Loss:19.35329\n",
      "Iteration: 663/277000 \t Elapsed time: 143.75 \t Loss:19.33402\n",
      "Iteration: 664/277000 \t Elapsed time: 143.90 \t Loss:19.31481\n",
      "Iteration: 665/277000 \t Elapsed time: 144.08 \t Loss:19.29560\n",
      "Iteration: 666/277000 \t Elapsed time: 144.32 \t Loss:19.27642\n",
      "Iteration: 667/277000 \t Elapsed time: 144.49 \t Loss:19.25720\n",
      "Iteration: 668/277000 \t Elapsed time: 144.66 \t Loss:19.23803\n",
      "Iteration: 669/277000 \t Elapsed time: 144.89 \t Loss:19.21899\n",
      "Iteration: 670/277000 \t Elapsed time: 145.04 \t Loss:19.19993\n",
      "Iteration: 671/277000 \t Elapsed time: 145.23 \t Loss:19.18089\n",
      "Iteration: 672/277000 \t Elapsed time: 145.46 \t Loss:19.16175\n",
      "Iteration: 673/277000 \t Elapsed time: 145.67 \t Loss:19.14272\n",
      "Iteration: 674/277000 \t Elapsed time: 145.89 \t Loss:19.12373\n",
      "Iteration: 675/277000 \t Elapsed time: 146.11 \t Loss:19.10476\n",
      "Iteration: 676/277000 \t Elapsed time: 146.26 \t Loss:19.08578\n",
      "Iteration: 677/277000 \t Elapsed time: 147.11 \t Loss:19.06683\n",
      "Iteration: 678/277000 \t Elapsed time: 147.30 \t Loss:19.04789\n",
      "Iteration: 679/277000 \t Elapsed time: 147.45 \t Loss:19.02896\n",
      "Iteration: 680/277000 \t Elapsed time: 147.61 \t Loss:19.01009\n",
      "Iteration: 681/277000 \t Elapsed time: 147.79 \t Loss:18.99127\n",
      "Iteration: 682/277000 \t Elapsed time: 147.99 \t Loss:18.97239\n",
      "Iteration: 683/277000 \t Elapsed time: 148.16 \t Loss:18.95353\n",
      "Iteration: 684/277000 \t Elapsed time: 148.31 \t Loss:18.93470\n",
      "Iteration: 685/277000 \t Elapsed time: 148.56 \t Loss:18.91590\n",
      "Iteration: 686/277000 \t Elapsed time: 148.71 \t Loss:18.89715\n",
      "Iteration: 687/277000 \t Elapsed time: 148.90 \t Loss:18.87836\n",
      "Iteration: 688/277000 \t Elapsed time: 149.07 \t Loss:18.85968\n",
      "Iteration: 689/277000 \t Elapsed time: 149.22 \t Loss:18.84091\n",
      "Iteration: 690/277000 \t Elapsed time: 149.37 \t Loss:18.82221\n",
      "Iteration: 691/277000 \t Elapsed time: 149.52 \t Loss:18.80352\n",
      "Iteration: 692/277000 \t Elapsed time: 149.70 \t Loss:18.78485\n",
      "Iteration: 693/277000 \t Elapsed time: 149.87 \t Loss:18.76617\n",
      "Iteration: 694/277000 \t Elapsed time: 150.04 \t Loss:18.74753\n",
      "Iteration: 695/277000 \t Elapsed time: 150.20 \t Loss:18.72893\n",
      "Iteration: 696/277000 \t Elapsed time: 150.37 \t Loss:18.71031\n",
      "Iteration: 697/277000 \t Elapsed time: 150.51 \t Loss:18.69169\n",
      "Iteration: 698/277000 \t Elapsed time: 150.66 \t Loss:18.67318\n",
      "Iteration: 699/277000 \t Elapsed time: 151.04 \t Loss:18.65460\n",
      "Iteration: 700/277000 \t Elapsed time: 151.20 \t Loss:18.63611\n",
      "Iteration: 701/277000 \t Elapsed time: 151.35 \t Loss:18.61773\n",
      "Iteration: 702/277000 \t Elapsed time: 151.50 \t Loss:18.59938\n",
      "Iteration: 703/277000 \t Elapsed time: 152.25 \t Loss:18.58128\n",
      "Iteration: 704/277000 \t Elapsed time: 152.44 \t Loss:18.56348\n",
      "Iteration: 705/277000 \t Elapsed time: 152.59 \t Loss:18.54524\n",
      "Iteration: 706/277000 \t Elapsed time: 152.78 \t Loss:18.52642\n",
      "Iteration: 707/277000 \t Elapsed time: 152.93 \t Loss:18.50741\n",
      "Iteration: 708/277000 \t Elapsed time: 153.10 \t Loss:18.48941\n",
      "Iteration: 709/277000 \t Elapsed time: 153.27 \t Loss:18.47105\n",
      "Iteration: 710/277000 \t Elapsed time: 153.41 \t Loss:18.45177\n",
      "Iteration: 711/277000 \t Elapsed time: 153.57 \t Loss:18.43363\n",
      "Iteration: 712/277000 \t Elapsed time: 153.75 \t Loss:18.41579\n",
      "Iteration: 713/277000 \t Elapsed time: 153.89 \t Loss:18.39693\n",
      "Iteration: 714/277000 \t Elapsed time: 154.08 \t Loss:18.37857\n",
      "Iteration: 715/277000 \t Elapsed time: 154.23 \t Loss:18.36060\n",
      "Iteration: 716/277000 \t Elapsed time: 154.41 \t Loss:18.34206\n",
      "Iteration: 717/277000 \t Elapsed time: 154.54 \t Loss:18.32395\n",
      "Iteration: 718/277000 \t Elapsed time: 154.69 \t Loss:18.30581\n",
      "Iteration: 719/277000 \t Elapsed time: 154.83 \t Loss:18.28752\n",
      "Iteration: 720/277000 \t Elapsed time: 154.97 \t Loss:18.26960\n",
      "Iteration: 721/277000 \t Elapsed time: 155.12 \t Loss:18.25123\n",
      "Iteration: 722/277000 \t Elapsed time: 155.26 \t Loss:18.23302\n",
      "Iteration: 723/277000 \t Elapsed time: 155.44 \t Loss:18.21494\n",
      "Iteration: 724/277000 \t Elapsed time: 155.60 \t Loss:18.19643\n",
      "Iteration: 725/277000 \t Elapsed time: 155.79 \t Loss:18.17833\n",
      "Iteration: 726/277000 \t Elapsed time: 155.97 \t Loss:18.16026\n",
      "Iteration: 727/277000 \t Elapsed time: 156.12 \t Loss:18.14213\n",
      "Iteration: 728/277000 \t Elapsed time: 156.65 \t Loss:18.12427\n",
      "Iteration: 729/277000 \t Elapsed time: 156.84 \t Loss:18.10610\n",
      "Iteration: 730/277000 \t Elapsed time: 157.05 \t Loss:18.08803\n",
      "Iteration: 731/277000 \t Elapsed time: 157.20 \t Loss:18.06996\n",
      "Iteration: 732/277000 \t Elapsed time: 157.34 \t Loss:18.05192\n",
      "Iteration: 733/277000 \t Elapsed time: 157.50 \t Loss:18.03402\n",
      "Iteration: 734/277000 \t Elapsed time: 157.66 \t Loss:18.01599\n",
      "Iteration: 735/277000 \t Elapsed time: 157.87 \t Loss:17.99804\n",
      "Iteration: 736/277000 \t Elapsed time: 158.00 \t Loss:17.98008\n",
      "Iteration: 737/277000 \t Elapsed time: 158.15 \t Loss:17.96210\n",
      "Iteration: 738/277000 \t Elapsed time: 158.36 \t Loss:17.94428\n",
      "Iteration: 739/277000 \t Elapsed time: 158.52 \t Loss:17.92641\n",
      "Iteration: 740/277000 \t Elapsed time: 158.66 \t Loss:17.90858\n",
      "Iteration: 741/277000 \t Elapsed time: 158.89 \t Loss:17.89068\n",
      "Iteration: 742/277000 \t Elapsed time: 159.04 \t Loss:17.87283\n",
      "Iteration: 743/277000 \t Elapsed time: 159.24 \t Loss:17.85503\n",
      "Iteration: 744/277000 \t Elapsed time: 159.39 \t Loss:17.83720\n",
      "Iteration: 745/277000 \t Elapsed time: 159.56 \t Loss:17.81939\n",
      "Iteration: 746/277000 \t Elapsed time: 159.72 \t Loss:17.80161\n",
      "Iteration: 747/277000 \t Elapsed time: 159.94 \t Loss:17.78386\n",
      "Iteration: 748/277000 \t Elapsed time: 160.17 \t Loss:17.76610\n",
      "Iteration: 749/277000 \t Elapsed time: 160.30 \t Loss:17.74833\n",
      "Iteration: 750/277000 \t Elapsed time: 160.57 \t Loss:17.73074\n",
      "Iteration: 751/277000 \t Elapsed time: 160.72 \t Loss:17.71306\n",
      "Iteration: 752/277000 \t Elapsed time: 160.93 \t Loss:17.69522\n",
      "Iteration: 753/277000 \t Elapsed time: 161.12 \t Loss:17.67758\n",
      "Iteration: 754/277000 \t Elapsed time: 161.34 \t Loss:17.65995\n",
      "Iteration: 755/277000 \t Elapsed time: 161.51 \t Loss:17.64232\n",
      "Iteration: 756/277000 \t Elapsed time: 161.67 \t Loss:17.62461\n",
      "Iteration: 757/277000 \t Elapsed time: 162.38 \t Loss:17.60699\n",
      "Iteration: 758/277000 \t Elapsed time: 162.58 \t Loss:17.58939\n",
      "Iteration: 759/277000 \t Elapsed time: 162.81 \t Loss:17.57184\n",
      "Iteration: 760/277000 \t Elapsed time: 162.96 \t Loss:17.55424\n",
      "Iteration: 761/277000 \t Elapsed time: 163.13 \t Loss:17.53674\n",
      "Iteration: 762/277000 \t Elapsed time: 163.29 \t Loss:17.51919\n",
      "Iteration: 763/277000 \t Elapsed time: 163.46 \t Loss:17.50175\n",
      "Iteration: 764/277000 \t Elapsed time: 163.64 \t Loss:17.48431\n",
      "Iteration: 765/277000 \t Elapsed time: 163.82 \t Loss:17.46690\n",
      "Iteration: 766/277000 \t Elapsed time: 163.98 \t Loss:17.44942\n",
      "Iteration: 767/277000 \t Elapsed time: 164.19 \t Loss:17.43184\n",
      "Iteration: 768/277000 \t Elapsed time: 164.39 \t Loss:17.41430\n",
      "Iteration: 769/277000 \t Elapsed time: 164.54 \t Loss:17.39688\n",
      "Iteration: 770/277000 \t Elapsed time: 164.73 \t Loss:17.37951\n",
      "Iteration: 771/277000 \t Elapsed time: 164.91 \t Loss:17.36212\n",
      "Iteration: 772/277000 \t Elapsed time: 165.10 \t Loss:17.34469\n",
      "Iteration: 773/277000 \t Elapsed time: 165.27 \t Loss:17.32739\n",
      "Iteration: 774/277000 \t Elapsed time: 165.43 \t Loss:17.31010\n",
      "Iteration: 775/277000 \t Elapsed time: 165.57 \t Loss:17.29296\n",
      "Iteration: 776/277000 \t Elapsed time: 165.78 \t Loss:17.27569\n",
      "Iteration: 777/277000 \t Elapsed time: 165.98 \t Loss:17.25837\n",
      "Iteration: 778/277000 \t Elapsed time: 166.29 \t Loss:17.24095\n",
      "Iteration: 779/277000 \t Elapsed time: 166.48 \t Loss:17.22367\n",
      "Iteration: 780/277000 \t Elapsed time: 167.19 \t Loss:17.20621\n",
      "Iteration: 781/277000 \t Elapsed time: 167.34 \t Loss:17.18892\n",
      "Iteration: 782/277000 \t Elapsed time: 167.53 \t Loss:17.17184\n",
      "Iteration: 783/277000 \t Elapsed time: 167.83 \t Loss:17.15458\n",
      "Iteration: 784/277000 \t Elapsed time: 168.02 \t Loss:17.13726\n",
      "Iteration: 785/277000 \t Elapsed time: 168.16 \t Loss:17.11997\n",
      "Iteration: 786/277000 \t Elapsed time: 168.40 \t Loss:17.10288\n",
      "Iteration: 787/277000 \t Elapsed time: 168.62 \t Loss:17.08581\n",
      "Iteration: 788/277000 \t Elapsed time: 168.80 \t Loss:17.06855\n",
      "Iteration: 789/277000 \t Elapsed time: 169.05 \t Loss:17.05134\n",
      "Iteration: 790/277000 \t Elapsed time: 169.20 \t Loss:17.03421\n",
      "Iteration: 791/277000 \t Elapsed time: 169.42 \t Loss:17.01716\n",
      "Iteration: 792/277000 \t Elapsed time: 169.56 \t Loss:17.00004\n",
      "Iteration: 793/277000 \t Elapsed time: 169.79 \t Loss:16.98290\n",
      "Iteration: 794/277000 \t Elapsed time: 169.94 \t Loss:16.96587\n",
      "Iteration: 795/277000 \t Elapsed time: 170.12 \t Loss:16.94883\n",
      "Iteration: 796/277000 \t Elapsed time: 170.35 \t Loss:16.93180\n",
      "Iteration: 797/277000 \t Elapsed time: 170.51 \t Loss:16.91481\n",
      "Iteration: 798/277000 \t Elapsed time: 170.74 \t Loss:16.89778\n",
      "Iteration: 799/277000 \t Elapsed time: 171.01 \t Loss:16.88076\n",
      "Iteration: 800/277000 \t Elapsed time: 171.19 \t Loss:16.86394\n",
      "Iteration: 801/277000 \t Elapsed time: 171.35 \t Loss:16.84689\n",
      "Iteration: 802/277000 \t Elapsed time: 171.58 \t Loss:16.82981\n",
      "Iteration: 803/277000 \t Elapsed time: 171.74 \t Loss:16.81303\n",
      "Iteration: 804/277000 \t Elapsed time: 171.97 \t Loss:16.79607\n",
      "Iteration: 805/277000 \t Elapsed time: 172.11 \t Loss:16.77904\n",
      "Iteration: 806/277000 \t Elapsed time: 172.25 \t Loss:16.76225\n",
      "Iteration: 807/277000 \t Elapsed time: 172.44 \t Loss:16.74555\n",
      "Iteration: 808/277000 \t Elapsed time: 172.59 \t Loss:16.72866\n",
      "Iteration: 809/277000 \t Elapsed time: 172.74 \t Loss:16.71160\n",
      "Iteration: 810/277000 \t Elapsed time: 172.92 \t Loss:16.69477\n",
      "Iteration: 811/277000 \t Elapsed time: 173.13 \t Loss:16.67790\n",
      "Iteration: 812/277000 \t Elapsed time: 173.31 \t Loss:16.66105\n",
      "Iteration: 813/277000 \t Elapsed time: 173.46 \t Loss:16.64428\n",
      "Iteration: 814/277000 \t Elapsed time: 173.67 \t Loss:16.62746\n",
      "Iteration: 815/277000 \t Elapsed time: 173.81 \t Loss:16.61069\n",
      "Iteration: 816/277000 \t Elapsed time: 173.95 \t Loss:16.59388\n",
      "Iteration: 817/277000 \t Elapsed time: 174.29 \t Loss:16.57713\n",
      "Iteration: 818/277000 \t Elapsed time: 174.49 \t Loss:16.56041\n",
      "Iteration: 819/277000 \t Elapsed time: 174.68 \t Loss:16.54369\n",
      "Iteration: 820/277000 \t Elapsed time: 174.89 \t Loss:16.52699\n",
      "Iteration: 821/277000 \t Elapsed time: 175.11 \t Loss:16.51028\n",
      "Iteration: 822/277000 \t Elapsed time: 175.32 \t Loss:16.49355\n",
      "Iteration: 823/277000 \t Elapsed time: 175.47 \t Loss:16.47686\n",
      "Iteration: 824/277000 \t Elapsed time: 175.64 \t Loss:16.46022\n",
      "Iteration: 825/277000 \t Elapsed time: 175.88 \t Loss:16.44356\n",
      "Iteration: 826/277000 \t Elapsed time: 176.06 \t Loss:16.42692\n",
      "Iteration: 827/277000 \t Elapsed time: 176.21 \t Loss:16.41030\n",
      "Iteration: 828/277000 \t Elapsed time: 176.40 \t Loss:16.39370\n",
      "Iteration: 829/277000 \t Elapsed time: 176.56 \t Loss:16.37716\n",
      "Iteration: 830/277000 \t Elapsed time: 176.72 \t Loss:16.36053\n",
      "Iteration: 831/277000 \t Elapsed time: 176.91 \t Loss:16.34395\n",
      "Iteration: 832/277000 \t Elapsed time: 177.06 \t Loss:16.32744\n",
      "Iteration: 833/277000 \t Elapsed time: 178.08 \t Loss:16.31092\n",
      "Iteration: 834/277000 \t Elapsed time: 178.24 \t Loss:16.29446\n",
      "Iteration: 835/277000 \t Elapsed time: 178.40 \t Loss:16.27806\n",
      "Iteration: 836/277000 \t Elapsed time: 178.53 \t Loss:16.26174\n",
      "Iteration: 837/277000 \t Elapsed time: 178.68 \t Loss:16.24554\n",
      "Iteration: 838/277000 \t Elapsed time: 178.84 \t Loss:16.22913\n",
      "Iteration: 839/277000 \t Elapsed time: 179.06 \t Loss:16.21246\n",
      "Iteration: 840/277000 \t Elapsed time: 179.25 \t Loss:16.19559\n",
      "Iteration: 841/277000 \t Elapsed time: 179.46 \t Loss:16.17900\n",
      "Iteration: 842/277000 \t Elapsed time: 179.71 \t Loss:16.16274\n",
      "Iteration: 843/277000 \t Elapsed time: 179.86 \t Loss:16.14647\n",
      "Iteration: 844/277000 \t Elapsed time: 180.06 \t Loss:16.13000\n",
      "Iteration: 845/277000 \t Elapsed time: 180.21 \t Loss:16.11340\n",
      "Iteration: 846/277000 \t Elapsed time: 180.36 \t Loss:16.09704\n",
      "Iteration: 847/277000 \t Elapsed time: 180.52 \t Loss:16.08084\n",
      "Iteration: 848/277000 \t Elapsed time: 180.68 \t Loss:16.06445\n",
      "Iteration: 849/277000 \t Elapsed time: 180.85 \t Loss:16.04798\n",
      "Iteration: 850/277000 \t Elapsed time: 180.99 \t Loss:16.03158\n",
      "Iteration: 851/277000 \t Elapsed time: 181.16 \t Loss:16.01540\n",
      "Iteration: 852/277000 \t Elapsed time: 181.30 \t Loss:15.99909\n",
      "Iteration: 853/277000 \t Elapsed time: 181.45 \t Loss:15.98273\n",
      "Iteration: 854/277000 \t Elapsed time: 181.91 \t Loss:15.96639\n",
      "Iteration: 855/277000 \t Elapsed time: 182.07 \t Loss:15.95024\n",
      "Iteration: 856/277000 \t Elapsed time: 182.24 \t Loss:15.93405\n",
      "Iteration: 857/277000 \t Elapsed time: 182.40 \t Loss:15.91786\n",
      "Iteration: 858/277000 \t Elapsed time: 182.58 \t Loss:15.90157\n",
      "Iteration: 859/277000 \t Elapsed time: 182.74 \t Loss:15.88530\n",
      "Iteration: 860/277000 \t Elapsed time: 182.88 \t Loss:15.86907\n",
      "Iteration: 861/277000 \t Elapsed time: 183.02 \t Loss:15.85295\n",
      "Iteration: 862/277000 \t Elapsed time: 183.18 \t Loss:15.83680\n",
      "Iteration: 863/277000 \t Elapsed time: 183.32 \t Loss:15.82062\n",
      "Iteration: 864/277000 \t Elapsed time: 183.58 \t Loss:15.80445\n",
      "Iteration: 865/277000 \t Elapsed time: 183.72 \t Loss:15.78824\n",
      "Iteration: 866/277000 \t Elapsed time: 183.92 \t Loss:15.77209\n",
      "Iteration: 867/277000 \t Elapsed time: 184.07 \t Loss:15.75599\n",
      "Iteration: 868/277000 \t Elapsed time: 184.29 \t Loss:15.73991\n",
      "Iteration: 869/277000 \t Elapsed time: 184.43 \t Loss:15.72383\n",
      "Iteration: 870/277000 \t Elapsed time: 184.59 \t Loss:15.70769\n",
      "Iteration: 871/277000 \t Elapsed time: 184.82 \t Loss:15.69164\n",
      "Iteration: 872/277000 \t Elapsed time: 185.03 \t Loss:15.67557\n",
      "Iteration: 873/277000 \t Elapsed time: 185.17 \t Loss:15.65948\n",
      "Iteration: 874/277000 \t Elapsed time: 185.34 \t Loss:15.64347\n",
      "Iteration: 875/277000 \t Elapsed time: 185.49 \t Loss:15.62749\n",
      "Iteration: 876/277000 \t Elapsed time: 185.65 \t Loss:15.61150\n",
      "Iteration: 877/277000 \t Elapsed time: 185.82 \t Loss:15.59543\n",
      "Iteration: 878/277000 \t Elapsed time: 186.04 \t Loss:15.57957\n",
      "Iteration: 879/277000 \t Elapsed time: 186.32 \t Loss:15.56364\n",
      "Iteration: 880/277000 \t Elapsed time: 186.48 \t Loss:15.54759\n",
      "Iteration: 881/277000 \t Elapsed time: 186.63 \t Loss:15.53167\n",
      "Iteration: 882/277000 \t Elapsed time: 186.80 \t Loss:15.51581\n",
      "Iteration: 883/277000 \t Elapsed time: 186.95 \t Loss:15.49980\n",
      "Iteration: 884/277000 \t Elapsed time: 187.48 \t Loss:15.48392\n",
      "Iteration: 885/277000 \t Elapsed time: 187.66 \t Loss:15.46806\n",
      "Iteration: 886/277000 \t Elapsed time: 187.85 \t Loss:15.45212\n",
      "Iteration: 887/277000 \t Elapsed time: 187.99 \t Loss:15.43632\n",
      "Iteration: 888/277000 \t Elapsed time: 188.31 \t Loss:15.42043\n",
      "Iteration: 889/277000 \t Elapsed time: 188.50 \t Loss:15.40450\n",
      "Iteration: 890/277000 \t Elapsed time: 188.65 \t Loss:15.38860\n",
      "Iteration: 891/277000 \t Elapsed time: 188.80 \t Loss:15.37279\n",
      "Iteration: 892/277000 \t Elapsed time: 189.10 \t Loss:15.35704\n",
      "Iteration: 893/277000 \t Elapsed time: 189.34 \t Loss:15.34122\n",
      "Iteration: 894/277000 \t Elapsed time: 189.49 \t Loss:15.32543\n",
      "Iteration: 895/277000 \t Elapsed time: 189.64 \t Loss:15.30956\n",
      "Iteration: 896/277000 \t Elapsed time: 189.85 \t Loss:15.29376\n",
      "Iteration: 897/277000 \t Elapsed time: 190.07 \t Loss:15.27804\n",
      "Iteration: 898/277000 \t Elapsed time: 190.26 \t Loss:15.26233\n",
      "Iteration: 899/277000 \t Elapsed time: 190.43 \t Loss:15.24661\n",
      "Iteration: 900/277000 \t Elapsed time: 190.57 \t Loss:15.23082\n",
      "Iteration: 901/277000 \t Elapsed time: 190.74 \t Loss:15.21505\n",
      "Iteration: 902/277000 \t Elapsed time: 190.92 \t Loss:15.19936\n",
      "Iteration: 903/277000 \t Elapsed time: 191.09 \t Loss:15.18372\n",
      "Iteration: 904/277000 \t Elapsed time: 191.24 \t Loss:15.16803\n",
      "Iteration: 905/277000 \t Elapsed time: 191.43 \t Loss:15.15233\n",
      "Iteration: 906/277000 \t Elapsed time: 191.61 \t Loss:15.13663\n",
      "Iteration: 907/277000 \t Elapsed time: 191.89 \t Loss:15.12098\n",
      "Iteration: 908/277000 \t Elapsed time: 192.15 \t Loss:15.10542\n",
      "Iteration: 909/277000 \t Elapsed time: 192.30 \t Loss:15.08978\n",
      "Iteration: 910/277000 \t Elapsed time: 192.46 \t Loss:15.07420\n",
      "Iteration: 911/277000 \t Elapsed time: 192.66 \t Loss:15.05859\n",
      "Iteration: 912/277000 \t Elapsed time: 193.18 \t Loss:15.04303\n",
      "Iteration: 913/277000 \t Elapsed time: 193.40 \t Loss:15.02747\n",
      "Iteration: 914/277000 \t Elapsed time: 193.56 \t Loss:15.01194\n",
      "Iteration: 915/277000 \t Elapsed time: 193.74 \t Loss:14.99630\n",
      "Iteration: 916/277000 \t Elapsed time: 193.89 \t Loss:14.98074\n",
      "Iteration: 917/277000 \t Elapsed time: 194.05 \t Loss:14.96515\n",
      "Iteration: 918/277000 \t Elapsed time: 194.27 \t Loss:14.94962\n",
      "Iteration: 919/277000 \t Elapsed time: 194.43 \t Loss:14.93408\n",
      "Iteration: 920/277000 \t Elapsed time: 194.59 \t Loss:14.91858\n",
      "Iteration: 921/277000 \t Elapsed time: 194.75 \t Loss:14.90315\n",
      "Iteration: 922/277000 \t Elapsed time: 194.90 \t Loss:14.88773\n",
      "Iteration: 923/277000 \t Elapsed time: 195.15 \t Loss:14.87242\n",
      "Iteration: 924/277000 \t Elapsed time: 195.36 \t Loss:14.85694\n",
      "Iteration: 925/277000 \t Elapsed time: 195.55 \t Loss:14.84139\n",
      "Iteration: 926/277000 \t Elapsed time: 195.70 \t Loss:14.82582\n",
      "Iteration: 927/277000 \t Elapsed time: 195.89 \t Loss:14.81036\n",
      "Iteration: 928/277000 \t Elapsed time: 196.08 \t Loss:14.79503\n",
      "Iteration: 929/277000 \t Elapsed time: 196.23 \t Loss:14.77972\n",
      "Iteration: 930/277000 \t Elapsed time: 196.51 \t Loss:14.76424\n",
      "Iteration: 931/277000 \t Elapsed time: 200.57 \t Loss:14.74882\n",
      "Iteration: 932/277000 \t Elapsed time: 200.95 \t Loss:14.73345\n",
      "Iteration: 933/277000 \t Elapsed time: 201.15 \t Loss:14.71803\n",
      "Iteration: 934/277000 \t Elapsed time: 201.35 \t Loss:14.70271\n",
      "Iteration: 935/277000 \t Elapsed time: 201.55 \t Loss:14.68738\n",
      "Iteration: 936/277000 \t Elapsed time: 201.74 \t Loss:14.67209\n",
      "Iteration: 937/277000 \t Elapsed time: 201.97 \t Loss:14.65672\n",
      "Iteration: 938/277000 \t Elapsed time: 202.11 \t Loss:14.64136\n",
      "Iteration: 939/277000 \t Elapsed time: 202.40 \t Loss:14.62605\n",
      "Iteration: 940/277000 \t Elapsed time: 202.59 \t Loss:14.61094\n",
      "Iteration: 941/277000 \t Elapsed time: 202.76 \t Loss:14.59580\n",
      "Iteration: 942/277000 \t Elapsed time: 202.92 \t Loss:14.58094\n",
      "Iteration: 943/277000 \t Elapsed time: 203.06 \t Loss:14.56558\n",
      "Iteration: 944/277000 \t Elapsed time: 203.28 \t Loss:14.54998\n",
      "Iteration: 945/277000 \t Elapsed time: 203.45 \t Loss:14.53472\n",
      "Iteration: 946/277000 \t Elapsed time: 203.60 \t Loss:14.51963\n",
      "Iteration: 947/277000 \t Elapsed time: 203.73 \t Loss:14.50469\n",
      "Iteration: 948/277000 \t Elapsed time: 203.92 \t Loss:14.48928\n",
      "Iteration: 949/277000 \t Elapsed time: 204.08 \t Loss:14.47407\n",
      "Iteration: 950/277000 \t Elapsed time: 204.23 \t Loss:14.45905\n",
      "Iteration: 951/277000 \t Elapsed time: 204.37 \t Loss:14.44409\n",
      "Iteration: 952/277000 \t Elapsed time: 204.53 \t Loss:14.42913\n",
      "Iteration: 953/277000 \t Elapsed time: 204.70 \t Loss:14.41393\n",
      "Iteration: 954/277000 \t Elapsed time: 204.85 \t Loss:14.39853\n",
      "Iteration: 955/277000 \t Elapsed time: 205.06 \t Loss:14.38304\n",
      "Iteration: 956/277000 \t Elapsed time: 205.26 \t Loss:14.36794\n",
      "Iteration: 957/277000 \t Elapsed time: 205.43 \t Loss:14.35295\n",
      "Iteration: 958/277000 \t Elapsed time: 205.63 \t Loss:14.33775\n",
      "Iteration: 959/277000 \t Elapsed time: 205.77 \t Loss:14.32262\n",
      "Iteration: 960/277000 \t Elapsed time: 205.91 \t Loss:14.30750\n",
      "Iteration: 961/277000 \t Elapsed time: 206.08 \t Loss:14.29243\n",
      "Iteration: 962/277000 \t Elapsed time: 206.26 \t Loss:14.27741\n",
      "Iteration: 963/277000 \t Elapsed time: 206.45 \t Loss:14.26228\n",
      "Iteration: 964/277000 \t Elapsed time: 206.60 \t Loss:14.24711\n",
      "Iteration: 965/277000 \t Elapsed time: 206.76 \t Loss:14.23232\n",
      "Iteration: 966/277000 \t Elapsed time: 206.90 \t Loss:14.21722\n",
      "Iteration: 967/277000 \t Elapsed time: 207.08 \t Loss:14.20218\n",
      "Iteration: 968/277000 \t Elapsed time: 207.25 \t Loss:14.18708\n",
      "Iteration: 969/277000 \t Elapsed time: 207.39 \t Loss:14.17206\n",
      "Iteration: 970/277000 \t Elapsed time: 207.52 \t Loss:14.15720\n",
      "Iteration: 971/277000 \t Elapsed time: 207.69 \t Loss:14.14218\n",
      "Iteration: 972/277000 \t Elapsed time: 207.83 \t Loss:14.12717\n",
      "Iteration: 973/277000 \t Elapsed time: 207.99 \t Loss:14.11231\n",
      "Iteration: 974/277000 \t Elapsed time: 208.15 \t Loss:14.09744\n",
      "Iteration: 975/277000 \t Elapsed time: 208.29 \t Loss:14.08253\n",
      "Iteration: 976/277000 \t Elapsed time: 208.46 \t Loss:14.06750\n",
      "Iteration: 977/277000 \t Elapsed time: 208.65 \t Loss:14.05252\n",
      "Iteration: 978/277000 \t Elapsed time: 208.83 \t Loss:14.03772\n",
      "Iteration: 979/277000 \t Elapsed time: 208.98 \t Loss:14.02290\n",
      "Iteration: 980/277000 \t Elapsed time: 209.12 \t Loss:14.00794\n",
      "Iteration: 981/277000 \t Elapsed time: 209.26 \t Loss:13.99300\n",
      "Iteration: 982/277000 \t Elapsed time: 209.41 \t Loss:13.97818\n",
      "Iteration: 983/277000 \t Elapsed time: 209.54 \t Loss:13.96329\n",
      "Iteration: 984/277000 \t Elapsed time: 209.74 \t Loss:13.94842\n",
      "Iteration: 985/277000 \t Elapsed time: 209.88 \t Loss:13.93364\n",
      "Iteration: 986/277000 \t Elapsed time: 210.03 \t Loss:13.91879\n",
      "Iteration: 987/277000 \t Elapsed time: 210.18 \t Loss:13.90396\n",
      "Iteration: 988/277000 \t Elapsed time: 210.41 \t Loss:13.88922\n",
      "Iteration: 989/277000 \t Elapsed time: 210.74 \t Loss:13.87445\n",
      "Iteration: 990/277000 \t Elapsed time: 210.88 \t Loss:13.85963\n",
      "Iteration: 991/277000 \t Elapsed time: 211.04 \t Loss:13.84486\n",
      "Iteration: 992/277000 \t Elapsed time: 211.22 \t Loss:13.83013\n",
      "Iteration: 993/277000 \t Elapsed time: 211.69 \t Loss:13.81539\n",
      "Iteration: 994/277000 \t Elapsed time: 211.84 \t Loss:13.80066\n",
      "Iteration: 995/277000 \t Elapsed time: 211.98 \t Loss:13.78599\n",
      "Iteration: 996/277000 \t Elapsed time: 212.13 \t Loss:13.77127\n",
      "Iteration: 997/277000 \t Elapsed time: 212.32 \t Loss:13.75647\n",
      "Iteration: 998/277000 \t Elapsed time: 212.45 \t Loss:13.74189\n",
      "Iteration: 999/277000 \t Elapsed time: 212.59 \t Loss:13.72714\n",
      "Iteration: 1000/277000 \t Elapsed time: 212.77 \t Loss:13.71247\n",
      "Start Validating\n",
      "Finish Validating\n",
      "Iteration: 1001/277000 \t Elapsed time: 237.90 \t Loss:13.69781\n",
      "Iteration: 1002/277000 \t Elapsed time: 238.05 \t Loss:13.68304\n",
      "Iteration: 1003/277000 \t Elapsed time: 238.20 \t Loss:13.66846\n",
      "Iteration: 1004/277000 \t Elapsed time: 238.40 \t Loss:13.65390\n",
      "Iteration: 1005/277000 \t Elapsed time: 238.61 \t Loss:13.63913\n",
      "Iteration: 1006/277000 \t Elapsed time: 238.76 \t Loss:13.62456\n",
      "Iteration: 1007/277000 \t Elapsed time: 238.98 \t Loss:13.60996\n",
      "Iteration: 1008/277000 \t Elapsed time: 239.16 \t Loss:13.59535\n",
      "Iteration: 1009/277000 \t Elapsed time: 239.32 \t Loss:13.58070\n",
      "Iteration: 1010/277000 \t Elapsed time: 239.54 \t Loss:13.56615\n",
      "Iteration: 1011/277000 \t Elapsed time: 239.73 \t Loss:13.55157\n",
      "Iteration: 1012/277000 \t Elapsed time: 239.94 \t Loss:13.53697\n",
      "Iteration: 1013/277000 \t Elapsed time: 240.20 \t Loss:13.52241\n",
      "Iteration: 1014/277000 \t Elapsed time: 240.45 \t Loss:13.50792\n",
      "Iteration: 1015/277000 \t Elapsed time: 240.69 \t Loss:13.49340\n",
      "Iteration: 1016/277000 \t Elapsed time: 240.86 \t Loss:13.47889\n",
      "Iteration: 1017/277000 \t Elapsed time: 241.08 \t Loss:13.46437\n",
      "Iteration: 1018/277000 \t Elapsed time: 241.31 \t Loss:13.44990\n",
      "Iteration: 1019/277000 \t Elapsed time: 241.55 \t Loss:13.43540\n",
      "Iteration: 1020/277000 \t Elapsed time: 241.71 \t Loss:13.42083\n",
      "Iteration: 1021/277000 \t Elapsed time: 241.89 \t Loss:13.40642\n",
      "Iteration: 1022/277000 \t Elapsed time: 242.30 \t Loss:13.39208\n",
      "Iteration: 1023/277000 \t Elapsed time: 242.46 \t Loss:13.37748\n",
      "Iteration: 1024/277000 \t Elapsed time: 242.64 \t Loss:13.36311\n",
      "Iteration: 1025/277000 \t Elapsed time: 242.86 \t Loss:13.34868\n",
      "Iteration: 1026/277000 \t Elapsed time: 243.07 \t Loss:13.33423\n",
      "Iteration: 1027/277000 \t Elapsed time: 243.21 \t Loss:13.31980\n",
      "Iteration: 1028/277000 \t Elapsed time: 243.38 \t Loss:13.30537\n",
      "Iteration: 1029/277000 \t Elapsed time: 243.60 \t Loss:13.29102\n",
      "Iteration: 1030/277000 \t Elapsed time: 243.80 \t Loss:13.27665\n",
      "Iteration: 1031/277000 \t Elapsed time: 243.98 \t Loss:13.26224\n",
      "Iteration: 1032/277000 \t Elapsed time: 244.16 \t Loss:13.24788\n",
      "Iteration: 1033/277000 \t Elapsed time: 244.50 \t Loss:13.23355\n",
      "Iteration: 1034/277000 \t Elapsed time: 244.68 \t Loss:13.21918\n",
      "Iteration: 1035/277000 \t Elapsed time: 244.87 \t Loss:13.20495\n",
      "Iteration: 1036/277000 \t Elapsed time: 245.04 \t Loss:13.19076\n",
      "Iteration: 1037/277000 \t Elapsed time: 245.27 \t Loss:13.17635\n",
      "Iteration: 1038/277000 \t Elapsed time: 245.46 \t Loss:13.16211\n",
      "Iteration: 1039/277000 \t Elapsed time: 245.62 \t Loss:13.14773\n",
      "Iteration: 1040/277000 \t Elapsed time: 245.78 \t Loss:13.13333\n",
      "Iteration: 1041/277000 \t Elapsed time: 246.03 \t Loss:13.11901\n",
      "Iteration: 1042/277000 \t Elapsed time: 246.25 \t Loss:13.10473\n",
      "Iteration: 1043/277000 \t Elapsed time: 246.42 \t Loss:13.09051\n",
      "Iteration: 1044/277000 \t Elapsed time: 246.65 \t Loss:13.07634\n",
      "Iteration: 1045/277000 \t Elapsed time: 246.84 \t Loss:13.06217\n",
      "Iteration: 1046/277000 \t Elapsed time: 247.36 \t Loss:13.04789\n",
      "Iteration: 1047/277000 \t Elapsed time: 247.55 \t Loss:13.03354\n",
      "Iteration: 1048/277000 \t Elapsed time: 249.40 \t Loss:13.01914\n",
      "Iteration: 1049/277000 \t Elapsed time: 249.63 \t Loss:13.00506\n",
      "Iteration: 1050/277000 \t Elapsed time: 249.79 \t Loss:12.99085\n",
      "Iteration: 1051/277000 \t Elapsed time: 249.96 \t Loss:12.97663\n",
      "Iteration: 1052/277000 \t Elapsed time: 250.22 \t Loss:12.96253\n",
      "Iteration: 1053/277000 \t Elapsed time: 250.38 \t Loss:12.94832\n",
      "Iteration: 1054/277000 \t Elapsed time: 250.62 \t Loss:12.93412\n",
      "Iteration: 1055/277000 \t Elapsed time: 250.77 \t Loss:12.91993\n",
      "Iteration: 1056/277000 \t Elapsed time: 250.97 \t Loss:12.90581\n",
      "Iteration: 1057/277000 \t Elapsed time: 251.14 \t Loss:12.89170\n",
      "Iteration: 1058/277000 \t Elapsed time: 251.29 \t Loss:12.87743\n",
      "Iteration: 1059/277000 \t Elapsed time: 251.52 \t Loss:12.86341\n",
      "Iteration: 1060/277000 \t Elapsed time: 251.79 \t Loss:12.84943\n",
      "Iteration: 1061/277000 \t Elapsed time: 251.94 \t Loss:12.83530\n",
      "Iteration: 1062/277000 \t Elapsed time: 252.12 \t Loss:12.82117\n",
      "Iteration: 1063/277000 \t Elapsed time: 252.58 \t Loss:12.80699\n",
      "Iteration: 1064/277000 \t Elapsed time: 252.75 \t Loss:12.79314\n",
      "Iteration: 1065/277000 \t Elapsed time: 252.91 \t Loss:12.77892\n",
      "Iteration: 1066/277000 \t Elapsed time: 253.05 \t Loss:12.76496\n",
      "Iteration: 1067/277000 \t Elapsed time: 253.20 \t Loss:12.75069\n",
      "Iteration: 1068/277000 \t Elapsed time: 253.38 \t Loss:12.73671\n",
      "Iteration: 1069/277000 \t Elapsed time: 253.63 \t Loss:12.72260\n",
      "Iteration: 1070/277000 \t Elapsed time: 253.86 \t Loss:12.70856\n",
      "Iteration: 1071/277000 \t Elapsed time: 254.06 \t Loss:12.69459\n",
      "Iteration: 1072/277000 \t Elapsed time: 254.21 \t Loss:12.68066\n",
      "Iteration: 1073/277000 \t Elapsed time: 254.37 \t Loss:12.66655\n",
      "Iteration: 1074/277000 \t Elapsed time: 254.61 \t Loss:12.65256\n",
      "Iteration: 1075/277000 \t Elapsed time: 254.80 \t Loss:12.63859\n",
      "Iteration: 1076/277000 \t Elapsed time: 254.94 \t Loss:12.62457\n",
      "Iteration: 1077/277000 \t Elapsed time: 255.11 \t Loss:12.61081\n",
      "Iteration: 1078/277000 \t Elapsed time: 255.27 \t Loss:12.59688\n",
      "Iteration: 1079/277000 \t Elapsed time: 255.42 \t Loss:12.58307\n",
      "Iteration: 1080/277000 \t Elapsed time: 255.57 \t Loss:12.56915\n",
      "Iteration: 1081/277000 \t Elapsed time: 255.72 \t Loss:12.55502\n",
      "Iteration: 1082/277000 \t Elapsed time: 255.87 \t Loss:12.54127\n",
      "Iteration: 1083/277000 \t Elapsed time: 256.02 \t Loss:12.52742\n",
      "Iteration: 1084/277000 \t Elapsed time: 256.19 \t Loss:12.51339\n",
      "Iteration: 1085/277000 \t Elapsed time: 256.44 \t Loss:12.49946\n",
      "Iteration: 1086/277000 \t Elapsed time: 256.71 \t Loss:12.48545\n",
      "Iteration: 1087/277000 \t Elapsed time: 256.86 \t Loss:12.47157\n",
      "Iteration: 1088/277000 \t Elapsed time: 257.04 \t Loss:12.45781\n",
      "Iteration: 1089/277000 \t Elapsed time: 257.22 \t Loss:12.44393\n",
      "Iteration: 1090/277000 \t Elapsed time: 257.38 \t Loss:12.42990\n",
      "Iteration: 1091/277000 \t Elapsed time: 257.62 \t Loss:12.41622\n",
      "Iteration: 1092/277000 \t Elapsed time: 257.81 \t Loss:12.40241\n",
      "Iteration: 1093/277000 \t Elapsed time: 257.98 \t Loss:12.38857\n",
      "Iteration: 1094/277000 \t Elapsed time: 258.15 \t Loss:12.37468\n",
      "Iteration: 1095/277000 \t Elapsed time: 258.32 \t Loss:12.36089\n",
      "Iteration: 1096/277000 \t Elapsed time: 258.48 \t Loss:12.34729\n",
      "Iteration: 1097/277000 \t Elapsed time: 258.65 \t Loss:12.33347\n",
      "Iteration: 1098/277000 \t Elapsed time: 258.79 \t Loss:12.31974\n",
      "Iteration: 1099/277000 \t Elapsed time: 258.92 \t Loss:12.30625\n",
      "Iteration: 1100/277000 \t Elapsed time: 259.06 \t Loss:12.29283\n",
      "Iteration: 1101/277000 \t Elapsed time: 259.22 \t Loss:12.27918\n",
      "Iteration: 1102/277000 \t Elapsed time: 259.36 \t Loss:12.26523\n",
      "Iteration: 1103/277000 \t Elapsed time: 259.53 \t Loss:12.25113\n",
      "Iteration: 1104/277000 \t Elapsed time: 259.75 \t Loss:12.23721\n",
      "Iteration: 1105/277000 \t Elapsed time: 259.92 \t Loss:12.22351\n",
      "Iteration: 1106/277000 \t Elapsed time: 260.08 \t Loss:12.20976\n",
      "Iteration: 1107/277000 \t Elapsed time: 260.21 \t Loss:12.19640\n",
      "Iteration: 1108/277000 \t Elapsed time: 260.38 \t Loss:12.18297\n",
      "Iteration: 1109/277000 \t Elapsed time: 260.55 \t Loss:12.16950\n",
      "Iteration: 1110/277000 \t Elapsed time: 260.69 \t Loss:12.15558\n",
      "Iteration: 1111/277000 \t Elapsed time: 260.84 \t Loss:12.14151\n",
      "Iteration: 1112/277000 \t Elapsed time: 261.05 \t Loss:12.12758\n",
      "Iteration: 1113/277000 \t Elapsed time: 261.19 \t Loss:12.11404\n",
      "Iteration: 1114/277000 \t Elapsed time: 261.33 \t Loss:12.10092\n",
      "Iteration: 1115/277000 \t Elapsed time: 261.49 \t Loss:12.08703\n",
      "Iteration: 1116/277000 \t Elapsed time: 261.62 \t Loss:12.07311\n",
      "Iteration: 1117/277000 \t Elapsed time: 261.77 \t Loss:12.05968\n",
      "Iteration: 1118/277000 \t Elapsed time: 261.94 \t Loss:12.04612\n",
      "Iteration: 1119/277000 \t Elapsed time: 262.12 \t Loss:12.03218\n",
      "Iteration: 1120/277000 \t Elapsed time: 262.26 \t Loss:12.01862\n",
      "Iteration: 1121/277000 \t Elapsed time: 262.43 \t Loss:12.00506\n",
      "Iteration: 1122/277000 \t Elapsed time: 262.63 \t Loss:11.99146\n",
      "Iteration: 1123/277000 \t Elapsed time: 262.77 \t Loss:11.97785\n",
      "Iteration: 1124/277000 \t Elapsed time: 262.96 \t Loss:11.96425\n",
      "Iteration: 1125/277000 \t Elapsed time: 263.11 \t Loss:11.95076\n",
      "Iteration: 1126/277000 \t Elapsed time: 263.58 \t Loss:11.93708\n",
      "Iteration: 1127/277000 \t Elapsed time: 263.82 \t Loss:11.92359\n",
      "Iteration: 1128/277000 \t Elapsed time: 264.00 \t Loss:11.91002\n",
      "Iteration: 1129/277000 \t Elapsed time: 264.14 \t Loss:11.89641\n",
      "Iteration: 1130/277000 \t Elapsed time: 264.30 \t Loss:11.88291\n",
      "Iteration: 1131/277000 \t Elapsed time: 264.49 \t Loss:11.86943\n",
      "Iteration: 1132/277000 \t Elapsed time: 264.63 \t Loss:11.85588\n",
      "Iteration: 1133/277000 \t Elapsed time: 265.03 \t Loss:11.84238\n",
      "Iteration: 1134/277000 \t Elapsed time: 265.18 \t Loss:11.82891\n",
      "Iteration: 1135/277000 \t Elapsed time: 265.31 \t Loss:11.81540\n",
      "Iteration: 1136/277000 \t Elapsed time: 265.50 \t Loss:11.80194\n",
      "Iteration: 1137/277000 \t Elapsed time: 265.70 \t Loss:11.78845\n",
      "Iteration: 1138/277000 \t Elapsed time: 265.89 \t Loss:11.77518\n",
      "Iteration: 1139/277000 \t Elapsed time: 266.06 \t Loss:11.76158\n",
      "Iteration: 1140/277000 \t Elapsed time: 266.23 \t Loss:11.74813\n",
      "Iteration: 1141/277000 \t Elapsed time: 266.41 \t Loss:11.73477\n",
      "Iteration: 1142/277000 \t Elapsed time: 266.68 \t Loss:11.72136\n",
      "Iteration: 1143/277000 \t Elapsed time: 266.84 \t Loss:11.70792\n",
      "Iteration: 1144/277000 \t Elapsed time: 267.04 \t Loss:11.69443\n",
      "Iteration: 1145/277000 \t Elapsed time: 267.21 \t Loss:11.68110\n",
      "Iteration: 1146/277000 \t Elapsed time: 267.37 \t Loss:11.66759\n",
      "Iteration: 1147/277000 \t Elapsed time: 267.54 \t Loss:11.65441\n",
      "Iteration: 1148/277000 \t Elapsed time: 267.77 \t Loss:11.64100\n",
      "Iteration: 1149/277000 \t Elapsed time: 268.21 \t Loss:11.62761\n",
      "Iteration: 1150/277000 \t Elapsed time: 268.35 \t Loss:11.61411\n",
      "Iteration: 1151/277000 \t Elapsed time: 268.49 \t Loss:11.60082\n",
      "Iteration: 1152/277000 \t Elapsed time: 268.65 \t Loss:11.58748\n",
      "Iteration: 1153/277000 \t Elapsed time: 268.85 \t Loss:11.57419\n",
      "Iteration: 1154/277000 \t Elapsed time: 269.01 \t Loss:11.56076\n",
      "Iteration: 1155/277000 \t Elapsed time: 269.19 \t Loss:11.54755\n",
      "Iteration: 1156/277000 \t Elapsed time: 269.33 \t Loss:11.53421\n",
      "Iteration: 1157/277000 \t Elapsed time: 269.56 \t Loss:11.52086\n",
      "Iteration: 1158/277000 \t Elapsed time: 269.86 \t Loss:11.50761\n",
      "Iteration: 1159/277000 \t Elapsed time: 270.00 \t Loss:11.49441\n",
      "Iteration: 1160/277000 \t Elapsed time: 270.17 \t Loss:11.48096\n",
      "Iteration: 1161/277000 \t Elapsed time: 270.32 \t Loss:11.46774\n",
      "Iteration: 1162/277000 \t Elapsed time: 270.50 \t Loss:11.45447\n",
      "Iteration: 1163/277000 \t Elapsed time: 270.74 \t Loss:11.44121\n",
      "Iteration: 1164/277000 \t Elapsed time: 270.91 \t Loss:11.42801\n",
      "Iteration: 1165/277000 \t Elapsed time: 271.06 \t Loss:11.41481\n",
      "Iteration: 1166/277000 \t Elapsed time: 271.29 \t Loss:11.40159\n",
      "Iteration: 1167/277000 \t Elapsed time: 271.52 \t Loss:11.38830\n",
      "Iteration: 1168/277000 \t Elapsed time: 271.66 \t Loss:11.37504\n",
      "Iteration: 1169/277000 \t Elapsed time: 271.87 \t Loss:11.36184\n",
      "Iteration: 1170/277000 \t Elapsed time: 272.03 \t Loss:11.34871\n",
      "Iteration: 1171/277000 \t Elapsed time: 272.21 \t Loss:11.33565\n",
      "Iteration: 1172/277000 \t Elapsed time: 272.39 \t Loss:11.32245\n",
      "Iteration: 1173/277000 \t Elapsed time: 272.58 \t Loss:11.30918\n",
      "Iteration: 1174/277000 \t Elapsed time: 272.73 \t Loss:11.29587\n",
      "Iteration: 1175/277000 \t Elapsed time: 272.89 \t Loss:11.28282\n",
      "Iteration: 1176/277000 \t Elapsed time: 273.04 \t Loss:11.26969\n",
      "Iteration: 1177/277000 \t Elapsed time: 273.22 \t Loss:11.25641\n",
      "Iteration: 1178/277000 \t Elapsed time: 273.37 \t Loss:11.24345\n",
      "Iteration: 1179/277000 \t Elapsed time: 273.53 \t Loss:11.23017\n",
      "Iteration: 1180/277000 \t Elapsed time: 273.70 \t Loss:11.21704\n",
      "Iteration: 1181/277000 \t Elapsed time: 273.90 \t Loss:11.20403\n",
      "Iteration: 1182/277000 \t Elapsed time: 274.28 \t Loss:11.19081\n",
      "Iteration: 1183/277000 \t Elapsed time: 274.42 \t Loss:11.17789\n",
      "Iteration: 1184/277000 \t Elapsed time: 274.63 \t Loss:11.16490\n",
      "Iteration: 1185/277000 \t Elapsed time: 274.83 \t Loss:11.15194\n",
      "Iteration: 1186/277000 \t Elapsed time: 275.02 \t Loss:11.13881\n",
      "Iteration: 1187/277000 \t Elapsed time: 275.19 \t Loss:11.12562\n",
      "Iteration: 1188/277000 \t Elapsed time: 275.38 \t Loss:11.11240\n",
      "Iteration: 1189/277000 \t Elapsed time: 275.63 \t Loss:11.09929\n",
      "Iteration: 1190/277000 \t Elapsed time: 275.90 \t Loss:11.08627\n",
      "Iteration: 1191/277000 \t Elapsed time: 276.12 \t Loss:11.07331\n",
      "Iteration: 1192/277000 \t Elapsed time: 276.27 \t Loss:11.06056\n",
      "Iteration: 1193/277000 \t Elapsed time: 276.41 \t Loss:11.04746\n",
      "Iteration: 1194/277000 \t Elapsed time: 276.64 \t Loss:11.03437\n",
      "Iteration: 1195/277000 \t Elapsed time: 276.82 \t Loss:11.02154\n",
      "Iteration: 1196/277000 \t Elapsed time: 276.98 \t Loss:11.00851\n",
      "Iteration: 1197/277000 \t Elapsed time: 278.09 \t Loss:10.99550\n",
      "Iteration: 1198/277000 \t Elapsed time: 278.25 \t Loss:10.98240\n",
      "Iteration: 1199/277000 \t Elapsed time: 278.41 \t Loss:10.96954\n",
      "Iteration: 1200/277000 \t Elapsed time: 278.60 \t Loss:10.95653\n",
      "Iteration: 1201/277000 \t Elapsed time: 278.81 \t Loss:10.94327\n",
      "Iteration: 1202/277000 \t Elapsed time: 278.98 \t Loss:10.93048\n",
      "Iteration: 1203/277000 \t Elapsed time: 279.12 \t Loss:10.91761\n",
      "Iteration: 1204/277000 \t Elapsed time: 279.27 \t Loss:10.90460\n",
      "Iteration: 1205/277000 \t Elapsed time: 279.41 \t Loss:10.89166\n",
      "Iteration: 1206/277000 \t Elapsed time: 279.58 \t Loss:10.87864\n",
      "Iteration: 1207/277000 \t Elapsed time: 279.73 \t Loss:10.86561\n",
      "Iteration: 1208/277000 \t Elapsed time: 279.87 \t Loss:10.85290\n",
      "Iteration: 1209/277000 \t Elapsed time: 280.03 \t Loss:10.84000\n",
      "Iteration: 1210/277000 \t Elapsed time: 280.19 \t Loss:10.82721\n",
      "Iteration: 1211/277000 \t Elapsed time: 280.37 \t Loss:10.81419\n",
      "Iteration: 1212/277000 \t Elapsed time: 280.52 \t Loss:10.80124\n",
      "Iteration: 1213/277000 \t Elapsed time: 280.79 \t Loss:10.78857\n",
      "Iteration: 1214/277000 \t Elapsed time: 280.94 \t Loss:10.77564\n",
      "Iteration: 1215/277000 \t Elapsed time: 281.09 \t Loss:10.76280\n",
      "Iteration: 1216/277000 \t Elapsed time: 281.22 \t Loss:10.74986\n",
      "Iteration: 1217/277000 \t Elapsed time: 281.37 \t Loss:10.73702\n",
      "Iteration: 1218/277000 \t Elapsed time: 281.65 \t Loss:10.72418\n",
      "Iteration: 1219/277000 \t Elapsed time: 281.83 \t Loss:10.71166\n",
      "Iteration: 1220/277000 \t Elapsed time: 282.04 \t Loss:10.69937\n",
      "Iteration: 1221/277000 \t Elapsed time: 282.36 \t Loss:10.68588\n",
      "Iteration: 1222/277000 \t Elapsed time: 282.52 \t Loss:10.67366\n",
      "Iteration: 1223/277000 \t Elapsed time: 282.66 \t Loss:10.66118\n",
      "Iteration: 1224/277000 \t Elapsed time: 282.81 \t Loss:10.64808\n",
      "Iteration: 1225/277000 \t Elapsed time: 282.97 \t Loss:10.63511\n",
      "Iteration: 1226/277000 \t Elapsed time: 283.12 \t Loss:10.62213\n",
      "Iteration: 1227/277000 \t Elapsed time: 283.29 \t Loss:10.60951\n",
      "Iteration: 1228/277000 \t Elapsed time: 283.54 \t Loss:10.59632\n",
      "Iteration: 1229/277000 \t Elapsed time: 283.74 \t Loss:10.58362\n",
      "Iteration: 1230/277000 \t Elapsed time: 283.94 \t Loss:10.57095\n",
      "Iteration: 1231/277000 \t Elapsed time: 284.11 \t Loss:10.55831\n",
      "Iteration: 1232/277000 \t Elapsed time: 284.25 \t Loss:10.54532\n",
      "Iteration: 1233/277000 \t Elapsed time: 284.45 \t Loss:10.53250\n",
      "Iteration: 1234/277000 \t Elapsed time: 284.66 \t Loss:10.51985\n",
      "Iteration: 1235/277000 \t Elapsed time: 284.82 \t Loss:10.50719\n",
      "Iteration: 1236/277000 \t Elapsed time: 284.97 \t Loss:10.49437\n",
      "Iteration: 1237/277000 \t Elapsed time: 285.11 \t Loss:10.48157\n",
      "Iteration: 1238/277000 \t Elapsed time: 285.27 \t Loss:10.46905\n",
      "Iteration: 1239/277000 \t Elapsed time: 285.44 \t Loss:10.45653\n",
      "Iteration: 1240/277000 \t Elapsed time: 285.60 \t Loss:10.44359\n",
      "Iteration: 1241/277000 \t Elapsed time: 285.74 \t Loss:10.43093\n",
      "Iteration: 1242/277000 \t Elapsed time: 285.89 \t Loss:10.41815\n",
      "Iteration: 1243/277000 \t Elapsed time: 286.20 \t Loss:10.40567\n",
      "Iteration: 1244/277000 \t Elapsed time: 286.36 \t Loss:10.39290\n",
      "Iteration: 1245/277000 \t Elapsed time: 286.50 \t Loss:10.38023\n",
      "Iteration: 1246/277000 \t Elapsed time: 286.65 \t Loss:10.36762\n",
      "Iteration: 1247/277000 \t Elapsed time: 286.80 \t Loss:10.35501\n",
      "Iteration: 1248/277000 \t Elapsed time: 286.97 \t Loss:10.34243\n",
      "Iteration: 1249/277000 \t Elapsed time: 287.11 \t Loss:10.32974\n",
      "Iteration: 1250/277000 \t Elapsed time: 287.26 \t Loss:10.31706\n",
      "Iteration: 1251/277000 \t Elapsed time: 287.41 \t Loss:10.30451\n",
      "Iteration: 1252/277000 \t Elapsed time: 287.54 \t Loss:10.29188\n",
      "Iteration: 1253/277000 \t Elapsed time: 287.72 \t Loss:10.27930\n",
      "Iteration: 1254/277000 \t Elapsed time: 288.09 \t Loss:10.26676\n",
      "Iteration: 1255/277000 \t Elapsed time: 288.25 \t Loss:10.25423\n",
      "Iteration: 1256/277000 \t Elapsed time: 288.39 \t Loss:10.24162\n",
      "Iteration: 1257/277000 \t Elapsed time: 288.58 \t Loss:10.22912\n",
      "Iteration: 1258/277000 \t Elapsed time: 288.84 \t Loss:10.21660\n",
      "Iteration: 1259/277000 \t Elapsed time: 289.00 \t Loss:10.20422\n",
      "Iteration: 1260/277000 \t Elapsed time: 289.24 \t Loss:10.19205\n",
      "Iteration: 1261/277000 \t Elapsed time: 289.41 \t Loss:10.17921\n",
      "Iteration: 1262/277000 \t Elapsed time: 289.58 \t Loss:10.16644\n",
      "Iteration: 1263/277000 \t Elapsed time: 289.75 \t Loss:10.15414\n",
      "Iteration: 1264/277000 \t Elapsed time: 289.91 \t Loss:10.14180\n",
      "Iteration: 1265/277000 \t Elapsed time: 290.07 \t Loss:10.12966\n",
      "Iteration: 1266/277000 \t Elapsed time: 290.25 \t Loss:10.11696\n",
      "Iteration: 1267/277000 \t Elapsed time: 290.41 \t Loss:10.10417\n",
      "Iteration: 1268/277000 \t Elapsed time: 290.62 \t Loss:10.09161\n",
      "Iteration: 1269/277000 \t Elapsed time: 290.79 \t Loss:10.07921\n",
      "Iteration: 1270/277000 \t Elapsed time: 290.93 \t Loss:10.06698\n",
      "Iteration: 1271/277000 \t Elapsed time: 291.13 \t Loss:10.05447\n",
      "Iteration: 1272/277000 \t Elapsed time: 291.29 \t Loss:10.04211\n",
      "Iteration: 1273/277000 \t Elapsed time: 291.47 \t Loss:10.02943\n",
      "Iteration: 1274/277000 \t Elapsed time: 292.01 \t Loss:10.01699\n",
      "Iteration: 1275/277000 \t Elapsed time: 292.18 \t Loss:10.00477\n",
      "Iteration: 1276/277000 \t Elapsed time: 292.97 \t Loss:9.99253\n",
      "Iteration: 1277/277000 \t Elapsed time: 293.11 \t Loss:9.98021\n",
      "Iteration: 1278/277000 \t Elapsed time: 293.30 \t Loss:9.96808\n",
      "Iteration: 1279/277000 \t Elapsed time: 293.45 \t Loss:9.95609\n",
      "Iteration: 1280/277000 \t Elapsed time: 293.71 \t Loss:9.94415\n",
      "Iteration: 1281/277000 \t Elapsed time: 293.87 \t Loss:9.93191\n",
      "Iteration: 1282/277000 \t Elapsed time: 294.01 \t Loss:9.91841\n",
      "Iteration: 1283/277000 \t Elapsed time: 294.19 \t Loss:9.90554\n",
      "Iteration: 1284/277000 \t Elapsed time: 294.49 \t Loss:9.89408\n",
      "Iteration: 1285/277000 \t Elapsed time: 294.64 \t Loss:9.88245\n",
      "Iteration: 1286/277000 \t Elapsed time: 294.80 \t Loss:9.86935\n",
      "Iteration: 1287/277000 \t Elapsed time: 294.97 \t Loss:9.85597\n",
      "Iteration: 1288/277000 \t Elapsed time: 295.15 \t Loss:9.84412\n",
      "Iteration: 1289/277000 \t Elapsed time: 295.35 \t Loss:9.83407\n",
      "Iteration: 1290/277000 \t Elapsed time: 295.57 \t Loss:9.82183\n",
      "Iteration: 1291/277000 \t Elapsed time: 295.74 \t Loss:9.80733\n",
      "Iteration: 1292/277000 \t Elapsed time: 295.90 \t Loss:9.79536\n",
      "Iteration: 1293/277000 \t Elapsed time: 296.28 \t Loss:9.78363\n",
      "Iteration: 1294/277000 \t Elapsed time: 296.43 \t Loss:9.77028\n",
      "Iteration: 1295/277000 \t Elapsed time: 296.61 \t Loss:9.75830\n",
      "Iteration: 1296/277000 \t Elapsed time: 296.80 \t Loss:9.74614\n",
      "Iteration: 1297/277000 \t Elapsed time: 296.95 \t Loss:9.73309\n",
      "Iteration: 1298/277000 \t Elapsed time: 297.36 \t Loss:9.72135\n",
      "Iteration: 1299/277000 \t Elapsed time: 297.56 \t Loss:9.70875\n",
      "Iteration: 1300/277000 \t Elapsed time: 297.72 \t Loss:9.69630\n",
      "Iteration: 1301/277000 \t Elapsed time: 297.96 \t Loss:9.68460\n",
      "Iteration: 1302/277000 \t Elapsed time: 298.25 \t Loss:9.67188\n",
      "Iteration: 1303/277000 \t Elapsed time: 298.42 \t Loss:9.65964\n",
      "Iteration: 1304/277000 \t Elapsed time: 298.66 \t Loss:9.64759\n",
      "Iteration: 1305/277000 \t Elapsed time: 298.83 \t Loss:9.63505\n",
      "Iteration: 1306/277000 \t Elapsed time: 298.98 \t Loss:9.62305\n",
      "Iteration: 1307/277000 \t Elapsed time: 299.13 \t Loss:9.61070\n",
      "Iteration: 1308/277000 \t Elapsed time: 299.31 \t Loss:9.59846\n",
      "Iteration: 1309/277000 \t Elapsed time: 299.46 \t Loss:9.58651\n",
      "Iteration: 1310/277000 \t Elapsed time: 299.62 \t Loss:9.57426\n",
      "Iteration: 1311/277000 \t Elapsed time: 299.80 \t Loss:9.56210\n",
      "Iteration: 1312/277000 \t Elapsed time: 301.77 \t Loss:9.54989\n",
      "Iteration: 1313/277000 \t Elapsed time: 301.96 \t Loss:9.53747\n",
      "Iteration: 1314/277000 \t Elapsed time: 302.18 \t Loss:9.52549\n",
      "Iteration: 1315/277000 \t Elapsed time: 302.37 \t Loss:9.51340\n",
      "Iteration: 1316/277000 \t Elapsed time: 302.88 \t Loss:9.50111\n",
      "Iteration: 1317/277000 \t Elapsed time: 303.07 \t Loss:9.48881\n",
      "Iteration: 1318/277000 \t Elapsed time: 303.22 \t Loss:9.47707\n",
      "Iteration: 1319/277000 \t Elapsed time: 303.47 \t Loss:9.46462\n",
      "Iteration: 1320/277000 \t Elapsed time: 303.65 \t Loss:9.45249\n",
      "Iteration: 1321/277000 \t Elapsed time: 303.79 \t Loss:9.44067\n",
      "Iteration: 1322/277000 \t Elapsed time: 303.97 \t Loss:9.42821\n",
      "Iteration: 1323/277000 \t Elapsed time: 304.24 \t Loss:9.41623\n",
      "Iteration: 1324/277000 \t Elapsed time: 304.38 \t Loss:9.40400\n",
      "Iteration: 1325/277000 \t Elapsed time: 304.57 \t Loss:9.39198\n",
      "Iteration: 1326/277000 \t Elapsed time: 304.83 \t Loss:9.37985\n",
      "Iteration: 1327/277000 \t Elapsed time: 305.00 \t Loss:9.36786\n",
      "Iteration: 1328/277000 \t Elapsed time: 305.19 \t Loss:9.35572\n",
      "Iteration: 1329/277000 \t Elapsed time: 305.34 \t Loss:9.34371\n",
      "Iteration: 1330/277000 \t Elapsed time: 305.48 \t Loss:9.33170\n",
      "Iteration: 1331/277000 \t Elapsed time: 305.64 \t Loss:9.31982\n",
      "Iteration: 1332/277000 \t Elapsed time: 305.79 \t Loss:9.30792\n",
      "Iteration: 1333/277000 \t Elapsed time: 305.94 \t Loss:9.29542\n",
      "Iteration: 1334/277000 \t Elapsed time: 306.09 \t Loss:9.28336\n",
      "Iteration: 1335/277000 \t Elapsed time: 306.25 \t Loss:9.27172\n",
      "Iteration: 1336/277000 \t Elapsed time: 306.43 \t Loss:9.26022\n",
      "Iteration: 1337/277000 \t Elapsed time: 306.61 \t Loss:9.24783\n",
      "Iteration: 1338/277000 \t Elapsed time: 306.78 \t Loss:9.23516\n",
      "Iteration: 1339/277000 \t Elapsed time: 306.94 \t Loss:9.22366\n",
      "Iteration: 1340/277000 \t Elapsed time: 307.11 \t Loss:9.21211\n",
      "Iteration: 1341/277000 \t Elapsed time: 307.25 \t Loss:9.20020\n",
      "Iteration: 1342/277000 \t Elapsed time: 307.40 \t Loss:9.18741\n",
      "Iteration: 1343/277000 \t Elapsed time: 307.56 \t Loss:9.17598\n",
      "Iteration: 1344/277000 \t Elapsed time: 307.71 \t Loss:9.16385\n",
      "Iteration: 1345/277000 \t Elapsed time: 307.85 \t Loss:9.15171\n",
      "Iteration: 1346/277000 \t Elapsed time: 308.00 \t Loss:9.13970\n",
      "Iteration: 1347/277000 \t Elapsed time: 308.13 \t Loss:9.12794\n",
      "Iteration: 1348/277000 \t Elapsed time: 308.41 \t Loss:9.11583\n",
      "Iteration: 1349/277000 \t Elapsed time: 308.58 \t Loss:9.10404\n",
      "Iteration: 1350/277000 \t Elapsed time: 308.73 \t Loss:9.09188\n",
      "Iteration: 1351/277000 \t Elapsed time: 308.87 \t Loss:9.07988\n",
      "Iteration: 1352/277000 \t Elapsed time: 309.12 \t Loss:9.06797\n",
      "Iteration: 1353/277000 \t Elapsed time: 309.27 \t Loss:9.05618\n",
      "Iteration: 1354/277000 \t Elapsed time: 309.48 \t Loss:9.04445\n",
      "Iteration: 1355/277000 \t Elapsed time: 309.67 \t Loss:9.03237\n",
      "Iteration: 1356/277000 \t Elapsed time: 309.82 \t Loss:9.02048\n",
      "Iteration: 1357/277000 \t Elapsed time: 309.96 \t Loss:9.00879\n",
      "Iteration: 1358/277000 \t Elapsed time: 310.14 \t Loss:8.99697\n",
      "Iteration: 1359/277000 \t Elapsed time: 310.28 \t Loss:8.98451\n",
      "Iteration: 1360/277000 \t Elapsed time: 310.42 \t Loss:8.97306\n",
      "Iteration: 1361/277000 \t Elapsed time: 310.58 \t Loss:8.96130\n",
      "Iteration: 1362/277000 \t Elapsed time: 310.72 \t Loss:8.94921\n",
      "Iteration: 1363/277000 \t Elapsed time: 311.14 \t Loss:8.93720\n",
      "Iteration: 1364/277000 \t Elapsed time: 311.30 \t Loss:8.92585\n",
      "Iteration: 1365/277000 \t Elapsed time: 311.60 \t Loss:8.91389\n",
      "Iteration: 1366/277000 \t Elapsed time: 311.76 \t Loss:8.90161\n",
      "Iteration: 1367/277000 \t Elapsed time: 312.03 \t Loss:8.88995\n",
      "Iteration: 1368/277000 \t Elapsed time: 312.24 \t Loss:8.87831\n",
      "Iteration: 1369/277000 \t Elapsed time: 312.44 \t Loss:8.86648\n",
      "Iteration: 1370/277000 \t Elapsed time: 312.58 \t Loss:8.85447\n",
      "Iteration: 1371/277000 \t Elapsed time: 312.73 \t Loss:8.84288\n",
      "Iteration: 1372/277000 \t Elapsed time: 312.88 \t Loss:8.83090\n",
      "Iteration: 1373/277000 \t Elapsed time: 313.10 \t Loss:8.81888\n",
      "Iteration: 1374/277000 \t Elapsed time: 313.33 \t Loss:8.80751\n",
      "Iteration: 1375/277000 \t Elapsed time: 313.55 \t Loss:8.79542\n",
      "Iteration: 1376/277000 \t Elapsed time: 313.70 \t Loss:8.78348\n",
      "Iteration: 1377/277000 \t Elapsed time: 313.87 \t Loss:8.77191\n",
      "Iteration: 1378/277000 \t Elapsed time: 314.01 \t Loss:8.76023\n",
      "Iteration: 1379/277000 \t Elapsed time: 314.25 \t Loss:8.74834\n",
      "Iteration: 1380/277000 \t Elapsed time: 314.39 \t Loss:8.73655\n",
      "Iteration: 1381/277000 \t Elapsed time: 314.56 \t Loss:8.72512\n",
      "Iteration: 1382/277000 \t Elapsed time: 314.71 \t Loss:8.71317\n",
      "Iteration: 1383/277000 \t Elapsed time: 314.92 \t Loss:8.70125\n",
      "Iteration: 1384/277000 \t Elapsed time: 315.08 \t Loss:8.68987\n",
      "Iteration: 1385/277000 \t Elapsed time: 315.24 \t Loss:8.67799\n",
      "Iteration: 1386/277000 \t Elapsed time: 315.44 \t Loss:8.66614\n",
      "Iteration: 1387/277000 \t Elapsed time: 315.67 \t Loss:8.65448\n",
      "Iteration: 1388/277000 \t Elapsed time: 315.81 \t Loss:8.64273\n",
      "Iteration: 1389/277000 \t Elapsed time: 315.96 \t Loss:8.63107\n",
      "Iteration: 1390/277000 \t Elapsed time: 316.17 \t Loss:8.61937\n",
      "Iteration: 1391/277000 \t Elapsed time: 316.33 \t Loss:8.60756\n",
      "Iteration: 1392/277000 \t Elapsed time: 316.48 \t Loss:8.59585\n",
      "Iteration: 1393/277000 \t Elapsed time: 316.66 \t Loss:8.58430\n",
      "Iteration: 1394/277000 \t Elapsed time: 316.81 \t Loss:8.57284\n",
      "Iteration: 1395/277000 \t Elapsed time: 317.06 \t Loss:8.56102\n",
      "Iteration: 1396/277000 \t Elapsed time: 317.26 \t Loss:8.54932\n",
      "Iteration: 1397/277000 \t Elapsed time: 317.42 \t Loss:8.53815\n",
      "Iteration: 1398/277000 \t Elapsed time: 318.06 \t Loss:8.52710\n",
      "Iteration: 1399/277000 \t Elapsed time: 318.28 \t Loss:8.51536\n",
      "Iteration: 1400/277000 \t Elapsed time: 318.66 \t Loss:8.50278\n",
      "Iteration: 1401/277000 \t Elapsed time: 318.83 \t Loss:8.49183\n",
      "Iteration: 1402/277000 \t Elapsed time: 319.00 \t Loss:8.48048\n",
      "Iteration: 1403/277000 \t Elapsed time: 319.21 \t Loss:8.46801\n",
      "Iteration: 1404/277000 \t Elapsed time: 319.36 \t Loss:8.45678\n",
      "Iteration: 1405/277000 \t Elapsed time: 319.67 \t Loss:8.44545\n",
      "Iteration: 1406/277000 \t Elapsed time: 319.87 \t Loss:8.43325\n",
      "Iteration: 1407/277000 \t Elapsed time: 320.06 \t Loss:8.42187\n",
      "Iteration: 1408/277000 \t Elapsed time: 320.29 \t Loss:8.41017\n",
      "Iteration: 1409/277000 \t Elapsed time: 320.51 \t Loss:8.39871\n",
      "Iteration: 1410/277000 \t Elapsed time: 320.68 \t Loss:8.38746\n",
      "Iteration: 1411/277000 \t Elapsed time: 320.83 \t Loss:8.37570\n",
      "Iteration: 1412/277000 \t Elapsed time: 320.99 \t Loss:8.36390\n",
      "Iteration: 1413/277000 \t Elapsed time: 321.13 \t Loss:8.35239\n",
      "Iteration: 1414/277000 \t Elapsed time: 321.38 \t Loss:8.34064\n",
      "Iteration: 1415/277000 \t Elapsed time: 321.63 \t Loss:8.32928\n",
      "Iteration: 1416/277000 \t Elapsed time: 321.82 \t Loss:8.31774\n",
      "Iteration: 1417/277000 \t Elapsed time: 321.98 \t Loss:8.30628\n",
      "Iteration: 1418/277000 \t Elapsed time: 322.25 \t Loss:8.29476\n",
      "Iteration: 1419/277000 \t Elapsed time: 322.45 \t Loss:8.28335\n",
      "Iteration: 1420/277000 \t Elapsed time: 322.59 \t Loss:8.27168\n",
      "Iteration: 1421/277000 \t Elapsed time: 322.75 \t Loss:8.26026\n",
      "Iteration: 1422/277000 \t Elapsed time: 322.91 \t Loss:8.24857\n",
      "Iteration: 1423/277000 \t Elapsed time: 323.32 \t Loss:8.23717\n",
      "Iteration: 1424/277000 \t Elapsed time: 323.49 \t Loss:8.22571\n",
      "Iteration: 1425/277000 \t Elapsed time: 323.66 \t Loss:8.21426\n",
      "Iteration: 1426/277000 \t Elapsed time: 323.80 \t Loss:8.20308\n",
      "Iteration: 1427/277000 \t Elapsed time: 324.01 \t Loss:8.19168\n",
      "Iteration: 1428/277000 \t Elapsed time: 324.28 \t Loss:8.18075\n",
      "Iteration: 1429/277000 \t Elapsed time: 324.44 \t Loss:8.16927\n",
      "Iteration: 1430/277000 \t Elapsed time: 324.67 \t Loss:8.15715\n",
      "Iteration: 1431/277000 \t Elapsed time: 324.81 \t Loss:8.14593\n",
      "Iteration: 1432/277000 \t Elapsed time: 325.00 \t Loss:8.13435\n",
      "Iteration: 1433/277000 \t Elapsed time: 325.56 \t Loss:8.12298\n",
      "Iteration: 1434/277000 \t Elapsed time: 326.19 \t Loss:8.11148\n",
      "Iteration: 1435/277000 \t Elapsed time: 326.36 \t Loss:8.10011\n",
      "Iteration: 1436/277000 \t Elapsed time: 326.54 \t Loss:8.08858\n",
      "Iteration: 1437/277000 \t Elapsed time: 326.68 \t Loss:8.07729\n",
      "Iteration: 1438/277000 \t Elapsed time: 326.83 \t Loss:8.06576\n",
      "Iteration: 1439/277000 \t Elapsed time: 326.97 \t Loss:8.05456\n",
      "Iteration: 1440/277000 \t Elapsed time: 327.18 \t Loss:8.04301\n",
      "Iteration: 1441/277000 \t Elapsed time: 327.37 \t Loss:8.03174\n",
      "Iteration: 1442/277000 \t Elapsed time: 328.92 \t Loss:8.02030\n",
      "Iteration: 1443/277000 \t Elapsed time: 329.13 \t Loss:8.00914\n",
      "Iteration: 1444/277000 \t Elapsed time: 329.26 \t Loss:7.99744\n",
      "Iteration: 1445/277000 \t Elapsed time: 329.52 \t Loss:7.98631\n",
      "Iteration: 1446/277000 \t Elapsed time: 329.72 \t Loss:7.97475\n",
      "Iteration: 1447/277000 \t Elapsed time: 329.87 \t Loss:7.96359\n",
      "Iteration: 1448/277000 \t Elapsed time: 330.01 \t Loss:7.95205\n",
      "Iteration: 1449/277000 \t Elapsed time: 330.16 \t Loss:7.94093\n",
      "Iteration: 1450/277000 \t Elapsed time: 330.31 \t Loss:7.92962\n",
      "Iteration: 1451/277000 \t Elapsed time: 330.45 \t Loss:7.91831\n",
      "Iteration: 1452/277000 \t Elapsed time: 330.61 \t Loss:7.90692\n",
      "Iteration: 1453/277000 \t Elapsed time: 330.76 \t Loss:7.89557\n",
      "Iteration: 1454/277000 \t Elapsed time: 330.93 \t Loss:7.88437\n",
      "Iteration: 1455/277000 \t Elapsed time: 331.09 \t Loss:7.87340\n",
      "Iteration: 1456/277000 \t Elapsed time: 331.25 \t Loss:7.86200\n",
      "Iteration: 1457/277000 \t Elapsed time: 331.45 \t Loss:7.85059\n",
      "Iteration: 1458/277000 \t Elapsed time: 331.83 \t Loss:7.83903\n",
      "Iteration: 1459/277000 \t Elapsed time: 332.04 \t Loss:7.82782\n",
      "Iteration: 1460/277000 \t Elapsed time: 332.19 \t Loss:7.81658\n",
      "Iteration: 1461/277000 \t Elapsed time: 332.38 \t Loss:7.80565\n",
      "Iteration: 1462/277000 \t Elapsed time: 332.54 \t Loss:7.79434\n",
      "Iteration: 1463/277000 \t Elapsed time: 332.67 \t Loss:7.78293\n",
      "Iteration: 1464/277000 \t Elapsed time: 332.81 \t Loss:7.77180\n",
      "Iteration: 1465/277000 \t Elapsed time: 332.96 \t Loss:7.76028\n",
      "Iteration: 1466/277000 \t Elapsed time: 333.11 \t Loss:7.74926\n",
      "Iteration: 1467/277000 \t Elapsed time: 333.25 \t Loss:7.73845\n",
      "Iteration: 1468/277000 \t Elapsed time: 333.41 \t Loss:7.72767\n",
      "Iteration: 1469/277000 \t Elapsed time: 333.55 \t Loss:7.71698\n",
      "Iteration: 1470/277000 \t Elapsed time: 333.72 \t Loss:7.70484\n",
      "Iteration: 1471/277000 \t Elapsed time: 333.97 \t Loss:7.69307\n",
      "Iteration: 1472/277000 \t Elapsed time: 334.12 \t Loss:7.68236\n",
      "Iteration: 1473/277000 \t Elapsed time: 334.27 \t Loss:7.67152\n",
      "Iteration: 1474/277000 \t Elapsed time: 334.42 \t Loss:7.66038\n",
      "Iteration: 1475/277000 \t Elapsed time: 334.59 \t Loss:7.64864\n",
      "Iteration: 1476/277000 \t Elapsed time: 334.74 \t Loss:7.63780\n",
      "Iteration: 1477/277000 \t Elapsed time: 335.04 \t Loss:7.62743\n",
      "Iteration: 1478/277000 \t Elapsed time: 335.18 \t Loss:7.61567\n",
      "Iteration: 1479/277000 \t Elapsed time: 335.34 \t Loss:7.60403\n",
      "Iteration: 1480/277000 \t Elapsed time: 335.50 \t Loss:7.59357\n",
      "Iteration: 1481/277000 \t Elapsed time: 335.65 \t Loss:7.58249\n",
      "Iteration: 1482/277000 \t Elapsed time: 335.79 \t Loss:7.57066\n",
      "Iteration: 1483/277000 \t Elapsed time: 335.94 \t Loss:7.55947\n",
      "Iteration: 1484/277000 \t Elapsed time: 336.10 \t Loss:7.54875\n",
      "Iteration: 1485/277000 \t Elapsed time: 336.25 \t Loss:7.53809\n",
      "Iteration: 1486/277000 \t Elapsed time: 336.39 \t Loss:7.52685\n",
      "Iteration: 1487/277000 \t Elapsed time: 336.54 \t Loss:7.51522\n",
      "Iteration: 1488/277000 \t Elapsed time: 336.69 \t Loss:7.50366\n",
      "Iteration: 1489/277000 \t Elapsed time: 336.84 \t Loss:7.49307\n",
      "Iteration: 1490/277000 \t Elapsed time: 336.97 \t Loss:7.48180\n",
      "Iteration: 1491/277000 \t Elapsed time: 337.12 \t Loss:7.47059\n",
      "Iteration: 1492/277000 \t Elapsed time: 337.27 \t Loss:7.45986\n",
      "Iteration: 1493/277000 \t Elapsed time: 337.41 \t Loss:7.44893\n",
      "Iteration: 1494/277000 \t Elapsed time: 337.57 \t Loss:7.43740\n",
      "Iteration: 1495/277000 \t Elapsed time: 337.71 \t Loss:7.42621\n",
      "Iteration: 1496/277000 \t Elapsed time: 337.87 \t Loss:7.41533\n",
      "Iteration: 1497/277000 \t Elapsed time: 338.11 \t Loss:7.40433\n",
      "Iteration: 1498/277000 \t Elapsed time: 338.26 \t Loss:7.39351\n",
      "Iteration: 1499/277000 \t Elapsed time: 338.40 \t Loss:7.38361\n",
      "Iteration: 1500/277000 \t Elapsed time: 338.62 \t Loss:7.37344\n",
      "Iteration: 1501/277000 \t Elapsed time: 338.82 \t Loss:7.36238\n",
      "Iteration: 1502/277000 \t Elapsed time: 338.97 \t Loss:7.34917\n",
      "Iteration: 1503/277000 \t Elapsed time: 339.19 \t Loss:7.33851\n",
      "Iteration: 1504/277000 \t Elapsed time: 339.50 \t Loss:7.32873\n",
      "Iteration: 1505/277000 \t Elapsed time: 339.66 \t Loss:7.31633\n",
      "Iteration: 1506/277000 \t Elapsed time: 339.84 \t Loss:7.30508\n",
      "Iteration: 1507/277000 \t Elapsed time: 340.00 \t Loss:7.29488\n",
      "Iteration: 1508/277000 \t Elapsed time: 340.17 \t Loss:7.28363\n",
      "Iteration: 1509/277000 \t Elapsed time: 340.32 \t Loss:7.27162\n",
      "Iteration: 1510/277000 \t Elapsed time: 340.48 \t Loss:7.26109\n",
      "Iteration: 1511/277000 \t Elapsed time: 340.67 \t Loss:7.24987\n",
      "Iteration: 1512/277000 \t Elapsed time: 340.81 \t Loss:7.23884\n",
      "Iteration: 1513/277000 \t Elapsed time: 340.96 \t Loss:7.22785\n",
      "Iteration: 1514/277000 \t Elapsed time: 341.29 \t Loss:7.21678\n",
      "Iteration: 1515/277000 \t Elapsed time: 341.49 \t Loss:7.20584\n",
      "Iteration: 1516/277000 \t Elapsed time: 341.86 \t Loss:7.19501\n",
      "Iteration: 1517/277000 \t Elapsed time: 342.07 \t Loss:7.18386\n",
      "Iteration: 1518/277000 \t Elapsed time: 342.24 \t Loss:7.17294\n",
      "Iteration: 1519/277000 \t Elapsed time: 342.42 \t Loss:7.16213\n",
      "Iteration: 1520/277000 \t Elapsed time: 342.57 \t Loss:7.15103\n",
      "Iteration: 1521/277000 \t Elapsed time: 343.09 \t Loss:7.14020\n",
      "Iteration: 1522/277000 \t Elapsed time: 343.25 \t Loss:7.12921\n",
      "Iteration: 1523/277000 \t Elapsed time: 343.55 \t Loss:7.11822\n",
      "Iteration: 1524/277000 \t Elapsed time: 343.70 \t Loss:7.10735\n",
      "Iteration: 1525/277000 \t Elapsed time: 343.84 \t Loss:7.09647\n",
      "Iteration: 1526/277000 \t Elapsed time: 344.26 \t Loss:7.08560\n",
      "Iteration: 1527/277000 \t Elapsed time: 344.47 \t Loss:7.07465\n",
      "Iteration: 1528/277000 \t Elapsed time: 344.60 \t Loss:7.06381\n",
      "Iteration: 1529/277000 \t Elapsed time: 344.76 \t Loss:7.05288\n",
      "Iteration: 1530/277000 \t Elapsed time: 344.91 \t Loss:7.04226\n",
      "Iteration: 1531/277000 \t Elapsed time: 345.15 \t Loss:7.03113\n",
      "Iteration: 1532/277000 \t Elapsed time: 345.29 \t Loss:7.02025\n",
      "Iteration: 1533/277000 \t Elapsed time: 345.45 \t Loss:7.00929\n",
      "Iteration: 1534/277000 \t Elapsed time: 345.59 \t Loss:6.99846\n",
      "Iteration: 1535/277000 \t Elapsed time: 345.78 \t Loss:6.98782\n",
      "Iteration: 1536/277000 \t Elapsed time: 345.96 \t Loss:6.97676\n",
      "Iteration: 1537/277000 \t Elapsed time: 346.10 \t Loss:6.96590\n",
      "Iteration: 1538/277000 \t Elapsed time: 346.26 \t Loss:6.95489\n",
      "Iteration: 1539/277000 \t Elapsed time: 346.42 \t Loss:6.94419\n",
      "Iteration: 1540/277000 \t Elapsed time: 346.57 \t Loss:6.93337\n",
      "Iteration: 1541/277000 \t Elapsed time: 346.85 \t Loss:6.92262\n",
      "Iteration: 1542/277000 \t Elapsed time: 347.10 \t Loss:6.91182\n",
      "Iteration: 1543/277000 \t Elapsed time: 347.25 \t Loss:6.90090\n",
      "Iteration: 1544/277000 \t Elapsed time: 347.43 \t Loss:6.89010\n",
      "Iteration: 1545/277000 \t Elapsed time: 347.59 \t Loss:6.87948\n",
      "Iteration: 1546/277000 \t Elapsed time: 347.77 \t Loss:6.86852\n",
      "Iteration: 1547/277000 \t Elapsed time: 348.21 \t Loss:6.85768\n",
      "Iteration: 1548/277000 \t Elapsed time: 348.39 \t Loss:6.84709\n",
      "Iteration: 1549/277000 \t Elapsed time: 348.56 \t Loss:6.83617\n",
      "Iteration: 1550/277000 \t Elapsed time: 348.72 \t Loss:6.82551\n",
      "Iteration: 1551/277000 \t Elapsed time: 348.86 \t Loss:6.81517\n",
      "Iteration: 1552/277000 \t Elapsed time: 349.11 \t Loss:6.80406\n",
      "Iteration: 1553/277000 \t Elapsed time: 349.29 \t Loss:6.79319\n",
      "Iteration: 1554/277000 \t Elapsed time: 349.53 \t Loss:6.78246\n",
      "Iteration: 1555/277000 \t Elapsed time: 349.69 \t Loss:6.77205\n",
      "Iteration: 1556/277000 \t Elapsed time: 349.90 \t Loss:6.76091\n",
      "Iteration: 1557/277000 \t Elapsed time: 352.24 \t Loss:6.75032\n",
      "Iteration: 1558/277000 \t Elapsed time: 352.41 \t Loss:6.73942\n",
      "Iteration: 1559/277000 \t Elapsed time: 352.56 \t Loss:6.72852\n",
      "Iteration: 1560/277000 \t Elapsed time: 352.71 \t Loss:6.71768\n",
      "Iteration: 1561/277000 \t Elapsed time: 352.87 \t Loss:6.70717\n",
      "Iteration: 1562/277000 \t Elapsed time: 353.07 \t Loss:6.69640\n",
      "Iteration: 1563/277000 \t Elapsed time: 353.76 \t Loss:6.68652\n",
      "Iteration: 1564/277000 \t Elapsed time: 353.96 \t Loss:6.67772\n",
      "Iteration: 1565/277000 \t Elapsed time: 354.13 \t Loss:6.66723\n",
      "Iteration: 1566/277000 \t Elapsed time: 354.27 \t Loss:6.65509\n",
      "Iteration: 1567/277000 \t Elapsed time: 354.48 \t Loss:6.64344\n",
      "Iteration: 1568/277000 \t Elapsed time: 354.70 \t Loss:6.63380\n",
      "Iteration: 1569/277000 \t Elapsed time: 354.96 \t Loss:6.62342\n",
      "Iteration: 1570/277000 \t Elapsed time: 355.18 \t Loss:6.61214\n",
      "Iteration: 1571/277000 \t Elapsed time: 355.37 \t Loss:6.60094\n",
      "Iteration: 1572/277000 \t Elapsed time: 355.52 \t Loss:6.59064\n",
      "Iteration: 1573/277000 \t Elapsed time: 355.66 \t Loss:6.57967\n",
      "Iteration: 1574/277000 \t Elapsed time: 355.82 \t Loss:6.56889\n",
      "Iteration: 1575/277000 \t Elapsed time: 356.00 \t Loss:6.55805\n",
      "Iteration: 1576/277000 \t Elapsed time: 356.15 \t Loss:6.54800\n",
      "Iteration: 1577/277000 \t Elapsed time: 356.30 \t Loss:6.53672\n",
      "Iteration: 1578/277000 \t Elapsed time: 356.45 \t Loss:6.52653\n",
      "Iteration: 1579/277000 \t Elapsed time: 356.60 \t Loss:6.51677\n",
      "Iteration: 1580/277000 \t Elapsed time: 356.76 \t Loss:6.50589\n",
      "Iteration: 1581/277000 \t Elapsed time: 356.93 \t Loss:6.49481\n",
      "Iteration: 1582/277000 \t Elapsed time: 357.07 \t Loss:6.48447\n",
      "Iteration: 1583/277000 \t Elapsed time: 357.23 \t Loss:6.47322\n",
      "Iteration: 1584/277000 \t Elapsed time: 357.38 \t Loss:6.46351\n",
      "Iteration: 1585/277000 \t Elapsed time: 357.53 \t Loss:6.45368\n",
      "Iteration: 1586/277000 \t Elapsed time: 357.70 \t Loss:6.44230\n",
      "Iteration: 1587/277000 \t Elapsed time: 357.85 \t Loss:6.43174\n",
      "Iteration: 1588/277000 \t Elapsed time: 358.00 \t Loss:6.42164\n",
      "Iteration: 1589/277000 \t Elapsed time: 358.15 \t Loss:6.41008\n",
      "Iteration: 1590/277000 \t Elapsed time: 358.30 \t Loss:6.39990\n",
      "Iteration: 1591/277000 \t Elapsed time: 358.44 \t Loss:6.38942\n",
      "Iteration: 1592/277000 \t Elapsed time: 358.59 \t Loss:6.37850\n",
      "Iteration: 1593/277000 \t Elapsed time: 358.75 \t Loss:6.36845\n",
      "Iteration: 1594/277000 \t Elapsed time: 358.90 \t Loss:6.35824\n",
      "Iteration: 1595/277000 \t Elapsed time: 359.06 \t Loss:6.34689\n",
      "Iteration: 1596/277000 \t Elapsed time: 359.25 \t Loss:6.33696\n",
      "Iteration: 1597/277000 \t Elapsed time: 359.39 \t Loss:6.32733\n",
      "Iteration: 1598/277000 \t Elapsed time: 359.55 \t Loss:6.32073\n",
      "Iteration: 1599/277000 \t Elapsed time: 359.68 \t Loss:6.31962\n",
      "Iteration: 1600/277000 \t Elapsed time: 359.85 \t Loss:6.30050\n",
      "Iteration: 1601/277000 \t Elapsed time: 359.99 \t Loss:6.28548\n",
      "Iteration: 1602/277000 \t Elapsed time: 360.14 \t Loss:6.28034\n",
      "Iteration: 1603/277000 \t Elapsed time: 360.31 \t Loss:6.26418\n",
      "Iteration: 1604/277000 \t Elapsed time: 360.48 \t Loss:6.25754\n",
      "Iteration: 1605/277000 \t Elapsed time: 360.63 \t Loss:6.24578\n",
      "Iteration: 1606/277000 \t Elapsed time: 360.82 \t Loss:6.23462\n",
      "Iteration: 1607/277000 \t Elapsed time: 360.96 \t Loss:6.22651\n",
      "Iteration: 1608/277000 \t Elapsed time: 361.15 \t Loss:6.21244\n",
      "Iteration: 1609/277000 \t Elapsed time: 361.31 \t Loss:6.20432\n",
      "Iteration: 1610/277000 \t Elapsed time: 361.48 \t Loss:6.19240\n",
      "Iteration: 1611/277000 \t Elapsed time: 361.66 \t Loss:6.18245\n",
      "Iteration: 1612/277000 \t Elapsed time: 361.88 \t Loss:6.17237\n",
      "Iteration: 1613/277000 \t Elapsed time: 362.03 \t Loss:6.16089\n",
      "Iteration: 1614/277000 \t Elapsed time: 362.20 \t Loss:6.15144\n",
      "Iteration: 1615/277000 \t Elapsed time: 362.34 \t Loss:6.14045\n",
      "Iteration: 1616/277000 \t Elapsed time: 362.58 \t Loss:6.13039\n",
      "Iteration: 1617/277000 \t Elapsed time: 362.78 \t Loss:6.12062\n",
      "Iteration: 1618/277000 \t Elapsed time: 362.96 \t Loss:6.10951\n",
      "Iteration: 1619/277000 \t Elapsed time: 363.22 \t Loss:6.09956\n",
      "Iteration: 1620/277000 \t Elapsed time: 363.62 \t Loss:6.08950\n",
      "Iteration: 1621/277000 \t Elapsed time: 363.84 \t Loss:6.07899\n",
      "Iteration: 1622/277000 \t Elapsed time: 364.06 \t Loss:6.06819\n",
      "Iteration: 1623/277000 \t Elapsed time: 364.23 \t Loss:6.05814\n",
      "Iteration: 1624/277000 \t Elapsed time: 364.37 \t Loss:6.04759\n",
      "Iteration: 1625/277000 \t Elapsed time: 364.53 \t Loss:6.03722\n",
      "Iteration: 1626/277000 \t Elapsed time: 364.69 \t Loss:6.02705\n",
      "Iteration: 1627/277000 \t Elapsed time: 364.85 \t Loss:6.01687\n",
      "Iteration: 1628/277000 \t Elapsed time: 365.00 \t Loss:6.00634\n",
      "Iteration: 1629/277000 \t Elapsed time: 365.24 \t Loss:5.99680\n",
      "Iteration: 1630/277000 \t Elapsed time: 365.43 \t Loss:5.98627\n",
      "Iteration: 1631/277000 \t Elapsed time: 365.60 \t Loss:5.97595\n",
      "Iteration: 1632/277000 \t Elapsed time: 365.75 \t Loss:5.96617\n",
      "Iteration: 1633/277000 \t Elapsed time: 365.95 \t Loss:5.95539\n",
      "Iteration: 1634/277000 \t Elapsed time: 366.19 \t Loss:5.94531\n",
      "Iteration: 1635/277000 \t Elapsed time: 366.34 \t Loss:5.93552\n",
      "Iteration: 1636/277000 \t Elapsed time: 366.49 \t Loss:5.92539\n",
      "Iteration: 1637/277000 \t Elapsed time: 366.63 \t Loss:5.91521\n",
      "Iteration: 1638/277000 \t Elapsed time: 366.78 \t Loss:5.90548\n",
      "Iteration: 1639/277000 \t Elapsed time: 367.10 \t Loss:5.89671\n",
      "Iteration: 1640/277000 \t Elapsed time: 367.33 \t Loss:5.88666\n",
      "Iteration: 1641/277000 \t Elapsed time: 367.50 \t Loss:5.87473\n",
      "Iteration: 1642/277000 \t Elapsed time: 367.87 \t Loss:5.86482\n",
      "Iteration: 1643/277000 \t Elapsed time: 368.02 \t Loss:5.85434\n",
      "Iteration: 1644/277000 \t Elapsed time: 368.17 \t Loss:5.84475\n",
      "Iteration: 1645/277000 \t Elapsed time: 368.65 \t Loss:5.83489\n",
      "Iteration: 1646/277000 \t Elapsed time: 368.85 \t Loss:5.82510\n",
      "Iteration: 1647/277000 \t Elapsed time: 369.03 \t Loss:5.81477\n",
      "Iteration: 1648/277000 \t Elapsed time: 369.27 \t Loss:5.80447\n",
      "Iteration: 1649/277000 \t Elapsed time: 369.60 \t Loss:5.79325\n",
      "Iteration: 1650/277000 \t Elapsed time: 369.85 \t Loss:5.78312\n",
      "Iteration: 1651/277000 \t Elapsed time: 370.00 \t Loss:5.77333\n",
      "Iteration: 1652/277000 \t Elapsed time: 370.15 \t Loss:5.76342\n",
      "Iteration: 1653/277000 \t Elapsed time: 370.83 \t Loss:5.75313\n",
      "Iteration: 1654/277000 \t Elapsed time: 371.00 \t Loss:5.74284\n",
      "Iteration: 1655/277000 \t Elapsed time: 371.16 \t Loss:5.73350\n",
      "Iteration: 1656/277000 \t Elapsed time: 371.31 \t Loss:5.72378\n",
      "Iteration: 1657/277000 \t Elapsed time: 371.45 \t Loss:5.71736\n",
      "Iteration: 1658/277000 \t Elapsed time: 371.60 \t Loss:5.71250\n",
      "Iteration: 1659/277000 \t Elapsed time: 371.75 \t Loss:5.70768\n",
      "Iteration: 1660/277000 \t Elapsed time: 371.92 \t Loss:5.69595\n",
      "Iteration: 1661/277000 \t Elapsed time: 372.12 \t Loss:5.67609\n",
      "Iteration: 1662/277000 \t Elapsed time: 372.38 \t Loss:5.66953\n",
      "Iteration: 1663/277000 \t Elapsed time: 372.61 \t Loss:5.66803\n",
      "Iteration: 1664/277000 \t Elapsed time: 372.82 \t Loss:5.64662\n",
      "Iteration: 1665/277000 \t Elapsed time: 372.99 \t Loss:5.63577\n",
      "Iteration: 1666/277000 \t Elapsed time: 373.17 \t Loss:5.62799\n",
      "Iteration: 1667/277000 \t Elapsed time: 373.30 \t Loss:5.61419\n",
      "Iteration: 1668/277000 \t Elapsed time: 373.87 \t Loss:5.60917\n",
      "Iteration: 1669/277000 \t Elapsed time: 374.05 \t Loss:5.59469\n",
      "Iteration: 1670/277000 \t Elapsed time: 374.24 \t Loss:5.58716\n",
      "Iteration: 1671/277000 \t Elapsed time: 374.41 \t Loss:5.58006\n",
      "Iteration: 1672/277000 \t Elapsed time: 374.60 \t Loss:5.56452\n",
      "Iteration: 1673/277000 \t Elapsed time: 374.81 \t Loss:5.55738\n",
      "Iteration: 1674/277000 \t Elapsed time: 374.99 \t Loss:5.54612\n",
      "Iteration: 1675/277000 \t Elapsed time: 375.18 \t Loss:5.53625\n",
      "Iteration: 1676/277000 \t Elapsed time: 375.36 \t Loss:5.53125\n",
      "Iteration: 1677/277000 \t Elapsed time: 375.54 \t Loss:5.51676\n",
      "Iteration: 1678/277000 \t Elapsed time: 375.70 \t Loss:5.50797\n",
      "Iteration: 1679/277000 \t Elapsed time: 375.87 \t Loss:5.49745\n",
      "Iteration: 1680/277000 \t Elapsed time: 376.02 \t Loss:5.48760\n",
      "Iteration: 1681/277000 \t Elapsed time: 376.20 \t Loss:5.47954\n",
      "Iteration: 1682/277000 \t Elapsed time: 376.36 \t Loss:5.46730\n",
      "Iteration: 1683/277000 \t Elapsed time: 376.56 \t Loss:5.45987\n",
      "Iteration: 1684/277000 \t Elapsed time: 376.75 \t Loss:5.44899\n",
      "Iteration: 1685/277000 \t Elapsed time: 377.01 \t Loss:5.43759\n",
      "Iteration: 1686/277000 \t Elapsed time: 377.32 \t Loss:5.42865\n",
      "Iteration: 1687/277000 \t Elapsed time: 377.48 \t Loss:5.42059\n",
      "Iteration: 1688/277000 \t Elapsed time: 377.65 \t Loss:5.41157\n",
      "Iteration: 1689/277000 \t Elapsed time: 377.88 \t Loss:5.40221\n",
      "Iteration: 1690/277000 \t Elapsed time: 378.03 \t Loss:5.38941\n",
      "Iteration: 1691/277000 \t Elapsed time: 379.37 \t Loss:5.37976\n",
      "Iteration: 1692/277000 \t Elapsed time: 379.52 \t Loss:5.37020\n",
      "Iteration: 1693/277000 \t Elapsed time: 379.66 \t Loss:5.36087\n",
      "Iteration: 1694/277000 \t Elapsed time: 379.85 \t Loss:5.35051\n",
      "Iteration: 1695/277000 \t Elapsed time: 380.02 \t Loss:5.34139\n",
      "Iteration: 1696/277000 \t Elapsed time: 380.16 \t Loss:5.33217\n",
      "Iteration: 1697/277000 \t Elapsed time: 380.35 \t Loss:5.32117\n",
      "Iteration: 1698/277000 \t Elapsed time: 380.50 \t Loss:5.31274\n",
      "Iteration: 1699/277000 \t Elapsed time: 380.65 \t Loss:5.30306\n",
      "Iteration: 1700/277000 \t Elapsed time: 380.85 \t Loss:5.29436\n",
      "Iteration: 1701/277000 \t Elapsed time: 381.00 \t Loss:5.28380\n",
      "Iteration: 1702/277000 \t Elapsed time: 381.25 \t Loss:5.27277\n",
      "Iteration: 1703/277000 \t Elapsed time: 381.41 \t Loss:5.26415\n",
      "Iteration: 1704/277000 \t Elapsed time: 381.55 \t Loss:5.25336\n",
      "Iteration: 1705/277000 \t Elapsed time: 381.70 \t Loss:5.24601\n",
      "Iteration: 1706/277000 \t Elapsed time: 382.00 \t Loss:5.23571\n",
      "Iteration: 1707/277000 \t Elapsed time: 382.16 \t Loss:5.22677\n",
      "Iteration: 1708/277000 \t Elapsed time: 382.30 \t Loss:5.21549\n",
      "Iteration: 1709/277000 \t Elapsed time: 382.46 \t Loss:5.20774\n",
      "Iteration: 1710/277000 \t Elapsed time: 382.62 \t Loss:5.19904\n",
      "Iteration: 1711/277000 \t Elapsed time: 382.84 \t Loss:5.19015\n",
      "Iteration: 1712/277000 \t Elapsed time: 383.05 \t Loss:5.18457\n",
      "Iteration: 1713/277000 \t Elapsed time: 383.20 \t Loss:5.17568\n",
      "Iteration: 1714/277000 \t Elapsed time: 383.36 \t Loss:5.15897\n",
      "Iteration: 1715/277000 \t Elapsed time: 383.56 \t Loss:5.15114\n",
      "Iteration: 1716/277000 \t Elapsed time: 383.72 \t Loss:5.14427\n",
      "Iteration: 1717/277000 \t Elapsed time: 383.87 \t Loss:5.13827\n",
      "Iteration: 1718/277000 \t Elapsed time: 384.17 \t Loss:5.12639\n",
      "Iteration: 1719/277000 \t Elapsed time: 384.31 \t Loss:5.10984\n",
      "Iteration: 1720/277000 \t Elapsed time: 384.49 \t Loss:5.10280\n",
      "Iteration: 1721/277000 \t Elapsed time: 384.64 \t Loss:5.09438\n",
      "Iteration: 1722/277000 \t Elapsed time: 384.78 \t Loss:5.08179\n",
      "Iteration: 1723/277000 \t Elapsed time: 385.03 \t Loss:5.07401\n",
      "Iteration: 1724/277000 \t Elapsed time: 385.24 \t Loss:5.06698\n",
      "Iteration: 1725/277000 \t Elapsed time: 385.44 \t Loss:5.05481\n",
      "Iteration: 1726/277000 \t Elapsed time: 385.63 \t Loss:5.04517\n",
      "Iteration: 1727/277000 \t Elapsed time: 385.77 \t Loss:5.03972\n",
      "Iteration: 1728/277000 \t Elapsed time: 385.92 \t Loss:5.02795\n",
      "Iteration: 1729/277000 \t Elapsed time: 386.08 \t Loss:5.01803\n",
      "Iteration: 1730/277000 \t Elapsed time: 386.21 \t Loss:5.01007\n",
      "Iteration: 1731/277000 \t Elapsed time: 386.36 \t Loss:5.00008\n",
      "Iteration: 1732/277000 \t Elapsed time: 386.54 \t Loss:4.98841\n",
      "Iteration: 1733/277000 \t Elapsed time: 386.76 \t Loss:4.98038\n",
      "Iteration: 1734/277000 \t Elapsed time: 387.11 \t Loss:4.97254\n",
      "Iteration: 1735/277000 \t Elapsed time: 387.28 \t Loss:4.96071\n",
      "Iteration: 1736/277000 \t Elapsed time: 387.46 \t Loss:4.95396\n",
      "Iteration: 1737/277000 \t Elapsed time: 387.62 \t Loss:4.94190\n",
      "Iteration: 1738/277000 \t Elapsed time: 387.76 \t Loss:4.93424\n",
      "Iteration: 1739/277000 \t Elapsed time: 387.94 \t Loss:4.92922\n",
      "Iteration: 1740/277000 \t Elapsed time: 388.09 \t Loss:4.92303\n",
      "Iteration: 1741/277000 \t Elapsed time: 388.25 \t Loss:4.92064\n",
      "Iteration: 1742/277000 \t Elapsed time: 388.55 \t Loss:4.90278\n",
      "Iteration: 1743/277000 \t Elapsed time: 388.78 \t Loss:4.88702\n",
      "Iteration: 1744/277000 \t Elapsed time: 388.92 \t Loss:4.88750\n",
      "Iteration: 1745/277000 \t Elapsed time: 389.07 \t Loss:4.87491\n",
      "Iteration: 1746/277000 \t Elapsed time: 389.40 \t Loss:4.86047\n",
      "Iteration: 1747/277000 \t Elapsed time: 389.58 \t Loss:4.85137\n",
      "Iteration: 1748/277000 \t Elapsed time: 389.74 \t Loss:4.84573\n",
      "Iteration: 1749/277000 \t Elapsed time: 389.88 \t Loss:4.83307\n",
      "Iteration: 1750/277000 \t Elapsed time: 390.03 \t Loss:4.82173\n",
      "Iteration: 1751/277000 \t Elapsed time: 390.23 \t Loss:4.81474\n",
      "Iteration: 1752/277000 \t Elapsed time: 390.37 \t Loss:4.80827\n",
      "Iteration: 1753/277000 \t Elapsed time: 391.40 \t Loss:4.79930\n",
      "Iteration: 1754/277000 \t Elapsed time: 391.54 \t Loss:4.78765\n",
      "Iteration: 1755/277000 \t Elapsed time: 391.73 \t Loss:4.78081\n",
      "Iteration: 1756/277000 \t Elapsed time: 391.92 \t Loss:4.76805\n",
      "Iteration: 1757/277000 \t Elapsed time: 392.08 \t Loss:4.76235\n",
      "Iteration: 1758/277000 \t Elapsed time: 392.29 \t Loss:4.75572\n",
      "Iteration: 1759/277000 \t Elapsed time: 392.48 \t Loss:4.74431\n",
      "Iteration: 1760/277000 \t Elapsed time: 392.67 \t Loss:4.73789\n",
      "Iteration: 1761/277000 \t Elapsed time: 392.83 \t Loss:4.72731\n",
      "Iteration: 1762/277000 \t Elapsed time: 393.03 \t Loss:4.72006\n",
      "Iteration: 1763/277000 \t Elapsed time: 393.34 \t Loss:4.70738\n",
      "Iteration: 1764/277000 \t Elapsed time: 393.52 \t Loss:4.70249\n",
      "Iteration: 1765/277000 \t Elapsed time: 393.79 \t Loss:4.69848\n",
      "Iteration: 1766/277000 \t Elapsed time: 393.99 \t Loss:4.67811\n",
      "Iteration: 1767/277000 \t Elapsed time: 394.15 \t Loss:4.68128\n",
      "Iteration: 1768/277000 \t Elapsed time: 394.48 \t Loss:4.67983\n",
      "Iteration: 1769/277000 \t Elapsed time: 394.65 \t Loss:4.65381\n",
      "Iteration: 1770/277000 \t Elapsed time: 394.82 \t Loss:4.65133\n",
      "Iteration: 1771/277000 \t Elapsed time: 394.98 \t Loss:4.63857\n",
      "Iteration: 1772/277000 \t Elapsed time: 395.13 \t Loss:4.62648\n",
      "Iteration: 1773/277000 \t Elapsed time: 395.40 \t Loss:4.61967\n",
      "Iteration: 1774/277000 \t Elapsed time: 395.61 \t Loss:4.60483\n",
      "Iteration: 1775/277000 \t Elapsed time: 395.79 \t Loss:4.60232\n",
      "Iteration: 1776/277000 \t Elapsed time: 395.96 \t Loss:4.58925\n",
      "Iteration: 1777/277000 \t Elapsed time: 396.51 \t Loss:4.58988\n",
      "Iteration: 1778/277000 \t Elapsed time: 396.70 \t Loss:4.57839\n",
      "Iteration: 1779/277000 \t Elapsed time: 396.87 \t Loss:4.57249\n",
      "Iteration: 1780/277000 \t Elapsed time: 397.09 \t Loss:4.55742\n",
      "Iteration: 1781/277000 \t Elapsed time: 397.25 \t Loss:4.55757\n",
      "Iteration: 1782/277000 \t Elapsed time: 397.47 \t Loss:4.54157\n",
      "Iteration: 1783/277000 \t Elapsed time: 397.70 \t Loss:4.53394\n",
      "Iteration: 1784/277000 \t Elapsed time: 397.91 \t Loss:4.52274\n",
      "Iteration: 1785/277000 \t Elapsed time: 398.10 \t Loss:4.51015\n",
      "Iteration: 1786/277000 \t Elapsed time: 398.26 \t Loss:4.50280\n",
      "Iteration: 1787/277000 \t Elapsed time: 398.46 \t Loss:4.50034\n",
      "Iteration: 1788/277000 \t Elapsed time: 398.60 \t Loss:4.48101\n",
      "Iteration: 1789/277000 \t Elapsed time: 398.83 \t Loss:4.47512\n",
      "Iteration: 1790/277000 \t Elapsed time: 398.98 \t Loss:4.47257\n",
      "Iteration: 1791/277000 \t Elapsed time: 399.16 \t Loss:4.46354\n",
      "Iteration: 1792/277000 \t Elapsed time: 399.39 \t Loss:4.45525\n",
      "Iteration: 1793/277000 \t Elapsed time: 399.67 \t Loss:4.44142\n",
      "Iteration: 1794/277000 \t Elapsed time: 399.87 \t Loss:4.43061\n",
      "Iteration: 1795/277000 \t Elapsed time: 400.11 \t Loss:4.42248\n",
      "Iteration: 1796/277000 \t Elapsed time: 400.32 \t Loss:4.41578\n",
      "Iteration: 1797/277000 \t Elapsed time: 400.56 \t Loss:4.40600\n",
      "Iteration: 1798/277000 \t Elapsed time: 400.73 \t Loss:4.40580\n",
      "Iteration: 1799/277000 \t Elapsed time: 400.86 \t Loss:4.39157\n",
      "Iteration: 1800/277000 \t Elapsed time: 401.05 \t Loss:4.38031\n",
      "Iteration: 1801/277000 \t Elapsed time: 401.22 \t Loss:4.36900\n",
      "Iteration: 1802/277000 \t Elapsed time: 401.39 \t Loss:4.36447\n",
      "Iteration: 1803/277000 \t Elapsed time: 401.55 \t Loss:4.35607\n",
      "Iteration: 1804/277000 \t Elapsed time: 401.69 \t Loss:4.34698\n",
      "Iteration: 1805/277000 \t Elapsed time: 401.85 \t Loss:4.33929\n",
      "Iteration: 1806/277000 \t Elapsed time: 402.01 \t Loss:4.33641\n",
      "Iteration: 1807/277000 \t Elapsed time: 402.16 \t Loss:4.34956\n",
      "Iteration: 1808/277000 \t Elapsed time: 402.35 \t Loss:4.33653\n",
      "Iteration: 1809/277000 \t Elapsed time: 402.53 \t Loss:4.31608\n",
      "Iteration: 1810/277000 \t Elapsed time: 402.67 \t Loss:4.30111\n",
      "Iteration: 1811/277000 \t Elapsed time: 402.83 \t Loss:4.29068\n",
      "Iteration: 1812/277000 \t Elapsed time: 403.26 \t Loss:4.28359\n",
      "Iteration: 1813/277000 \t Elapsed time: 403.45 \t Loss:4.27332\n",
      "Iteration: 1814/277000 \t Elapsed time: 403.61 \t Loss:4.26217\n",
      "Iteration: 1815/277000 \t Elapsed time: 403.75 \t Loss:4.24861\n",
      "Iteration: 1816/277000 \t Elapsed time: 403.90 \t Loss:4.25111\n",
      "Iteration: 1817/277000 \t Elapsed time: 404.04 \t Loss:4.23214\n",
      "Iteration: 1818/277000 \t Elapsed time: 404.22 \t Loss:4.22562\n",
      "Iteration: 1819/277000 \t Elapsed time: 404.37 \t Loss:4.21391\n",
      "Iteration: 1820/277000 \t Elapsed time: 404.51 \t Loss:4.20592\n",
      "Iteration: 1821/277000 \t Elapsed time: 404.64 \t Loss:4.19581\n",
      "Iteration: 1822/277000 \t Elapsed time: 404.79 \t Loss:4.19005\n",
      "Iteration: 1823/277000 \t Elapsed time: 404.92 \t Loss:4.18153\n",
      "Iteration: 1824/277000 \t Elapsed time: 405.06 \t Loss:4.17322\n",
      "Iteration: 1825/277000 \t Elapsed time: 405.21 \t Loss:4.16218\n",
      "Iteration: 1826/277000 \t Elapsed time: 405.36 \t Loss:4.15799\n",
      "Iteration: 1827/277000 \t Elapsed time: 405.51 \t Loss:4.14648\n",
      "Iteration: 1828/277000 \t Elapsed time: 405.66 \t Loss:4.13717\n",
      "Iteration: 1829/277000 \t Elapsed time: 405.82 \t Loss:4.13115\n",
      "Iteration: 1830/277000 \t Elapsed time: 405.96 \t Loss:4.11912\n",
      "Iteration: 1831/277000 \t Elapsed time: 406.10 \t Loss:4.11747\n",
      "Iteration: 1832/277000 \t Elapsed time: 406.25 \t Loss:4.10569\n",
      "Iteration: 1833/277000 \t Elapsed time: 406.39 \t Loss:4.09839\n",
      "Iteration: 1834/277000 \t Elapsed time: 406.54 \t Loss:4.09868\n",
      "Iteration: 1835/277000 \t Elapsed time: 406.68 \t Loss:4.11112\n",
      "Iteration: 1836/277000 \t Elapsed time: 406.84 \t Loss:4.11105\n",
      "Iteration: 1837/277000 \t Elapsed time: 407.00 \t Loss:4.09462\n",
      "Iteration: 1838/277000 \t Elapsed time: 408.65 \t Loss:4.05944\n",
      "Iteration: 1839/277000 \t Elapsed time: 408.84 \t Loss:4.06962\n",
      "Iteration: 1840/277000 \t Elapsed time: 409.03 \t Loss:4.08297\n",
      "Iteration: 1841/277000 \t Elapsed time: 409.21 \t Loss:4.04136\n",
      "Iteration: 1842/277000 \t Elapsed time: 409.45 \t Loss:4.04038\n",
      "Iteration: 1843/277000 \t Elapsed time: 409.61 \t Loss:4.05049\n",
      "Iteration: 1844/277000 \t Elapsed time: 409.78 \t Loss:4.01761\n",
      "Iteration: 1845/277000 \t Elapsed time: 410.06 \t Loss:4.02501\n",
      "Iteration: 1846/277000 \t Elapsed time: 410.20 \t Loss:4.00025\n",
      "Iteration: 1847/277000 \t Elapsed time: 410.38 \t Loss:4.00300\n",
      "Iteration: 1848/277000 \t Elapsed time: 410.57 \t Loss:3.97517\n",
      "Iteration: 1849/277000 \t Elapsed time: 410.71 \t Loss:3.97440\n",
      "Iteration: 1850/277000 \t Elapsed time: 410.86 \t Loss:3.96470\n",
      "Iteration: 1851/277000 \t Elapsed time: 411.05 \t Loss:3.95279\n",
      "Iteration: 1852/277000 \t Elapsed time: 411.21 \t Loss:3.94409\n",
      "Iteration: 1853/277000 \t Elapsed time: 411.41 \t Loss:3.94090\n",
      "Iteration: 1854/277000 \t Elapsed time: 411.59 \t Loss:3.93197\n",
      "Iteration: 1855/277000 \t Elapsed time: 411.77 \t Loss:3.92450\n",
      "Iteration: 1856/277000 \t Elapsed time: 411.95 \t Loss:3.93467\n",
      "Iteration: 1857/277000 \t Elapsed time: 412.08 \t Loss:3.92058\n",
      "Iteration: 1858/277000 \t Elapsed time: 412.26 \t Loss:3.90903\n",
      "Iteration: 1859/277000 \t Elapsed time: 412.49 \t Loss:3.89745\n",
      "Iteration: 1860/277000 \t Elapsed time: 412.63 \t Loss:3.88401\n",
      "Iteration: 1861/277000 \t Elapsed time: 412.80 \t Loss:3.88356\n",
      "Iteration: 1862/277000 \t Elapsed time: 412.95 \t Loss:3.87107\n",
      "Iteration: 1863/277000 \t Elapsed time: 413.10 \t Loss:3.86402\n",
      "Iteration: 1864/277000 \t Elapsed time: 413.28 \t Loss:3.85441\n",
      "Iteration: 1865/277000 \t Elapsed time: 413.43 \t Loss:3.84498\n",
      "Iteration: 1866/277000 \t Elapsed time: 413.65 \t Loss:3.83461\n",
      "Iteration: 1867/277000 \t Elapsed time: 413.84 \t Loss:3.83175\n",
      "Iteration: 1868/277000 \t Elapsed time: 414.07 \t Loss:3.82462\n",
      "Iteration: 1869/277000 \t Elapsed time: 414.49 \t Loss:3.82049\n",
      "Iteration: 1870/277000 \t Elapsed time: 414.63 \t Loss:3.80072\n",
      "Iteration: 1871/277000 \t Elapsed time: 414.83 \t Loss:3.79973\n",
      "Iteration: 1872/277000 \t Elapsed time: 415.03 \t Loss:3.78955\n",
      "Iteration: 1873/277000 \t Elapsed time: 415.17 \t Loss:3.77496\n",
      "Iteration: 1874/277000 \t Elapsed time: 415.34 \t Loss:3.77578\n",
      "Iteration: 1875/277000 \t Elapsed time: 415.53 \t Loss:3.76464\n",
      "Iteration: 1876/277000 \t Elapsed time: 415.75 \t Loss:3.78195\n",
      "Iteration: 1877/277000 \t Elapsed time: 415.91 \t Loss:3.77119\n",
      "Iteration: 1878/277000 \t Elapsed time: 416.06 \t Loss:3.77130\n",
      "Iteration: 1879/277000 \t Elapsed time: 416.23 \t Loss:3.74391\n",
      "Iteration: 1880/277000 \t Elapsed time: 416.40 \t Loss:3.73295\n",
      "Iteration: 1881/277000 \t Elapsed time: 416.64 \t Loss:3.73014\n",
      "Iteration: 1882/277000 \t Elapsed time: 416.86 \t Loss:3.71580\n",
      "Iteration: 1883/277000 \t Elapsed time: 416.99 \t Loss:3.70148\n",
      "Iteration: 1884/277000 \t Elapsed time: 417.18 \t Loss:3.69916\n",
      "Iteration: 1885/277000 \t Elapsed time: 417.37 \t Loss:3.68343\n",
      "Iteration: 1886/277000 \t Elapsed time: 417.57 \t Loss:3.67630\n",
      "Iteration: 1887/277000 \t Elapsed time: 417.78 \t Loss:3.66627\n",
      "Iteration: 1888/277000 \t Elapsed time: 417.92 \t Loss:3.66829\n",
      "Iteration: 1889/277000 \t Elapsed time: 418.07 \t Loss:3.67180\n",
      "Iteration: 1890/277000 \t Elapsed time: 418.27 \t Loss:3.66233\n",
      "Iteration: 1891/277000 \t Elapsed time: 418.42 \t Loss:3.64447\n",
      "Iteration: 1892/277000 \t Elapsed time: 418.58 \t Loss:3.64210\n",
      "Iteration: 1893/277000 \t Elapsed time: 418.74 \t Loss:3.62167\n",
      "Iteration: 1894/277000 \t Elapsed time: 418.91 \t Loss:3.62476\n",
      "Iteration: 1895/277000 \t Elapsed time: 419.37 \t Loss:3.61422\n",
      "Iteration: 1896/277000 \t Elapsed time: 419.55 \t Loss:3.60616\n",
      "Iteration: 1897/277000 \t Elapsed time: 420.12 \t Loss:3.58953\n",
      "Iteration: 1898/277000 \t Elapsed time: 420.26 \t Loss:3.59627\n",
      "Iteration: 1899/277000 \t Elapsed time: 420.46 \t Loss:3.57453\n",
      "Iteration: 1900/277000 \t Elapsed time: 420.62 \t Loss:3.57593\n",
      "Iteration: 1901/277000 \t Elapsed time: 420.77 \t Loss:3.56927\n",
      "Iteration: 1902/277000 \t Elapsed time: 420.94 \t Loss:3.56009\n",
      "Iteration: 1903/277000 \t Elapsed time: 421.09 \t Loss:3.54826\n",
      "Iteration: 1904/277000 \t Elapsed time: 421.23 \t Loss:3.53574\n",
      "Iteration: 1905/277000 \t Elapsed time: 421.38 \t Loss:3.54127\n",
      "Iteration: 1906/277000 \t Elapsed time: 421.53 \t Loss:3.51587\n",
      "Iteration: 1907/277000 \t Elapsed time: 421.77 \t Loss:3.51734\n",
      "Iteration: 1908/277000 \t Elapsed time: 421.97 \t Loss:3.50590\n",
      "Iteration: 1909/277000 \t Elapsed time: 422.15 \t Loss:3.49305\n",
      "Iteration: 1910/277000 \t Elapsed time: 422.35 \t Loss:3.49001\n",
      "Iteration: 1911/277000 \t Elapsed time: 422.49 \t Loss:3.48406\n",
      "Iteration: 1912/277000 \t Elapsed time: 422.63 \t Loss:3.48938\n",
      "Iteration: 1913/277000 \t Elapsed time: 422.79 \t Loss:3.48771\n",
      "Iteration: 1914/277000 \t Elapsed time: 422.95 \t Loss:3.54470\n",
      "Iteration: 1915/277000 \t Elapsed time: 423.10 \t Loss:3.74372\n",
      "Iteration: 1916/277000 \t Elapsed time: 423.31 \t Loss:3.60091\n",
      "Iteration: 1917/277000 \t Elapsed time: 423.50 \t Loss:3.48249\n",
      "Iteration: 1918/277000 \t Elapsed time: 423.64 \t Loss:3.54270\n",
      "Iteration: 1919/277000 \t Elapsed time: 423.80 \t Loss:3.46730\n",
      "Iteration: 1920/277000 \t Elapsed time: 424.05 \t Loss:3.46316\n",
      "Iteration: 1921/277000 \t Elapsed time: 424.28 \t Loss:3.47098\n",
      "Iteration: 1922/277000 \t Elapsed time: 424.49 \t Loss:3.42932\n",
      "Iteration: 1923/277000 \t Elapsed time: 425.03 \t Loss:3.43475\n",
      "Iteration: 1924/277000 \t Elapsed time: 425.43 \t Loss:3.42519\n",
      "Iteration: 1925/277000 \t Elapsed time: 425.66 \t Loss:3.40032\n",
      "Iteration: 1926/277000 \t Elapsed time: 425.83 \t Loss:3.41296\n",
      "Iteration: 1927/277000 \t Elapsed time: 425.98 \t Loss:3.39340\n",
      "Iteration: 1928/277000 \t Elapsed time: 426.13 \t Loss:3.37306\n",
      "Iteration: 1929/277000 \t Elapsed time: 426.29 \t Loss:3.37845\n",
      "Iteration: 1930/277000 \t Elapsed time: 426.47 \t Loss:3.36450\n",
      "Iteration: 1931/277000 \t Elapsed time: 426.63 \t Loss:3.34554\n",
      "Iteration: 1932/277000 \t Elapsed time: 427.00 \t Loss:3.35456\n",
      "Iteration: 1933/277000 \t Elapsed time: 427.14 \t Loss:3.33691\n",
      "Iteration: 1934/277000 \t Elapsed time: 427.35 \t Loss:3.35431\n",
      "Iteration: 1935/277000 \t Elapsed time: 427.49 \t Loss:3.32359\n",
      "Iteration: 1936/277000 \t Elapsed time: 427.65 \t Loss:3.31831\n",
      "Iteration: 1937/277000 \t Elapsed time: 427.80 \t Loss:3.30259\n",
      "Iteration: 1938/277000 \t Elapsed time: 427.99 \t Loss:3.29730\n",
      "Iteration: 1939/277000 \t Elapsed time: 428.16 \t Loss:3.31112\n",
      "Iteration: 1940/277000 \t Elapsed time: 428.33 \t Loss:3.29792\n",
      "Iteration: 1941/277000 \t Elapsed time: 428.48 \t Loss:3.27736\n",
      "Iteration: 1942/277000 \t Elapsed time: 428.65 \t Loss:3.26924\n",
      "Iteration: 1943/277000 \t Elapsed time: 428.79 \t Loss:3.26834\n",
      "Iteration: 1944/277000 \t Elapsed time: 428.94 \t Loss:3.26897\n",
      "Iteration: 1945/277000 \t Elapsed time: 429.18 \t Loss:3.25961\n",
      "Iteration: 1946/277000 \t Elapsed time: 429.32 \t Loss:3.24391\n",
      "Iteration: 1947/277000 \t Elapsed time: 429.52 \t Loss:3.24580\n",
      "Iteration: 1948/277000 \t Elapsed time: 429.67 \t Loss:3.23537\n",
      "Iteration: 1949/277000 \t Elapsed time: 429.80 \t Loss:3.22095\n",
      "Iteration: 1950/277000 \t Elapsed time: 429.98 \t Loss:3.20748\n",
      "Iteration: 1951/277000 \t Elapsed time: 430.12 \t Loss:3.21000\n",
      "Iteration: 1952/277000 \t Elapsed time: 430.28 \t Loss:3.21974\n",
      "Iteration: 1953/277000 \t Elapsed time: 430.41 \t Loss:3.19412\n",
      "Iteration: 1954/277000 \t Elapsed time: 430.58 \t Loss:3.18195\n",
      "Iteration: 1955/277000 \t Elapsed time: 430.75 \t Loss:3.20933\n",
      "Iteration: 1956/277000 \t Elapsed time: 430.90 \t Loss:3.18584\n",
      "Iteration: 1957/277000 \t Elapsed time: 431.09 \t Loss:3.16913\n",
      "Iteration: 1958/277000 \t Elapsed time: 432.25 \t Loss:3.15551\n",
      "Iteration: 1959/277000 \t Elapsed time: 432.40 \t Loss:3.15529\n",
      "Iteration: 1960/277000 \t Elapsed time: 432.56 \t Loss:3.14618\n",
      "Iteration: 1961/277000 \t Elapsed time: 432.70 \t Loss:3.14895\n",
      "Iteration: 1962/277000 \t Elapsed time: 432.87 \t Loss:3.16154\n",
      "Iteration: 1963/277000 \t Elapsed time: 433.28 \t Loss:3.14026\n",
      "Iteration: 1964/277000 \t Elapsed time: 433.46 \t Loss:3.12826\n",
      "Iteration: 1965/277000 \t Elapsed time: 433.66 \t Loss:3.10505\n",
      "Iteration: 1966/277000 \t Elapsed time: 433.82 \t Loss:3.11657\n",
      "Iteration: 1967/277000 \t Elapsed time: 434.18 \t Loss:3.09538\n",
      "Iteration: 1968/277000 \t Elapsed time: 434.33 \t Loss:3.09753\n",
      "Iteration: 1969/277000 \t Elapsed time: 434.56 \t Loss:3.08300\n",
      "Iteration: 1970/277000 \t Elapsed time: 434.73 \t Loss:3.08354\n",
      "Iteration: 1971/277000 \t Elapsed time: 434.89 \t Loss:3.06408\n",
      "Iteration: 1972/277000 \t Elapsed time: 435.13 \t Loss:3.05588\n",
      "Iteration: 1973/277000 \t Elapsed time: 435.37 \t Loss:3.04337\n",
      "Iteration: 1974/277000 \t Elapsed time: 435.52 \t Loss:3.05516\n",
      "Iteration: 1975/277000 \t Elapsed time: 435.67 \t Loss:3.03445\n",
      "Iteration: 1976/277000 \t Elapsed time: 435.87 \t Loss:3.02366\n",
      "Iteration: 1977/277000 \t Elapsed time: 436.03 \t Loss:3.01877\n",
      "Iteration: 1978/277000 \t Elapsed time: 436.22 \t Loss:3.00793\n",
      "Iteration: 1979/277000 \t Elapsed time: 436.47 \t Loss:3.00669\n",
      "Iteration: 1980/277000 \t Elapsed time: 436.62 \t Loss:3.00625\n",
      "Iteration: 1981/277000 \t Elapsed time: 436.77 \t Loss:2.98848\n",
      "Iteration: 1982/277000 \t Elapsed time: 437.14 \t Loss:2.99559\n",
      "Iteration: 1983/277000 \t Elapsed time: 437.29 \t Loss:2.98470\n",
      "Iteration: 1984/277000 \t Elapsed time: 437.54 \t Loss:2.99135\n",
      "Iteration: 1985/277000 \t Elapsed time: 437.68 \t Loss:2.97763\n",
      "Iteration: 1986/277000 \t Elapsed time: 437.95 \t Loss:2.96546\n",
      "Iteration: 1987/277000 \t Elapsed time: 438.10 \t Loss:2.96614\n",
      "Iteration: 1988/277000 \t Elapsed time: 438.37 \t Loss:2.94208\n",
      "Iteration: 1989/277000 \t Elapsed time: 438.58 \t Loss:2.95158\n",
      "Iteration: 1990/277000 \t Elapsed time: 439.07 \t Loss:2.94217\n",
      "Iteration: 1991/277000 \t Elapsed time: 439.21 \t Loss:2.91900\n",
      "Iteration: 1992/277000 \t Elapsed time: 439.37 \t Loss:2.92358\n",
      "Iteration: 1993/277000 \t Elapsed time: 439.51 \t Loss:2.91333\n",
      "Iteration: 1994/277000 \t Elapsed time: 439.69 \t Loss:2.90446\n",
      "Iteration: 1995/277000 \t Elapsed time: 439.84 \t Loss:2.89261\n",
      "Iteration: 1996/277000 \t Elapsed time: 440.38 \t Loss:2.92121\n",
      "Iteration: 1997/277000 \t Elapsed time: 440.54 \t Loss:2.90343\n",
      "Iteration: 1998/277000 \t Elapsed time: 440.69 \t Loss:2.88938\n",
      "Iteration: 1999/277000 \t Elapsed time: 440.85 \t Loss:2.88979\n",
      "Iteration: 2000/277000 \t Elapsed time: 441.05 \t Loss:2.89560\n",
      "Start Validating\n",
      "Finish Validating\n",
      "Iteration: 2001/277000 \t Elapsed time: 465.70 \t Loss:2.86125\n",
      "Iteration: 2002/277000 \t Elapsed time: 465.88 \t Loss:2.86338\n",
      "Iteration: 2003/277000 \t Elapsed time: 466.12 \t Loss:2.85264\n",
      "Iteration: 2004/277000 \t Elapsed time: 466.29 \t Loss:2.85892\n",
      "Iteration: 2005/277000 \t Elapsed time: 466.57 \t Loss:2.84776\n",
      "Iteration: 2006/277000 \t Elapsed time: 466.71 \t Loss:2.83533\n",
      "Iteration: 2007/277000 \t Elapsed time: 466.88 \t Loss:2.84190\n",
      "Iteration: 2008/277000 \t Elapsed time: 467.42 \t Loss:2.88602\n",
      "Iteration: 2009/277000 \t Elapsed time: 467.65 \t Loss:2.83691\n",
      "Iteration: 2010/277000 \t Elapsed time: 467.83 \t Loss:2.80798\n",
      "Iteration: 2011/277000 \t Elapsed time: 467.97 \t Loss:2.79985\n",
      "Iteration: 2012/277000 \t Elapsed time: 468.95 \t Loss:2.79517\n",
      "Iteration: 2013/277000 \t Elapsed time: 469.10 \t Loss:2.80681\n",
      "Iteration: 2014/277000 \t Elapsed time: 469.25 \t Loss:2.76746\n",
      "Iteration: 2015/277000 \t Elapsed time: 469.52 \t Loss:2.77085\n",
      "Iteration: 2016/277000 \t Elapsed time: 469.74 \t Loss:2.75185\n",
      "Iteration: 2017/277000 \t Elapsed time: 470.01 \t Loss:2.75109\n",
      "Iteration: 2018/277000 \t Elapsed time: 470.16 \t Loss:2.76544\n",
      "Iteration: 2019/277000 \t Elapsed time: 470.41 \t Loss:2.74456\n",
      "Iteration: 2020/277000 \t Elapsed time: 470.62 \t Loss:2.73805\n",
      "Iteration: 2021/277000 \t Elapsed time: 470.78 \t Loss:2.74849\n",
      "Iteration: 2022/277000 \t Elapsed time: 470.96 \t Loss:2.73777\n",
      "Iteration: 2023/277000 \t Elapsed time: 471.13 \t Loss:2.70696\n",
      "Iteration: 2024/277000 \t Elapsed time: 471.33 \t Loss:2.73922\n",
      "Iteration: 2025/277000 \t Elapsed time: 471.52 \t Loss:2.71151\n",
      "Iteration: 2026/277000 \t Elapsed time: 471.69 \t Loss:2.72618\n",
      "Iteration: 2027/277000 \t Elapsed time: 471.83 \t Loss:2.70270\n",
      "Iteration: 2028/277000 \t Elapsed time: 472.05 \t Loss:2.70900\n",
      "Iteration: 2029/277000 \t Elapsed time: 472.26 \t Loss:2.69953\n",
      "Iteration: 2030/277000 \t Elapsed time: 472.51 \t Loss:2.70819\n",
      "Iteration: 2031/277000 \t Elapsed time: 472.74 \t Loss:2.70704\n",
      "Iteration: 2032/277000 \t Elapsed time: 472.91 \t Loss:2.67560\n",
      "Iteration: 2033/277000 \t Elapsed time: 473.14 \t Loss:2.66314\n",
      "Iteration: 2034/277000 \t Elapsed time: 473.30 \t Loss:2.67513\n",
      "Iteration: 2035/277000 \t Elapsed time: 473.46 \t Loss:2.64980\n",
      "Iteration: 2036/277000 \t Elapsed time: 473.62 \t Loss:2.63895\n",
      "Iteration: 2037/277000 \t Elapsed time: 473.77 \t Loss:2.63213\n",
      "Iteration: 2038/277000 \t Elapsed time: 473.91 \t Loss:2.62390\n",
      "Iteration: 2039/277000 \t Elapsed time: 474.05 \t Loss:2.60930\n",
      "Iteration: 2040/277000 \t Elapsed time: 474.41 \t Loss:2.63192\n",
      "Iteration: 2041/277000 \t Elapsed time: 474.57 \t Loss:2.59524\n",
      "Iteration: 2042/277000 \t Elapsed time: 474.70 \t Loss:2.59661\n",
      "Iteration: 2043/277000 \t Elapsed time: 474.85 \t Loss:2.57909\n",
      "Iteration: 2044/277000 \t Elapsed time: 475.03 \t Loss:2.60273\n",
      "Iteration: 2045/277000 \t Elapsed time: 475.18 \t Loss:2.56923\n",
      "Iteration: 2046/277000 \t Elapsed time: 475.35 \t Loss:2.56425\n",
      "Iteration: 2047/277000 \t Elapsed time: 475.54 \t Loss:2.56371\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[53], line 116\u001b[0m\n\u001b[1;32m    114\u001b[0m \u001b[39m# begin to train\u001b[39;00m\n\u001b[1;32m    115\u001b[0m trainer \u001b[39m=\u001b[39m Trainer(model, optim, scheduler, training_set, valid_set, args, cuda)\n\u001b[0;32m--> 116\u001b[0m trainer\u001b[39m.\u001b[39;49mtrain()\n",
      "Cell \u001b[0;32mIn[49], line 190\u001b[0m, in \u001b[0;36mTrainer.train\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    188\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mglobal_step \u001b[39m%\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mnll_gap \u001b[39m==\u001b[39m \u001b[39m0\u001b[39m:\n\u001b[1;32m    189\u001b[0m     \u001b[39mwith\u001b[39;00m \u001b[39mopen\u001b[39m(os\u001b[39m.\u001b[39mpath\u001b[39m.\u001b[39mjoin(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mlog_dir, \u001b[39m\"\u001b[39m\u001b[39mNLL.txt\u001b[39m\u001b[39m\"\u001b[39m), \u001b[39m\"\u001b[39m\u001b[39ma\u001b[39m\u001b[39m\"\u001b[39m) \u001b[39mas\u001b[39;00m nll_file:\n\u001b[0;32m--> 190\u001b[0m         nll_file\u001b[39m.\u001b[39mwrite(\u001b[39mstr\u001b[39m(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mglobal_step) \u001b[39m+\u001b[39m \u001b[39m\"\u001b[39m\u001b[39m \u001b[39m\u001b[39m\\t\u001b[39;00m\u001b[39m \u001b[39m\u001b[39m\"\u001b[39m \u001b[39m+\u001b[39m \u001b[39m\"\u001b[39m\u001b[39m{:.2f}\u001b[39;00m\u001b[39m \u001b[39m\u001b[39m\\t\u001b[39;00m\u001b[39m \u001b[39m\u001b[39m{:.5f}\u001b[39;00m\u001b[39m\"\u001b[39m\u001b[39m.\u001b[39mformat(elapsed, loss\u001b[39m.\u001b[39mdata) \u001b[39m+\u001b[39m \u001b[39m\"\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39m\"\u001b[39m)\n\u001b[1;32m    194\u001b[0m \u001b[39m# checkpoint\u001b[39;00m\n\u001b[1;32m    195\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mglobal_step \u001b[39m%\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcheckpoints_gap \u001b[39m==\u001b[39m \u001b[39m0\u001b[39m \u001b[39mand\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mglobal_step \u001b[39m>\u001b[39m \u001b[39m0\u001b[39m:\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# @title Train\n",
    "import argparse\n",
    "import torch\n",
    "from torchvision import datasets\n",
    "# import Learner\n",
    "# import datasets\n",
    "# import utils\n",
    "# from CGlowModel import CondGlowModel\n",
    "\n",
    "\n",
    "if __name__==\"__main__\":\n",
    "\n",
    "    parser = argparse.ArgumentParser(description='train c-Glow')\n",
    "\n",
    "    # input output path\n",
    "    parser.add_argument(\"-d\", \"--dataset_name\", type=str, default=\"horse\")\n",
    "    parser.add_argument(\"-r\", \"--dataset_root\", type=str, default=\"weizmann_horse_db\")\n",
    "\n",
    "    # log root\n",
    "    parser.add_argument(\"--log_root\", type=str, default=\"\")\n",
    "\n",
    "    # C-Glow parameters\n",
    "    # we are calculating Y GIVEN X\n",
    "    parser.add_argument(\"--x_size\", type=tuple, default=(1,8))\n",
    "    parser.add_argument(\"--y_size\", type=tuple, default=(1,16,16))\n",
    "    parser.add_argument(\"--x_hidden_channels\", type=int, default=128)\n",
    "    parser.add_argument(\"--x_hidden_size\", type=int, default=64)\n",
    "    parser.add_argument(\"--y_hidden_channels\", type=int, default=256)\n",
    "    parser.add_argument(\"-K\", \"--depth_flow\", type=int, default=8)\n",
    "    parser.add_argument(\"-L\", \"--num_levels\", type=int, default=3)\n",
    "    parser.add_argument(\"--learn_top\", type=bool, default=False)\n",
    "\n",
    "    # Dataset preprocess parameters\n",
    "    parser.add_argument(\"--label_scale\", type=float, default=1)\n",
    "    parser.add_argument(\"--label_bias\", type=float, default=0.5)\n",
    "    parser.add_argument(\"--x_bins\", type=float, default=256.0)\n",
    "    parser.add_argument(\"--y_bins\", type=float, default=2.0)\n",
    "\n",
    "\n",
    "    # Optimizer parameters\n",
    "    parser.add_argument(\"--optimizer\", type=str, default=\"adam\")\n",
    "    parser.add_argument(\"--lr\", type=float, default=0.0002)\n",
    "    parser.add_argument(\"--betas\", type=tuple, default=(0.9,0.9999))\n",
    "    parser.add_argument(\"--eps\", type=float, default=1e-8)\n",
    "    parser.add_argument(\"--regularizer\", type=float, default=0.0)\n",
    "    parser.add_argument(\"--num_steps\", type=int, default=0)\n",
    "\n",
    "    # Trainer parameters\n",
    "    parser.add_argument(\"--num_epochs\", type=int, default=1000)\n",
    "    parser.add_argument(\"--batch_size\", type=int, default=2)\n",
    "    parser.add_argument(\"--max_grad_clip\", type=float, default=5)\n",
    "    parser.add_argument(\"--max_grad_norm\", type=float, default=0)\n",
    "    parser.add_argument(\"--checkpoints_gap\", type=int, default=1000)\n",
    "    parser.add_argument(\"--nll_gap\", type=int, default=1)\n",
    "    parser.add_argument(\"--inference_gap\", type=int, default=1000)\n",
    "    parser.add_argument(\"--save_gap\", type=int, default=1000)\n",
    "\n",
    "    # model path\n",
    "    parser.add_argument(\"--model_path\", type=str, default=\"\")#\"/pdo/users/jlupoiii/TESS/model_conditional_norm_flow/log_20230821_2340/checkpoints/checkpoint_4000.pth.tar\") # \"/pdo/users/jlupoiii/TESS/model_conditional_norm_flow/log_20230821_2340/checkpoints/checkpoint_4000.pth.tar\"\n",
    "\n",
    "    # args = parser.parse_args()\n",
    "    args = parser.parse_known_args()[0]\n",
    "    cuda = torch.cuda.is_available()\n",
    "\n",
    "\n",
    "    # # HORSE DATASET\n",
    "    # dataset_root = 'weizmann_horse_db/'\n",
    "    # # Define any image transformations you want to apply (resize, normalization, etc.)\n",
    "    # transform = transforms.Compose([\n",
    "    #     transforms.Resize((64, 64)),\n",
    "    #     transforms.ToTensor(),\n",
    "    #     # transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "    # ])\n",
    "    # target_transform = transforms.Compose([\n",
    "    #     transforms.Resize((64, 64)),\n",
    "    #     transforms.ToTensor(),\n",
    "    #     # transforms.Normalize(mean=[0.456], std=[0.225])\n",
    "    # ])\n",
    "    # # Create the dataset\n",
    "    # training_set = WeizmannHorseDataset(root_dir=dataset_root, transform=transform, target_transform=target_transform)\n",
    "    # valid_set = WeizmannHorseDataset(root_dir=dataset_root, transform=transform, target_transform=target_transform)\n",
    "\n",
    "    # create TESS dataset\n",
    "    torch.manual_seed(42)\n",
    "    full_dataset = TESSDataset()\n",
    "    train_ratio = 0.8\n",
    "    num_train_samples = int(train_ratio * len(full_dataset))\n",
    "    num_valid_samples = len(full_dataset) - num_train_samples\n",
    "    training_set, valid_set = random_split(full_dataset, [num_train_samples, num_valid_samples])\n",
    "\n",
    "    model = CondGlowModel(args)\n",
    "    if cuda:\n",
    "        model = model.cuda()\n",
    "\n",
    "    # from utils import count_parameters\n",
    "    print(\"number of param: {}\".format(count_parameters(model)))\n",
    "\n",
    "    # optimizer\n",
    "    optim = torch.optim.Adam(model.parameters(), lr=args.lr,betas=args.betas, weight_decay=args.regularizer)\n",
    "\n",
    "    # scheduler\n",
    "    scheduler = None\n",
    "\n",
    "    if args.model_path != \"\":\n",
    "        state = load_state(args.model_path, cuda)\n",
    "        optim.load_state_dict(state[\"optim\"])\n",
    "        model.load_state_dict(state[\"model\"])\n",
    "        args.steps = state[\"iteration\"] + 1\n",
    "        if scheduler is not None and state.get(\"scheduler\", None) is not None:\n",
    "            scheduler.load_state_dict(state[\"scheduler\"])\n",
    "        del state\n",
    "        print('Realoaded model stored at', args.model_path, 'starting at step', args.steps)\n",
    "\n",
    "    # begin to train\n",
    "    trainer = Trainer(model, optim, scheduler, training_set, valid_set, args, cuda)\n",
    "    trainer.train()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "-pMvfdKknvWb"
   },
   "outputs": [],
   "source": [
    "# plt.imshow(y_sample.cpu()[0][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {
    "id": "E0vt1suwnvYt"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "load to gpu\n",
      "Batch IDs: 0\n",
      "Samples: 0/10\n",
      "Samples: 1/10\n",
      "Samples: 2/10\n",
      "Samples: 3/10\n",
      "Samples: 4/10\n",
      "Samples: 5/10\n",
      "Samples: 6/10\n",
      "Samples: 7/10\n",
      "Samples: 8/10\n",
      "Samples: 9/10\n",
      "E-el: 106.69999694824219, E-az: 233.89999389648438, M-el: 86.30000305175781, M-az: 177.3000030517578, 1/ED: 0.018315019086003304, 1/MD: 0.015625, 1/MD^2: 0.0003354398941155523, 1/ED^2: 0.000244140625]\n",
      "E-el: 94.69999694824219, E-az: 185.10000610351562, M-el: 91.4000015258789, M-az: 119.9000015258789, 1/ED: 0.02958579920232296, 1/MD: 0.015748031437397003, 1/MD^2: 0.0008753194706514478, 1/ED^2: 0.00024800049141049385]\n",
      "E-el: 104.4000015258789, E-az: 300.0, M-el: 78.5999984741211, M-az: 247.8000030517578, 1/ED: 0.022123893722891808, 1/MD: 0.013642564415931702, 1/MD^2: 0.0004894666490145028, 1/ED^2: 0.00018611957784742117]\n",
      "E-el: 107.9000015258789, E-az: 244.6999969482422, M-el: 84.4000015258789, M-az: 192.39999389648438, 1/ED: 0.0173611119389534, 1/MD: 0.014880952425301075, 1/MD^2: 0.000301408173982054, 1/ED^2: 0.00022144273680169135]\n",
      "E-el: 85.9000015258789, E-az: 158.89999389648438, M-el: 91.80000305175781, M-az: 101.19999694824219, 1/ED: 0.038314174860715866, 1/MD: 0.014880952425301075, 1/MD^2: 0.0014679761370643973, 1/ED^2: 0.00022144273680169135]\n",
      "E-el: 108.30000305175781, E-az: 250.39999389648438, M-el: 83.5, M-az: 200.0, 1/ED: 0.017094017937779427, 1/MD: 0.014471779577434063, 1/MD^2: 0.00029220542637631297, 1/ED^2: 0.00020943241543136537]\n",
      "E-el: 106.30000305175781, E-az: 231.5, M-el: 86.80000305175781, M-az: 174.0, 1/ED: 0.01862197369337082, 1/MD: 0.015772869810461998, 1/MD^2: 0.0003467779024504125, 1/ED^2: 0.0002487834426574409]\n",
      "E-el: 82.30000305175781, E-az: 14.0, M-el: 69.80000305175781, M-az: 257.70001220703125, 1/ED: 0.0458715595304966, 1/MD: 0.019267823547124863, 1/MD^2: 0.002104199957102537, 1/ED^2: 0.00037124898517504334]\n",
      "E-el: 108.30000305175781, E-az: 269.8999938964844, M-el: 81.19999694824219, M-az: 223.39999389648438, 1/ED: 0.017513135448098183, 1/MD: 0.013386880978941917, 1/MD^2: 0.00030670990236103535, 1/ED^2: 0.00017920858226716518]\n",
      "E-el: 75.69999694824219, E-az: 39.5, M-el: 68.5, M-az: 252.3000030517578, 1/ED: 0.052356019616127014, 1/MD: 0.02049180306494236, 1/MD^2: 0.002741152886301279, 1/ED^2: 0.00041991399484686553]\n",
      "Batch IDs: 1\n",
      "Samples: 0/10\n",
      "Samples: 1/10\n",
      "Samples: 2/10\n",
      "Samples: 3/10\n",
      "Samples: 4/10\n",
      "Samples: 5/10\n",
      "Samples: 6/10\n",
      "Samples: 7/10\n",
      "Samples: 8/10\n",
      "Samples: 9/10\n",
      "E-el: 101.80000305175781, E-az: 209.5, M-el: 90.0, M-az: 144.89999389648438, 1/ED: 0.022675737738609314, 1/MD: 0.016313213855028152, 1/MD^2: 0.000514189072418958, 1/ED^2: 0.00026612094370648265]\n",
      "E-el: 107.5, E-az: 280.29998779296875, M-el: 80.30000305175781, M-az: 233.5, 1/ED: 0.01858736015856266, 1/MD: 0.013192611746490002, 1/MD^2: 0.0003454899706412107, 1/ED^2: 0.00017404500977136195]\n",
      "E-el: 107.4000015258789, E-az: 239.5, M-el: 85.30000305175781, M-az: 185.1999969482422, 1/ED: 0.01773049682378769, 1/MD: 0.015267175622284412, 1/MD^2: 0.00031437049619853497, 1/ED^2: 0.0002330866554984823]\n",
      "E-el: 107.9000015258789, E-az: 244.3000030517578, M-el: 84.5, M-az: 191.8000030517578, 1/ED: 0.017391303554177284, 1/MD: 0.014925372786819935, 1/MD^2: 0.0003024574543815106, 1/ED^2: 0.00022276675736065954]\n",
      "E-el: 108.4000015258789, E-az: 268.6000061035156, M-el: 81.30000305175781, M-az: 222.0, 1/ED: 0.017421603202819824, 1/MD: 0.01342281885445118, 1/MD^2: 0.00030351223540492356, 1/ED^2: 0.00018017206457443535]\n",
      "E-el: 107.69999694824219, E-az: 242.1999969482422, M-el: 84.80000305175781, M-az: 188.89999389648438, 1/ED: 0.017513135448098183, 1/MD: 0.015082956291735172, 1/MD^2: 0.00030670990236103535, 1/ED^2: 0.00022749556228518486]\n",
      "E-el: 108.5, E-az: 265.0, M-el: 81.5999984741211, M-az: 218.10000610351562, 1/ED: 0.017211703583598137, 1/MD: 0.013568521477282047, 1/MD^2: 0.00029624273884110153, 1/ED^2: 0.00018410476332064718]\n",
      "E-el: 88.30000305175781, E-az: 166.0, M-el: 91.69999694824219, M-az: 105.5999984741211, 1/ED: 0.035971224308013916, 1/MD: 0.015105740167200565, 1/MD^2: 0.0012939288280904293, 1/ED^2: 0.00022818338766228408]\n",
      "E-el: 94.80000305175781, E-az: 185.6999969482422, M-el: 91.4000015258789, M-az: 120.4000015258789, 1/ED: 0.029411764815449715, 1/MD: 0.015772869810461998, 1/MD^2: 0.0008650519303046167, 1/ED^2: 0.0002487834426574409]\n",
      "E-el: 98.0, E-az: 325.20001220703125, M-el: 75.69999694824219, M-az: 257.70001220703125, 1/ED: 0.029154518619179726, 1/MD: 0.015313935466110706, 1/MD^2: 0.000849985983222723, 1/ED^2: 0.0002345166285522282]\n",
      "Batch IDs: 2\n",
      "Samples: 0/10\n",
      "Samples: 1/10\n",
      "Samples: 2/10\n",
      "Samples: 3/10\n",
      "Samples: 4/10\n",
      "Samples: 5/10\n",
      "Samples: 6/10\n",
      "Samples: 7/10\n",
      "Samples: 8/10\n",
      "Samples: 9/10\n",
      "E-el: 107.69999694824219, E-az: 242.8000030517578, M-el: 84.69999694824219, M-az: 189.8000030517578, 1/ED: 0.017482517287135124, 1/MD: 0.015037594363093376, 1/MD^2: 0.00030563841573894024, 1/ED^2: 0.0002261292393086478]\n",
      "E-el: 97.30000305175781, E-az: 327.3999938964844, M-el: 75.4000015258789, M-az: 258.20001220703125, 1/ED: 0.02994011901319027, 1/MD: 0.01550387591123581, 1/MD^2: 0.000896410783752799, 1/ED^2: 0.00024037016555666924]\n",
      "E-el: 82.19999694824219, E-az: 147.3000030517578, M-el: 91.9000015258789, M-az: 94.9000015258789, 1/ED: 0.042016807943582535, 1/MD: 0.014556040987372398, 1/MD^2: 0.0017654120456427336, 1/ED^2: 0.0002118783158948645]\n",
      "E-el: 93.0999984741211, E-az: 340.6000061035156, M-el: 73.5999984741211, M-az: 259.8999938964844, 1/ED: 0.03448275849223137, 1/MD: 0.016611294820904732, 1/MD^2: 0.0011890606256201863, 1/ED^2: 0.0002759351336862892]\n",
      "E-el: 98.4000015258789, E-az: 197.0, M-el: 91.0, M-az: 131.1999969482422, 1/ED: 0.02590673603117466, 1/MD: 0.016129031777381897, 1/MD^2: 0.0006711589521728456, 1/ED^2: 0.00026014569448307157]\n",
      "E-el: 108.5, E-az: 261.8999938964844, M-el: 82.0, M-az: 214.5, 1/ED: 0.017094017937779427, 1/MD: 0.01373626384884119, 1/MD^2: 0.00029220542637631297, 1/ED^2: 0.00018868493498302996]\n",
      "E-el: 75.0, E-az: 43.20000076293945, M-el: 68.5, M-az: 251.39999389648438, 1/ED: 0.05291005223989487, 1/MD: 0.020576132461428642, 1/MD^2: 0.0027994737029075623, 1/ED^2: 0.00042337720515206456]\n",
      "E-el: 108.5, E-az: 260.20001220703125, M-el: 82.0999984741211, M-az: 212.5, 1/ED: 0.01706484705209732, 1/MD: 0.013812154531478882, 1/MD^2: 0.0002912089694291353, 1/ED^2: 0.00019077562319580466]\n",
      "E-el: 77.5, E-az: 130.3000030517578, M-el: 92.19999694824219, M-az: 87.0, 1/ED: 0.04694835841655731, 1/MD: 0.014184396713972092, 1/MD^2: 0.0022041480988264084, 1/ED^2: 0.00020119712280575186]\n",
      "E-el: 108.5, E-az: 257.79998779296875, M-el: 82.4000015258789, M-az: 209.5, 1/ED: 0.017006803303956985, 1/MD: 0.013966480270028114, 1/MD^2: 0.0002892313350457698, 1/ED^2: 0.00019506257376633584]\n",
      "Batch IDs: 3\n",
      "Samples: 0/10\n",
      "Samples: 1/10\n",
      "Samples: 2/10\n",
      "Samples: 3/10\n",
      "Samples: 4/10\n",
      "Samples: 5/10\n",
      "Samples: 6/10\n",
      "Samples: 7/10\n",
      "Samples: 8/10\n",
      "Samples: 9/10\n",
      "E-el: 107.5999984741211, E-az: 241.5, M-el: 84.9000015258789, M-az: 188.0, 1/ED: 0.017574692144989967, 1/MD: 0.01512859296053648, 1/MD^2: 0.0003088698140345514, 1/ED^2: 0.00022887432714924216]\n",
      "E-el: 102.80000305175781, E-az: 213.39999389648438, M-el: 89.5999984741211, M-az: 149.8000030517578, 1/ED: 0.021786492317914963, 1/MD: 0.016313213855028152, 1/MD^2: 0.0004746512568090111, 1/ED^2: 0.00026612094370648265]\n",
      "E-el: 95.5, E-az: 187.8000030517578, M-el: 91.30000305175781, M-az: 122.4000015258789, 1/ED: 0.02865329571068287, 1/MD: 0.015847859904170036, 1/MD^2: 0.0008210113155655563, 1/ED^2: 0.0002511546772439033]\n",
      "E-el: 73.0999984741211, E-az: 54.79999923706055, M-el: 95.5, M-az: 60.0, 1/ED: 0.052356019616127014, 1/MD: 0.01342281885445118, 1/MD^2: 0.002741152886301279, 1/ED^2: 0.00018017206457443535]\n",
      "E-el: 107.30000305175781, E-az: 238.39999389648438, M-el: 85.5, M-az: 183.6999969482422, 1/ED: 0.017825311049818993, 1/MD: 0.015337423421442509, 1/MD^2: 0.0003177417383994907, 1/ED^2: 0.0002352365554543212]\n",
      "E-el: 103.69999694824219, E-az: 303.3999938964844, M-el: 78.30000305175781, M-az: 249.60000610351562, 1/ED: 0.022883296012878418, 1/MD: 0.013812154531478882, 1/MD^2: 0.0005236451979726553, 1/ED^2: 0.00019077562319580466]\n",
      "E-el: 108.0, E-az: 245.8000030517578, M-el: 84.19999694824219, M-az: 193.89999389648438, 1/ED: 0.017301037907600403, 1/MD: 0.014814814552664757, 1/MD^2: 0.0002993259113281965, 1/ED^2: 0.00021947873756289482]\n",
      "E-el: 99.69999694824219, E-az: 201.6999969482422, M-el: 90.69999694824219, M-az: 136.0, 1/ED: 0.02463054098188877, 1/MD: 0.016233766451478004, 1/MD^2: 0.0006066635833121836, 1/ED^2: 0.00026353515568189323]\n",
      "E-el: 108.4000015258789, E-az: 267.8999938964844, M-el: 81.30000305175781, M-az: 221.3000030517578, 1/ED: 0.017391303554177284, 1/MD: 0.01344086043536663, 1/MD^2: 0.0003024574543815106, 1/ED^2: 0.0001806567161111161]\n",
      "E-el: 108.5, E-az: 257.1000061035156, M-el: 82.5, M-az: 208.6999969482422, 1/ED: 0.017006803303956985, 1/MD: 0.014005602337419987, 1/MD^2: 0.0002892313350457698, 1/ED^2: 0.00019615689234342426]\n",
      "Batch IDs: 4\n",
      "Samples: 0/10\n",
      "Samples: 1/10\n",
      "Samples: 2/10\n",
      "Samples: 3/10\n",
      "Samples: 4/10\n",
      "Samples: 5/10\n",
      "Samples: 6/10\n",
      "Samples: 7/10\n",
      "Samples: 8/10\n",
      "Samples: 9/10\n",
      "E-el: 72.4000015258789, E-az: 101.0999984741211, M-el: 93.0, M-az: 75.80000305175781, 1/ED: 0.052356019616127014, 1/MD: 0.01373626384884119, 1/MD^2: 0.002741152886301279, 1/ED^2: 0.00018868493498302996]\n",
      "E-el: 87.5, E-az: 357.3999938964844, M-el: 71.4000015258789, M-az: 259.79998779296875, 1/ED: 0.04032257944345474, 1/MD: 0.018050542101264, 1/MD^2: 0.0016259105177596211, 1/ED^2: 0.0003258220385760069]\n",
      "E-el: 100.80000305175781, E-az: 315.29998779296875, M-el: 76.9000015258789, M-az: 254.8000030517578, 1/ED: 0.026109660044312477, 1/MD: 0.014556040987372398, 1/MD^2: 0.0006817143876105547, 1/ED^2: 0.0002118783158948645]\n",
      "E-el: 107.30000305175781, E-az: 239.10000610351562, M-el: 85.4000015258789, M-az: 184.60000610351562, 1/ED: 0.017761988565325737, 1/MD: 0.015290520153939724, 1/MD^2: 0.0003154882579110563, 1/ED^2: 0.00023380000493489206]\n",
      "E-el: 108.5, E-az: 260.6000061035156, M-el: 82.0999984741211, M-az: 213.0, 1/ED: 0.01706484705209732, 1/MD: 0.013793103396892548, 1/MD^2: 0.0002912089694291353, 1/ED^2: 0.00019024970242753625]\n",
      "E-el: 74.30000305175781, E-az: 47.099998474121094, M-el: 68.5, M-az: 250.5, 1/ED: 0.05347593501210213, 1/MD: 0.020661156624555588, 1/MD^2: 0.0028596757911145687, 1/ED^2: 0.0004268834018148482]\n",
      "E-el: 81.5, E-az: 145.0, M-el: 91.9000015258789, M-az: 93.69999694824219, 1/ED: 0.04273504391312599, 1/MD: 0.014492753893136978, 1/MD^2: 0.0018262838711962104, 1/ED^2: 0.000210039914236404]\n",
      "E-el: 99.5999984741211, E-az: 201.3000030517578, M-el: 90.69999694824219, M-az: 135.60000610351562, 1/ED: 0.024752475321292877, 1/MD: 0.016207454726099968, 1/MD^2: 0.00061268504941836, 1/ED^2: 0.00026268159854225814]\n",
      "E-el: 108.19999694824219, E-az: 249.3000030517578, M-el: 83.5999984741211, M-az: 198.60000610351562, 1/ED: 0.017123287543654442, 1/MD: 0.014534884132444859, 1/MD^2: 0.00029320697649382055, 1/ED^2: 0.00021126284264028072]\n",
      "E-el: 106.5, E-az: 287.8999938964844, M-el: 79.69999694824219, M-az: 239.6999969482422, 1/ED: 0.019723866134881973, 1/MD: 0.013245033100247383, 1/MD^2: 0.00038903087261132896, 1/ED^2: 0.00017543090507388115]\n",
      "Batch IDs: 5\n",
      "Samples: 0/10\n",
      "Samples: 1/10\n",
      "Samples: 2/10\n",
      "Samples: 3/10\n",
      "Samples: 4/10\n",
      "Samples: 5/10\n",
      "Samples: 6/10\n",
      "Samples: 7/10\n",
      "Samples: 8/10\n",
      "Samples: 9/10\n",
      "E-el: 99.0999984741211, E-az: 321.5, M-el: 76.19999694824219, M-az: 256.70001220703125, 1/ED: 0.028011204674839973, 1/MD: 0.015015015378594398, 1/MD^2: 0.000784627569373697, 1/ED^2: 0.00022545066894963384]\n",
      "E-el: 108.30000305175781, E-az: 269.0, M-el: 81.19999694824219, M-az: 222.5, 1/ED: 0.01745200715959072, 1/MD: 0.013404825702309608, 1/MD^2: 0.0003045725461561233, 1/ED^2: 0.00017968934844247997]\n",
      "E-el: 108.0, E-az: 274.70001220703125, M-el: 80.69999694824219, M-az: 228.3000030517578, 1/ED: 0.01795332133769989, 1/MD: 0.01326259970664978, 1/MD^2: 0.000322321749990806, 1/ED^2: 0.00017589655180927366]\n",
      "E-el: 105.0999984741211, E-az: 296.5, M-el: 78.9000015258789, M-az: 245.6999969482422, 1/ED: 0.021321961656212807, 1/MD: 0.013495276682078838, 1/MD^2: 0.00045462604612112045, 1/ED^2: 0.00018212248687632382]\n",
      "E-el: 107.19999694824219, E-az: 283.0, M-el: 80.0999984741211, M-az: 235.8000030517578, 1/ED: 0.018939394503831863, 1/MD: 0.013192611746490002, 1/MD^2: 0.00035870063584297895, 1/ED^2: 0.00017404500977136195]\n",
      "E-el: 74.5, E-az: 115.80000305175781, M-el: 92.5999984741211, M-az: 81.19999694824219, 1/ED: 0.05000000074505806, 1/MD: 0.013927577063441277, 1/MD^2: 0.0024999999441206455, 1/ED^2: 0.00019397739379201084]\n",
      "E-el: 107.0, E-az: 284.79998779296875, M-el: 79.9000015258789, M-az: 237.3000030517578, 1/ED: 0.0191938579082489, 1/MD: 0.013210039585828781, 1/MD^2: 0.000368404173059389, 1/ED^2: 0.0001745051413308829]\n",
      "E-el: 108.0999984741211, E-az: 247.3000030517578, M-el: 84.0, M-az: 195.89999389648438, 1/ED: 0.017211703583598137, 1/MD: 0.014705882407724857, 1/MD^2: 0.00029624273884110153, 1/ED^2: 0.00021626298257615417]\n",
      "E-el: 72.19999694824219, E-az: 99.19999694824219, M-el: 93.0999984741211, M-az: 75.19999694824219, 1/ED: 0.052356019616127014, 1/MD: 0.01371742133051157, 1/MD^2: 0.002741152886301279, 1/ED^2: 0.00018816764350049198]\n",
      "E-el: 96.5999984741211, E-az: 329.79998779296875, M-el: 75.0999984741211, M-az: 258.6000061035156, 1/ED: 0.030674846842885017, 1/MD: 0.015698587521910667, 1/MD^2: 0.0009409462218172848, 1/ED^2: 0.0002464456483721733]\n",
      "Batch IDs: 6\n",
      "Samples: 0/10\n",
      "Samples: 1/10\n",
      "Samples: 2/10\n",
      "Samples: 3/10\n",
      "Samples: 4/10\n",
      "Samples: 5/10\n",
      "Samples: 6/10\n",
      "Samples: 7/10\n",
      "Samples: 8/10\n",
      "Samples: 9/10\n",
      "E-el: 85.30000305175781, E-az: 156.89999389648438, M-el: 91.80000305175781, M-az: 100.0999984741211, 1/ED: 0.038910504430532455, 1/MD: 0.014814814552664757, 1/MD^2: 0.0015140274772420526, 1/ED^2: 0.00021947873756289482]\n",
      "E-el: 108.4000015258789, E-az: 252.39999389648438, M-el: 83.19999694824219, M-az: 202.6999969482422, 1/ED: 0.017035774886608124, 1/MD: 0.014326647855341434, 1/MD^2: 0.00029021763475611806, 1/ED^2: 0.00020525282889138907]\n",
      "E-el: 95.80000305175781, E-az: 332.29998779296875, M-el: 74.69999694824219, M-az: 259.0, 1/ED: 0.031545739620923996, 1/MD: 0.01589825190603733, 1/MD^2: 0.0009951337706297636, 1/ED^2: 0.0002527543983887881]\n",
      "E-el: 102.5999984741211, E-az: 212.8000030517578, M-el: 89.5999984741211, M-az: 149.0, 1/ED: 0.021881837397813797, 1/MD: 0.016313213855028152, 1/MD^2: 0.0004788148507941514, 1/ED^2: 0.00026612094370648265]\n",
      "E-el: 107.9000015258789, E-az: 245.1999969482422, M-el: 84.30000305175781, M-az: 193.0, 1/ED: 0.017331022769212723, 1/MD: 0.014858840964734554, 1/MD^2: 0.0003003643359988928, 1/ED^2: 0.00022078515030443668]\n",
      "E-el: 106.69999694824219, E-az: 286.79998779296875, M-el: 79.80000305175781, M-az: 238.89999389648438, 1/ED: 0.01953125, 1/MD: 0.013227513059973717, 1/MD^2: 0.0003814697265625, 1/ED^2: 0.00017496710643172264]\n",
      "E-el: 108.5, E-az: 261.0, M-el: 82.0, M-az: 213.5, 1/ED: 0.01706484705209732, 1/MD: 0.013774104416370392, 1/MD^2: 0.0002912089694291353, 1/ED^2: 0.0001897259644465521]\n",
      "E-el: 108.5, E-az: 262.8999938964844, M-el: 81.80000305175781, M-az: 215.6999969482422, 1/ED: 0.017123287543654442, 1/MD: 0.013679890893399715, 1/MD^2: 0.00029320697649382055, 1/ED^2: 0.00018713940517045557]\n",
      "E-el: 104.5, E-az: 299.70001220703125, M-el: 78.5999984741211, M-az: 247.60000610351562, 1/ED: 0.022026430815458298, 1/MD: 0.0136239780113101, 1/MD^2: 0.0004851637058891356, 1/ED^2: 0.00018561277829576284]\n",
      "E-el: 96.19999694824219, E-az: 189.89999389648438, M-el: 91.30000305175781, M-az: 124.30000305175781, 1/ED: 0.028011204674839973, 1/MD: 0.01592356711626053, 1/MD^2: 0.000784627569373697, 1/ED^2: 0.0002535599924158305]\n",
      "Batch IDs: 7\n",
      "Samples: 0/10\n",
      "Samples: 1/10\n",
      "Samples: 2/10\n",
      "Samples: 3/10\n",
      "Samples: 4/10\n",
      "Samples: 5/10\n",
      "Samples: 6/10\n",
      "Samples: 7/10\n",
      "Samples: 8/10\n",
      "Samples: 9/10\n",
      "E-el: 91.80000305175781, E-az: 176.5, M-el: 91.5999984741211, M-az: 113.0, 1/ED: 0.03236246109008789, 1/MD: 0.015455950982868671, 1/MD^2: 0.0010473288130015135, 1/ED^2: 0.00023888640862423927]\n",
      "E-el: 85.5999984741211, E-az: 157.89999389648438, M-el: 91.80000305175781, M-az: 100.69999694824219, 1/ED: 0.03861003741621971, 1/MD: 0.014836795628070831, 1/MD^2: 0.0014907350996509194, 1/ED^2: 0.00022013048874214292]\n",
      "E-el: 107.0999984741211, E-az: 236.89999389648438, M-el: 85.80000305175781, M-az: 181.5, 1/ED: 0.017985612154006958, 1/MD: 0.015432098880410194, 1/MD^2: 0.0003234822070226073, 1/ED^2: 0.0002381496742600575]\n",
      "E-el: 99.9000015258789, E-az: 202.10000610351562, M-el: 90.5999984741211, M-az: 136.5, 1/ED: 0.02450980432331562, 1/MD: 0.016233766451478004, 1/MD^2: 0.0006007304764352739, 1/ED^2: 0.00026353515568189323]\n",
      "E-el: 108.5, E-az: 265.3999938964844, M-el: 81.5999984741211, M-az: 218.5, 1/ED: 0.017241379246115685, 1/MD: 0.013550135307013988, 1/MD^2: 0.0002972651564050466, 1/ED^2: 0.00018360617104917765]\n",
      "E-el: 91.0, E-az: 347.0, M-el: 72.69999694824219, M-az: 260.20001220703125, 1/ED: 0.036764707416296005, 1/MD: 0.017182130366563797, 1/MD^2: 0.0013516435865312815, 1/ED^2: 0.0002952256181742996]\n",
      "E-el: 98.19999694824219, E-az: 324.6000061035156, M-el: 75.69999694824219, M-az: 257.5, 1/ED: 0.028985507786273956, 1/MD: 0.015267175622284412, 1/MD^2: 0.000840159656945616, 1/ED^2: 0.0002330866554984823]\n",
      "E-el: 83.0999984741211, E-az: 11.300000190734863, M-el: 70.0, M-az: 258.1000061035156, 1/ED: 0.045045044273138046, 1/MD: 0.019083969295024872, 1/MD^2: 0.0020290561951696873, 1/ED^2: 0.00036419788375496864]\n",
      "E-el: 108.5, E-az: 259.0, M-el: 82.30000305175781, M-az: 211.0, 1/ED: 0.017035774886608124, 1/MD: 0.013888888992369175, 1/MD^2: 0.00029021763475611806, 1/ED^2: 0.00019290123600512743]\n",
      "E-el: 108.0999984741211, E-az: 247.89999389648438, M-el: 83.9000015258789, M-az: 196.6999969482422, 1/ED: 0.017182130366563797, 1/MD: 0.014641288667917252, 1/MD^2: 0.0002952256181742996, 1/ED^2: 0.00021436733368318528]\n",
      "Batch IDs: 8\n",
      "Samples: 0/10\n",
      "Samples: 1/10\n",
      "Samples: 2/10\n",
      "Samples: 3/10\n",
      "Samples: 4/10\n",
      "Samples: 5/10\n",
      "Samples: 6/10\n",
      "Samples: 7/10\n",
      "Samples: 8/10\n",
      "Samples: 9/10\n",
      "E-el: 108.5, E-az: 265.79998779296875, M-el: 81.5, M-az: 219.0, 1/ED: 0.01727115735411644, 1/MD: 0.013531799428164959, 1/MD^2: 0.00029829287086613476, 1/ED^2: 0.00018310960149392486]\n",
      "E-el: 107.4000015258789, E-az: 239.8000030517578, M-el: 85.30000305175781, M-az: 185.5, 1/ED: 0.017699114978313446, 1/MD: 0.015243902802467346, 1/MD^2: 0.0003132586716674268, 1/ED^2: 0.00023237656569108367]\n",
      "E-el: 104.5999984741211, E-az: 299.0, M-el: 78.69999694824219, M-az: 247.1999969482422, 1/ED: 0.021881837397813797, 1/MD: 0.013605441898107529, 1/MD^2: 0.0004788148507941514, 1/ED^2: 0.00018510805966798216]\n",
      "E-el: 108.5, E-az: 258.79998779296875, M-el: 82.30000305175781, M-az: 210.8000030517578, 1/ED: 0.017035774886608124, 1/MD: 0.013908205553889275, 1/MD^2: 0.00029021763475611806, 1/ED^2: 0.00019343818712513894]\n",
      "E-el: 107.0999984741211, E-az: 237.3000030517578, M-el: 85.69999694824219, M-az: 182.10000610351562, 1/ED: 0.01795332133769989, 1/MD: 0.015408320352435112, 1/MD^2: 0.000322321749990806, 1/ED^2: 0.0002374163450440392]\n",
      "E-el: 102.9000015258789, E-az: 213.6999969482422, M-el: 89.5, M-az: 150.1999969482422, 1/ED: 0.021691974252462387, 1/MD: 0.016313213855028152, 1/MD^2: 0.0004705417377408594, 1/ED^2: 0.00026612094370648265]\n",
      "E-el: 96.80000305175781, E-az: 329.20001220703125, M-el: 75.0999984741211, M-az: 258.5, 1/ED: 0.030487805604934692, 1/MD: 0.015649452805519104, 1/MD^2: 0.0009295062627643347, 1/ED^2: 0.00024490535724908113]\n",
      "E-el: 96.5, E-az: 190.89999389648438, M-el: 91.19999694824219, M-az: 125.19999694824219, 1/ED: 0.02770083025097847, 1/MD: 0.015948962420225143, 1/MD^2: 0.0007673360523767769, 1/ED^2: 0.00025436942814849317]\n",
      "E-el: 91.30000305175781, E-az: 175.10000610351562, M-el: 91.5999984741211, M-az: 112.0, 1/ED: 0.032894738018512726, 1/MD: 0.015408320352435112, 1/MD^2: 0.0010820637689903378, 1/ED^2: 0.0002374163450440392]\n",
      "E-el: 73.69999694824219, E-az: 51.0, M-el: 95.80000305175781, M-az: 58.599998474121094, 1/ED: 0.05181347206234932, 1/MD: 0.013404825702309608, 1/MD^2: 0.0026846358086913824, 1/ED^2: 0.00017968934844247997]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[97], line 71\u001b[0m\n\u001b[1;32m     68\u001b[0m predictor \u001b[39m=\u001b[39m Inferencer(model, valid_set, args, cuda)\n\u001b[1;32m     70\u001b[0m \u001b[39m# predict\u001b[39;00m\n\u001b[0;32m---> 71\u001b[0m predictor\u001b[39m.\u001b[39;49msampled_based_prediction(args\u001b[39m.\u001b[39;49mnum_samples)\n",
      "Cell \u001b[0;32mIn[96], line 369\u001b[0m, in \u001b[0;36mInferencer.sampled_based_prediction\u001b[0;34m(self, n_samples)\u001b[0m\n\u001b[1;32m    366\u001b[0m im_latent \u001b[39m=\u001b[39m axes[\u001b[39m1\u001b[39m]\u001b[39m.\u001b[39mimshow(pred_img\u001b[39m.\u001b[39mcpu()[\u001b[39m0\u001b[39m], cmap\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mgray\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m    367\u001b[0m fig\u001b[39m.\u001b[39mcolorbar(im_latent, fraction\u001b[39m=\u001b[39m\u001b[39m0.046\u001b[39m, pad\u001b[39m=\u001b[39m\u001b[39m0.04\u001b[39m)\n\u001b[0;32m--> 369\u001b[0m fig\u001b[39m.\u001b[39;49msavefig(os\u001b[39m.\u001b[39;49mpath\u001b[39m.\u001b[39;49mjoin(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mout_root, \u001b[39mf\u001b[39;49m\u001b[39m\"\u001b[39;49m\u001b[39mbatch_xandy-\u001b[39;49m\u001b[39m{\u001b[39;49;00mi_batch\u001b[39m}\u001b[39;49;00m\u001b[39m-\u001b[39;49m\u001b[39m{\u001b[39;49;00mi\u001b[39m}\u001b[39;49;00m\u001b[39m.pdf\u001b[39;49m\u001b[39m\"\u001b[39;49m))\n\u001b[1;32m    370\u001b[0m plt\u001b[39m.\u001b[39mclose()\n",
      "File \u001b[0;32m~/miniconda3/envs/pytorch_env/lib/python3.9/site-packages/matplotlib/figure.py:3378\u001b[0m, in \u001b[0;36mFigure.savefig\u001b[0;34m(self, fname, transparent, **kwargs)\u001b[0m\n\u001b[1;32m   3374\u001b[0m     \u001b[39mfor\u001b[39;00m ax \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39maxes:\n\u001b[1;32m   3375\u001b[0m         stack\u001b[39m.\u001b[39menter_context(\n\u001b[1;32m   3376\u001b[0m             ax\u001b[39m.\u001b[39mpatch\u001b[39m.\u001b[39m_cm_set(facecolor\u001b[39m=\u001b[39m\u001b[39m'\u001b[39m\u001b[39mnone\u001b[39m\u001b[39m'\u001b[39m, edgecolor\u001b[39m=\u001b[39m\u001b[39m'\u001b[39m\u001b[39mnone\u001b[39m\u001b[39m'\u001b[39m))\n\u001b[0;32m-> 3378\u001b[0m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mcanvas\u001b[39m.\u001b[39;49mprint_figure(fname, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[0;32m~/miniconda3/envs/pytorch_env/lib/python3.9/site-packages/matplotlib/backend_bases.py:2366\u001b[0m, in \u001b[0;36mFigureCanvasBase.print_figure\u001b[0;34m(self, filename, dpi, facecolor, edgecolor, orientation, format, bbox_inches, pad_inches, bbox_extra_artists, backend, **kwargs)\u001b[0m\n\u001b[1;32m   2362\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m   2363\u001b[0m     \u001b[39m# _get_renderer may change the figure dpi (as vector formats\u001b[39;00m\n\u001b[1;32m   2364\u001b[0m     \u001b[39m# force the figure dpi to 72), so we need to set it again here.\u001b[39;00m\n\u001b[1;32m   2365\u001b[0m     \u001b[39mwith\u001b[39;00m cbook\u001b[39m.\u001b[39m_setattr_cm(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mfigure, dpi\u001b[39m=\u001b[39mdpi):\n\u001b[0;32m-> 2366\u001b[0m         result \u001b[39m=\u001b[39m print_method(\n\u001b[1;32m   2367\u001b[0m             filename,\n\u001b[1;32m   2368\u001b[0m             facecolor\u001b[39m=\u001b[39;49mfacecolor,\n\u001b[1;32m   2369\u001b[0m             edgecolor\u001b[39m=\u001b[39;49medgecolor,\n\u001b[1;32m   2370\u001b[0m             orientation\u001b[39m=\u001b[39;49morientation,\n\u001b[1;32m   2371\u001b[0m             bbox_inches_restore\u001b[39m=\u001b[39;49m_bbox_inches_restore,\n\u001b[1;32m   2372\u001b[0m             \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   2373\u001b[0m \u001b[39mfinally\u001b[39;00m:\n\u001b[1;32m   2374\u001b[0m     \u001b[39mif\u001b[39;00m bbox_inches \u001b[39mand\u001b[39;00m restore_bbox:\n",
      "File \u001b[0;32m~/miniconda3/envs/pytorch_env/lib/python3.9/site-packages/matplotlib/backend_bases.py:2232\u001b[0m, in \u001b[0;36mFigureCanvasBase._switch_canvas_and_return_print_method.<locals>.<lambda>\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m   2228\u001b[0m     optional_kws \u001b[39m=\u001b[39m {  \u001b[39m# Passed by print_figure for other renderers.\u001b[39;00m\n\u001b[1;32m   2229\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mdpi\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39m\"\u001b[39m\u001b[39mfacecolor\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39m\"\u001b[39m\u001b[39medgecolor\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39m\"\u001b[39m\u001b[39morientation\u001b[39m\u001b[39m\"\u001b[39m,\n\u001b[1;32m   2230\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mbbox_inches_restore\u001b[39m\u001b[39m\"\u001b[39m}\n\u001b[1;32m   2231\u001b[0m     skip \u001b[39m=\u001b[39m optional_kws \u001b[39m-\u001b[39m {\u001b[39m*\u001b[39minspect\u001b[39m.\u001b[39msignature(meth)\u001b[39m.\u001b[39mparameters}\n\u001b[0;32m-> 2232\u001b[0m     print_method \u001b[39m=\u001b[39m functools\u001b[39m.\u001b[39mwraps(meth)(\u001b[39mlambda\u001b[39;00m \u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs: meth(\n\u001b[1;32m   2233\u001b[0m         \u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49m{k: v \u001b[39mfor\u001b[39;49;00m k, v \u001b[39min\u001b[39;49;00m kwargs\u001b[39m.\u001b[39;49mitems() \u001b[39mif\u001b[39;49;00m k \u001b[39mnot\u001b[39;49;00m \u001b[39min\u001b[39;49;00m skip}))\n\u001b[1;32m   2234\u001b[0m \u001b[39melse\u001b[39;00m:  \u001b[39m# Let third-parties do as they see fit.\u001b[39;00m\n\u001b[1;32m   2235\u001b[0m     print_method \u001b[39m=\u001b[39m meth\n",
      "File \u001b[0;32m~/miniconda3/envs/pytorch_env/lib/python3.9/site-packages/matplotlib/backends/backend_pdf.py:2820\u001b[0m, in \u001b[0;36mFigureCanvasPdf.print_pdf\u001b[0;34m(self, filename, bbox_inches_restore, metadata)\u001b[0m\n\u001b[1;32m   2815\u001b[0m file\u001b[39m.\u001b[39mnewPage(width, height)\n\u001b[1;32m   2816\u001b[0m renderer \u001b[39m=\u001b[39m MixedModeRenderer(\n\u001b[1;32m   2817\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mfigure, width, height, dpi,\n\u001b[1;32m   2818\u001b[0m     RendererPdf(file, dpi, height, width),\n\u001b[1;32m   2819\u001b[0m     bbox_inches_restore\u001b[39m=\u001b[39mbbox_inches_restore)\n\u001b[0;32m-> 2820\u001b[0m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mfigure\u001b[39m.\u001b[39;49mdraw(renderer)\n\u001b[1;32m   2821\u001b[0m renderer\u001b[39m.\u001b[39mfinalize()\n\u001b[1;32m   2822\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39misinstance\u001b[39m(filename, PdfPages):\n",
      "File \u001b[0;32m~/miniconda3/envs/pytorch_env/lib/python3.9/site-packages/matplotlib/artist.py:95\u001b[0m, in \u001b[0;36m_finalize_rasterization.<locals>.draw_wrapper\u001b[0;34m(artist, renderer, *args, **kwargs)\u001b[0m\n\u001b[1;32m     93\u001b[0m \u001b[39m@wraps\u001b[39m(draw)\n\u001b[1;32m     94\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mdraw_wrapper\u001b[39m(artist, renderer, \u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs):\n\u001b[0;32m---> 95\u001b[0m     result \u001b[39m=\u001b[39m draw(artist, renderer, \u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m     96\u001b[0m     \u001b[39mif\u001b[39;00m renderer\u001b[39m.\u001b[39m_rasterizing:\n\u001b[1;32m     97\u001b[0m         renderer\u001b[39m.\u001b[39mstop_rasterizing()\n",
      "File \u001b[0;32m~/miniconda3/envs/pytorch_env/lib/python3.9/site-packages/matplotlib/artist.py:72\u001b[0m, in \u001b[0;36mallow_rasterization.<locals>.draw_wrapper\u001b[0;34m(artist, renderer)\u001b[0m\n\u001b[1;32m     69\u001b[0m     \u001b[39mif\u001b[39;00m artist\u001b[39m.\u001b[39mget_agg_filter() \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m     70\u001b[0m         renderer\u001b[39m.\u001b[39mstart_filter()\n\u001b[0;32m---> 72\u001b[0m     \u001b[39mreturn\u001b[39;00m draw(artist, renderer)\n\u001b[1;32m     73\u001b[0m \u001b[39mfinally\u001b[39;00m:\n\u001b[1;32m     74\u001b[0m     \u001b[39mif\u001b[39;00m artist\u001b[39m.\u001b[39mget_agg_filter() \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n",
      "File \u001b[0;32m~/miniconda3/envs/pytorch_env/lib/python3.9/site-packages/matplotlib/figure.py:3175\u001b[0m, in \u001b[0;36mFigure.draw\u001b[0;34m(self, renderer)\u001b[0m\n\u001b[1;32m   3172\u001b[0m         \u001b[39m# ValueError can occur when resizing a window.\u001b[39;00m\n\u001b[1;32m   3174\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mpatch\u001b[39m.\u001b[39mdraw(renderer)\n\u001b[0;32m-> 3175\u001b[0m mimage\u001b[39m.\u001b[39;49m_draw_list_compositing_images(\n\u001b[1;32m   3176\u001b[0m     renderer, \u001b[39mself\u001b[39;49m, artists, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49msuppressComposite)\n\u001b[1;32m   3178\u001b[0m \u001b[39mfor\u001b[39;00m sfig \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39msubfigs:\n\u001b[1;32m   3179\u001b[0m     sfig\u001b[39m.\u001b[39mdraw(renderer)\n",
      "File \u001b[0;32m~/miniconda3/envs/pytorch_env/lib/python3.9/site-packages/matplotlib/image.py:131\u001b[0m, in \u001b[0;36m_draw_list_compositing_images\u001b[0;34m(renderer, parent, artists, suppress_composite)\u001b[0m\n\u001b[1;32m    129\u001b[0m \u001b[39mif\u001b[39;00m not_composite \u001b[39mor\u001b[39;00m \u001b[39mnot\u001b[39;00m has_images:\n\u001b[1;32m    130\u001b[0m     \u001b[39mfor\u001b[39;00m a \u001b[39min\u001b[39;00m artists:\n\u001b[0;32m--> 131\u001b[0m         a\u001b[39m.\u001b[39;49mdraw(renderer)\n\u001b[1;32m    132\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    133\u001b[0m     \u001b[39m# Composite any adjacent images together\u001b[39;00m\n\u001b[1;32m    134\u001b[0m     image_group \u001b[39m=\u001b[39m []\n",
      "File \u001b[0;32m~/miniconda3/envs/pytorch_env/lib/python3.9/site-packages/matplotlib/artist.py:72\u001b[0m, in \u001b[0;36mallow_rasterization.<locals>.draw_wrapper\u001b[0;34m(artist, renderer)\u001b[0m\n\u001b[1;32m     69\u001b[0m     \u001b[39mif\u001b[39;00m artist\u001b[39m.\u001b[39mget_agg_filter() \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m     70\u001b[0m         renderer\u001b[39m.\u001b[39mstart_filter()\n\u001b[0;32m---> 72\u001b[0m     \u001b[39mreturn\u001b[39;00m draw(artist, renderer)\n\u001b[1;32m     73\u001b[0m \u001b[39mfinally\u001b[39;00m:\n\u001b[1;32m     74\u001b[0m     \u001b[39mif\u001b[39;00m artist\u001b[39m.\u001b[39mget_agg_filter() \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n",
      "File \u001b[0;32m~/miniconda3/envs/pytorch_env/lib/python3.9/site-packages/matplotlib/axes/_base.py:3028\u001b[0m, in \u001b[0;36m_AxesBase.draw\u001b[0;34m(self, renderer)\u001b[0m\n\u001b[1;32m   3025\u001b[0m     \u001b[39mfor\u001b[39;00m spine \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mspines\u001b[39m.\u001b[39mvalues():\n\u001b[1;32m   3026\u001b[0m         artists\u001b[39m.\u001b[39mremove(spine)\n\u001b[0;32m-> 3028\u001b[0m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_update_title_position(renderer)\n\u001b[1;32m   3030\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39maxison:\n\u001b[1;32m   3031\u001b[0m     \u001b[39mfor\u001b[39;00m _axis \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_axis_map\u001b[39m.\u001b[39mvalues():\n",
      "File \u001b[0;32m~/miniconda3/envs/pytorch_env/lib/python3.9/site-packages/matplotlib/axes/_base.py:2972\u001b[0m, in \u001b[0;36m_AxesBase._update_title_position\u001b[0;34m(self, renderer)\u001b[0m\n\u001b[1;32m   2970\u001b[0m top \u001b[39m=\u001b[39m \u001b[39mmax\u001b[39m(top, bb\u001b[39m.\u001b[39mymax)\n\u001b[1;32m   2971\u001b[0m \u001b[39mif\u001b[39;00m title\u001b[39m.\u001b[39mget_text():\n\u001b[0;32m-> 2972\u001b[0m     ax\u001b[39m.\u001b[39;49myaxis\u001b[39m.\u001b[39;49mget_tightbbox(renderer)  \u001b[39m# update offsetText\u001b[39;00m\n\u001b[1;32m   2973\u001b[0m     \u001b[39mif\u001b[39;00m ax\u001b[39m.\u001b[39myaxis\u001b[39m.\u001b[39moffsetText\u001b[39m.\u001b[39mget_text():\n\u001b[1;32m   2974\u001b[0m         bb \u001b[39m=\u001b[39m ax\u001b[39m.\u001b[39myaxis\u001b[39m.\u001b[39moffsetText\u001b[39m.\u001b[39mget_tightbbox(renderer)\n",
      "File \u001b[0;32m~/miniconda3/envs/pytorch_env/lib/python3.9/site-packages/matplotlib/axis.py:1323\u001b[0m, in \u001b[0;36mAxis.get_tightbbox\u001b[0;34m(self, renderer, for_layout_only)\u001b[0m\n\u001b[1;32m   1321\u001b[0m \u001b[39mif\u001b[39;00m renderer \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m   1322\u001b[0m     renderer \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mfigure\u001b[39m.\u001b[39m_get_renderer()\n\u001b[0;32m-> 1323\u001b[0m ticks_to_draw \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_update_ticks()\n\u001b[1;32m   1325\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_update_label_position(renderer)\n\u001b[1;32m   1327\u001b[0m \u001b[39m# go back to just this axis's tick labels\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/pytorch_env/lib/python3.9/site-packages/matplotlib/axis.py:1264\u001b[0m, in \u001b[0;36mAxis._update_ticks\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1262\u001b[0m major_locs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mget_majorticklocs()\n\u001b[1;32m   1263\u001b[0m major_labels \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmajor\u001b[39m.\u001b[39mformatter\u001b[39m.\u001b[39mformat_ticks(major_locs)\n\u001b[0;32m-> 1264\u001b[0m major_ticks \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mget_major_ticks(\u001b[39mlen\u001b[39;49m(major_locs))\n\u001b[1;32m   1265\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmajor\u001b[39m.\u001b[39mformatter\u001b[39m.\u001b[39mset_locs(major_locs)\n\u001b[1;32m   1266\u001b[0m \u001b[39mfor\u001b[39;00m tick, loc, label \u001b[39min\u001b[39;00m \u001b[39mzip\u001b[39m(major_ticks, major_locs, major_labels):\n",
      "File \u001b[0;32m~/miniconda3/envs/pytorch_env/lib/python3.9/site-packages/matplotlib/axis.py:1604\u001b[0m, in \u001b[0;36mAxis.get_major_ticks\u001b[0;34m(self, numticks)\u001b[0m\n\u001b[1;32m   1602\u001b[0m     tick \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_get_tick(major\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m)\n\u001b[1;32m   1603\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmajorTicks\u001b[39m.\u001b[39mappend(tick)\n\u001b[0;32m-> 1604\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_copy_tick_props(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mmajorTicks[\u001b[39m0\u001b[39;49m], tick)\n\u001b[1;32m   1606\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmajorTicks[:numticks]\n",
      "File \u001b[0;32m~/miniconda3/envs/pytorch_env/lib/python3.9/site-packages/matplotlib/axis.py:1573\u001b[0m, in \u001b[0;36mAxis._copy_tick_props\u001b[0;34m(self, src, dest)\u001b[0m\n\u001b[1;32m   1571\u001b[0m dest\u001b[39m.\u001b[39mtick1line\u001b[39m.\u001b[39mupdate_from(src\u001b[39m.\u001b[39mtick1line)\n\u001b[1;32m   1572\u001b[0m dest\u001b[39m.\u001b[39mtick2line\u001b[39m.\u001b[39mupdate_from(src\u001b[39m.\u001b[39mtick2line)\n\u001b[0;32m-> 1573\u001b[0m dest\u001b[39m.\u001b[39;49mgridline\u001b[39m.\u001b[39;49mupdate_from(src\u001b[39m.\u001b[39;49mgridline)\n",
      "File \u001b[0;32m~/miniconda3/envs/pytorch_env/lib/python3.9/site-packages/matplotlib/lines.py:1354\u001b[0m, in \u001b[0;36mLine2D.update_from\u001b[0;34m(self, other)\u001b[0m\n\u001b[1;32m   1351\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_solidjoinstyle \u001b[39m=\u001b[39m other\u001b[39m.\u001b[39m_solidjoinstyle\n\u001b[1;32m   1353\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_linestyle \u001b[39m=\u001b[39m other\u001b[39m.\u001b[39m_linestyle\n\u001b[0;32m-> 1354\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_marker \u001b[39m=\u001b[39m MarkerStyle(marker\u001b[39m=\u001b[39;49mother\u001b[39m.\u001b[39;49m_marker)\n\u001b[1;32m   1355\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_drawstyle \u001b[39m=\u001b[39m other\u001b[39m.\u001b[39m_drawstyle\n",
      "File \u001b[0;32m~/miniconda3/envs/pytorch_env/lib/python3.9/site-packages/matplotlib/markers.py:272\u001b[0m, in \u001b[0;36mMarkerStyle.__init__\u001b[0;34m(self, marker, fillstyle, transform, capstyle, joinstyle)\u001b[0m\n\u001b[1;32m    267\u001b[0m     marker \u001b[39m=\u001b[39m \u001b[39m\"\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    268\u001b[0m     _api\u001b[39m.\u001b[39mwarn_deprecated(\n\u001b[1;32m    269\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39m3.6\u001b[39m\u001b[39m\"\u001b[39m, message\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mMarkerStyle(None) is deprecated since \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    270\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39m%(since)s\u001b[39;00m\u001b[39m; support will be removed \u001b[39m\u001b[39m%(removal)s\u001b[39;00m\u001b[39m.  Use \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    271\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mMarkerStyle(\u001b[39m\u001b[39m'\u001b[39m\u001b[39m'\u001b[39m\u001b[39m) to construct an empty MarkerStyle.\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[0;32m--> 272\u001b[0m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_set_marker(marker)\n",
      "File \u001b[0;32m~/miniconda3/envs/pytorch_env/lib/python3.9/site-packages/matplotlib/markers.py:355\u001b[0m, in \u001b[0;36mMarkerStyle._set_marker\u001b[0;34m(self, marker)\u001b[0m\n\u001b[1;32m    352\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_marker_function \u001b[39m=\u001b[39m \u001b[39mgetattr\u001b[39m(\n\u001b[1;32m    353\u001b[0m         \u001b[39mself\u001b[39m, \u001b[39m'\u001b[39m\u001b[39m_set_\u001b[39m\u001b[39m'\u001b[39m \u001b[39m+\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmarkers[marker])\n\u001b[1;32m    354\u001b[0m \u001b[39melif\u001b[39;00m \u001b[39misinstance\u001b[39m(marker, MarkerStyle):\n\u001b[0;32m--> 355\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m\u001b[39m__dict__\u001b[39m \u001b[39m=\u001b[39m copy\u001b[39m.\u001b[39;49mdeepcopy(marker\u001b[39m.\u001b[39;49m\u001b[39m__dict__\u001b[39;49m)\n\u001b[1;32m    357\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    358\u001b[0m     \u001b[39mtry\u001b[39;00m:\n",
      "File \u001b[0;32m~/miniconda3/envs/pytorch_env/lib/python3.9/copy.py:146\u001b[0m, in \u001b[0;36mdeepcopy\u001b[0;34m(x, memo, _nil)\u001b[0m\n\u001b[1;32m    144\u001b[0m copier \u001b[39m=\u001b[39m _deepcopy_dispatch\u001b[39m.\u001b[39mget(\u001b[39mcls\u001b[39m)\n\u001b[1;32m    145\u001b[0m \u001b[39mif\u001b[39;00m copier \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m--> 146\u001b[0m     y \u001b[39m=\u001b[39m copier(x, memo)\n\u001b[1;32m    147\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    148\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39missubclass\u001b[39m(\u001b[39mcls\u001b[39m, \u001b[39mtype\u001b[39m):\n",
      "File \u001b[0;32m~/miniconda3/envs/pytorch_env/lib/python3.9/copy.py:230\u001b[0m, in \u001b[0;36m_deepcopy_dict\u001b[0;34m(x, memo, deepcopy)\u001b[0m\n\u001b[1;32m    228\u001b[0m memo[\u001b[39mid\u001b[39m(x)] \u001b[39m=\u001b[39m y\n\u001b[1;32m    229\u001b[0m \u001b[39mfor\u001b[39;00m key, value \u001b[39min\u001b[39;00m x\u001b[39m.\u001b[39mitems():\n\u001b[0;32m--> 230\u001b[0m     y[deepcopy(key, memo)] \u001b[39m=\u001b[39m deepcopy(value, memo)\n\u001b[1;32m    231\u001b[0m \u001b[39mreturn\u001b[39;00m y\n",
      "File \u001b[0;32m~/miniconda3/envs/pytorch_env/lib/python3.9/copy.py:153\u001b[0m, in \u001b[0;36mdeepcopy\u001b[0;34m(x, memo, _nil)\u001b[0m\n\u001b[1;32m    151\u001b[0m copier \u001b[39m=\u001b[39m \u001b[39mgetattr\u001b[39m(x, \u001b[39m\"\u001b[39m\u001b[39m__deepcopy__\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39mNone\u001b[39;00m)\n\u001b[1;32m    152\u001b[0m \u001b[39mif\u001b[39;00m copier \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m--> 153\u001b[0m     y \u001b[39m=\u001b[39m copier(memo)\n\u001b[1;32m    154\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    155\u001b[0m     reductor \u001b[39m=\u001b[39m dispatch_table\u001b[39m.\u001b[39mget(\u001b[39mcls\u001b[39m)\n",
      "File \u001b[0;32m~/miniconda3/envs/pytorch_env/lib/python3.9/site-packages/matplotlib/path.py:287\u001b[0m, in \u001b[0;36mPath.__deepcopy__\u001b[0;34m(self, memo)\u001b[0m\n\u001b[1;32m    282\u001b[0m \u001b[39m\u001b[39m\u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m    283\u001b[0m \u001b[39mReturn a deepcopy of the `Path`.  The `Path` will not be\u001b[39;00m\n\u001b[1;32m    284\u001b[0m \u001b[39mreadonly, even if the source `Path` is.\u001b[39;00m\n\u001b[1;32m    285\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m    286\u001b[0m \u001b[39m# Deepcopying arrays (vertices, codes) strips the writeable=False flag.\u001b[39;00m\n\u001b[0;32m--> 287\u001b[0m p \u001b[39m=\u001b[39m copy\u001b[39m.\u001b[39;49mdeepcopy(\u001b[39msuper\u001b[39;49m(), memo)\n\u001b[1;32m    288\u001b[0m p\u001b[39m.\u001b[39m_readonly \u001b[39m=\u001b[39m \u001b[39mFalse\u001b[39;00m\n\u001b[1;32m    289\u001b[0m \u001b[39mreturn\u001b[39;00m p\n",
      "File \u001b[0;32m~/miniconda3/envs/pytorch_env/lib/python3.9/copy.py:161\u001b[0m, in \u001b[0;36mdeepcopy\u001b[0;34m(x, memo, _nil)\u001b[0m\n\u001b[1;32m    159\u001b[0m reductor \u001b[39m=\u001b[39m \u001b[39mgetattr\u001b[39m(x, \u001b[39m\"\u001b[39m\u001b[39m__reduce_ex__\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39mNone\u001b[39;00m)\n\u001b[1;32m    160\u001b[0m \u001b[39mif\u001b[39;00m reductor \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m--> 161\u001b[0m     rv \u001b[39m=\u001b[39m reductor(\u001b[39m4\u001b[39;49m)\n\u001b[1;32m    162\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    163\u001b[0m     reductor \u001b[39m=\u001b[39m \u001b[39mgetattr\u001b[39m(x, \u001b[39m\"\u001b[39m\u001b[39m__reduce__\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39mNone\u001b[39;00m)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 2000x800 with 4 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import argparse\n",
    "import torch\n",
    "# import Learner\n",
    "# import datasets\n",
    "# import utils\n",
    "# from CGlowModel import CondGlowModel\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "\n",
    "    parser = argparse.ArgumentParser(description='predict c-Glow')\n",
    "\n",
    "    # model parameters\n",
    "    # parser.add_argument(\"--x_size\", type=tuple, default=(3,64,64))\n",
    "    # parser.add_argument(\"--y_size\", type=tuple, default=(1,64,64))\n",
    "    parser.add_argument(\"--x_size\", type=tuple, default=(1,8))\n",
    "    parser.add_argument(\"--y_size\", type=tuple, default=(1,16,16))\n",
    "    parser.add_argument(\"--x_hidden_channels\", type=int, default=128)\n",
    "    parser.add_argument(\"--x_hidden_size\", type=int, default=64)\n",
    "    parser.add_argument(\"--y_hidden_channels\", type=int, default=256)\n",
    "    parser.add_argument(\"-K\", \"--depth_flow\", type=int, default=8)\n",
    "    parser.add_argument(\"-L\", \"--num_levels\", type=int, default=3)\n",
    "    parser.add_argument(\"--learn_top\", type=bool, default=False)\n",
    "\n",
    "    # dataset\n",
    "    parser.add_argument(\"-d\", \"--dataset_name\", type=str, default=\"horse\")\n",
    "    parser.add_argument(\"-r\", \"--dataset_root\", type=str, default=\"\")\n",
    "    parser.add_argument(\"--label_scale\", type=float, default=1)\n",
    "    parser.add_argument(\"--label_bias\", type=float, default=0.5)\n",
    "    parser.add_argument(\"--num_labels\", type=int, default=2)\n",
    "    parser.add_argument(\"--x_bins\", type=float, default=256.0)\n",
    "    parser.add_argument(\"--y_bins\", type=float, default=2.0)\n",
    "\n",
    "    # output\n",
    "    parser.add_argument(\"-o\", \"--out_root\", type=str, default=\"out_dir\")\n",
    "\n",
    "    # model path\n",
    "    parser.add_argument(\"--model_path\", type=str, default=\"/pdo/users/jlupoiii/TESS/model_conditional_norm_flow/log_20230822_1646/checkpoints/checkpoint_2000.pth.tar\")\n",
    "\n",
    "    # predictor parameters\n",
    "    parser.add_argument(\"--batch_size\", type=int, default=10)\n",
    "    parser.add_argument(\"--num_samples\", type=int, default=10)\n",
    "\n",
    "\n",
    "    # args = parser.parse_args()\n",
    "    args = parser.parse_known_args()[0]\n",
    "    cuda = torch.cuda.is_available()\n",
    "\n",
    "    # create TESS dataset\n",
    "    torch.manual_seed(42)\n",
    "    full_dataset = TESSDataset()\n",
    "    train_ratio = 0.8\n",
    "    num_train_samples = int(train_ratio * len(full_dataset))\n",
    "    num_valid_samples = len(full_dataset) - num_train_samples\n",
    "    training_set, valid_set = random_split(full_dataset, [num_train_samples, num_valid_samples])\n",
    "\n",
    "\n",
    "    # model\n",
    "    model = CondGlowModel(args)\n",
    "    if cuda:\n",
    "        model = model.cuda()\n",
    "    state = load_state(args.model_path, cuda)\n",
    "    model.load_state_dict(state[\"model\"])\n",
    "    del state\n",
    "\n",
    "    # predictor\n",
    "    predictor = Inferencer(model, valid_set, args, cuda)\n",
    "\n",
    "    # predict\n",
    "    predictor.sampled_based_prediction(args.num_samples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "hKoex5akoLe7"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "authorship_tag": "ABX9TyMbQ56iI/lZFYZCaH1MfPGD",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "pytorch_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.17"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
