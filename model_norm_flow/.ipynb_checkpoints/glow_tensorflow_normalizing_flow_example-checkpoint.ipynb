{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Code from https://github.com/simonwestberg/Glow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TensorFlow\n",
    "import tensorflow as tf\n",
    "import tensorflow_probability as tf_prob\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras.layers import Layer\n",
    "import numpy as np\n",
    "\n",
    "### LAYERS\n",
    "\n",
    "## ActNorm\n",
    "class ActNorm(Layer):\n",
    "\n",
    "    def __init__(self):\n",
    "        super(ActNorm, self).__init__()\n",
    "\n",
    "    # Create the state of the layer. ActNorm initialization is done in call()\n",
    "    def build(self, input_shape):\n",
    "        b, h, w, c = input_shape[0]  # Batch size, height, width, channels\n",
    "\n",
    "        # Scale parameters per channel, called 's' in Glow\n",
    "        self.scale = self.add_weight(\n",
    "            shape=(1, 1, 1, c),\n",
    "            trainable=True,\n",
    "            name=\"actnorm_scale_\" + str(np.random.randint(0, 1e6)))\n",
    "\n",
    "        # Bias parameter per channel, called 'b' in Glow\n",
    "        self.bias = self.add_weight(\n",
    "            shape=(1, 1, 1, c),\n",
    "            trainable=True,\n",
    "            name=\"actnorm_bias_\" + str(np.random.randint(0, 1e6))\n",
    "        )\n",
    "\n",
    "        # Used to check if scale and bias have been initialized\n",
    "        self.initialized = self.add_weight(\n",
    "            trainable=False,\n",
    "            dtype=tf.bool,\n",
    "            name=\"actnorm_initialized_\" + str(np.random.randint(0, 1e6))\n",
    "        )\n",
    "\n",
    "        self.initialized.assign(False)\n",
    "\n",
    "    @tf.function\n",
    "    def call(self, inputs, forward=True):\n",
    "        \"\"\"\n",
    "        inputs: list containing [input tensor, log_det]\n",
    "        returns: output tensor\n",
    "        \"\"\"\n",
    "\n",
    "        x = inputs[0]\n",
    "        log_det = inputs[1]\n",
    "\n",
    "        b, h, w, c = x.shape  # Batch size, height, width, channels\n",
    "\n",
    "        if not self.initialized:\n",
    "            \"\"\"\n",
    "            Given an initial batch X, setting\n",
    "            scale = 1 / mean(X) and\n",
    "            bias = -mean(X) / std(X)\n",
    "            where mean and std is calculated per channel in X,\n",
    "            results in post-actnorm activations X = X*s + b having zero mean\n",
    "            and unit variance per channel.\n",
    "            \"\"\"\n",
    "\n",
    "            assert (len(x.shape) == 4)\n",
    "\n",
    "            # Calculate mean per channel\n",
    "            mean = tf.math.reduce_mean(x, axis=[0, 1, 2], keepdims=True)\n",
    "\n",
    "            # Calculate standard deviation per channel\n",
    "            std = tf.math.reduce_std(x, axis=[0, 1, 2], keepdims=True)\n",
    "\n",
    "            # Add small value to std to avoid division by zero\n",
    "            eps = tf.constant(1e-6, shape=std.shape, dtype=std.dtype)\n",
    "            std = tf.math.add(std, eps)\n",
    "\n",
    "            self.scale.assign(tf.math.divide(1.0, std))\n",
    "            self.bias.assign(-mean)\n",
    "\n",
    "            self.initialized.assign(True)\n",
    "\n",
    "        if forward:\n",
    "\n",
    "            outputs = tf.math.multiply(x + self.bias, self.scale)\n",
    "\n",
    "            # log-determinant of ActNorm layer\n",
    "            log_s = tf.math.log(tf.math.abs(self.scale))\n",
    "            log_det += h * w * tf.math.reduce_sum(log_s)\n",
    "\n",
    "            return outputs, log_det\n",
    "\n",
    "        else:\n",
    "            # Reverse operation\n",
    "            outputs = (x / self.scale) - self.bias\n",
    "\n",
    "            # log-determinant of ActNorm layer\n",
    "            log_s = tf.math.log(tf.math.abs(self.scale))\n",
    "            log_det -= h * w * tf.math.reduce_sum(log_s)\n",
    "            return outputs, log_det\n",
    "\n",
    "\n",
    "## Permutation (1x1 convolution, reverse, or shuffle)\n",
    "class Permutation(Layer):\n",
    "\n",
    "    def __init__(self, perm_type=\"1x1\"):\n",
    "        super(Permutation, self).__init__()\n",
    "\n",
    "        self.types = [\"1x1\", \"reverse\", \"shuffle\"]\n",
    "        self.perm_type = perm_type\n",
    "\n",
    "        if perm_type not in self.types:\n",
    "            raise ValueError(\"Incorrect permutation type, should be either \"\n",
    "                             \"'1x1', 'reverse', or 'shuffle'\")\n",
    "\n",
    "    # Create the state of the layer (weights)\n",
    "    def build(self, input_shape):\n",
    "        b, h, w, c = input_shape[0]\n",
    "\n",
    "        if self.perm_type == \"1x1\":\n",
    "            self.W = self.add_weight(\n",
    "                shape=(1, 1, c, c),\n",
    "                trainable=True,\n",
    "                initializer=tf.keras.initializers.orthogonal,\n",
    "                name=\"1x1_W_\" + str(np.random.randint(0, 1e6))\n",
    "            )\n",
    "        elif self.perm_type == \"reverse\":\n",
    "            pass\n",
    "\n",
    "        elif self.perm_type == \"shuffle\":\n",
    "            rng_seed = abs(hash(self.perm_type)) % 10000000\n",
    "            self.indices = np.random.RandomState(seed=rng_seed).permutation(np.arange(c))\n",
    "            self.reverse_indicies = [0] * c\n",
    "            for i in range(c):\n",
    "                self.reverse_indicies[self.indices[i]] = i\n",
    "\n",
    "    # Defines the computation from inputs to outputs\n",
    "    def call(self, inputs, forward=True):\n",
    "        \"\"\"\n",
    "        inputs: input tensor or list containing [input tensor, log_det]\n",
    "        returns: output tensor\n",
    "        \"\"\"\n",
    "\n",
    "        x = inputs[0]\n",
    "        log_det = inputs[1]\n",
    "\n",
    "        b, h, w, c = x.shape\n",
    "\n",
    "        if forward:\n",
    "            if self.perm_type == \"1x1\":\n",
    "                outputs = tf.nn.conv2d(x,\n",
    "                                       self.W,\n",
    "                                       strides=[1, 1, 1, 1],\n",
    "                                       padding=\"SAME\")\n",
    "\n",
    "                # Log-determinant\n",
    "                det = tf.math.reduce_sum(tf.linalg.det(self.W))\n",
    "                log_det += h * w * tf.math.log(tf.math.abs(det))\n",
    "\n",
    "                return outputs, log_det\n",
    "\n",
    "            elif self.perm_type == \"reverse\":\n",
    "                output = x[:, :, :, ::-1]\n",
    "                log_det += 0\n",
    "\n",
    "                return output, log_det\n",
    "\n",
    "            elif self.perm_type == \"shuffle\":\n",
    "                permuted_output = tf.gather(x, self.indices, axis=3)\n",
    "                log_det += 0\n",
    "\n",
    "                return permuted_output, log_det\n",
    "\n",
    "        else:\n",
    "            if self.perm_type == \"1x1\":\n",
    "                W_inv = tf.linalg.inv(self.W)\n",
    "                outputs = tf.nn.conv2d(x,\n",
    "                                       W_inv,\n",
    "                                       strides=[1, 1, 1, 1],\n",
    "                                       padding=\"SAME\")\n",
    "\n",
    "                # Log-determinant\n",
    "                det = tf.math.reduce_sum(tf.linalg.det(self.W))\n",
    "                log_det -= h * w * tf.math.log(tf.math.abs(det))\n",
    "\n",
    "                return outputs, log_det\n",
    "\n",
    "            elif self.perm_type == \"reverse\":\n",
    "                outputs = x[:, :, :, ::-1]\n",
    "\n",
    "                log_det -= 0\n",
    "\n",
    "                return outputs, log_det\n",
    "\n",
    "            elif self.perm_type == \"shuffle\":\n",
    "                reverse_permute_output = tf.gather(x, self.reverse_indicies, axis=3)\n",
    "\n",
    "                log_det -= 0\n",
    "\n",
    "                return reverse_permute_output, log_det\n",
    "\n",
    "\n",
    "## Affine coupling\n",
    "class AffineCoupling(Layer):\n",
    "\n",
    "    def __init__(self, hidden_channels):\n",
    "        \"\"\"\n",
    "        :param hidden_channels: Number of filters used for the hidden layers of the NN() function, see GLOW paper\n",
    "        \"\"\"\n",
    "        super(AffineCoupling, self).__init__()\n",
    "\n",
    "        self.NN = tf.keras.Sequential()\n",
    "        self.hidden_channels = hidden_channels\n",
    "\n",
    "    def build(self, input_shape):\n",
    "        b, h, w, c = input_shape[0]\n",
    "\n",
    "        self.NN.add(tf.keras.layers.Conv2D(self.hidden_channels, kernel_size=3,\n",
    "                                           activation='relu', strides=(1, 1),\n",
    "                                           padding='same'))\n",
    "        self.NN.add(tf.keras.layers.Conv2D(self.hidden_channels, kernel_size=1,\n",
    "                                           activation='relu', strides=(1, 1),\n",
    "                                           padding='same'))\n",
    "        self.NN.add(Conv2D_zeros(c))\n",
    "\n",
    "    # Defines the computation from inputs to outputs\n",
    "    def call(self, inputs, forward=True):\n",
    "        \"\"\"\n",
    "        Computes the forward/reverse calculations of the affine coupling layer\n",
    "        inputs: list containing [input tensor, log_det]\n",
    "        returns: A tensor, same dimensions as input tensor, for next step of flow and the scalar log determinant\n",
    "        \"\"\"\n",
    "\n",
    "        x = inputs[0]\n",
    "        log_det = inputs[1]\n",
    "\n",
    "        if forward:\n",
    "            # split along the channels, which is axis=3\n",
    "            x_a, x_b = tf.split(x, num_or_size_splits=2, axis=3)\n",
    "\n",
    "            nn_output = self.NN(x_b)\n",
    "            log_s = nn_output[:, :, :, 0::2]\n",
    "            t = nn_output[:, :, :, 1::2]\n",
    "            s = tf.math.sigmoid(log_s + 2)\n",
    "\n",
    "            y_a = tf.math.multiply(s, x_a + t)\n",
    "\n",
    "            y_b = x_b\n",
    "            output = tf.concat((y_a, y_b), axis=3)\n",
    "\n",
    "            _log_det = tf.math.log(tf.math.abs(s))\n",
    "            _log_det = tf.math.reduce_sum(_log_det, axis=[1, 2, 3])\n",
    "            log_det += tf.math.reduce_mean(_log_det)\n",
    "\n",
    "            return output, log_det\n",
    "\n",
    "        # the reverse calculations, if forward is False\n",
    "        else:\n",
    "            y_a, y_b = tf.split(x, num_or_size_splits=2, axis=3)\n",
    "\n",
    "            nn_output = self.NN(y_b)\n",
    "            log_s = nn_output[:, :, :, 0::2]\n",
    "            t = nn_output[:, :, :, 1::2]\n",
    "\n",
    "            s = tf.math.sigmoid(log_s + 2)\n",
    "\n",
    "            x_a = tf.math.divide(y_a, s) - t\n",
    "            x_b = y_b\n",
    "            output = tf.concat((x_a, x_b), axis=3)\n",
    "\n",
    "            _log_det = tf.math.log(tf.math.abs(s))\n",
    "            _log_det = tf.math.reduce_sum(_log_det, axis=[1, 2, 3])\n",
    "            log_det -= tf.math.reduce_mean(_log_det)\n",
    "\n",
    "            return output, log_det\n",
    "\n",
    "\n",
    "## Squeeze\n",
    "class Squeeze(Layer):\n",
    "\n",
    "    def __init__(self):\n",
    "        super(Squeeze, self).__init__()\n",
    "\n",
    "    # Defines the computation from inputs to outputs\n",
    "    def call(self, inputs, forward=True):\n",
    "        \"\"\"\n",
    "        inputs: input tensor\n",
    "        returns: output tensor\n",
    "        \"\"\"\n",
    "        if forward:\n",
    "            outputs = tf.nn.space_to_depth(inputs, block_size=2)\n",
    "        else:\n",
    "            outputs = tf.nn.depth_to_space(inputs, block_size=2)\n",
    "\n",
    "        return outputs\n",
    "\n",
    "\n",
    "class Split(Layer):\n",
    "\n",
    "    def __init__(self, split=True):\n",
    "        super(Split, self).__init__()\n",
    "        self.split = split\n",
    "\n",
    "    def build(self, input_shape):\n",
    "        b, h, w, c = input_shape\n",
    "\n",
    "        # Uses learnt prior instead of fixed mean and std\n",
    "        if self.split:\n",
    "            self.prior = Conv2D_zeros(filters=c)\n",
    "        else:\n",
    "            self.prior = Conv2D_zeros(filters=2 * c)\n",
    "\n",
    "    def call(self, inputs, forward=True, reconstruct=False):\n",
    "\n",
    "        if forward:\n",
    "            b, h, w, c = inputs.shape\n",
    "\n",
    "            if self.split:\n",
    "                x, z = tf.split(inputs, num_or_size_splits=2, axis=3)\n",
    "                mean, log_sd = tf.split(self.prior(x), num_or_size_splits=2, axis=3)\n",
    "                log_p = gaussian_logprob(z, mean, log_sd)\n",
    "                log_p = tf.math.reduce_sum(log_p, axis=[1, 2, 3])\n",
    "                log_p = tf.math.reduce_mean(log_p, axis=0)\n",
    "\n",
    "                return x, z, log_p\n",
    "            else:\n",
    "                zero = tf.zeros_like(inputs)\n",
    "                mean, log_sd = tf.split(self.prior(zero), num_or_size_splits=2, axis=3)\n",
    "                log_p = gaussian_logprob(inputs, mean, log_sd)\n",
    "                log_p = tf.math.reduce_sum(log_p, axis=[1, 2, 3])\n",
    "                log_p = tf.math.reduce_mean(log_p, axis=0)\n",
    "                z = inputs\n",
    "\n",
    "                return z, log_p\n",
    "\n",
    "        else:\n",
    "            if reconstruct:\n",
    "                if self.split:\n",
    "                    z, z_add = inputs[0], inputs[1]\n",
    "                    return tf.concat([z, z_add], axis=3)\n",
    "\n",
    "                else:\n",
    "                    return inputs\n",
    "\n",
    "            if self.split:\n",
    "                z, z_add = inputs[0], inputs[1]\n",
    "                mean, log_sd = tf.split(self.prior(z), num_or_size_splits=2, axis=3)\n",
    "                z_sample = mean + tf.math.exp(log_sd) * z_add\n",
    "\n",
    "                z = tf.concat([z, z_sample], axis=3)\n",
    "                return z\n",
    "            else:\n",
    "                zero = tf.zeros_like(inputs)\n",
    "                mean, log_sd = tf.split(self.prior(zero), num_or_size_splits=2, axis=3)\n",
    "                z = mean + tf.math.exp(log_sd) * inputs\n",
    "                return z\n",
    "\n",
    "\n",
    "class Conv2D_zeros(Layer):\n",
    "\n",
    "    def __init__(self, filters):\n",
    "        super(Conv2D_zeros, self).__init__()\n",
    "        self.filters = filters\n",
    "\n",
    "    def build(self, input_shape):\n",
    "        # Get shape\n",
    "        b, h, w, c = input_shape\n",
    "\n",
    "        self.conv = tf.keras.layers.Conv2D(self.filters,\n",
    "                                           kernel_size=3,\n",
    "                                           strides=(1, 1),\n",
    "                                           padding=\"same\",\n",
    "                                           activation=None,\n",
    "                                           kernel_initializer=\"zeros\",\n",
    "                                           bias_initializer=\"zeros\")\n",
    "\n",
    "        self.scale = self.add_weight(\n",
    "            shape=(1, 1, 1, self.filters),\n",
    "            trainable=True,\n",
    "            initializer=\"zeros\",\n",
    "            name=\"conv2D_zeros_scale_\" + str(np.random.randint(0, 1e6))\n",
    "        )\n",
    "\n",
    "    def call(self, inputs):\n",
    "        x = self.conv(inputs)\n",
    "        x = x * tf.math.exp(self.scale * 3)\n",
    "\n",
    "        return x\n",
    "\n",
    "\n",
    "def gaussian_logprob(x, mean, log_sd):\n",
    "    return -0.5 * tf.math.log(2 * np.pi) - log_sd - 0.5 * (x - mean) ** 2 / tf.math.exp(2 * log_sd)\n",
    "\n",
    "\n",
    "## Step of flow\n",
    "class FlowStep(Layer):\n",
    "\n",
    "    def __init__(self, hidden_channels, perm_type=\"1x1\"):\n",
    "        super(FlowStep, self).__init__()\n",
    "        self.hidden_channels = hidden_channels\n",
    "\n",
    "        self.actnorm = ActNorm()\n",
    "        self.perm = Permutation(perm_type)\n",
    "        self.coupling = AffineCoupling(hidden_channels)\n",
    "\n",
    "    def call(self, inputs, forward=True):\n",
    "        \"\"\"\n",
    "        inputs: list containing [input tensor, log_det]\n",
    "        returns: output tensor\n",
    "        \"\"\"\n",
    "\n",
    "        x = inputs[0]\n",
    "        log_det = inputs[1]\n",
    "\n",
    "        if forward:\n",
    "            x, log_det = self.actnorm([x, log_det], forward)\n",
    "            x, log_det = self.perm([x, log_det], forward)\n",
    "            x, log_det = self.coupling([x, log_det], forward)\n",
    "        else:\n",
    "            x, log_det = self.coupling([x, log_det], forward)\n",
    "            x, log_det = self.perm([x, log_det], forward)\n",
    "            x, log_det = self.actnorm([x, log_det], forward)\n",
    "\n",
    "        return x, log_det\n",
    "\n",
    "\n",
    "### Glow model\n",
    "\n",
    "class Glow(keras.Model):\n",
    "\n",
    "    def __init__(self, steps, levels, img_shape, hidden_channels, perm_type=\"1x1\"):\n",
    "        super(Glow, self).__init__()\n",
    "\n",
    "        assert (len(img_shape) == 3)\n",
    "\n",
    "        self.steps = steps  # Number of steps in each flow, K in the paper\n",
    "        self.levels = levels  # Number of levels, L in the paper\n",
    "        self.height, self.width, self.channels = img_shape\n",
    "        self.dimension = self.height * self.width * self.channels  # Dimension of input/latent space\n",
    "        self.hidden_channels = hidden_channels\n",
    "        self.perm_type = perm_type\n",
    "\n",
    "        # Normal distribution with 0 mean and std=1, defined over R^dimension\n",
    "        self.latent_distribution = tf_prob.distributions.MultivariateNormalDiag(\n",
    "            loc=[0.0] * self.dimension)\n",
    "\n",
    "        self.squeeze = Squeeze()\n",
    "        self.flow_layers = []\n",
    "        self.split_layers = []\n",
    "\n",
    "        for l in range(levels):\n",
    "            flows = []\n",
    "\n",
    "            for k in range(steps):\n",
    "                flows.append(FlowStep(hidden_channels, perm_type))\n",
    "\n",
    "            self.flow_layers.append(flows)\n",
    "\n",
    "            if l != levels - 1:\n",
    "                self.split_layers.append(Split(split=True))\n",
    "            else:\n",
    "                self.split_layers.append(Split(split=False))\n",
    "\n",
    "    def preprocess(self, x):\n",
    "        \"\"\"Expects images x without any pre-processing, with pixel values in [0, 255]\"\"\"\n",
    "        x = x / 255.0 - 0.5  # Normalize to lie in [-0.5, 0.5]\n",
    "        # Add uniform noise\n",
    "        x = x + tf.random.uniform(shape=tf.shape(x), minval=0, maxval=1 / 256.0, dtype=x.dtype)\n",
    "\n",
    "        return x\n",
    "\n",
    "    def sample_image(self, temperature):\n",
    "\n",
    "        # Sample latent variable\n",
    "        z = self.latent_distribution.sample() * temperature\n",
    "        # Decode z\n",
    "        x = self(z, forward=False)\n",
    "\n",
    "        x = np.clip(x, -0.5, 0.5)\n",
    "        x = x + 0.5\n",
    "        return x\n",
    "\n",
    "    def call(self, inputs, forward=True, reconstruct=False):\n",
    "\n",
    "        if forward:\n",
    "            x = inputs\n",
    "            log_det = 0.0\n",
    "            sum_log_p = 0.0\n",
    "            x = self.preprocess(x)\n",
    "\n",
    "            latent_variables = []\n",
    "\n",
    "            for l in range(self.levels - 1):\n",
    "\n",
    "                x = self.squeeze(x, forward=forward)\n",
    "\n",
    "                # K steps of flow\n",
    "                for k in range(self.steps):\n",
    "                    x, log_det = self.flow_layers[l][k]([x, log_det], forward=forward)\n",
    "\n",
    "                # Split\n",
    "                x, z, log_p = self.split_layers[l](x, forward=forward)\n",
    "                sum_log_p += log_p\n",
    "\n",
    "                latent_dim = np.prod(z.shape[1:])  # Dimension of extracted z\n",
    "                latent_variables.append(tf.reshape(z, [-1, latent_dim]))\n",
    "\n",
    "            # Last squeeze\n",
    "            x = self.squeeze(x, forward=forward)\n",
    "\n",
    "            # Last steps of flow\n",
    "            for k in range(self.steps):\n",
    "                x, log_det = self.flow_layers[-1][k]([x, log_det], forward=forward)\n",
    "\n",
    "            z, log_p = self.split_layers[-1](x, forward=forward)\n",
    "            sum_log_p += log_p\n",
    "\n",
    "            latent_dim = np.prod(z.shape[1:])  # Dimension of last latent variable\n",
    "            latent_variables.append(tf.reshape(z, [-1, latent_dim]))\n",
    "\n",
    "            # Concatenate latent variables\n",
    "            latent_variables = tf.concat(latent_variables, axis=1)\n",
    "\n",
    "            c = -self.dimension * np.log(1 / 256)\n",
    "\n",
    "            # Average NLL of the images in bits/dimension\n",
    "            NLL = (-sum_log_p - log_det + c) / (np.log(2) * self.dimension)\n",
    "\n",
    "            self.add_loss(NLL)\n",
    "\n",
    "            return latent_variables, NLL\n",
    "\n",
    "        else:\n",
    "            # Run the model backwards, assuming that inputs is a sampled latent variable of full dimension\n",
    "\n",
    "            # Extract slices of the latent variables to be used in reverse split function\n",
    "            latent_variables = []\n",
    "\n",
    "            start = 0  # Starting index of the slice for z_i\n",
    "            stop = 0  # Stopping index of the slice for z_i\n",
    "\n",
    "            for l in range(self.levels - 1):\n",
    "                stop += self.dimension // (2 ** (l + 1))\n",
    "                latent_variables.append(inputs[start:stop])\n",
    "                start = stop\n",
    "\n",
    "            latent_variables.append(inputs[start:])\n",
    "\n",
    "            log_det = 0.0\n",
    "\n",
    "            # Extract last latent variable and reshape\n",
    "            z = latent_variables[-1]\n",
    "            c_last = self.channels * 2 ** (self.levels + 1)  # nr of channels in the last latent output\n",
    "            h_last = self.height // (2 ** self.levels)  # height of the last latent output\n",
    "            w_last = self.width // (2 ** self.levels)  # width of the last latent output\n",
    "            z = tf.reshape(z, shape=(1, h_last, w_last, c_last))\n",
    "\n",
    "            z = self.split_layers[-1](z, forward=forward, reconstruct=reconstruct)\n",
    "\n",
    "            # Last steps of flow\n",
    "            for k in reversed(range(self.steps)):\n",
    "                z, log_det = self.flow_layers[-1][k]([z, log_det], forward=forward)\n",
    "\n",
    "            # Last squeeze\n",
    "            z = self.squeeze(z, forward=forward)\n",
    "\n",
    "            for l in reversed(range(self.levels - 1)):\n",
    "\n",
    "                # Extract latent variable, reshape, and concatenate along channel dimension (reverse split)\n",
    "                z_add = latent_variables[l]\n",
    "                z_add = tf.reshape(z_add, shape=z.shape)\n",
    "\n",
    "                z = self.split_layers[l]([z, z_add], forward=forward, reconstruct=reconstruct)\n",
    "\n",
    "                # K steps of flow\n",
    "                for k in reversed(range(self.steps)):\n",
    "                    z, log_det = self.flow_layers[l][k]([z, log_det], forward=forward)\n",
    "\n",
    "                z = self.squeeze(z, forward=forward)\n",
    "\n",
    "            x = z\n",
    "\n",
    "            return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from matplotlib import pyplot as plt\n",
    "from keras.datasets import fashion_mnist\n",
    "from keras.datasets import mnist\n",
    "import tensorflow as tf\n",
    "import tensorflow_datasets as tfds\n",
    "from tensorflow import keras\n",
    "import numpy as np\n",
    "\n",
    "# from model import *\n",
    "\n",
    "\n",
    "def preprocess(data_set, type=None):\n",
    "    if type is not None:\n",
    "        (X_train, Y_train), (X_test, Y_test) = data_set.load_data(type=type)\n",
    "    else:\n",
    "        (X_train, Y_train), (X_test, Y_test) = data_set.load_data()\n",
    "    X_train = np.expand_dims(X_train, axis=3)\n",
    "    X_test = np.expand_dims(X_test, axis=3)\n",
    "\n",
    "    X_train = X_train[0:, :, :, :]\n",
    "    X_test = X_test[0:, :, :, :]\n",
    "\n",
    "    # Add zero-padding to get 32x32 images\n",
    "    X_train = np.pad(X_train, pad_width=((0, 0), (2, 2), (2, 2), (0, 0)))\n",
    "    X_test = np.pad(X_test, pad_width=((0, 0), (2, 2), (2, 2), (0, 0)))\n",
    "\n",
    "    X_train = X_train.astype(np.float32)\n",
    "    X_test = X_test.astype(np.float32)\n",
    "\n",
    "    return X_train, Y_train, X_test, Y_test\n",
    "\n",
    "\n",
    "# Load data sets\n",
    "X_train, Y_train, X_test, Y_test = preprocess(mnist)\n",
    "# X_train_letters, Y_train_letters, X_test_letters, Y_test_letters = preprocess(emnist, type='letters')\n",
    "# X_train_fashion, _, X_test_fashion, _ = preprocess(fashion_mnist)\n",
    "\n",
    "np.random.shuffle(X_test)\n",
    "# np.random.shuffle(X_test_letters)\n",
    "# np.random.shuffle(X_test_fashion)\n",
    "\n",
    "# Create balanced MNIST training set\n",
    "\n",
    "num = 4000  # samples per class\n",
    "\n",
    "X_balanced = np.zeros((num * 10, 32, 32, 1))\n",
    "count = [0 for i in range(10)]\n",
    "\n",
    "idx = 0\n",
    "for i, x in enumerate(X_train):\n",
    "    if count[Y_train[i]] < num:\n",
    "        X_balanced[idx] = x\n",
    "        count[Y_train[i]] += 1\n",
    "        idx += 1\n",
    "\n",
    "assert (idx == X_balanced.shape[0])\n",
    "\n",
    "np.random.shuffle(X_balanced)\n",
    "\n",
    "# Train the model\n",
    "lr = 1e-4\n",
    "\n",
    "model = Glow(steps=32, levels=3, img_shape=(32, 32, 1), hidden_channels=512, perm_type=\"1x1\")\n",
    "adam = keras.optimizers.Adam(learning_rate=lr)\n",
    "model.compile(optimizer=adam)\n",
    "\n",
    "model.fit(X_balanced, epochs=10, batch_size=128)\n",
    "\n",
    "# Plot loss\n",
    "loss = model.history.history[\"loss\"]\n",
    "epochs = np.arange(len(loss))\n",
    "plt.plot(epochs, loss, color=\"r\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# LINEAR INTERPOLATION\n",
    "\n",
    "# These are pairs of different examples of the same digit to illustrate the latent space produces smooth transitions\n",
    "images = [X_test[np.where(Y_test == 0)[0][0:2]],\n",
    "          X_test[np.where(Y_test == 1)[0][0:2]],\n",
    "          X_test[np.where(Y_test == 2)[0][0:2]],\n",
    "          X_test[np.where(Y_test == 3)[0][0:2]],\n",
    "          X_test[np.where(Y_test == 4)[0][0:2]],\n",
    "          X_test[np.where(Y_test == 5)[0][0:2]],\n",
    "          X_test[np.where(Y_test == 6)[0][0:2]],\n",
    "          X_test[np.where(Y_test == 7)[0][0:2]],\n",
    "          X_test[np.where(Y_test == 8)[0][0:2]],\n",
    "          X_test[np.where(Y_test == 9)[0][0:2]]\n",
    "          ]\n",
    "\n",
    "zs = []\n",
    "\n",
    "for pair in images:\n",
    "    x1, x2 = pair[0:1], pair[1:2]\n",
    "\n",
    "    # Encode\n",
    "    z1, _ = model(x1, forward=True)\n",
    "    z1 = z1[0]\n",
    "    z2, _ = model(x2, forward=True)\n",
    "    z2 = z2[0]\n",
    "\n",
    "    zs.append([z1, z2])\n",
    "\n",
    "# Interpolate\n",
    "num = 10\n",
    "latents_full = []\n",
    "\n",
    "for pair in zs:\n",
    "    z1, z2 = pair[0], pair[1]\n",
    "    lis = []\n",
    "    alpha = 0\n",
    "    for i in range(num):\n",
    "        lis.append((1 - alpha) * z1 + alpha * z2)\n",
    "        alpha += 1 / (num - 1.0)\n",
    "    latents_full.append(lis)\n",
    "\n",
    "# Decode\n",
    "decodings = []\n",
    "\n",
    "for i, latents in enumerate(latents_full):\n",
    "    print(f\"decoding {i} of {len(latents_full)}\")\n",
    "    imgs = []\n",
    "    for i, z in enumerate(latents):\n",
    "        output = model(z, forward=False, reconstruct=True)\n",
    "        imgs.append(output)\n",
    "    decodings.append(imgs)\n",
    "\n",
    "# Plot\n",
    "fig = plt.figure(figsize=(8, 8))\n",
    "\n",
    "for row in range(10):\n",
    "    for col in range(num):\n",
    "        fig.add_subplot(10, num, row * num + col + 1)\n",
    "        plt.imshow(decodings[row][col][0, :, :, 0], cmap=\"gray\")\n",
    "        plt.axis('off')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reconstruct one image\n",
    "\n",
    "plt.figure() # plot input\n",
    "x_in = X_test[1:2]\n",
    "plt.imshow(x_in[0])\n",
    "\n",
    "plt.figure() # plot latent\n",
    "z, _ = model(x_in, forward=True)\n",
    "plt.imshow(np.array(z).reshape((32,32)))\n",
    "\n",
    "plt.figure() # plot outpur\n",
    "x_out = model(z[0], forward=False, reconstruct=True)\n",
    "plt.imshow(x_out[0])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Take 30 samples of output and plot the mean image and std image\n",
    "samples = []\n",
    "for i in range(30):\n",
    "    samples.append(model(z[0], forward=False, reconstruct=False))\n",
    "\n",
    "plt.figure()\n",
    "plt.imshow(np.mean(np.array(samples), axis=0)[0])\n",
    "\n",
    "plt.figure()\n",
    "plt.imshow(np.std(np.array(samples), axis=0)[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.mean(np.array(samples), axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SAMPLE NEW IMAGES\n",
    "\n",
    "rows = 10\n",
    "cols = 10\n",
    "\n",
    "images = []\n",
    "\n",
    "for r in range(rows):\n",
    "    for c in range(cols):\n",
    "        x = model.sample_image(temperature=4)\n",
    "        images.append(x)\n",
    "\n",
    "# Plot\n",
    "fig = plt.figure(figsize=(8, 8))\n",
    "\n",
    "for r in range(rows):\n",
    "    for c in range(cols):\n",
    "        fig.add_subplot(rows, cols, r * cols + c + 1)\n",
    "        plt.imshow(images[r * cols + c][0, :, :, 0], cmap=\"gray\")\n",
    "        plt.axis('off')\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
